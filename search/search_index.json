{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Solutions to Introduction to Algorithms Third Edition \ud83d\udcda A crowdsourced work contributed from nice people all around the world. Getting Started This website contains nearly complete solutions to the bible textbook - Introduction to Algorithms Third Edition , published by Thomas H. Cormen , Charles E. Leiserson , Ronald L. Rivest , and Clifford Stein . I hope to organize solutions to help people and myself study algorithms. By using Markdown (.md) files and KaTeX math library, this page is much more readable on portable devices. \"Many a little makes a mickle.\" Contributors Thanks to the authors of CLRS Solutions , Michelle Bodnar (who writes the even-numbered problems) and Andrew Lohr (who writes the odd-numbered problems), @skanev , @CyberZHG , @yinyanghu , @Gutdub , etc. Thanks to all contributors on GitHub , you guys make this repository a better reference! Special thanks to @JeffreyCA , who fixed math rendering on iOS Safari in #26 . If I miss your name here, please tell me! Motivation I build this website since I want to help everyone learn algorithms by providing something easy to read on mobile devices. Therefore, if any adjustment is needed or you have the same motivation to contribute to this work, please don't hesitate to give me your feedback. You can press the \"pencil icon\" in the upper right corner to edit the content or open an issue in this repository . Your solution will be rebased after I review it and make some form modifications to your pull request. There're lots of issues regarding to solutions in this repository, if you have time, please take a look and try to help people on the internet :) Thank you very much, and I hope that everyone will learn algorithms smoothly. How I Generate the Website? I use the static site generator MkDocs and the beautiful theme Material for MkDocs to build this website. As for rendering math equations, I use KaTeX , which is fast and beautiful. I also add overflow-x: auto to prevent the overflow issue on small screen devices so that you can scroll horizontally in the math display equations. More Information For a clear commit history, I rebase my repository regularly. Therefore, if you have forked the repository before, consider re-forking it again. For more information, please visit my GitHub . Updated to this new page on April 13, 2018, at 04:48 (GMT+8) . Revised on July 21, 2019. License Licensed under the MIT License.","title":"Preface"},{"location":"#solutions-to-introduction-to-algorithms-third-edition","text":"\ud83d\udcda A crowdsourced work contributed from nice people all around the world.","title":"Solutions to Introduction to Algorithms Third Edition"},{"location":"#getting-started","text":"This website contains nearly complete solutions to the bible textbook - Introduction to Algorithms Third Edition , published by Thomas H. Cormen , Charles E. Leiserson , Ronald L. Rivest , and Clifford Stein . I hope to organize solutions to help people and myself study algorithms. By using Markdown (.md) files and KaTeX math library, this page is much more readable on portable devices. \"Many a little makes a mickle.\"","title":"Getting Started"},{"location":"#contributors","text":"Thanks to the authors of CLRS Solutions , Michelle Bodnar (who writes the even-numbered problems) and Andrew Lohr (who writes the odd-numbered problems), @skanev , @CyberZHG , @yinyanghu , @Gutdub , etc. Thanks to all contributors on GitHub , you guys make this repository a better reference! Special thanks to @JeffreyCA , who fixed math rendering on iOS Safari in #26 . If I miss your name here, please tell me!","title":"Contributors"},{"location":"#motivation","text":"I build this website since I want to help everyone learn algorithms by providing something easy to read on mobile devices. Therefore, if any adjustment is needed or you have the same motivation to contribute to this work, please don't hesitate to give me your feedback. You can press the \"pencil icon\" in the upper right corner to edit the content or open an issue in this repository . Your solution will be rebased after I review it and make some form modifications to your pull request. There're lots of issues regarding to solutions in this repository, if you have time, please take a look and try to help people on the internet :) Thank you very much, and I hope that everyone will learn algorithms smoothly.","title":"Motivation"},{"location":"#how-i-generate-the-website","text":"I use the static site generator MkDocs and the beautiful theme Material for MkDocs to build this website. As for rendering math equations, I use KaTeX , which is fast and beautiful. I also add overflow-x: auto to prevent the overflow issue on small screen devices so that you can scroll horizontally in the math display equations.","title":"How I Generate the Website?"},{"location":"#more-information","text":"For a clear commit history, I rebase my repository regularly. Therefore, if you have forked the repository before, consider re-forking it again. For more information, please visit my GitHub . Updated to this new page on April 13, 2018, at 04:48 (GMT+8) . Revised on July 21, 2019.","title":"More Information"},{"location":"#license","text":"Licensed under the MIT License.","title":"License"},{"location":"Chap01/1.1/","text":"1.1-1 Give a real-world example that requires sorting or a real-world example that requires computing a convex hull. Sorting: browse the price of the restaurants with ascending prices on NTU street. Convex hull: computing the diameter of set of points. 1.1-2 Other than speed, what other measures of efficiency might one use in a real-world setting? Memory efficiency and coding efficiency. 1.1-3 Select a data structure that you have seen previously, and discuss its strengths and limitations. Linked-list: Strengths: insertion and deletion. Limitations: random access. 1.1-4 How are the shortest-path and traveling-salesman problems given above similar? How are they different? Similar: finding path with shortest distance. Different: traveling-salesman has more constraints. 1.1-5 Come up with a real-world problem in which only the best solution will do. Then come up with one in which a solution that is \"approximately\" the best is good enough. Best: find the GCD of two positive integer numbers. Approximately: find the solution of differential equations.","title":"1.1 Algorithms"},{"location":"Chap01/1.1/#11-1","text":"Give a real-world example that requires sorting or a real-world example that requires computing a convex hull. Sorting: browse the price of the restaurants with ascending prices on NTU street. Convex hull: computing the diameter of set of points.","title":"1.1-1"},{"location":"Chap01/1.1/#11-2","text":"Other than speed, what other measures of efficiency might one use in a real-world setting? Memory efficiency and coding efficiency.","title":"1.1-2"},{"location":"Chap01/1.1/#11-3","text":"Select a data structure that you have seen previously, and discuss its strengths and limitations. Linked-list: Strengths: insertion and deletion. Limitations: random access.","title":"1.1-3"},{"location":"Chap01/1.1/#11-4","text":"How are the shortest-path and traveling-salesman problems given above similar? How are they different? Similar: finding path with shortest distance. Different: traveling-salesman has more constraints.","title":"1.1-4"},{"location":"Chap01/1.1/#11-5","text":"Come up with a real-world problem in which only the best solution will do. Then come up with one in which a solution that is \"approximately\" the best is good enough. Best: find the GCD of two positive integer numbers. Approximately: find the solution of differential equations.","title":"1.1-5"},{"location":"Chap01/1.2/","text":"1.2-1 Give an example of an application that requires algorithmic content at the application level, and discuss the function of the algorithms involved. Drive navigation. 1.2-2 Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$ , insertion sort runs in $8n^2$ steps, while merge sort runs in $64n\\lg n$ steps. For which values of $n$ does insertion sort beat merge sort? $$ \\begin{aligned} 8n^2 & < 64n\\lg n \\\\ 2^n & < n^8 \\\\ 2 \\le n & \\le 43. \\end{aligned} $$ 1.2-3 What is the smallest value of $n$ such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine? $$ \\begin{aligned} 100n^2 & < 2^n \\\\ n & \\ge 15. \\end{aligned} $$","title":"1.2 Algorithms as a technology"},{"location":"Chap01/1.2/#12-1","text":"Give an example of an application that requires algorithmic content at the application level, and discuss the function of the algorithms involved. Drive navigation.","title":"1.2-1"},{"location":"Chap01/1.2/#12-2","text":"Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$ , insertion sort runs in $8n^2$ steps, while merge sort runs in $64n\\lg n$ steps. For which values of $n$ does insertion sort beat merge sort? $$ \\begin{aligned} 8n^2 & < 64n\\lg n \\\\ 2^n & < n^8 \\\\ 2 \\le n & \\le 43. \\end{aligned} $$","title":"1.2-2"},{"location":"Chap01/1.2/#12-3","text":"What is the smallest value of $n$ such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine? $$ \\begin{aligned} 100n^2 & < 2^n \\\\ n & \\ge 15. \\end{aligned} $$","title":"1.2-3"},{"location":"Chap01/Problems/1-1/","text":"For each function $f(n)$ and time $t$ in the following table, determine the largest size $n$ of a problem that can be solved in time $t$, assuming that the algorithm to solve the problem takes $f(n)$ microseconds. $$ \\begin{array}{cccccccc} & \\text{1 second} & \\text{1 minute} & \\text{1 hour} & \\text{1 day} & \\text{1 month} & \\text{1 year} & \\text{1 century} \\\\ \\hline \\lg n & 2^{10^6} & 2^{6 \\times 10^7} & 2^{3.6 \\times 10^9} & 2^{8.64 \\times 10^{10}} & 2^{2.59 \\times 10^{12}} & 2^{3.15 \\times 10^{13}} & 2^{3.15 \\times 10^{15}} \\\\ \\sqrt n & 10^{12} & 3.6 \\times 10^{15} & 1.3 \\times 10^{19} & 7.46 \\times 10^{21} & 6.72 \\times 10^{24} & 9.95 \\times 10^{26} & 9.95 \\times 10^{30} \\\\ n & 10^6 & 6 \\times 10^7 & 3.6 \\times 10^9 & 8.64 \\times 10^{10} & 2.59 \\times 10^{12} & 3.15 \\times 10^{13} & 3.15 \\times 10^{15} \\\\ n\\lg n & 6.24 \\times 10^4 & 2.8 \\times 10^6 & 1.33 \\times 10^8 & 2.76 \\times 10^9 & 7.19 \\times 10^{10} & 7.98 \\times 10^{11} & 6.86 \\times 10^{13} \\\\ n^2 & 1000 & 7745 & 60000 & 293938 & 1609968 & 5615692 & 56156922 \\\\ n^3 & 100 & 391 & 1532 & 4420 & 13736 & 31593 & 146645 \\\\ 2^n & 19 & 25 & 31 & 36 & 41 & 44 & 51 \\\\ n! & 9 & 11 & 12 & 13 & 15 & 16 & 17 \\end{array} $$","title":"Problem 1-1"},{"location":"Chap02/2.1/","text":"2.1-1 Using Figure 2.2 as a model, illustrate the operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$. The operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$. Array indices appear above the rectangles, and values stored in the array positions appear within the rectangles. (a)-(e) are iterations of the for loop of lines 1-8. In each iteration, the black rectangle holds the key taken from $A[i]$, which is compared with the values in shaded rectangles to its left in the test of line 5. Dotted arrows show array values moved one position to the right in line 6. and solid arrows indicate where the key moves to in line 8. (f) is the final sorted array. The changes of array A during traversal: $$ \\begin{aligned} A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 59, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 59, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 58, 59 \\rangle \\end{aligned} $$ 2.1-2 Rewrite the $\\text{INSERTION-SORT}$ procedure to sort into nonincreasing instead of nondecreasing order. INSERTION - SORT ( A ) for j = 2 to A . length key = A [ j ] i = j - 1 while i > 0 and A [ i ] < key A [ i + 1 ] = A [ i ] i = i - 1 A [ i + 1 ] = key 2.1-3 Consider the searching problem : Input : A sequence of $n$ numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and a value $v$. Output: An index $i$ such that $v = A[i]$ or the special value $\\text{NIL}$ if $v$ does not appear in $A$. Write pseudocode for linear search , which scans through the sequence, looking for $v$. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties. LINEAR - SEARCH ( A , v ) for i = 1 to A . length if A [ i ] == v return i return NIL Loop invariant: At the start of each iteration of the for loop, the subarray $A[1..i - 1]$ consists of elements that are different than $v$. Initialization: Before the first loop iteration ($i = 1$), the subarray is the empty array, so the proof is trivial. Maintenance: During each loop iteration, we compare $v$ with $A[i]$. If they are the same, we return $i$, which is the correct result. Otherwise, we continue to the next iteration of the loop. At the end of each loop iteration, we know the subarray $A[1..i]$ does not contain $v$, so the loop invariant holds true. Incrementing $i$ for the next iteration of the for loop then preserves the loop invariant. Termination: The loop terminates when $i > A.length = n$. Since $i$ increases by $1$, we must have $i = n + 1$ at that time. Substituting $n + 1$, for $i$ in the wording of the loop invariant, we have that the subarray $A[1..n]$ consists of elements that are different than $v$. Thus, we return $\\text{NIL}$. Observing that $A[1..n]$, we conclude that the entire array does not have any element equal to $v$. Hence the algorithm is correct. 2.1-4 Consider the problem of adding two $n$-bit binary integers, stored in two $n$-element arrays $A$ and $B$. The sum of the two integers should be stored in binary form in an $(n + 1)$-element array $C$. State the problem formally and write pseudocode for adding the two integers. Input: An array of booleans $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and an array of booleans $B = \\langle b_1, b_2, \\ldots, b_n \\rangle$, each representing an integer stored in binary format (each digit is a number, either $0$ or $1$, least-significant digit first) and each of length $n$. Output: An array $C = \\langle c_1, c_2, \\ldots, c_{n + 1} \\rangle$ such that $C' = A' + B'$ where $A'$, $B'$ and $C'$ are the integers, represented by $A$, $B$ and $C$. ADD - BINARY ( A , B ) C = new integer [ A . length + 1 ] carry = 0 for i = 1 to A . length C [ i ] = ( A [ i ] + B [ i ] + carry ) % 2 // remainder carry = ( A [ i ] + B [ i ] + carry ) / 2 // quotient C [ i + 1 ] = carry return C","title":"2.1 Insertion sort"},{"location":"Chap02/2.1/#21-1","text":"Using Figure 2.2 as a model, illustrate the operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$. The operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$. Array indices appear above the rectangles, and values stored in the array positions appear within the rectangles. (a)-(e) are iterations of the for loop of lines 1-8. In each iteration, the black rectangle holds the key taken from $A[i]$, which is compared with the values in shaded rectangles to its left in the test of line 5. Dotted arrows show array values moved one position to the right in line 6. and solid arrows indicate where the key moves to in line 8. (f) is the final sorted array. The changes of array A during traversal: $$ \\begin{aligned} A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 31, 41, 59, 26, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 59, 41, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 59, 58 \\rangle \\\\ A & = \\langle 26, 31, 41, 41, 58, 59 \\rangle \\end{aligned} $$","title":"2.1-1"},{"location":"Chap02/2.1/#21-2","text":"Rewrite the $\\text{INSERTION-SORT}$ procedure to sort into nonincreasing instead of nondecreasing order. INSERTION - SORT ( A ) for j = 2 to A . length key = A [ j ] i = j - 1 while i > 0 and A [ i ] < key A [ i + 1 ] = A [ i ] i = i - 1 A [ i + 1 ] = key","title":"2.1-2"},{"location":"Chap02/2.1/#21-3","text":"Consider the searching problem : Input : A sequence of $n$ numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and a value $v$. Output: An index $i$ such that $v = A[i]$ or the special value $\\text{NIL}$ if $v$ does not appear in $A$. Write pseudocode for linear search , which scans through the sequence, looking for $v$. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties. LINEAR - SEARCH ( A , v ) for i = 1 to A . length if A [ i ] == v return i return NIL Loop invariant: At the start of each iteration of the for loop, the subarray $A[1..i - 1]$ consists of elements that are different than $v$. Initialization: Before the first loop iteration ($i = 1$), the subarray is the empty array, so the proof is trivial. Maintenance: During each loop iteration, we compare $v$ with $A[i]$. If they are the same, we return $i$, which is the correct result. Otherwise, we continue to the next iteration of the loop. At the end of each loop iteration, we know the subarray $A[1..i]$ does not contain $v$, so the loop invariant holds true. Incrementing $i$ for the next iteration of the for loop then preserves the loop invariant. Termination: The loop terminates when $i > A.length = n$. Since $i$ increases by $1$, we must have $i = n + 1$ at that time. Substituting $n + 1$, for $i$ in the wording of the loop invariant, we have that the subarray $A[1..n]$ consists of elements that are different than $v$. Thus, we return $\\text{NIL}$. Observing that $A[1..n]$, we conclude that the entire array does not have any element equal to $v$. Hence the algorithm is correct.","title":"2.1-3"},{"location":"Chap02/2.1/#21-4","text":"Consider the problem of adding two $n$-bit binary integers, stored in two $n$-element arrays $A$ and $B$. The sum of the two integers should be stored in binary form in an $(n + 1)$-element array $C$. State the problem formally and write pseudocode for adding the two integers. Input: An array of booleans $A = \\langle a_1, a_2, \\ldots, a_n \\rangle$ and an array of booleans $B = \\langle b_1, b_2, \\ldots, b_n \\rangle$, each representing an integer stored in binary format (each digit is a number, either $0$ or $1$, least-significant digit first) and each of length $n$. Output: An array $C = \\langle c_1, c_2, \\ldots, c_{n + 1} \\rangle$ such that $C' = A' + B'$ where $A'$, $B'$ and $C'$ are the integers, represented by $A$, $B$ and $C$. ADD - BINARY ( A , B ) C = new integer [ A . length + 1 ] carry = 0 for i = 1 to A . length C [ i ] = ( A [ i ] + B [ i ] + carry ) % 2 // remainder carry = ( A [ i ] + B [ i ] + carry ) / 2 // quotient C [ i + 1 ] = carry return C","title":"2.1-4"},{"location":"Chap02/2.2/","text":"2.2-1 Express the function $n^3 / 1000 - 100n^2 - 100n + 3n$ in terms of $\\Theta$-notation. $\\Theta(n^3)$. 2.2-2 Consider sorting $n$ numbers stored in array $A$ by first finding the smallest element of $A$ and exchanging it with the element in $A[1]$. Then find the second smallest element of $A$, and exchange it with $A[2]$. Continue in this manner for the first $n - 1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort . What loop invariant does this algorithm maintain? Why does it need to run for only the first $n - 1$ elements, rather than for all $n$ elements? Give the best-case and worst-case running times of selection sort in $\\Theta$-notation. Pseudocode: n = A . length for i = 1 to n - 1 minIndex = i for j = i + 1 to n if A [ j ] < A [ minIndex ] minIndex = j swap ( A [ i ], A [ minIndex ]) Loop invariant: At the start of the loop in line 1, the subarray $A[1..i - 1]$ consists of the smallest $i - 1$ elements in array $A$ with sorted order. Why does it need to run for only the first $n - 1$ elements, rather than for all $n$ elements? After $n - 1$ iterations, the subarray $A[1..n - 1]$ consists of the smallest $i - 1$ elements in array $A$ with sorted order. Therefore, $A[n]$ is already the largest element. Running time: $\\Theta(n^2)$. 2.2-3 Consider linear search again (see Exercise 2.1-3). How many elements of the in- put sequence need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case? What are the average-case and worst-case running times of linear search in $\\Theta$-notation? Justify your answers. If the element is present in the sequence, half of the elements are likely to be checked before it is found in the average case. In the worst case, all of them will be checked. That is, $n / 2$ checks for the average case and $n$ for the worst case. Both of them are $\\Theta(n)$. 2.2-4 How can we modify almost any algorithm to have a good best-case running time? You can modify any algorithm to have a best case time complexity by adding a special case. If the input matches this special case, return the pre-computed answer.","title":"2.2 Analyzing algorithms"},{"location":"Chap02/2.2/#22-1","text":"Express the function $n^3 / 1000 - 100n^2 - 100n + 3n$ in terms of $\\Theta$-notation. $\\Theta(n^3)$.","title":"2.2-1"},{"location":"Chap02/2.2/#22-2","text":"Consider sorting $n$ numbers stored in array $A$ by first finding the smallest element of $A$ and exchanging it with the element in $A[1]$. Then find the second smallest element of $A$, and exchange it with $A[2]$. Continue in this manner for the first $n - 1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort . What loop invariant does this algorithm maintain? Why does it need to run for only the first $n - 1$ elements, rather than for all $n$ elements? Give the best-case and worst-case running times of selection sort in $\\Theta$-notation. Pseudocode: n = A . length for i = 1 to n - 1 minIndex = i for j = i + 1 to n if A [ j ] < A [ minIndex ] minIndex = j swap ( A [ i ], A [ minIndex ]) Loop invariant: At the start of the loop in line 1, the subarray $A[1..i - 1]$ consists of the smallest $i - 1$ elements in array $A$ with sorted order. Why does it need to run for only the first $n - 1$ elements, rather than for all $n$ elements? After $n - 1$ iterations, the subarray $A[1..n - 1]$ consists of the smallest $i - 1$ elements in array $A$ with sorted order. Therefore, $A[n]$ is already the largest element. Running time: $\\Theta(n^2)$.","title":"2.2-2"},{"location":"Chap02/2.2/#22-3","text":"Consider linear search again (see Exercise 2.1-3). How many elements of the in- put sequence need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case? What are the average-case and worst-case running times of linear search in $\\Theta$-notation? Justify your answers. If the element is present in the sequence, half of the elements are likely to be checked before it is found in the average case. In the worst case, all of them will be checked. That is, $n / 2$ checks for the average case and $n$ for the worst case. Both of them are $\\Theta(n)$.","title":"2.2-3"},{"location":"Chap02/2.2/#22-4","text":"How can we modify almost any algorithm to have a good best-case running time? You can modify any algorithm to have a best case time complexity by adding a special case. If the input matches this special case, return the pre-computed answer.","title":"2.2-4"},{"location":"Chap02/2.3/","text":"2.3-1 Using Figure 2.4 as a model, illustrate the operation of merge sort on the array $A = \\langle 3, 41, 52, 26, 38, 57, 9, 49 \\rangle$. $$[3] \\quad [41] \\quad [52] \\quad [26] \\quad [38] \\quad [57] \\quad [9] \\quad [49]$$ $$\\downarrow$$ $$[3|41] \\quad [26|52] \\quad [38|57] \\quad [9|49]$$ $$\\downarrow$$ $$[3|26|41|52] \\quad [9|38|49|57]$$ $$\\downarrow$$ $$[3|9|26|38|41|49|52|57]$$ 2.3-2 Rewrite the $\\text{MERGE}$ procedure so that it does not use sentinels, instead stopping once either array $L$ or $R$ has had all its elements copied back to $A$ and then copying the remainder of the other array back into $A$. MERGE ( A , p , q , r ) n1 = q - p + 1 n2 = r - q let L [ 1. . n1 ] and R [ 1. . n2 ] be new arrays for i = 1 to n1 L [ i ] = A [ p + i - 1 ] for j = 1 to n2 R [ j ] = A [ q + j ] i = 1 j = 1 for k = p to r if i > n1 A [ k ] = R [ j ] j = j + 1 else if j > n2 A [ k ] = L [ i ] i = i + 1 else if L [ i ] \u2264 R [ j ] A [ k ] = L [ i ] i = i + 1 else A [ k ] = R [ j ] j = j + 1 2.3-3 Use mathematical induction to show that when $n$ is an exact power of $2$, the solution of the recurrence $$ T(n) = \\begin{cases} 2 & \\text{if } n = 2, \\\\ 2T(n / 2) + n & \\text{if } n = 2^k, \\text{for } k > 1 \\end{cases} $$ is $T(n) = n\\lg n$. Base case For $n = 2^1$, $T(n) = 2\\lg 2 = 2$. Suppose $n = 2^k$, $T(n) = n\\lg n = 2^k \\lg 2^k = 2^kk$. For $n = 2^{k + 1}$, $$ \\begin{aligned} T(n) & = 2T(2^{k + 1} / 2) + 2^{k + 1} \\\\ & = 2T(2^k) + 2^{k + 1} \\\\ & = 2 \\cdot 2^kk + 2^{k + 1} \\\\ & = 2^{k + 1}(k + 1) \\\\ & = 2^{k + 1} \\lg 2^{k + 1} \\\\ & = n\\lg n. \\end{aligned} $$ By P.M.I., $T(n) = n\\lg n$, when $n$ is an exact power of $2$. 2.3-4 We can express insertion sort as a recursive procedure as follows. In order to sort $A[1..n]$, we recursively sort $A[1..n - 1]$ and then insert $A[n]$ into the sorted array $A[1..n - 1]$. Write a recurrence for the running time of this recursive version of insertion sort. It takes $\\Theta(n)$ time in the worst case to insert $A[n]$ into the sorted array $A[1..n - 1]$. Therefore, the recurrence $$ T(n) = \\begin{cases} \\Theta(1) & \\text{if } n = 1, \\\\ T(n - 1) + \\Theta(n) & \\text{if } n > 1. \\end{cases} $$ The solution of the recurrence is $\\Theta(n^2)$. 2.3-5 Referring back to the searching problem (see Exercise 2.1-3), observe that if the sequence $A$ is sorted, we can check the midpoint of the sequence against $v$ and eliminate half of the sequence from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\\Theta(\\lg n)$. Iterative: ITERATIVE - BINARY - SEARCH ( A , v , low , high ) while low \u2264 high mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] low = mid + 1 else high = mid - 1 return NIL Recursive: RECURSIVE - BINARY - SEARCH ( A , v , low , high ) if low > high return NIL mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , v , mid + 1 , high ) else return RECURSIVE - BINARY - SEARCH ( A , v , low , mid - 1 ) Each time we do the comparison of $v$ with the middle element, the search range continues with range halved. The recurrence $$ T(n) = \\begin{cases} \\Theta(1) & \\text{if } n = 1, \\\\ T(n / 2) + \\Theta(1) & \\text{if } n > 1. \\end{cases} $$ The solution of the recurrence is $T(n) = \\Theta(\\lg n)$. 2.3-6 Observe that the while loop of lines 5\u20137 of the $\\text{INSERTION-SORT}$ procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[i..j - 1]$. Can we use a binary search (see Exercise 2.3-5) instead to improve the overall worst-case running time of insertion sort to $\\Theta(n\\lg n)$? Each time the while loop of lines 5-7 of $\\text{INSERTION-SORT}$ scans backward through the sorted array $A[1..j - 1]$. The loop not only searches for the proper place for $A[j]$, but it also moves each of the array elements that are bigger than $A[j]$ one position to the right (line 6). These movements takes $\\Theta(j)$ time, which occurs when all the $j - 1$ elements preceding $A[j]$ are larger than $A[j]$. The running time of using binary search to search is $\\Theta(\\lg j)$, which is still dominated by the running time of moving element $\\Theta(j)$. Therefore, we can't improve the overrall worst-case running time of insertion sort to $\\Theta(n\\lg n)$. 2.3-7 $\\star$ Describe a $\\Theta(n\\lg n)$-time algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether or not there exist two elements in $S$ whose sum is exactly $x$. First, sort $S$, which takes $\\Theta(n\\lg n)$. Then, for each element $s_i$ in $S$, $i = 1, \\dots, n$, search $A[i + 1..n]$ for $s_i' = x - s_i$ by binary search, which takes $\\Theta(\\lg n)$. If $s_i'$ is found, return its position; otherwise, continue for next iteration. The time complexity of the algorithm is $\\Theta(n\\lg n) + n \\cdot \\Theta(\\lg n) = \\Theta(n\\lg n)$.","title":"2.3 Designing algorithms"},{"location":"Chap02/2.3/#23-1","text":"Using Figure 2.4 as a model, illustrate the operation of merge sort on the array $A = \\langle 3, 41, 52, 26, 38, 57, 9, 49 \\rangle$. $$[3] \\quad [41] \\quad [52] \\quad [26] \\quad [38] \\quad [57] \\quad [9] \\quad [49]$$ $$\\downarrow$$ $$[3|41] \\quad [26|52] \\quad [38|57] \\quad [9|49]$$ $$\\downarrow$$ $$[3|26|41|52] \\quad [9|38|49|57]$$ $$\\downarrow$$ $$[3|9|26|38|41|49|52|57]$$","title":"2.3-1"},{"location":"Chap02/2.3/#23-2","text":"Rewrite the $\\text{MERGE}$ procedure so that it does not use sentinels, instead stopping once either array $L$ or $R$ has had all its elements copied back to $A$ and then copying the remainder of the other array back into $A$. MERGE ( A , p , q , r ) n1 = q - p + 1 n2 = r - q let L [ 1. . n1 ] and R [ 1. . n2 ] be new arrays for i = 1 to n1 L [ i ] = A [ p + i - 1 ] for j = 1 to n2 R [ j ] = A [ q + j ] i = 1 j = 1 for k = p to r if i > n1 A [ k ] = R [ j ] j = j + 1 else if j > n2 A [ k ] = L [ i ] i = i + 1 else if L [ i ] \u2264 R [ j ] A [ k ] = L [ i ] i = i + 1 else A [ k ] = R [ j ] j = j + 1","title":"2.3-2"},{"location":"Chap02/2.3/#23-3","text":"Use mathematical induction to show that when $n$ is an exact power of $2$, the solution of the recurrence $$ T(n) = \\begin{cases} 2 & \\text{if } n = 2, \\\\ 2T(n / 2) + n & \\text{if } n = 2^k, \\text{for } k > 1 \\end{cases} $$ is $T(n) = n\\lg n$. Base case For $n = 2^1$, $T(n) = 2\\lg 2 = 2$. Suppose $n = 2^k$, $T(n) = n\\lg n = 2^k \\lg 2^k = 2^kk$. For $n = 2^{k + 1}$, $$ \\begin{aligned} T(n) & = 2T(2^{k + 1} / 2) + 2^{k + 1} \\\\ & = 2T(2^k) + 2^{k + 1} \\\\ & = 2 \\cdot 2^kk + 2^{k + 1} \\\\ & = 2^{k + 1}(k + 1) \\\\ & = 2^{k + 1} \\lg 2^{k + 1} \\\\ & = n\\lg n. \\end{aligned} $$ By P.M.I., $T(n) = n\\lg n$, when $n$ is an exact power of $2$.","title":"2.3-3"},{"location":"Chap02/2.3/#23-4","text":"We can express insertion sort as a recursive procedure as follows. In order to sort $A[1..n]$, we recursively sort $A[1..n - 1]$ and then insert $A[n]$ into the sorted array $A[1..n - 1]$. Write a recurrence for the running time of this recursive version of insertion sort. It takes $\\Theta(n)$ time in the worst case to insert $A[n]$ into the sorted array $A[1..n - 1]$. Therefore, the recurrence $$ T(n) = \\begin{cases} \\Theta(1) & \\text{if } n = 1, \\\\ T(n - 1) + \\Theta(n) & \\text{if } n > 1. \\end{cases} $$ The solution of the recurrence is $\\Theta(n^2)$.","title":"2.3-4"},{"location":"Chap02/2.3/#23-5","text":"Referring back to the searching problem (see Exercise 2.1-3), observe that if the sequence $A$ is sorted, we can check the midpoint of the sequence against $v$ and eliminate half of the sequence from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\\Theta(\\lg n)$. Iterative: ITERATIVE - BINARY - SEARCH ( A , v , low , high ) while low \u2264 high mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] low = mid + 1 else high = mid - 1 return NIL Recursive: RECURSIVE - BINARY - SEARCH ( A , v , low , high ) if low > high return NIL mid = floor (( low + high ) / 2 ) if v == A [ mid ] return mid else if v > A [ mid ] return RECURSIVE - BINARY - SEARCH ( A , v , mid + 1 , high ) else return RECURSIVE - BINARY - SEARCH ( A , v , low , mid - 1 ) Each time we do the comparison of $v$ with the middle element, the search range continues with range halved. The recurrence $$ T(n) = \\begin{cases} \\Theta(1) & \\text{if } n = 1, \\\\ T(n / 2) + \\Theta(1) & \\text{if } n > 1. \\end{cases} $$ The solution of the recurrence is $T(n) = \\Theta(\\lg n)$.","title":"2.3-5"},{"location":"Chap02/2.3/#23-6","text":"Observe that the while loop of lines 5\u20137 of the $\\text{INSERTION-SORT}$ procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[i..j - 1]$. Can we use a binary search (see Exercise 2.3-5) instead to improve the overall worst-case running time of insertion sort to $\\Theta(n\\lg n)$? Each time the while loop of lines 5-7 of $\\text{INSERTION-SORT}$ scans backward through the sorted array $A[1..j - 1]$. The loop not only searches for the proper place for $A[j]$, but it also moves each of the array elements that are bigger than $A[j]$ one position to the right (line 6). These movements takes $\\Theta(j)$ time, which occurs when all the $j - 1$ elements preceding $A[j]$ are larger than $A[j]$. The running time of using binary search to search is $\\Theta(\\lg j)$, which is still dominated by the running time of moving element $\\Theta(j)$. Therefore, we can't improve the overrall worst-case running time of insertion sort to $\\Theta(n\\lg n)$.","title":"2.3-6"},{"location":"Chap02/2.3/#23-7-star","text":"Describe a $\\Theta(n\\lg n)$-time algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether or not there exist two elements in $S$ whose sum is exactly $x$. First, sort $S$, which takes $\\Theta(n\\lg n)$. Then, for each element $s_i$ in $S$, $i = 1, \\dots, n$, search $A[i + 1..n]$ for $s_i' = x - s_i$ by binary search, which takes $\\Theta(\\lg n)$. If $s_i'$ is found, return its position; otherwise, continue for next iteration. The time complexity of the algorithm is $\\Theta(n\\lg n) + n \\cdot \\Theta(\\lg n) = \\Theta(n\\lg n)$.","title":"2.3-7 $\\star$"},{"location":"Chap02/Problems/2-1/","text":"Although merge sort runs in $\\Theta(n\\lg n)$ worst-case time and insertion sort runs in $\\Theta(n^2)$ worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus, it makes sense to coarsen the leaves of the recursion by using insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in which $n / k$ sublists of length $k$ are sorted using insertion sort and then merged using the standard merging mechanism, where $k$ is a value to be determined. a. Show that insertion sort can sort the $n / k$ sublists, each of length $k$, in $\\Theta(nk)$ worst-case time. b. Show how to merge the sublists in $\\Theta(n\\lg(n / k))$ worst-case time. c. Given that the modified algorithm runs in $\\Theta(nk + n\\lg(n / k))$ worst-case time, what is the largest value of $k$ as a function of $n$ for which the modified algorithm has the same running time as standard merge sort, in terms of $\\Theta$-notation? d. How should we choose $k$ in practice? a. The worst-case time to sort a list of length $k$ by insertion sort is $\\Theta(k^2)$. Therefore, sorting $n / k$ sublists, each of length $k$ takes $\\Theta(k^2 \\cdot n / k) = \\Theta(nk)$ worst-case time. b. We have $n / k$ sorted sublists each of length $k$. To merge these $n / k$ sorted sublists to a single sorted list of length $n$, we have to take $2$ sublists at a time and continue to merge them. This will result in $\\lg(n / k)$ steps and we compare $n$ elements in each step. Therefore, the worst-case time to merge the sublists is $\\Theta(n\\lg(n / k))$. c. The modified algorithm has time complexity as standard merge sort when $\\Theta(nk + n\\lg(n / k)) = \\Theta(n\\lg n)$. Assume $k = \\Theta(\\lg n)$, $$ \\begin{aligned} \\Theta(nk + n\\lg(n / k)) & = \\Theta(nk + n\\lg n - n\\lg k) \\\\ & = \\Theta(n\\lg n + n\\lg n - n\\lg(\\lg n)) \\\\ & = \\Theta(2n\\lg n - n\\lg(\\lg n)) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ d. Choose $k$ be the largest length of sublist on which insertion sort is faster than merge sort.","title":"2-1 Insertion sort on small arrays in merge sort"},{"location":"Chap02/Problems/2-2/","text":"Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly swapping adjacent elements that are out of order. BUBBLESORT ( A ) for i = 1 to A . length - 1 for j = A . length downto i + 1 if A [ j ] < A [ j - 1 ] exchange A [ j ] with A [ j - 1 ] a. Let $A'$ denote the output of $\\text{BUBBLESORT}(A)$ To prove that $\\text{BUBBLESORT}$ is correct, we need to prove that it terminates and that $$A'[1] \\le A'[2] \\le \\cdots \\le A'[n], \\tag{2.3}$$ where $n = A.length$. In order to show that $\\text{BUBBLESORT}$ actually sorts, what else do we need to prove? The next two parts will prove inequality $\\text{(2.3)}$. b. State precisely a loop invariant for the for loop in lines 2\u20134, and prove that this loop invariant holds. Your proof should use the structure of the loop invariant proof presented in this chapter. c. Using the termination condition of the loop invariant proved in part (b), state a loop invariant for the for loop in lines 1\u20134 that will allow you to prove inequality $\\text{(2.3)}$. Your proof should use the structure of the loop invariant proof presented in this chapter. d. What is the worst-case running time of bubblesort? How does it compare to the running time of insertion sort? a. $A'$ consists of the elements in $A$ but in sorted order. b. Loop invariant: At the start of each iteration of the for loop of lines 2-4, the subarray $A[j..n]$ consists of the elements originally in $A[j..n]$ before entering the loop but possibly in a different order and the first element $A[j]$ is the smallest among them. Initialization: Initially the subarray contains only the last element $A[n]$, which is trivially the smallest element of the subarray. Maintenance: In every step we compare $A[j]$ with $A[j - 1]$ and make $A[j - 1]$ the smallest among them. After the iteration, the length of the subarray increases by one and the first element is the smallest of the subarray. Termination: The loop terminates when $j = i$. According to the statement of loop invariant, $A[i]$ is the smallest among $A[i..n]$ and $A[i..n]$ consists of the elements originally in $A[i..n]$ before entering the loop. c. Loop invariant: At the start of each iteration of the for loop of lines 1-4, the subarray $A[1..i \u2212 1]$ consists of the $i - 1$ smallest elements in $A[1..n]$ in sorted order. $A[i..n]$ consists of the $n - i + 1$ remaining elements in $A[1..n]$. Initialization: Initially the subarray $A[1..i \u2212 1]$ is empty and trivially this is the smallest element of the subarray. Maintenance: From part (b), after the execution of the inner loop, $A[i]$ will be the smallest element of the subarray $A[i..n]$. And in the beginning of the outer loop, $A[1..i \u2212 1]$ consists of elements that are smaller than the elements of $A[i..n]$, in sorted order. So, after the execution of the outer loop, subarray $A[1..i]$ will consists of elements that are smaller than the elements of $A[i + 1..n]$, in sorted order. Termination: The loop terminates when $i = A.length$. At that point the array $A[1..n]$ will consists of all elements in sorted order. d. The $i$th iteration of the for loop of lines 1-4 will cause $n \u2212 i$ iterations of the for loop of lines 2-4, each with constant time execution, so the worst-case running time of bubble sort is $\\Theta(n^2)$ which is same as the worst-case running time of insertion sort.","title":"2-2 Correctness of bubblesort"},{"location":"Chap02/Problems/2-3/","text":"The following code fragment implements Horner's rule for evaluating a polynomial $$ \\begin{aligned} P(x) & = \\sum_{k = 0}^n a_k x^k \\\\ & = a_0 + x(a_1 + x (a_2 + \\cdots + x(a_{n - 1} + x a_n) \\cdots)), \\end{aligned} $$ given the coefficients $a_0, a_1, \\ldots, a_n$ and a value of $x$: y = 0 for i = n downto 0 y = a [ i ] + x * y a. In terms of $\\Theta$-notation, what is the running time of this code fragment for Horner's rule? b. Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare to Horner's rule c. Consider the following loop invariant: At the start of each iteration of the for loop of lines 2-3, $$y = \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1} x^k.$$ Interpret a summation with no terms as equaling $0$. Following the structure of the loop invariant proof presented in this chapter, use this loop invariant to show that, at termination, $y = \\sum_{k = 0}^n a_k x^k$. d. Conclude by arguing that the given code fragment correctly evaluates a polynomial characterized by the coefficients $a_0, a_1, \\ldots, a_n$. a. $\\Theta(n)$. b. NAIVE - HORNER () y = 0 for k = 0 to n temp = 1 for i = 1 to k temp = temp * x y = y + a [ k ] * temp The running time is $\\Theta(n^2)$, because of the nested loop. It is obviously slower. c. Initialization: It is pretty trivial, since the summation has no terms which implies $y = 0$. Maintenance: By using the loop invariant, in the end of the $i$-th iteration, we have $$ \\begin{aligned} y & = a_i + x \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1} x^k \\\\ & = a_i x^0 + \\sum_{k = 0}^{n - i - 1} a_{k + i + 1} x^{k + 1} \\\\ & = a_i x^0 + \\sum_{k = 1}^{n - i} a_{k + i} x^k \\\\ & = \\sum_{k = 0}^{n - i} a_{k + i} x^k. \\end{aligned} $$ Termination: The loop terminates at $i = -1$. If we substitute, $$y = \\sum_{k = 0}^{n - i - 1} a_{k + i + 1} x^k = \\sum_{k = 0}^n a_k x^k.$$ d. The invariant of the loop is a sum that equals a polynomial with the given coefficients.","title":"2-3 Correctness of Horner's rule"},{"location":"Chap02/Problems/2-4/","text":"Let $A[1..n]$ be an array of $n$ distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair $(i, j)$ is called an inversion of $A$. a. List the five inversions in the array $\\langle 2, 3, 8, 6, 1 \\rangle$. b. What array with elements from the set $\\{1, 2, \\ldots, n\\}$ has the most inversions? How many does it have? c. What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer. d. Give an algorithm that determines the number of inversions in any permutation of $n$ elements in $\\Theta(n\\lg n)$ worst-case time. ($\\textit{Hint:}$ Modify merge sort). a. $(1, 5)$, $(2, 5)$, $(3, 4)$, $(3, 5)$, $(4, 5)$. b. The array $\\langle n, n - 1, \\dots, 1 \\rangle$ has the most inversions $(n - 1) + (n - 2) + \\cdots + 1 = n(n - 1) / 2$. c. The running time of insertion sort is a constant times the number of inversions. Let $I(i)$ denote the number of $j < i$ such that $A[j] > A[i]$. Then $\\sum_{i = 1}^n I(i)$ equals the number of inversions in $A$. Now consider the while loop on lines 5-7 of the insertion sort algorithm. The loop will execute once for each element of $A$ which has index less than $j$ is larger than $A[j]$. Thus, it will execute $I(j)$ times. We reach this while loop once for each iteration of the for loop, so the number of constant time steps of insertion sort is $\\sum_{j = 1}^n I(j)$ which is exactly the inversion number of $A$. d. We'll call our algorithm $\\text{COUNT-INVERSIONS}$ for modified merge sort. In addition to sorting $A$, it will also keep track of the number of inversions. $\\text{COUNT-INVERSIONS}(A, p, r)$ sorts $A[p..r]$ and returns the number of inversions in the elements of $A[p..r]$, so $left$ and $right$ track the number of inversions of the form $(i, j)$ where $i$ and $j$ are both in the same half of $A$. $\\text{MERGE-INVERSIONS}(A, p, q, r)$ returns the number of inversions of the form $(i, j)$ where $i$ is in the first half of the array and $j$ is in the second half. Summing these up gives the total number of inversions in $A$. The runtime of the modified algorithm is $\\Theta(n\\lg n)$, which is same as merge sort since we only add an additional constant-time operation to some of the iterations in some of the loops. COUNT - INVERSIONS ( A , p , r ) if p < r q = floor (( p + r ) / 2 ) left = COUNT - INVERSIONS ( A , p , q ) right = COUNT - INVERSIONS ( A , q + 1 , r ) inversions = MERGE - INVERSIONS ( A , p , q , r ) + left + right return inversions MERGE - INVERSIONS ( A , p , q , r ) n1 = q - p + 1 n2 = r - q let L [ 1. . n1 + 1 ] and R [ 1. . n2 + 1 ] be new arrays for i = 1 to n1 L [ i ] = A [ p + i - 1 ] for j = 1 to n2 R [ j ] = A [ q + j ] L [ n1 + 1 ] = \u221e R [ n2 + 1 ] = \u221e i = 1 j = 1 inversions = 0 for k = p to r if L [ i ] <= R [ j ] A [ k ] = L [ i ] i = i + 1 else inversions = inversions + n1 - i + 1 A [ k ] = R [ j ] j = j + 1 return inversions","title":"2-4 Inversions"},{"location":"Chap03/3.1/","text":"3.1-1 Let $f(n) + g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\\Theta$-notation, prove that $\\max(f(n), g(n)) = \\Theta(f(n) + g(n))$. For asymptotically nonnegative functions $f(n)$ and $g(n)$, we know that $$ \\begin{aligned} \\exists n_1, n_2: & f(n) \\ge 0 & \\text{for} \\, n > n_1 \\\\ & g(n) \\ge 0 & \\text{for} \\, n > n_2. \\end{aligned} $$ Let $n_0 = \\max(n_1, n_2)$ and we know the equations below would be true for $n > n_0$: $$ \\begin{aligned} f(n) & \\le \\max(f(n), g(n)) \\\\ g(n) & \\le \\max(f(n), g(n)) \\\\ (f(n) + g(n))/2 & \\le \\max(f(n), g(n)) \\\\ \\max(f(n), g(n)) & \\le (f(n) + g(n)). \\end{aligned} $$ Then we can combine last two inequalities: $$0 \\le \\frac{f(n) + g(n)}{2} \\le \\max{(f(n), g(n))} \\le f(n) + g(n).$$ Which is the definition of $\\Theta{(f(n) + g(n))}$ with $c_1 = \\frac{1}{2}$ and $c_2 = 1$ 3.1-2 Show that for any real constants $a$ and $b$, where $b > 0$, $$(n + a)^b = \\Theta(n^b). \\tag{3.2}$$ Expand $(n + a)^b$ by the Binomial Expansion, we have $$(n + a)^b = C_0^b n^b a^0 + C_1^b n^{b - 1} a^1 + \\cdots + C_b^b n^0 a^b.$$ Besides, we know below is true for any polynomial when $x \\ge 1$. $$a_0 x^0 + a_1 x^1 + \\cdots + a_n x^n \\le (a_0 + a_1 + \\cdots + a_n) x^n.$$ Thus, $$C_0^b n^b \\le C_0^b n^b a^0 + C_1^b n^{b - 1} a^1 + \\cdots + C_b^b n^0 a^b \\le (C_0^b + C_1^b + \\cdots + C_b^b) n^b = 2^b n^b.$$ $$\\implies (n + a)^b = \\Theta(n^b).$$ 3.1-3 Explain why the statement, \"The running time of algorithm $A$ is at least $O(n^2)$,\" is meaningless. $T(n)$: running time of algorithm $A$. We just care about the upper bound and the lower bound of $T(n)$. The statement: $T(n)$ is at least $O(n^2)$. Upper bound: Because \"$T(n)$ is at least $O(n^2)$\", there's no information about the upper bound of $T(n)$. Lower bound: Assume $f(n) = O(n^2)$, then the statement: $T(n) \\ge f(n)$, but $f(n)$ could be any fuction that is \"smaller\" than $n^2$. For example, constant, $n$, etc, so there's no conclusion about the lower bound of $T(n)$, too. Therefore, the statement, \"The running time of algorithm $A$ is at least $O(n^2)$,\" is meaningless. 3.1-4 Is $2^{n + 1} = O(2^n)$? Is $2^{2n} = O(2^n)$? True. Note that $2^{n + 1} = 2 \\times 2^n$. We can choose $c \\ge 2$ and $n_0 = 0$, such that $0 \\le 2^{n + 1} \\le c \\times 2^n$ for all $n \\ge n_0$. By definition, $2^{n + 1} = O(2^n)$. False. Note that $2^{2n} = 2^n \\times 2^n = 4^n$. We can't find any $c$ and $n_0$, such that $0 \\le 2^{2n} = 4^n \\le c \\times 2^n$ for all $n \\ge n_0$. 3.1-5 Prove Theorem 3.1. The theorem states: For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. From $f = \\Theta(g(n))$, we have that $$0 \\le c_1 g(n) \\le f(n) \\le c_2g(n) \\text{ for } n > n_0.$$ We can pick the constants from here and use them in the definitions of $O$ and $\\Omega$ to show that both hold. From $f(n) = \\Omega(g(n))$ and $f(n) = O(g(n))$, we have that $$ \\begin{aligned} & 0 \\le c_3g(n) \\le f(n) & \\text{ for all } n \\ge n_1 \\\\ \\text{and } & 0 \\le f(n) \\le c_4g(n) & \\text{ for all } n \\ge n_2. \\end{aligned} $$ If we let $n_3 = \\max(n_1, n_2)$ and merge the inequalities, we get $$0 \\le c_3g(n) \\le f(n) \\le c_4g(n) \\text{ for all } n > n_3.$$ Which is the definition of $\\Theta$. 3.1-6 Prove that the running time of an algorithm is $\\Theta(g(n))$ if and only if its worst-case running time is $O(g(n))$ and its best-case running time is $\\Omega(g(n))$. If $T_w$ is the worst-case running time and $T_b$ is the best-case running time, we know that $$ \\begin{aligned} & 0 \\le c_1g(n) \\le T_b(n) & \\text{ for } n > n_b \\\\ \\text{and } & 0 \\le T_w(n) \\le c_2g(n) & \\text{ for } n > n_w. \\end{aligned} $$ Combining them we get $$0 \\le c_1g(n) \\le T_b(n) \\le T_w(n) \\le c_2g(n) \\text{ for } n > \\max(n_b, n_w).$$ Since the running time is bound between $T_b$ and $T_w$ and the above is the definition of the $\\Theta$-notation, proved. 3.1-7 Prove $o(g(n)) \\cap w(g(n))$ is the empty set. Let $f(n) = o(g(n)) \\cap w(g(n))$. We know that for any $c_1 > 0$, $c_2 > 0$, $$ \\begin{aligned} & \\exists n_1 > 0: 0 \\le f(n) < c_1g(n) \\\\ \\text{and } & \\exists n_2 > 0: 0 \\le c_2g(n) < f(n). \\end{aligned} $$ If we pick $n_0 = \\max(n_1, n_2)$, and let $c_1 = c_2$, from the problem definition we get $$c_1g(n) < f(n) < c_1g(n).$$ There is no solutions, which means that the intersection is the empty set. 3.1-8 We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n, m)$ we denote $O(g(n, m))$ the set of functions: $$ \\begin{aligned} O(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le f(n, m) \\le cg(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$ Give corresponding definitions for $\\Omega(g(n, m))$ and $\\Theta(g(n, m))$. $$ \\begin{aligned} \\Omega(g(n, m)) = \\{ f(n, m): & \\text{ there exist positive constants $c$, $n_0$, and $m_0$ such that } \\\\ & \\text{ $0 \\le cg(n, m) \\le f(n, m)$ for all $n \\ge n_0$ and $m \\ge m_0$}.\\} \\end{aligned} $$ $$ \\begin{aligned} \\Theta(g(n, m)) = \\{ f(n, m): & \\text{ there exist positive constants $c_1$, $c_2$, $n_0$, and $m_0$ such that } \\\\ & \\text{ $0 \\le c_1 g(n, m) \\le f(n, m) \\le c_2 g(n, m)$ for all $n \\ge n_0$ and $m \\ge m_0$}.\\} \\end{aligned} $$","title":"3.1 Asymptotic notation"},{"location":"Chap03/3.1/#31-1","text":"Let $f(n) + g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\\Theta$-notation, prove that $\\max(f(n), g(n)) = \\Theta(f(n) + g(n))$. For asymptotically nonnegative functions $f(n)$ and $g(n)$, we know that $$ \\begin{aligned} \\exists n_1, n_2: & f(n) \\ge 0 & \\text{for} \\, n > n_1 \\\\ & g(n) \\ge 0 & \\text{for} \\, n > n_2. \\end{aligned} $$ Let $n_0 = \\max(n_1, n_2)$ and we know the equations below would be true for $n > n_0$: $$ \\begin{aligned} f(n) & \\le \\max(f(n), g(n)) \\\\ g(n) & \\le \\max(f(n), g(n)) \\\\ (f(n) + g(n))/2 & \\le \\max(f(n), g(n)) \\\\ \\max(f(n), g(n)) & \\le (f(n) + g(n)). \\end{aligned} $$ Then we can combine last two inequalities: $$0 \\le \\frac{f(n) + g(n)}{2} \\le \\max{(f(n), g(n))} \\le f(n) + g(n).$$ Which is the definition of $\\Theta{(f(n) + g(n))}$ with $c_1 = \\frac{1}{2}$ and $c_2 = 1$","title":"3.1-1"},{"location":"Chap03/3.1/#31-2","text":"Show that for any real constants $a$ and $b$, where $b > 0$, $$(n + a)^b = \\Theta(n^b). \\tag{3.2}$$ Expand $(n + a)^b$ by the Binomial Expansion, we have $$(n + a)^b = C_0^b n^b a^0 + C_1^b n^{b - 1} a^1 + \\cdots + C_b^b n^0 a^b.$$ Besides, we know below is true for any polynomial when $x \\ge 1$. $$a_0 x^0 + a_1 x^1 + \\cdots + a_n x^n \\le (a_0 + a_1 + \\cdots + a_n) x^n.$$ Thus, $$C_0^b n^b \\le C_0^b n^b a^0 + C_1^b n^{b - 1} a^1 + \\cdots + C_b^b n^0 a^b \\le (C_0^b + C_1^b + \\cdots + C_b^b) n^b = 2^b n^b.$$ $$\\implies (n + a)^b = \\Theta(n^b).$$","title":"3.1-2"},{"location":"Chap03/3.1/#31-3","text":"Explain why the statement, \"The running time of algorithm $A$ is at least $O(n^2)$,\" is meaningless. $T(n)$: running time of algorithm $A$. We just care about the upper bound and the lower bound of $T(n)$. The statement: $T(n)$ is at least $O(n^2)$. Upper bound: Because \"$T(n)$ is at least $O(n^2)$\", there's no information about the upper bound of $T(n)$. Lower bound: Assume $f(n) = O(n^2)$, then the statement: $T(n) \\ge f(n)$, but $f(n)$ could be any fuction that is \"smaller\" than $n^2$. For example, constant, $n$, etc, so there's no conclusion about the lower bound of $T(n)$, too. Therefore, the statement, \"The running time of algorithm $A$ is at least $O(n^2)$,\" is meaningless.","title":"3.1-3"},{"location":"Chap03/3.1/#31-4","text":"Is $2^{n + 1} = O(2^n)$? Is $2^{2n} = O(2^n)$? True. Note that $2^{n + 1} = 2 \\times 2^n$. We can choose $c \\ge 2$ and $n_0 = 0$, such that $0 \\le 2^{n + 1} \\le c \\times 2^n$ for all $n \\ge n_0$. By definition, $2^{n + 1} = O(2^n)$. False. Note that $2^{2n} = 2^n \\times 2^n = 4^n$. We can't find any $c$ and $n_0$, such that $0 \\le 2^{2n} = 4^n \\le c \\times 2^n$ for all $n \\ge n_0$.","title":"3.1-4"},{"location":"Chap03/3.1/#31-5","text":"Prove Theorem 3.1. The theorem states: For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. From $f = \\Theta(g(n))$, we have that $$0 \\le c_1 g(n) \\le f(n) \\le c_2g(n) \\text{ for } n > n_0.$$ We can pick the constants from here and use them in the definitions of $O$ and $\\Omega$ to show that both hold. From $f(n) = \\Omega(g(n))$ and $f(n) = O(g(n))$, we have that $$ \\begin{aligned} & 0 \\le c_3g(n) \\le f(n) & \\text{ for all } n \\ge n_1 \\\\ \\text{and } & 0 \\le f(n) \\le c_4g(n) & \\text{ for all } n \\ge n_2. \\end{aligned} $$ If we let $n_3 = \\max(n_1, n_2)$ and merge the inequalities, we get $$0 \\le c_3g(n) \\le f(n) \\le c_4g(n) \\text{ for all } n > n_3.$$ Which is the definition of $\\Theta$.","title":"3.1-5"},{"location":"Chap03/3.1/#31-6","text":"Prove that the running time of an algorithm is $\\Theta(g(n))$ if and only if its worst-case running time is $O(g(n))$ and its best-case running time is $\\Omega(g(n))$. If $T_w$ is the worst-case running time and $T_b$ is the best-case running time, we know that $$ \\begin{aligned} & 0 \\le c_1g(n) \\le T_b(n) & \\text{ for } n > n_b \\\\ \\text{and } & 0 \\le T_w(n) \\le c_2g(n) & \\text{ for } n > n_w. \\end{aligned} $$ Combining them we get $$0 \\le c_1g(n) \\le T_b(n) \\le T_w(n) \\le c_2g(n) \\text{ for } n > \\max(n_b, n_w).$$ Since the running time is bound between $T_b$ and $T_w$ and the above is the definition of the $\\Theta$-notation, proved.","title":"3.1-6"},{"location":"Chap03/3.1/#31-7","text":"Prove $o(g(n)) \\cap w(g(n))$ is the empty set. Let $f(n) = o(g(n)) \\cap w(g(n))$. We know that for any $c_1 > 0$, $c_2 > 0$, $$ \\begin{aligned} & \\exists n_1 > 0: 0 \\le f(n) < c_1g(n) \\\\ \\text{and } & \\exists n_2 > 0: 0 \\le c_2g(n) < f(n). \\end{aligned} $$ If we pick $n_0 = \\max(n_1, n_2)$, and let $c_1 = c_2$, from the problem definition we get $$c_1g(n) < f(n) < c_1g(n).$$ There is no solutions, which means that the intersection is the empty set.","title":"3.1-7"},{"location":"Chap03/3.1/#31-8","text":"We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n, m)$ we denote $O(g(n, m))$ the set of functions: $$ \\begin{aligned} O(g(n, m)) = \\{f(n, m): & \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\\\ & \\text{ such that } 0 \\le f(n, m) \\le cg(n, m) \\\\ & \\text{ for all } n \\ge n_0 \\text{ or } m \\ge m_0.\\} \\end{aligned} $$ Give corresponding definitions for $\\Omega(g(n, m))$ and $\\Theta(g(n, m))$. $$ \\begin{aligned} \\Omega(g(n, m)) = \\{ f(n, m): & \\text{ there exist positive constants $c$, $n_0$, and $m_0$ such that } \\\\ & \\text{ $0 \\le cg(n, m) \\le f(n, m)$ for all $n \\ge n_0$ and $m \\ge m_0$}.\\} \\end{aligned} $$ $$ \\begin{aligned} \\Theta(g(n, m)) = \\{ f(n, m): & \\text{ there exist positive constants $c_1$, $c_2$, $n_0$, and $m_0$ such that } \\\\ & \\text{ $0 \\le c_1 g(n, m) \\le f(n, m) \\le c_2 g(n, m)$ for all $n \\ge n_0$ and $m \\ge m_0$}.\\} \\end{aligned} $$","title":"3.1-8"},{"location":"Chap03/3.2/","text":"3.2-1 Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n) + g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n) \\cdot g(n)$ is monotonically increasing. $$ \\begin{aligned} f(m) & \\le f(n) \\quad \\text{ for } m \\le n \\\\ g(m) & \\le g(n) \\quad \\text{ for } m \\le n, \\\\ \\to f(m) + g(m) & \\le f(n) + g(n), \\end{aligned} $$ which proves the first function. Then $$f(g(m)) \\le f(g(n)) \\text{ for } m \\le n.$$ This is true, since $g(m) \\le g(n)$ and $f(n)$ is monotonically increasing. If both functions are nonnegative, then we can multiply the two equalities and we get $$f(m) \\cdot g(m) \\le f(n) \\cdot g(n).$$ 3.2-2 Prove equation $\\text{(3.16)}$. $$ \\begin{aligned} a^{\\log_b c} = a^\\frac{\\log_a c}{\\log_a b} = (a^{\\log_a c})^{\\frac{1}{\\log_a b}} = c^{\\log_b a} \\end{aligned} $$ 3.2-3 Prove equation $\\text{(3.19)}$. Also prove that $n! \\ne \\omega(2^n)$ and $n! \\ne o(n^n)$. $$\\lg(n!) = \\Theta(n\\lg n) \\tag{3.19}$$ We can use Stirling's approximation to prove these three equations. For equation $\\text{(3.19)}$, $$ \\begin{aligned} \\lg(n!) & = \\lg\\Bigg(\\sqrt{2\\pi n}\\Big(\\frac{n}{e}\\Big)^n\\Big(1 + \\Theta(\\frac{1}{n})\\Big)\\Bigg) \\\\ & = \\lg\\sqrt{2\\pi n } + \\lg\\Big(\\frac{n}{e}\\Big)^n + \\lg\\Big(1+\\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + n\\lg{\\frac{n}{e}} + \\lg\\Big(\\Theta(1) + \\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + \\Theta(n\\lg n) + \\Theta(\\frac{1}{n}) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ For $n! \\ne \\omega(2^n)$, $$ \\begin{aligned} \\lim_{n \\to \\infty} \\frac{2^n}{n!} & = \\lim_{n \\to \\infty} \\frac{2^n}{\\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\\\ & = \\lim_{n \\to \\infty} \\frac{1}{\\sqrt{2\\pi n} \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\left(\\frac{2e}{n}\\right)^n \\\\ & \\le \\lim_{n \\to \\infty} \\left(\\frac{2e}{n}\\right)^n \\\\ & \\le \\lim_{n \\to \\infty} \\frac{1}{2^n} = 0, \\end{aligned} $$ where the last step holds for $n > 4e$. For $n! \\ne o(n^n)$, $$ \\begin{aligned} \\lim_{n \\to \\infty} \\frac{n^n}{n!} & = \\lim_{n \\to \\infty} \\frac{n^n}{\\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\\\ & = \\lim_{n \\to \\infty} \\frac{e^n}{\\sqrt{2\\pi n} \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\\\ & = \\lim_{n \\to \\infty} O(\\frac{1}{\\sqrt n})e^n \\\\ & \\ge \\lim_{n \\to \\infty} \\frac{e^n}{c\\sqrt n} & \\text{(for some constant $c > 0$)}\\\\ & \\ge \\lim_{n \\to \\infty} \\frac{e^n}{cn} \\\\ & = \\lim_{n \\to \\infty} \\frac{e^n}{c} = \\infty. \\end{aligned} $$ 3.2-4 $\\star$ Is the function $\\lceil \\lg n \\rceil!$ polynomially bounded? Is the function $\\lceil \\lg\\lg n \\rceil!$ polynomially bounded? Proving that a function $f(n)$ is polynomially bounded is equivalent to proving that $\\lg(f(n)) = O(\\lg n)$ for the following reasons. If $f$ is polynomially bounded, then there exist constants $c$, $k$, $n_0$ such that for all $n \\ge n_0$, $f(n) \\le cn^k$. Hence, $\\lg(f(n)) \\le kc\\lg n$, which means that $\\lg(f(n)) = O(\\lg n)$. If $\\lg(f(n)) = O(\\lg n)$, then $f$ is polynomially bounded. In the following proofs, we will make use of the following two facts: $\\lg(n!) = \\Theta(n\\lg n)$ $\\lceil \\lg n \\rceil = \\Theta(\\lg n)$ $\\lceil \\lg n \\rceil!$ is not polynomially bounded because $$ \\begin{aligned} \\lg(\\lceil \\lg n \\rceil!) & = \\Theta(\\lceil \\lg n \\rceil \\lg \\lceil \\lg n \\rceil) \\\\ & = \\Theta(\\lg n\\lg\\lg n) \\\\ & = \\omega(\\lg n) \\\\ & \\ne O(\\lg n). \\end{aligned} $$ $\\lceil \\lg\\lg n \\rceil!$ is polynomially bounded because $$ \\begin{aligned} \\lg(\\lceil \\lg\\lg n \\rceil!) & = \\Theta(\\lceil \\lg\\lg n \\rceil \\lg \\lceil \\lg\\lg n \\rceil) \\\\ & = \\Theta(\\lg\\lg n\\lg\\lg\\lg n) \\\\ & = o((\\lg\\lg n)^2) \\\\ & = o(\\lg^2(\\lg n)) \\\\ & = o(\\lg n) \\\\ & = O(\\lg n). \\end{aligned} $$ The last step above follows from the property that any polylogarithmic function grows more slowly than any positive polynomial function, i.e., that for constants $a, b > 0$, we have $\\lg^b n = o(n^a)$. Substitute $\\lg n$ for $n$, $2$ for $b$, and $1$ for $a$, giving $\\lg^2(\\lg n) = o(\\lg n)$. Therefore, $\\lg(\\lceil \\lg\\lg n \\rceil!) = O(\\lg n)$, and so $\\lceil \\lg\\lg n \\rceil!$ is polynomially bounded. 3.2-5 $\\star$ Which is asymptotically larger: $\\lg(\\lg^*n)$ or $\\lg^*(\\lg n)$? We have $\\lg^* 2^n = 1 + \\lg^* n$, $$ \\begin{aligned} \\lim_{n \\to \\infty} \\frac{\\lg(\\lg^*n)}{\\lg^*(\\lg n)} & = \\lim_{n \\to \\infty} \\frac{\\lg(\\lg^* 2^n)}{\\lg^*(\\lg 2^n)} \\\\ & = \\lim_{n \\to \\infty} \\frac{\\lg(1 + \\lg^* n)}{\\lg^* n} \\\\ & = \\lim_{n \\to \\infty} \\frac{\\lg(1 + n)}{n} \\\\ & = \\lim_{n \\to \\infty} \\frac{1}{1 + n} \\\\ & = 0. \\end{aligned} $$ Therefore, we have that $\\lg^*(\\lg n)$ is asymptotically larger. 3.2-6 Show that the golden ratio $\\phi$ and its conjugate $\\hat \\phi$ both satisfy the equation $x^2 = x + 1$. $$ \\begin{aligned} \\phi^2 & = \\Bigg(\\frac{1 + \\sqrt 5}{2}\\Bigg)^2 = \\frac{6 + 2\\sqrt 5}{4} = 1 + \\frac{1 + \\sqrt 5}{2} = 1 + \\phi \\\\ \\hat\\phi^2 & = \\Bigg(\\frac{1 - \\sqrt 5}{2}\\Bigg)^2 = \\frac{6 - 2\\sqrt 5}{4} = 1 + \\frac{1 - \\sqrt 5}{2} = 1 + \\hat\\phi. \\end{aligned} $$ 3.2-7 Prove by induction that the $i$th Fibonacci number satisfies the equality $$F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5},$$ where $\\phi$ is the golden ratio and $\\hat\\phi$ is its conjugate. Base case For $i = 0$, $$ \\begin{aligned} \\frac{\\phi^0 - \\hat\\phi^0}{\\sqrt 5} & = \\frac{1 - 1}{\\sqrt 5} \\\\ & = 0 \\\\ & = F_0. \\end{aligned} $$ For $i = 1$, $$ \\begin{aligned} \\frac{\\phi^1 - \\hat\\phi^1}{\\sqrt 5} & = \\frac{(1 + \\sqrt 5) - (1 - \\sqrt 5)}{2 \\sqrt 5} \\\\ & = 1 \\\\ & = F_1. \\end{aligned} $$ Assume $F_{i - 1} = (\\phi^{i - 1} - \\hat\\phi^{i - 1}) / \\sqrt 5$ and $F_{i - 2} = (\\phi^{i - 2} - \\hat\\phi^{i - 2}) / \\sqrt 5$, $$ \\begin{aligned} F_i & = F_{i - 1} + F_{i - 2} \\\\ & = \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt 5} + \\frac{\\phi^{i - 2} - \\hat\\phi^{i - 2}}{\\sqrt 5} \\\\ & = \\frac{\\phi^{i - 2}(\\phi + 1) - \\hat\\phi^{i - 2}(\\hat\\phi + 1)}{\\sqrt 5} \\\\ & = \\frac{\\phi^{i - 2}\\phi^2 - \\hat\\phi^{i - 2}\\hat\\phi^2}{\\sqrt 5} \\\\ & = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5}. \\end{aligned} $$ 3.2-8 Show that $k\\ln k = \\Theta(n)$ implies $k = \\Theta(n / \\lg n)$. From the symmetry of $\\Theta$, $$k\\ln k = \\Theta(n) \\Rightarrow n = \\Theta(k\\ln k).$$ Let's find $\\ln n$, $$\\ln n = \\Theta(\\ln(k\\ln k)) = \\Theta(\\ln k + \\ln\\ln k) = \\Theta(\\ln k).$$ Let's divide the two, $$\\frac{n}{\\ln n} = \\frac{\\Theta(k\\ln k)}{\\Theta(\\ln k)} = \\Theta\\Big({\\frac{k\\ln k}{\\ln k}}\\Big) = \\Theta(k).$$","title":"3.2 Standard notations and common functions"},{"location":"Chap03/3.2/#32-1","text":"Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n) + g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n) \\cdot g(n)$ is monotonically increasing. $$ \\begin{aligned} f(m) & \\le f(n) \\quad \\text{ for } m \\le n \\\\ g(m) & \\le g(n) \\quad \\text{ for } m \\le n, \\\\ \\to f(m) + g(m) & \\le f(n) + g(n), \\end{aligned} $$ which proves the first function. Then $$f(g(m)) \\le f(g(n)) \\text{ for } m \\le n.$$ This is true, since $g(m) \\le g(n)$ and $f(n)$ is monotonically increasing. If both functions are nonnegative, then we can multiply the two equalities and we get $$f(m) \\cdot g(m) \\le f(n) \\cdot g(n).$$","title":"3.2-1"},{"location":"Chap03/3.2/#32-2","text":"Prove equation $\\text{(3.16)}$. $$ \\begin{aligned} a^{\\log_b c} = a^\\frac{\\log_a c}{\\log_a b} = (a^{\\log_a c})^{\\frac{1}{\\log_a b}} = c^{\\log_b a} \\end{aligned} $$","title":"3.2-2"},{"location":"Chap03/3.2/#32-3","text":"Prove equation $\\text{(3.19)}$. Also prove that $n! \\ne \\omega(2^n)$ and $n! \\ne o(n^n)$. $$\\lg(n!) = \\Theta(n\\lg n) \\tag{3.19}$$ We can use Stirling's approximation to prove these three equations. For equation $\\text{(3.19)}$, $$ \\begin{aligned} \\lg(n!) & = \\lg\\Bigg(\\sqrt{2\\pi n}\\Big(\\frac{n}{e}\\Big)^n\\Big(1 + \\Theta(\\frac{1}{n})\\Big)\\Bigg) \\\\ & = \\lg\\sqrt{2\\pi n } + \\lg\\Big(\\frac{n}{e}\\Big)^n + \\lg\\Big(1+\\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + n\\lg{\\frac{n}{e}} + \\lg\\Big(\\Theta(1) + \\Theta(\\frac{1}{n})\\Big) \\\\ & = \\Theta(\\sqrt n) + \\Theta(n\\lg n) + \\Theta(\\frac{1}{n}) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ For $n! \\ne \\omega(2^n)$, $$ \\begin{aligned} \\lim_{n \\to \\infty} \\frac{2^n}{n!} & = \\lim_{n \\to \\infty} \\frac{2^n}{\\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\\\ & = \\lim_{n \\to \\infty} \\frac{1}{\\sqrt{2\\pi n} \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\left(\\frac{2e}{n}\\right)^n \\\\ & \\le \\lim_{n \\to \\infty} \\left(\\frac{2e}{n}\\right)^n \\\\ & \\le \\lim_{n \\to \\infty} \\frac{1}{2^n} = 0, \\end{aligned} $$ where the last step holds for $n > 4e$. For $n! \\ne o(n^n)$, $$ \\begin{aligned} \\lim_{n \\to \\infty} \\frac{n^n}{n!} & = \\lim_{n \\to \\infty} \\frac{n^n}{\\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\\\ & = \\lim_{n \\to \\infty} \\frac{e^n}{\\sqrt{2\\pi n} \\left(1 + \\Theta\\left(\\frac{1}{n}\\right)\\right)} \\\\ & = \\lim_{n \\to \\infty} O(\\frac{1}{\\sqrt n})e^n \\\\ & \\ge \\lim_{n \\to \\infty} \\frac{e^n}{c\\sqrt n} & \\text{(for some constant $c > 0$)}\\\\ & \\ge \\lim_{n \\to \\infty} \\frac{e^n}{cn} \\\\ & = \\lim_{n \\to \\infty} \\frac{e^n}{c} = \\infty. \\end{aligned} $$","title":"3.2-3"},{"location":"Chap03/3.2/#32-4-star","text":"Is the function $\\lceil \\lg n \\rceil!$ polynomially bounded? Is the function $\\lceil \\lg\\lg n \\rceil!$ polynomially bounded? Proving that a function $f(n)$ is polynomially bounded is equivalent to proving that $\\lg(f(n)) = O(\\lg n)$ for the following reasons. If $f$ is polynomially bounded, then there exist constants $c$, $k$, $n_0$ such that for all $n \\ge n_0$, $f(n) \\le cn^k$. Hence, $\\lg(f(n)) \\le kc\\lg n$, which means that $\\lg(f(n)) = O(\\lg n)$. If $\\lg(f(n)) = O(\\lg n)$, then $f$ is polynomially bounded. In the following proofs, we will make use of the following two facts: $\\lg(n!) = \\Theta(n\\lg n)$ $\\lceil \\lg n \\rceil = \\Theta(\\lg n)$ $\\lceil \\lg n \\rceil!$ is not polynomially bounded because $$ \\begin{aligned} \\lg(\\lceil \\lg n \\rceil!) & = \\Theta(\\lceil \\lg n \\rceil \\lg \\lceil \\lg n \\rceil) \\\\ & = \\Theta(\\lg n\\lg\\lg n) \\\\ & = \\omega(\\lg n) \\\\ & \\ne O(\\lg n). \\end{aligned} $$ $\\lceil \\lg\\lg n \\rceil!$ is polynomially bounded because $$ \\begin{aligned} \\lg(\\lceil \\lg\\lg n \\rceil!) & = \\Theta(\\lceil \\lg\\lg n \\rceil \\lg \\lceil \\lg\\lg n \\rceil) \\\\ & = \\Theta(\\lg\\lg n\\lg\\lg\\lg n) \\\\ & = o((\\lg\\lg n)^2) \\\\ & = o(\\lg^2(\\lg n)) \\\\ & = o(\\lg n) \\\\ & = O(\\lg n). \\end{aligned} $$ The last step above follows from the property that any polylogarithmic function grows more slowly than any positive polynomial function, i.e., that for constants $a, b > 0$, we have $\\lg^b n = o(n^a)$. Substitute $\\lg n$ for $n$, $2$ for $b$, and $1$ for $a$, giving $\\lg^2(\\lg n) = o(\\lg n)$. Therefore, $\\lg(\\lceil \\lg\\lg n \\rceil!) = O(\\lg n)$, and so $\\lceil \\lg\\lg n \\rceil!$ is polynomially bounded.","title":"3.2-4 $\\star$"},{"location":"Chap03/3.2/#32-5-star","text":"Which is asymptotically larger: $\\lg(\\lg^*n)$ or $\\lg^*(\\lg n)$? We have $\\lg^* 2^n = 1 + \\lg^* n$, $$ \\begin{aligned} \\lim_{n \\to \\infty} \\frac{\\lg(\\lg^*n)}{\\lg^*(\\lg n)} & = \\lim_{n \\to \\infty} \\frac{\\lg(\\lg^* 2^n)}{\\lg^*(\\lg 2^n)} \\\\ & = \\lim_{n \\to \\infty} \\frac{\\lg(1 + \\lg^* n)}{\\lg^* n} \\\\ & = \\lim_{n \\to \\infty} \\frac{\\lg(1 + n)}{n} \\\\ & = \\lim_{n \\to \\infty} \\frac{1}{1 + n} \\\\ & = 0. \\end{aligned} $$ Therefore, we have that $\\lg^*(\\lg n)$ is asymptotically larger.","title":"3.2-5 $\\star$"},{"location":"Chap03/3.2/#32-6","text":"Show that the golden ratio $\\phi$ and its conjugate $\\hat \\phi$ both satisfy the equation $x^2 = x + 1$. $$ \\begin{aligned} \\phi^2 & = \\Bigg(\\frac{1 + \\sqrt 5}{2}\\Bigg)^2 = \\frac{6 + 2\\sqrt 5}{4} = 1 + \\frac{1 + \\sqrt 5}{2} = 1 + \\phi \\\\ \\hat\\phi^2 & = \\Bigg(\\frac{1 - \\sqrt 5}{2}\\Bigg)^2 = \\frac{6 - 2\\sqrt 5}{4} = 1 + \\frac{1 - \\sqrt 5}{2} = 1 + \\hat\\phi. \\end{aligned} $$","title":"3.2-6"},{"location":"Chap03/3.2/#32-7","text":"Prove by induction that the $i$th Fibonacci number satisfies the equality $$F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5},$$ where $\\phi$ is the golden ratio and $\\hat\\phi$ is its conjugate. Base case For $i = 0$, $$ \\begin{aligned} \\frac{\\phi^0 - \\hat\\phi^0}{\\sqrt 5} & = \\frac{1 - 1}{\\sqrt 5} \\\\ & = 0 \\\\ & = F_0. \\end{aligned} $$ For $i = 1$, $$ \\begin{aligned} \\frac{\\phi^1 - \\hat\\phi^1}{\\sqrt 5} & = \\frac{(1 + \\sqrt 5) - (1 - \\sqrt 5)}{2 \\sqrt 5} \\\\ & = 1 \\\\ & = F_1. \\end{aligned} $$ Assume $F_{i - 1} = (\\phi^{i - 1} - \\hat\\phi^{i - 1}) / \\sqrt 5$ and $F_{i - 2} = (\\phi^{i - 2} - \\hat\\phi^{i - 2}) / \\sqrt 5$, $$ \\begin{aligned} F_i & = F_{i - 1} + F_{i - 2} \\\\ & = \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt 5} + \\frac{\\phi^{i - 2} - \\hat\\phi^{i - 2}}{\\sqrt 5} \\\\ & = \\frac{\\phi^{i - 2}(\\phi + 1) - \\hat\\phi^{i - 2}(\\hat\\phi + 1)}{\\sqrt 5} \\\\ & = \\frac{\\phi^{i - 2}\\phi^2 - \\hat\\phi^{i - 2}\\hat\\phi^2}{\\sqrt 5} \\\\ & = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt 5}. \\end{aligned} $$","title":"3.2-7"},{"location":"Chap03/3.2/#32-8","text":"Show that $k\\ln k = \\Theta(n)$ implies $k = \\Theta(n / \\lg n)$. From the symmetry of $\\Theta$, $$k\\ln k = \\Theta(n) \\Rightarrow n = \\Theta(k\\ln k).$$ Let's find $\\ln n$, $$\\ln n = \\Theta(\\ln(k\\ln k)) = \\Theta(\\ln k + \\ln\\ln k) = \\Theta(\\ln k).$$ Let's divide the two, $$\\frac{n}{\\ln n} = \\frac{\\Theta(k\\ln k)}{\\Theta(\\ln k)} = \\Theta\\Big({\\frac{k\\ln k}{\\ln k}}\\Big) = \\Theta(k).$$","title":"3.2-8"},{"location":"Chap03/Problems/3-1/","text":"Let $$p(n) = \\sum_{i = 0}^d a_i n^i,$$ where $a_d > 0$, be a degree-$d$ polynomial in $n$, and let $k$ be a constant. Use the definitions of the asymptotic notations to prove the following properties. a. If $k \\ge d$, then $p(n) = O(n^k)$. b. If $k \\le d$, then $p(n) = \\Omega(n^k)$. c. If $k = d$, then $p(n) = \\Theta(n^k)$. d. If $k > d$, then $p(n) = o(n^k)$. e. If $k < d$, then $p(n) = \\omega(n^k)$. Let's see that $p(n) = O(n^d)$. We need do pick $c = a_d + b$, such that $$\\sum\\limits_{i = 0}^d a_i n^i = a_d n^d + a_{d - 1}n^{d - 1} + \\cdots + a_1n + a_0 \\le cn^d.$$ When we divide by $n^d$, we get $$c = a_d + b \\ge a_d + \\frac{a_{d - 1}}n + \\frac{a_{d - 2}}{n^2} + \\cdots + \\frac{a_0}{n^d}.$$ and $$b \\ge \\frac{a_{d - 1}}n + \\frac{a_{d - 2}}{n^2} + \\cdots + \\frac{a_0}{n^d}.$$ If we choose $b = 1$, then we can choose $n_0$, $$n_0 = \\max(da_{d - 1}, d\\sqrt{a_{d - 2}}, \\ldots, d\\sqrt[d]{a_0}).$$ Now we have $n_0$ and $c$, such that $$p(n) \\le cn^d \\quad \\text{for } n \\ge n_0,$$ which is the definition of $O(n^d)$. By chosing $b = -1$ we can prove the $\\Omega(n^d)$ inequality and thus the $\\Theta(n^d)$ inequality. It is very similar to prove the other inequalities.","title":"3-1 Asymptotic behavior of polynomials"},{"location":"Chap03/Problems/3-2/","text":"Indicate for each pair of expressions $(A, B)$ in the table below, whether $A$ is $O$, $o$, $\\Omega$, $\\omega$, or $\\Theta$ of $B$. Assume that $k \\ge 1$, $\\epsilon > 0$, and $c > 1$ are constants. Your answer should be in the form of the table with \"yes\" or \"no\" written in each box. $$ \\begin{array}{ccccccc} A & B & O & o & \\Omega & \\omega & \\Theta \\\\ \\hline \\lg^k n & n^\\epsilon & yes & yes & no & no & no \\\\ n^k & c^n & yes & yes & no & no & no \\\\ \\sqrt n & n^{\\sin n} & no & no & no & no & no \\\\ 2^n & 2^{n / 2} & no & no & yes & yes & no \\\\ n^{\\lg c} & c^{\\lg n} & yes & no & yes & no & yes \\\\ \\lg(n!) & \\lg(n^n) & yes & no & yes & no & yes \\end{array} $$","title":"3-2 Relative asymptotic growths"},{"location":"Chap03/Problems/3-3/","text":"a. Rank the following functions by order of growth; that is, find an arrangement $g_1, g_2, \\ldots , g_{30}$ of the functions $g_1 = \\Omega(g_2), g_2 = \\Omega(g_3), \\ldots, g_{29} = \\Omega(g_{30})$. Partition your list into equivalence classes such that functions $f(n)$ and $g(n)$ are in the same class if and only if $f(n) = \\Theta(g(n))$. $$ \\begin{array}{cccccc} \\lg(\\lg^{^*}n) \\quad & \\quad 2^{\\lg^*n} \\quad & \\quad (\\sqrt 2)^{\\lg n} \\quad & \\quad n^2 \\quad & \\quad n! \\quad & \\quad (\\lg n)! \\\\ (\\frac{3}{2})^n \\quad & \\quad n^3 \\quad & \\quad \\lg^2 n \\quad & \\quad \\lg(n!) \\quad & \\quad 2^{2^n} \\quad & \\quad n^{1/\\lg n} \\\\ \\lg\\lg n \\quad & \\quad \\lg^* n \\quad & \\quad n\\cdot 2^n \\quad & \\quad n^{\\lg\\lg n} \\quad & \\quad \\lg n \\quad & \\quad 1 \\\\ 2^{\\lg n} \\quad & \\quad (\\lg n)^{\\lg n} \\quad & \\quad e^n \\quad & \\quad 4^{\\lg n} \\quad & \\quad (n + 1)! \\quad & \\quad \\sqrt{\\lg n} \\\\ \\lg^*(\\lg n) \\quad & \\quad 2^{\\sqrt{2\\lg n}} \\quad & \\quad n \\quad & \\quad 2^n \\quad & \\quad n\\lg n \\quad & \\quad 2^{2^{n + 1}} \\end{array} $$ b. Give an example of a single nonnegative function $f(n)$ such that for all functions $g_i(n)$ in part (a), $f(n)$ is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$. $$ \\begin{array}{ll} 2^{2^{n + 1}} & \\\\ 2^{2^n} & \\\\ (n + 1)! & \\\\ n! & \\\\ e^n & \\\\ n\\cdot 2^n & \\\\ 2^n & \\\\ (3 / 2)^n & \\\\ (\\lg n)^{\\lg n} = n^{\\lg\\lg n} & \\\\ (\\lg n)! & \\\\ n^3 & \\\\ n^2 = 4^{\\lg n} & \\\\ n\\lg n \\text{ and } \\lg(n!) & \\\\ n = 2^{\\lg n} & \\\\ (\\sqrt 2)^{\\lg n}(= \\sqrt n) & \\\\ 2^{\\sqrt{2\\lg n}} & \\\\ \\lg^2 n & \\\\ \\ln n & \\\\ \\sqrt{\\lg n} & \\\\ \\ln\\ln n & \\\\ 2^{\\lg^*n} & \\\\ \\lg^*n \\text{ and } \\lg^*(\\lg n) & \\\\ \\lg(\\lg^*n) & \\\\ n^{1 / \\lg n}(= 2) \\text{ and } 1 & \\end{array} $$ b. For example, $$ f(n) = \\begin{cases} 2^{2^{n + 2}} & \\text{if $n$ is even}, \\\\ 0 & \\text{if $n$ is odd}. \\end{cases} $$ for all functions $g_i(n)$ in part (a), $f(n)$ is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$.","title":"3-3 Ordering by asymptotic growth rates"},{"location":"Chap03/Problems/3-4/","text":"Let $f(n)$ and $g(n)$ by asymptotically positive functions. Prove or disprove each of the following conjectures. a. $f(n) = O(g(n))$ implies $g(n) = O(f(n))$. b. $f(n) + g(n) = \\Theta(\\min(f(n), g(n)))$. c. $f(n) = O(g(n))$ implies $\\lg(f(n)) = O(\\lg(g(n)))$, where $\\lg(g(n)) \\ge 1$ and $f(n) \\ge 1$ for all sufficiently large $n$. d. $f(n) = O(g(n))$ implies $2^{f(n)} = O(2^{g(n)})$. e. $f(n) = O((f(n))^2)$. f. $f(n) = O(g(n))$ implies $g(n) = \\Omega(f(n))$. g. $f(n) = \\Theta(f(n / 2))$. h. $f(n) + o(f(n)) = \\Theta(f(n))$. a. Disprove, $n = O(n^2)$, but $n^2 \\ne O(n)$. b. Disprove, $n^2 + n \\ne \\Theta(\\min(n^2, n)) = \\Theta(n)$. c. Prove, because $f(n) \\ge 1$ after a certain $n \\ge n_0$. $$ \\begin{aligned} \\exists c, n_0: \\forall n \\ge n_0, 0 \\le f(n) \\le cg(n) \\\\ \\Rightarrow 0 \\le \\lg f(n) \\le \\lg (cg(n)) = \\lg c + \\lg g(n). \\end{aligned} $$ We need to prove that $$\\lg f(n) \\le d\\lg g(n).$$ We can find $d$, $$d = \\frac{\\lg c + \\lg g(n)}{\\lg g(n)} = \\frac{\\lg c}{\\lg g(n)} + 1 \\le \\lg c + 1,$$ where the last step is valid, because $\\lg g(n) \\ge 1$. d. Disprove, because $2n = O(n)$, but $2^{2n} = 4^n \\ne O(2^n)$. e. Prove, $0 \\le f(n) \\le cf^2(n)$ is trivial when $f(n) \\ge 1$, but if $f(n) < 1$ for all $n$, it's not correct. However, we don't care this case. f. Prove, from the first, we know that $0 \\le f(n) \\le cg(n)$ and we need to prove that $0 \\le df(n) \\le g(n)$, which is straightforward with $d = 1 / c$. g. Disprove, let's pick $f(n) = 2^n$. We will need to prove that $$\\exists c_1, c_2, n_0: \\forall n \\ge n_0, 0 \\le c_1 \\cdot 2^{n / 2} \\le 2^n \\le c_2 \\cdot 2^{n / 2},$$ which is obviously untrue. h. Prove, let $g(n) = o(f(n))$. Then $$\\exists c, n_0: \\forall n \\ge n_0, 0 \\le g(n) < cf(n).$$ We need to prove that $$\\exists c_1, c_2, n_0: \\forall n \\ge n_0, 0 \\le c_1f(n) \\le f(n) + g(n) \\le c_2f(n).$$ Thus, if we pick $c_1 = 1$ and $c_2 = c + 1$, it holds.","title":"3-4 Asymptotic notation properties"},{"location":"Chap03/Problems/3-5/","text":"Some authors define $\\Omega$ in a slightly different way than we do; let's use ${\\Omega}^{\\infty}$ (read \"omega infinity\") for this alternative definition. We say that $f(n) = {\\Omega}^{\\infty}(g(n))$ if there exists a positive constant $c$ such that $f(n) \\ge cg(n) \\ge 0$ for infinitely many integers $n$. a. Show that for any two functions $f(n)$ and $g(n)$ that are asymptotically nonnegative, either $f(n) = O(g(n))$ or $f(n) = {\\Omega}^{\\infty}(g(n))$ or both, whereas this is not true if we use $\\Omega$ in place of ${\\Omega}^{\\infty}$. b. Describe the potential advantages and disadvantages of using ${\\Omega}^{\\infty}$ instead of $\\Omega$ to characterize the running times of programs. Some authors also define $O$ in a slightly different manner; let's use $O'$ for the alternative definition. We say that $f(n) = O'(g(n))$ if and only if $|f(n)| = O(g(n))$. c. What happens to each direction of the \"if and only if\" in Theorem 3.1 if we substitute $O'$ for $O$ but we still use $\\Omega$? Some authors define $\\tilde O$ (read \"soft-oh\") to mean $O$ with logarithmic factors ignored: $$ \\begin{aligned} \\tilde{O}(g(n)) = \\{f(n): & \\text{ there exist positive constants $c$, $k$, and $n_0$ such that } \\\\ & \\text{ $0 \\le f(n) \\le cg(n) \\lg^k(n)$ for all $n \\ge n_0$ }.\\} \\end{aligned} $$ d. Define $\\tilde\\Omega$ and $\\tilde\\Theta$ in a similar manner. Prove the corresponding analog to Theorem 3.1. a. We have $$ f(n) = \\begin{cases} O(g(n)) \\text{ and } {\\Omega}^{\\infty}(g(n)) & \\text{if $f(n) = \\Theta(g(n))$}, \\\\ O(g(n)) & \\text{if $0 \\le f(n) \\le cg(n)$}, \\\\ {\\Omega}^{\\infty}(g(n)) & \\text{if $0 \\le cg(n) \\le f(n)$, for infinitely many integers $n$}. \\end{cases} $$ If there are only finite $n$ such that $f(n) \\ge cg(n) \\ge 0$. When $n \\to \\infty$, $0 \\le f(n) \\le cg(n)$, i.e., $f(n) = O(g(n))$. Obviously, it's not hold when we use $\\Omega$ in place of ${\\Omega}^{\\infty}$. b. Advantages: We can characterize all the relationships between all functions. Disadvantages: We cannot characterize precisely. c. For any two functions $f(n)$ and $g(n)$, we have if $f(n) = \\Theta(g(n))$ then $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$ and $f(n) = \\Omega(g(n))$. But the conversion is not true. d. We have $$ \\begin{aligned} \\tilde\\Omega(g(n)) = \\{f(n): & \\text{ there exist positive constants $c$, $k$, and $n_0$ such that } \\\\ & \\text{ $0 \\le cg(n)\\lg^k(n) \\le f(n)$ for all $n \\ge n_0$}.\\} \\end{aligned} $$ $$ \\begin{aligned} \\tilde{\\Theta}(g(n)) = \\{f(n): & \\text{ there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, and $n_0$ such that } \\\\ & \\text{ $0\\le c_1 g(n) \\lg^{k_1}(n) \\le f(n)\\le c_2g (n) \\lg^{k_2}(n)$ for all $n\\ge n_0$.}\\} \\end{aligned} $$ For any two functions $f(n)$ and $g(n)$, we have $f(n) = \\tilde\\Theta(g(n))$ if and only if $f(n) = \\tilde O(g(n))$ and $f(n) = \\tilde\\Omega(g(n))$.","title":"3-5 Variations on $O$ and $\\Omega$"},{"location":"Chap03/Problems/3-6/","text":"We can apply the iteration operator $^*$ used in the $\\lg^*$ function to any monotonically increasing function $f(n)$ over the reals. For a given constant $c \\in \\mathbb R$, we define the iterated function ${f_c}^*$ by ${f_c}^*(n) = \\min \\{i \\ge 0 : f^{(i)}(n) \\le c \\}$ which need not be well defined in all cases. In other words, the quantity ${f_c}^*(n)$ is the number of iterated applications of the function $f$ required to reduce its argument down to $c$ or less. For each of the following functions $f(n)$ and constants $c$, give as tight a bound as possible on ${f_c}^*(n)$. $$ \\begin{array}{ccl} f(n) & c & {f_c}^* \\\\ \\hline n - 1 & 0 & \\Theta(n) \\\\ \\lg n & 1 & \\Theta(\\lg^*{n}) \\\\ n / 2 & 1 & \\Theta(\\lg n) \\\\ n / 2 & 2 & \\Theta(\\lg n) \\\\ \\sqrt 2 & 2 & \\Theta(\\lg\\lg n) \\\\ \\sqrt 2 & 1 & \\text{does not converge} \\\\ n^{1 / 3} & 2 & \\Theta(\\log_3{\\lg n}) \\\\ n / \\lg n & 2 & \\omega(\\lg\\lg n), o(\\lg n) \\end{array} $$","title":"3-6 Iterated functions"},{"location":"Chap04/4.1/","text":"4.1-1 What does $\\text{FIND-MAXIMUM-SUBARRAY}$ return when all elements of $A$ are negative? It will return the greatest element of $A$. 4.1-2 Write pseudocode for the brute-force method of solving the maximum-subarray problem. Your procedure should run in $\\Theta(n^2)$ time. BRUTE - FORCE - FIND - MAXIMUM - SUBARRAY ( A ) n = A . length max - sum = - \u221e for l = 1 to n sum = 0 for h = l to n sum = sum + A [ h ] if sum > max - sum max - sum = sum low = l high = h return ( low , high , max - sum ) 4.1-3 Implement both the brute-force and recursive algorithms for the maximum-subarray problem on your own computer. What problem size $n_0$ gives the crossover point at which the recursive algorithm beats the brute-force algorithm? Then, change the base case of the recursive algorithm to use the brute-force algorithm whenever the problem size is less than $n_0$. Does that change the crossover point? On my computer, $n_0$ is $37$. If the algorithm is modified to used divide and conquer when $n \\ge 37$ and the brute-force approach when $n$ is less, the performance at the crossover point almost doubles. The performance at $n_0 - 1$ stays the same, though (or even gets worse, because of the added overhead). What I find interesting is that if we set $n_0 = 20$ and used the mixed approach to sort $40$ elements, it is still faster than both. 4.1-4 Suppose we change the definition of the maximum-subarray problem to allow the result to be an empty subarray, where the sum of the values of an empty subarray is $0$. How would you change any of the algorithms that do not allow empty subarrays to permit an empty subarray to be the result? If the original algorithm returns a negative sum, returning an empty subarray instead. 4.1-5 Use the following ideas to develop a nonrecursive, linear-time algorithm for the maximum-subarray problem. Start at the left end of the array, and progress toward the right, keeping track of the maximum subarray seen so far. Knowing a maximum subarray $A[1..j]$, extend the answer to find a maximum subarray ending at index $j + 1$ by using the following observation: a maximum subarray $A[i..j + 1]$, for some $1 \\le i \\le j + 1$. Determine a maximum subarray of the form $A[i..j + 1]$ in constant time based on knowing a maximum subarray ending at index $j$. ITERATIVE - FIND - MAXIMUM - SUBARRAY ( A ) n = A . length max - sum = - \u221e sum = - \u221e for j = 1 to n currentHigh = j if sum > 0 sum = sum + A [ j ] else currentLow = j sum = A [ j ] if sum > max - sum max - sum = sum low = currentLow high = currentHigh return ( low , high , max - sum )","title":"4.1 The maximum-subarray problem"},{"location":"Chap04/4.1/#41-1","text":"What does $\\text{FIND-MAXIMUM-SUBARRAY}$ return when all elements of $A$ are negative? It will return the greatest element of $A$.","title":"4.1-1"},{"location":"Chap04/4.1/#41-2","text":"Write pseudocode for the brute-force method of solving the maximum-subarray problem. Your procedure should run in $\\Theta(n^2)$ time. BRUTE - FORCE - FIND - MAXIMUM - SUBARRAY ( A ) n = A . length max - sum = - \u221e for l = 1 to n sum = 0 for h = l to n sum = sum + A [ h ] if sum > max - sum max - sum = sum low = l high = h return ( low , high , max - sum )","title":"4.1-2"},{"location":"Chap04/4.1/#41-3","text":"Implement both the brute-force and recursive algorithms for the maximum-subarray problem on your own computer. What problem size $n_0$ gives the crossover point at which the recursive algorithm beats the brute-force algorithm? Then, change the base case of the recursive algorithm to use the brute-force algorithm whenever the problem size is less than $n_0$. Does that change the crossover point? On my computer, $n_0$ is $37$. If the algorithm is modified to used divide and conquer when $n \\ge 37$ and the brute-force approach when $n$ is less, the performance at the crossover point almost doubles. The performance at $n_0 - 1$ stays the same, though (or even gets worse, because of the added overhead). What I find interesting is that if we set $n_0 = 20$ and used the mixed approach to sort $40$ elements, it is still faster than both.","title":"4.1-3"},{"location":"Chap04/4.1/#41-4","text":"Suppose we change the definition of the maximum-subarray problem to allow the result to be an empty subarray, where the sum of the values of an empty subarray is $0$. How would you change any of the algorithms that do not allow empty subarrays to permit an empty subarray to be the result? If the original algorithm returns a negative sum, returning an empty subarray instead.","title":"4.1-4"},{"location":"Chap04/4.1/#41-5","text":"Use the following ideas to develop a nonrecursive, linear-time algorithm for the maximum-subarray problem. Start at the left end of the array, and progress toward the right, keeping track of the maximum subarray seen so far. Knowing a maximum subarray $A[1..j]$, extend the answer to find a maximum subarray ending at index $j + 1$ by using the following observation: a maximum subarray $A[i..j + 1]$, for some $1 \\le i \\le j + 1$. Determine a maximum subarray of the form $A[i..j + 1]$ in constant time based on knowing a maximum subarray ending at index $j$. ITERATIVE - FIND - MAXIMUM - SUBARRAY ( A ) n = A . length max - sum = - \u221e sum = - \u221e for j = 1 to n currentHigh = j if sum > 0 sum = sum + A [ j ] else currentLow = j sum = A [ j ] if sum > max - sum max - sum = sum low = currentLow high = currentHigh return ( low , high , max - sum )","title":"4.1-5"},{"location":"Chap04/4.2/","text":"4.2-1 Use Strassen's algorithm to compute the matrix product $$ \\begin{pmatrix} 1 & 3 \\\\ 7 & 5 \\end{pmatrix} \\begin{pmatrix} 6 & 8 \\\\ 4 & 2 \\end{pmatrix} . $$ Show your work. The first matrices are $$ \\begin{array}{ll} S_1 = 6 & S_6 = 8 \\\\ S_2 = 4 & S_7 = -2 \\\\ S_3 = 12 & S_8 = 6 \\\\ S_4 = -2 & S_9 = -6 \\\\ S_5 = 6 & S_{10} = 14. \\end{array} $$ The products are $$ \\begin{aligned} P_1 & = 1 \\cdot 6 = 6 \\\\ P_2 & = 4 \\cdot 2 = 8 \\\\ P_3 & = 6 \\cdot 12 = 72 \\\\ P_4 & = -2 \\cdot 5 = -10 \\\\ P_5 & = 6 \\cdot 8 = 48 \\\\ P_6 & = -2 \\cdot 6 = -12 \\\\ P_7 & = -6 \\cdot 14 = -84. \\end{aligned} $$ The four matrices are $$ \\begin{aligned} C_{11} & = 48 + (-10) - 8 + (-12) = 18 \\\\ C_{12} & = 6 + 8 = 14 \\\\ C_{21} & = 72 + (-10) = 62 \\\\ C_{22} & = 48 + 6 - 72 - (-84) = 66. \\end{aligned} $$ The result is $$ \\begin{pmatrix} 18 & 14 \\\\ 62 & 66 \\end{pmatrix} . $$ 4.2-2 Write pseudocode for Strassen's algorithm. STRASSEN ( A , B ) n = A . rows if n == 1 return a [ 1 , 1 ] * b [ 1 , 1 ] let C be a new n \u00d7 n matrix A [ 1 , 1 ] = A [ 1. . n / 2 ][ 1. . n / 2 ] A [ 1 , 2 ] = A [ 1. . n / 2 ][ n / 2 + 1. . n ] A [ 2 , 1 ] = A [ n / 2 + 1. . n ][ 1. . n / 2 ] A [ 2 , 2 ] = A [ n / 2 + 1. . n ][ n / 2 + 1. . n ] B [ 1 , 1 ] = B [ 1. . n / 2 ][ 1. . n / 2 ] B [ 1 , 2 ] = B [ 1. . n / 2 ][ n / 2 + 1. . n ] B [ 2 , 1 ] = B [ n / 2 + 1. . n ][ 1. . n / 2 ] B [ 2 , 2 ] = B [ n / 2 + 1. . n ][ n / 2 + 1. . n ] S [ 1 ] = B [ 1 , 2 ] - B [ 2 , 2 ] S [ 2 ] = A [ 1 , 1 ] + A [ 1 , 2 ] S [ 3 ] = A [ 2 , 1 ] + A [ 2 , 2 ] S [ 4 ] = B [ 2 , 1 ] - B [ 1 , 1 ] S [ 5 ] = A [ 1 , 1 ] + A [ 2 , 2 ] S [ 6 ] = B [ 1 , 1 ] + B [ 2 , 2 ] S [ 7 ] = A [ 1 , 2 ] - A [ 2 , 2 ] S [ 8 ] = B [ 2 , 1 ] + B [ 2 , 2 ] S [ 9 ] = A [ 1 , 1 ] - A [ 2 , 1 ] S [ 10 ] = B [ 1 , 1 ] + B [ 1 , 2 ] P [ 1 ] = STRASSEN ( A [ 1 , 1 ], S [ 1 ]) P [ 2 ] = STRASSEN ( S [ 2 ], B [ 2 , 2 ]) P [ 3 ] = STRASSEN ( S [ 3 ], B [ 1 , 1 ]) P [ 4 ] = STRASSEN ( A [ 2 , 2 ], S [ 4 ]) P [ 5 ] = STRASSEN ( S [ 5 ], S [ 6 ]) P [ 6 ] = STRASSEN ( S [ 7 ], S [ 8 ]) P [ 7 ] = STRASSEN ( S [ 9 ], S [ 10 ]) C [ 1. . n / 2 ][ 1. . n / 2 ] = P [ 5 ] + P [ 4 ] - P [ 2 ] + P [ 6 ] C [ 1. . n / 2 ][ n / 2 + 1. . n ] = P [ 1 ] + P [ 2 ] C [ n / 2 + 1. . n ][ 1. . n / 2 ] = P [ 3 ] + P [ 4 ] C [ n / 2 + 1. . n ][ n / 2 + 1. . n ] = P [ 5 ] + P [ 1 ] - P [ 3 ] - P [ 7 ] return C 4.2-3 How would you modify Strassen's algorithm to multiply $n \\times n$ matrices in which $n$ is not an exact power of $2$? Show that the resulting algorithm runs in time $\\Theta(n^{\\lg7})$. We can just extend it to an $n \\times n$ matrix and pad it with zeroes. It's obviously $\\Theta(n^{\\lg7})$. 4.2-4 What is the largest $k$ such that if you can multiply $3 \\times 3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n \\times n$ matrices is time $o(n^{\\lg 7})$? What would the running time of this algorithm be? Assume $n = 3^m$ for some $m$. Then, using block matrix multiplication, we obtain the recursive running time $T(n) = kT(n / 3) + O(1)$. By master theorem, we can find the largest $k$ to satisfy $\\log_3 k < \\lg 7$ is $k = 21$. 4.2-5 V. Pan has discovered a way of multiplying $68 \\times 68$ matrices using $132464$ multiplications, a way of multiplying $70 \\times 70$ matrices using $143640$ multiplications, and a way of multiplying $72 \\times 72$ matrices using $155424$ multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare to Strassen's algorithm? Using what we know from the last exercise, we need to pick the smallest of the following $$ \\begin{aligned} \\log_{68} 132464 & \\approx 2.795128 \\\\ \\log_{70} 143640 & \\approx 2.795122 \\\\ \\log_{72} 155424 & \\approx 2.795147. \\end{aligned} $$ The fastest one asymptotically is $70 \\times 70$ using $143640$. 4.2-6 How quickly can you multiply a $kn \\times n$ matrix by an $n \\times kn$ matrix, using Strassen's algorithm as a subroutine? Answer the same question with the order of the input matrices reversed. $(kn \\times n)(n \\times kn)$ produces a $kn \\times kn$ matrix. This produces $k^2$ multiplications of $n \\times n$ matrices. $(n \\times kn)(kn \\times n)$ produces an $n \\times n$ matrix. This produces $k$ multiplications and $k - 1$ additions. 4.2-7 Show how to multiply the complex numbers $a + bi$ and $c + di$ using only three multiplications of real numbers. The algorithm should take $a$, $b$, $c$ and $d$ as input and produce the real component $ac - bd$ and the imaginary component $ad + bc$ separately. The three matrices are $$ \\begin{aligned} A & = (a + b)(c + d) = ac + ad + bc + bd \\\\ B & = ac \\\\ C & = bd. \\end{aligned} $$ The result is $$(B - C) + (A - B - C)i.$$","title":"4.2 Strassen's algorithm for matrix multiplication"},{"location":"Chap04/4.2/#42-1","text":"Use Strassen's algorithm to compute the matrix product $$ \\begin{pmatrix} 1 & 3 \\\\ 7 & 5 \\end{pmatrix} \\begin{pmatrix} 6 & 8 \\\\ 4 & 2 \\end{pmatrix} . $$ Show your work. The first matrices are $$ \\begin{array}{ll} S_1 = 6 & S_6 = 8 \\\\ S_2 = 4 & S_7 = -2 \\\\ S_3 = 12 & S_8 = 6 \\\\ S_4 = -2 & S_9 = -6 \\\\ S_5 = 6 & S_{10} = 14. \\end{array} $$ The products are $$ \\begin{aligned} P_1 & = 1 \\cdot 6 = 6 \\\\ P_2 & = 4 \\cdot 2 = 8 \\\\ P_3 & = 6 \\cdot 12 = 72 \\\\ P_4 & = -2 \\cdot 5 = -10 \\\\ P_5 & = 6 \\cdot 8 = 48 \\\\ P_6 & = -2 \\cdot 6 = -12 \\\\ P_7 & = -6 \\cdot 14 = -84. \\end{aligned} $$ The four matrices are $$ \\begin{aligned} C_{11} & = 48 + (-10) - 8 + (-12) = 18 \\\\ C_{12} & = 6 + 8 = 14 \\\\ C_{21} & = 72 + (-10) = 62 \\\\ C_{22} & = 48 + 6 - 72 - (-84) = 66. \\end{aligned} $$ The result is $$ \\begin{pmatrix} 18 & 14 \\\\ 62 & 66 \\end{pmatrix} . $$","title":"4.2-1"},{"location":"Chap04/4.2/#42-2","text":"Write pseudocode for Strassen's algorithm. STRASSEN ( A , B ) n = A . rows if n == 1 return a [ 1 , 1 ] * b [ 1 , 1 ] let C be a new n \u00d7 n matrix A [ 1 , 1 ] = A [ 1. . n / 2 ][ 1. . n / 2 ] A [ 1 , 2 ] = A [ 1. . n / 2 ][ n / 2 + 1. . n ] A [ 2 , 1 ] = A [ n / 2 + 1. . n ][ 1. . n / 2 ] A [ 2 , 2 ] = A [ n / 2 + 1. . n ][ n / 2 + 1. . n ] B [ 1 , 1 ] = B [ 1. . n / 2 ][ 1. . n / 2 ] B [ 1 , 2 ] = B [ 1. . n / 2 ][ n / 2 + 1. . n ] B [ 2 , 1 ] = B [ n / 2 + 1. . n ][ 1. . n / 2 ] B [ 2 , 2 ] = B [ n / 2 + 1. . n ][ n / 2 + 1. . n ] S [ 1 ] = B [ 1 , 2 ] - B [ 2 , 2 ] S [ 2 ] = A [ 1 , 1 ] + A [ 1 , 2 ] S [ 3 ] = A [ 2 , 1 ] + A [ 2 , 2 ] S [ 4 ] = B [ 2 , 1 ] - B [ 1 , 1 ] S [ 5 ] = A [ 1 , 1 ] + A [ 2 , 2 ] S [ 6 ] = B [ 1 , 1 ] + B [ 2 , 2 ] S [ 7 ] = A [ 1 , 2 ] - A [ 2 , 2 ] S [ 8 ] = B [ 2 , 1 ] + B [ 2 , 2 ] S [ 9 ] = A [ 1 , 1 ] - A [ 2 , 1 ] S [ 10 ] = B [ 1 , 1 ] + B [ 1 , 2 ] P [ 1 ] = STRASSEN ( A [ 1 , 1 ], S [ 1 ]) P [ 2 ] = STRASSEN ( S [ 2 ], B [ 2 , 2 ]) P [ 3 ] = STRASSEN ( S [ 3 ], B [ 1 , 1 ]) P [ 4 ] = STRASSEN ( A [ 2 , 2 ], S [ 4 ]) P [ 5 ] = STRASSEN ( S [ 5 ], S [ 6 ]) P [ 6 ] = STRASSEN ( S [ 7 ], S [ 8 ]) P [ 7 ] = STRASSEN ( S [ 9 ], S [ 10 ]) C [ 1. . n / 2 ][ 1. . n / 2 ] = P [ 5 ] + P [ 4 ] - P [ 2 ] + P [ 6 ] C [ 1. . n / 2 ][ n / 2 + 1. . n ] = P [ 1 ] + P [ 2 ] C [ n / 2 + 1. . n ][ 1. . n / 2 ] = P [ 3 ] + P [ 4 ] C [ n / 2 + 1. . n ][ n / 2 + 1. . n ] = P [ 5 ] + P [ 1 ] - P [ 3 ] - P [ 7 ] return C","title":"4.2-2"},{"location":"Chap04/4.2/#42-3","text":"How would you modify Strassen's algorithm to multiply $n \\times n$ matrices in which $n$ is not an exact power of $2$? Show that the resulting algorithm runs in time $\\Theta(n^{\\lg7})$. We can just extend it to an $n \\times n$ matrix and pad it with zeroes. It's obviously $\\Theta(n^{\\lg7})$.","title":"4.2-3"},{"location":"Chap04/4.2/#42-4","text":"What is the largest $k$ such that if you can multiply $3 \\times 3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n \\times n$ matrices is time $o(n^{\\lg 7})$? What would the running time of this algorithm be? Assume $n = 3^m$ for some $m$. Then, using block matrix multiplication, we obtain the recursive running time $T(n) = kT(n / 3) + O(1)$. By master theorem, we can find the largest $k$ to satisfy $\\log_3 k < \\lg 7$ is $k = 21$.","title":"4.2-4"},{"location":"Chap04/4.2/#42-5","text":"V. Pan has discovered a way of multiplying $68 \\times 68$ matrices using $132464$ multiplications, a way of multiplying $70 \\times 70$ matrices using $143640$ multiplications, and a way of multiplying $72 \\times 72$ matrices using $155424$ multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare to Strassen's algorithm? Using what we know from the last exercise, we need to pick the smallest of the following $$ \\begin{aligned} \\log_{68} 132464 & \\approx 2.795128 \\\\ \\log_{70} 143640 & \\approx 2.795122 \\\\ \\log_{72} 155424 & \\approx 2.795147. \\end{aligned} $$ The fastest one asymptotically is $70 \\times 70$ using $143640$.","title":"4.2-5"},{"location":"Chap04/4.2/#42-6","text":"How quickly can you multiply a $kn \\times n$ matrix by an $n \\times kn$ matrix, using Strassen's algorithm as a subroutine? Answer the same question with the order of the input matrices reversed. $(kn \\times n)(n \\times kn)$ produces a $kn \\times kn$ matrix. This produces $k^2$ multiplications of $n \\times n$ matrices. $(n \\times kn)(kn \\times n)$ produces an $n \\times n$ matrix. This produces $k$ multiplications and $k - 1$ additions.","title":"4.2-6"},{"location":"Chap04/4.2/#42-7","text":"Show how to multiply the complex numbers $a + bi$ and $c + di$ using only three multiplications of real numbers. The algorithm should take $a$, $b$, $c$ and $d$ as input and produce the real component $ac - bd$ and the imaginary component $ad + bc$ separately. The three matrices are $$ \\begin{aligned} A & = (a + b)(c + d) = ac + ad + bc + bd \\\\ B & = ac \\\\ C & = bd. \\end{aligned} $$ The result is $$(B - C) + (A - B - C)i.$$","title":"4.2-7"},{"location":"Chap04/4.3/","text":"4.3-1 Show that the solution of $T(n) = T(n - 1) + n$ is $O(n^2)$. We guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n - 1)^2 + n \\\\ & = cn^2 - 2cn + c + n \\\\ & = cn^2 + n(1 - 2c) + c \\\\ & \\le cn^2, \\end{aligned} $$ where the last step holds for $c > \\frac{1}{2}$. 4.3-2 Show that the solution of $T(n) = T(\\lceil n / 2 \\rceil) + 1$ is $O(\\lg n)$. We guess $T(n) \\le c\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c\\lg(\\lceil n / 2 \\rceil - 2) + 1 \\\\ & \\le c\\lg(n / 2 + 1 - 2) + 1 \\\\ & = c\\lg((n - 2) / 2) + 1 \\\\ & = c\\lg(n - 2) - c\\lg2 + 1 \\\\ & \\le c\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c \\ge 1$. 4.3-3 We saw that the solution of $T(n) = 2T(\\lfloor n / 2 \\rfloor) + n$ is $O(n\\lg n)$. Show that the solution of this recurrence is also $\\Omega(n\\lg n)$. Conclude that the solution is $\\Theta(n\\lg n)$. First, we guess $T(n) \\le cn\\lg n$, $$ \\begin{aligned} T(n) & \\le 2c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + n \\\\ & \\le cn\\lg(n / 2) + n \\\\ & = cn\\lg n - cn\\lg 2 + n \\\\ & = cn\\lg n + (1 - c)n \\\\ & \\le cn\\lg n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. Next, we guess $T(n) \\ge c(n + 2)\\lg(n + 2)$, $$ \\begin{aligned} T(n) & \\ge 2c(\\lfloor n / 2 \\rfloor + 2)(\\lg(\\lfloor n / 2 \\rfloor + 2) + n \\\\ & \\ge 2c(n / 2 - 1 + 2)(\\lg(n / 2 - 1 + 2) + n \\\\ & = 2c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + n \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2)\\lg 2 + n \\\\ & = c(n + 2)\\lg(n + 2) + (1 - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $n \\ge \\frac{2c}{1 - c}$, $0 \\le c < 1$. 4.3-4 Show that by making a different inductive hyptohesis, we can overcome the difficulty with the boundary condition $T(1) = 1$ for recurrence $\\text{(4.19)}$ without adjusting the boundary conditions for the inductive proof. We guess $T(n) \\le n\\lg n + n$, $$ \\begin{aligned} T(n) & \\le 2(c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + \\lfloor n / 2 \\rfloor) + n \\\\ & \\le 2c(n / 2)\\lg(n / 2) + 2(n / 2) + n \\\\ & = cn\\lg(n / 2) + 2n \\\\ & = cn\\lg n - cn\\lg{2} + 2n \\\\ & = cn\\lg n + (2 - c)n \\\\ & \\le cn\\lg n + n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. This time, the boundary condition is $$T(1) = 1 \\le cn\\lg n + n = 0 + 1 = 1.$$ 4.3-5 Show that $\\Theta(n\\lg n)$ is the solution to the \"exact\" recurrence $\\text{(4.3)}$ for merge sort. The recurrence is $$T(n) = T(\\lceil n / 2 \\rceil) + T(\\lfloor n / 2 \\rfloor) + \\Theta(n) \\tag{4.3}$$ To show $\\Theta$ bound, separately show $O$ and $\\Omega$ bounds. For $O(n\\lg n)$, we guess $T(n) \\le c(n - 2)\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c(\\lceil n / 2 \\rceil -2 )\\lg(\\lceil n / 2 \\rceil - 2) + c(\\lfloor n / 2 \\rfloor - 2)\\lg(\\lfloor n / 2 \\rfloor - 2) + dn \\\\ & \\le c(n / 2 + 1 -2 )\\lg(n / 2 + 1 - 2) + c(n / 2 - 2)\\lg(n / 2 - 2) + dn \\\\ & \\le c(n / 2 - 1 )\\lg(n / 2 - 1) + c(n / 2 - 1)\\lg(n / 2 - 1) + dn \\\\ & = c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg(n - 2) - c(n - 2) + dn \\\\ & = c(n - 2)\\lg(n - 2) + (d - c)n + 2c \\\\ & \\le c(n - 2)\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c > d$. For $\\Omega(n\\lg n)$, we guess $T(n) \\ge c(n + 2)\\lg (n + 2)$, $$ \\begin{aligned} T(n) & \\ge c(\\lceil n / 2 \\rceil +2 )\\lg(\\lceil n / 2 \\rceil + 2) + c(\\lfloor n / 2 \\rfloor + 2)\\lg(\\lfloor n / 2 \\rfloor + 2) + dn \\\\ & \\ge c(n / 2 + 2)\\lg(n / 2 + 2) + c(n / 2-1+2)\\lg(n / 2-1+2) + dn \\\\ & \\ge c(n / 2 + 1 )\\lg(n / 2 + 1) + c(n / 2 + 1)\\lg(n / 2 + 1) + dn \\\\ & \\ge c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2) + dn \\\\ & = c(n + 2)\\lg(n + 2) + (d - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $d > c$. 4.3-6 Show that the solution to $T(n) = 2T(\\lfloor n / 2 \\rfloor + 17) + n$ is $O(n\\lg n)$. We guess $T(n) \\le c(n - a)\\lg(n - a)$, $$ \\begin{aligned} T(n) & \\le 2c(\\lfloor n / 2 \\rfloor + 17 - a)\\lg(\\lfloor n / 2 \\rfloor + 17 - a) + n \\\\ & \\le 2c(n / 2 + 1 + 17 - a)\\lg(n / 2 + 1 + 17 - a) + n \\\\ & = c(n + 36 - 2a)\\lg\\frac{n + 36 - 2a}{2} + n \\\\ & = c(n + 36 - 2a)\\lg(n + 36 - 2a) - c(n + 36 - 2a) + n & (c > 1, n > n_0 = f(a))\\\\ & \\le c(n + 36 - 2a)\\lg(n + 36 - 2a) & (a \\ge 36) \\\\ & \\le c(n - a)\\lg(n - a). \\end{aligned} $$ 4.3-7 Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 3) + n$ is $T(n) = \\Theta(n^{\\log_3 4})$. Show that a substitution proof with the assumption $T(n) \\le cn^{\\log_3 4}$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. We guess $T(n) \\le cn^{\\log_3 4}$ first, $$ \\begin{aligned} T(n) & \\le 4c(n / 3)^{\\log_3 4} + n \\\\ & = cn^{\\log_3 4} + n. \\end{aligned} $$ We stuck here. We guess $T(n) \\le cn^{\\log_3 4} - dn$ again, $$ \\begin{aligned} T(n) & \\le 4(c(n / 3)^{\\log_3 4} - dn / 3) + n \\\\ & = 4(cn^{\\log_3 4} / 4 - dn / 3) + n \\\\ & = cn^{\\log_3 4} - \\frac{4}{3}dn + n \\\\ & \\le cn^{\\log_3 4} - dn, \\end{aligned} $$ where the last step holds for $d \\ge 3$. 4.3-8 Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 2) + n$ is $T(n) = \\Theta(n^2)$. Show that a substitution proof with the assumption $T(n) \\le cn^2$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. First, let's try the guess $T(n) \\le cn^2$. Then, we have $$ \\begin{aligned} T(n) &= 4T(n / 2) + n \\\\ &\\le 4c(n / 2)^2 + n \\\\ &= cn^2 + n. \\end{aligned} $$ We can't proceed any further from the inequality above to conclude $T(n) \\le cn^2$. Alternatively, let us try the guess $$T(n) \\le cn^2 - cn,$$ which subtracts off a lower-order term. Now we have $$ \\begin{aligned} T(n) &= 4T(n / 2) + n \\\\ &= 4\\left(c(n / 2)^2 - c(n / 2)\\right) + n \\\\ &= 4c(n / 2)^2 - 4c(n / 2) + n \\\\ &= cn^2 + (1 - 2c)n \\\\ &\\le cn^2, \\end{aligned} $$ where the last step holds for $c \\ge 1 / 2$. 4.3-9 Solve the recurrence $T(n) = 3T(\\sqrt n) + \\log n$ by making a change of variables. Your solution should be asymptotically tight. Do not worry about whether values are integral. First, $$ \\begin{aligned} T(n) & = 3T(\\sqrt n) + \\lg n & \\text{ let } m = \\lg n \\\\ T(2^m) & = 3T(2^{m / 2}) + m \\\\ S(m) & = 3S(m / 2) + m. \\end{aligned} $$ Now we guess $S(m) \\le cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\le 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\le cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\le -2) \\\\ & \\le cm^{\\lg 3} + dm. \\end{aligned} $$ Then we guess $S(m) \\ge cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\ge 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\ge cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\ge -2) \\\\ & \\ge cm^{\\lg 3} + dm. \\end{aligned} $$ Thus, $$ \\begin{aligned} S(m) & = \\Theta(m^{\\lg 3}) \\\\ T(n) & = \\Theta(\\lg^{\\lg 3}{n}). \\end{aligned} $$","title":"4.3 The substitution method for solving recurrences"},{"location":"Chap04/4.3/#43-1","text":"Show that the solution of $T(n) = T(n - 1) + n$ is $O(n^2)$. We guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n - 1)^2 + n \\\\ & = cn^2 - 2cn + c + n \\\\ & = cn^2 + n(1 - 2c) + c \\\\ & \\le cn^2, \\end{aligned} $$ where the last step holds for $c > \\frac{1}{2}$.","title":"4.3-1"},{"location":"Chap04/4.3/#43-2","text":"Show that the solution of $T(n) = T(\\lceil n / 2 \\rceil) + 1$ is $O(\\lg n)$. We guess $T(n) \\le c\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c\\lg(\\lceil n / 2 \\rceil - 2) + 1 \\\\ & \\le c\\lg(n / 2 + 1 - 2) + 1 \\\\ & = c\\lg((n - 2) / 2) + 1 \\\\ & = c\\lg(n - 2) - c\\lg2 + 1 \\\\ & \\le c\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c \\ge 1$.","title":"4.3-2"},{"location":"Chap04/4.3/#43-3","text":"We saw that the solution of $T(n) = 2T(\\lfloor n / 2 \\rfloor) + n$ is $O(n\\lg n)$. Show that the solution of this recurrence is also $\\Omega(n\\lg n)$. Conclude that the solution is $\\Theta(n\\lg n)$. First, we guess $T(n) \\le cn\\lg n$, $$ \\begin{aligned} T(n) & \\le 2c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + n \\\\ & \\le cn\\lg(n / 2) + n \\\\ & = cn\\lg n - cn\\lg 2 + n \\\\ & = cn\\lg n + (1 - c)n \\\\ & \\le cn\\lg n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. Next, we guess $T(n) \\ge c(n + 2)\\lg(n + 2)$, $$ \\begin{aligned} T(n) & \\ge 2c(\\lfloor n / 2 \\rfloor + 2)(\\lg(\\lfloor n / 2 \\rfloor + 2) + n \\\\ & \\ge 2c(n / 2 - 1 + 2)(\\lg(n / 2 - 1 + 2) + n \\\\ & = 2c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + n \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2)\\lg 2 + n \\\\ & = c(n + 2)\\lg(n + 2) + (1 - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $n \\ge \\frac{2c}{1 - c}$, $0 \\le c < 1$.","title":"4.3-3"},{"location":"Chap04/4.3/#43-4","text":"Show that by making a different inductive hyptohesis, we can overcome the difficulty with the boundary condition $T(1) = 1$ for recurrence $\\text{(4.19)}$ without adjusting the boundary conditions for the inductive proof. We guess $T(n) \\le n\\lg n + n$, $$ \\begin{aligned} T(n) & \\le 2(c\\lfloor n / 2 \\rfloor\\lg{\\lfloor n / 2 \\rfloor} + \\lfloor n / 2 \\rfloor) + n \\\\ & \\le 2c(n / 2)\\lg(n / 2) + 2(n / 2) + n \\\\ & = cn\\lg(n / 2) + 2n \\\\ & = cn\\lg n - cn\\lg{2} + 2n \\\\ & = cn\\lg n + (2 - c)n \\\\ & \\le cn\\lg n + n, \\end{aligned} $$ where the last step holds for $c \\ge 1$. This time, the boundary condition is $$T(1) = 1 \\le cn\\lg n + n = 0 + 1 = 1.$$","title":"4.3-4"},{"location":"Chap04/4.3/#43-5","text":"Show that $\\Theta(n\\lg n)$ is the solution to the \"exact\" recurrence $\\text{(4.3)}$ for merge sort. The recurrence is $$T(n) = T(\\lceil n / 2 \\rceil) + T(\\lfloor n / 2 \\rfloor) + \\Theta(n) \\tag{4.3}$$ To show $\\Theta$ bound, separately show $O$ and $\\Omega$ bounds. For $O(n\\lg n)$, we guess $T(n) \\le c(n - 2)\\lg(n - 2)$, $$ \\begin{aligned} T(n) & \\le c(\\lceil n / 2 \\rceil -2 )\\lg(\\lceil n / 2 \\rceil - 2) + c(\\lfloor n / 2 \\rfloor - 2)\\lg(\\lfloor n / 2 \\rfloor - 2) + dn \\\\ & \\le c(n / 2 + 1 -2 )\\lg(n / 2 + 1 - 2) + c(n / 2 - 2)\\lg(n / 2 - 2) + dn \\\\ & \\le c(n / 2 - 1 )\\lg(n / 2 - 1) + c(n / 2 - 1)\\lg(n / 2 - 1) + dn \\\\ & = c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + c\\frac{n - 2}{2}\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg\\frac{n - 2}{2} + dn \\\\ & = c(n - 2)\\lg(n - 2) - c(n - 2) + dn \\\\ & = c(n - 2)\\lg(n - 2) + (d - c)n + 2c \\\\ & \\le c(n - 2)\\lg(n - 2), \\end{aligned} $$ where the last step holds for $c > d$. For $\\Omega(n\\lg n)$, we guess $T(n) \\ge c(n + 2)\\lg (n + 2)$, $$ \\begin{aligned} T(n) & \\ge c(\\lceil n / 2 \\rceil +2 )\\lg(\\lceil n / 2 \\rceil + 2) + c(\\lfloor n / 2 \\rfloor + 2)\\lg(\\lfloor n / 2 \\rfloor + 2) + dn \\\\ & \\ge c(n / 2 + 2)\\lg(n / 2 + 2) + c(n / 2-1+2)\\lg(n / 2-1+2) + dn \\\\ & \\ge c(n / 2 + 1 )\\lg(n / 2 + 1) + c(n / 2 + 1)\\lg(n / 2 + 1) + dn \\\\ & \\ge c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + c\\frac{n + 2}{2}\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg\\frac{n + 2}{2} + dn \\\\ & = c(n + 2)\\lg(n + 2) - c(n + 2) + dn \\\\ & = c(n + 2)\\lg(n + 2) + (d - c)n - 2c \\\\ & \\ge c(n + 2)\\lg(n + 2), \\end{aligned} $$ where the last step holds for $d > c$.","title":"4.3-5"},{"location":"Chap04/4.3/#43-6","text":"Show that the solution to $T(n) = 2T(\\lfloor n / 2 \\rfloor + 17) + n$ is $O(n\\lg n)$. We guess $T(n) \\le c(n - a)\\lg(n - a)$, $$ \\begin{aligned} T(n) & \\le 2c(\\lfloor n / 2 \\rfloor + 17 - a)\\lg(\\lfloor n / 2 \\rfloor + 17 - a) + n \\\\ & \\le 2c(n / 2 + 1 + 17 - a)\\lg(n / 2 + 1 + 17 - a) + n \\\\ & = c(n + 36 - 2a)\\lg\\frac{n + 36 - 2a}{2} + n \\\\ & = c(n + 36 - 2a)\\lg(n + 36 - 2a) - c(n + 36 - 2a) + n & (c > 1, n > n_0 = f(a))\\\\ & \\le c(n + 36 - 2a)\\lg(n + 36 - 2a) & (a \\ge 36) \\\\ & \\le c(n - a)\\lg(n - a). \\end{aligned} $$","title":"4.3-6"},{"location":"Chap04/4.3/#43-7","text":"Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 3) + n$ is $T(n) = \\Theta(n^{\\log_3 4})$. Show that a substitution proof with the assumption $T(n) \\le cn^{\\log_3 4}$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. We guess $T(n) \\le cn^{\\log_3 4}$ first, $$ \\begin{aligned} T(n) & \\le 4c(n / 3)^{\\log_3 4} + n \\\\ & = cn^{\\log_3 4} + n. \\end{aligned} $$ We stuck here. We guess $T(n) \\le cn^{\\log_3 4} - dn$ again, $$ \\begin{aligned} T(n) & \\le 4(c(n / 3)^{\\log_3 4} - dn / 3) + n \\\\ & = 4(cn^{\\log_3 4} / 4 - dn / 3) + n \\\\ & = cn^{\\log_3 4} - \\frac{4}{3}dn + n \\\\ & \\le cn^{\\log_3 4} - dn, \\end{aligned} $$ where the last step holds for $d \\ge 3$.","title":"4.3-7"},{"location":"Chap04/4.3/#43-8","text":"Using the master method in Section 4.5, you can show that the solution to the recurrence $T(n) = 4T(n / 2) + n$ is $T(n) = \\Theta(n^2)$. Show that a substitution proof with the assumption $T(n) \\le cn^2$ fails. Then show how to subtract off a lower-order term to make the substitution proof work. First, let's try the guess $T(n) \\le cn^2$. Then, we have $$ \\begin{aligned} T(n) &= 4T(n / 2) + n \\\\ &\\le 4c(n / 2)^2 + n \\\\ &= cn^2 + n. \\end{aligned} $$ We can't proceed any further from the inequality above to conclude $T(n) \\le cn^2$. Alternatively, let us try the guess $$T(n) \\le cn^2 - cn,$$ which subtracts off a lower-order term. Now we have $$ \\begin{aligned} T(n) &= 4T(n / 2) + n \\\\ &= 4\\left(c(n / 2)^2 - c(n / 2)\\right) + n \\\\ &= 4c(n / 2)^2 - 4c(n / 2) + n \\\\ &= cn^2 + (1 - 2c)n \\\\ &\\le cn^2, \\end{aligned} $$ where the last step holds for $c \\ge 1 / 2$.","title":"4.3-8"},{"location":"Chap04/4.3/#43-9","text":"Solve the recurrence $T(n) = 3T(\\sqrt n) + \\log n$ by making a change of variables. Your solution should be asymptotically tight. Do not worry about whether values are integral. First, $$ \\begin{aligned} T(n) & = 3T(\\sqrt n) + \\lg n & \\text{ let } m = \\lg n \\\\ T(2^m) & = 3T(2^{m / 2}) + m \\\\ S(m) & = 3S(m / 2) + m. \\end{aligned} $$ Now we guess $S(m) \\le cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\le 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\le cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\le -2) \\\\ & \\le cm^{\\lg 3} + dm. \\end{aligned} $$ Then we guess $S(m) \\ge cm^{\\lg 3} + dm$, $$ \\begin{aligned} S(m) & \\ge 3\\Big(c(m / 2)^{\\lg 3} + d(m / 2)\\Big) + m \\\\ & \\ge cm^{\\lg 3} + (\\frac{3}{2}d + 1)m & (d \\ge -2) \\\\ & \\ge cm^{\\lg 3} + dm. \\end{aligned} $$ Thus, $$ \\begin{aligned} S(m) & = \\Theta(m^{\\lg 3}) \\\\ T(n) & = \\Theta(\\lg^{\\lg 3}{n}). \\end{aligned} $$","title":"4.3-9"},{"location":"Chap04/4.4/","text":"4.4-1 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 3T(\\lfloor n / 2 \\rfloor) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $3^{\\lg n} = n^{\\lg 3}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $3^i(n / 2^i) = (3 / 2)^i n$. $$ \\begin{aligned} T(n) & = n + \\frac{3}{2}n + \\Big(\\frac{3}{2}\\Big)^2 n + \\cdots + \\Big(\\frac{3}{2}\\Big)^{\\lg n - 1} n + \\Theta(n^{\\lg 3}) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{3}{2}\\Big)^i n + \\Theta(n^{\\lg 3}) \\\\ & = \\frac{(3 / 2)^{\\lg n} - 1}{(3 / 2) - 1}n + \\Theta(n^{\\lg 3}) \\\\ & = 2[(3 / 2)^{\\lg n} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg(3 / 2)} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - \\lg 2} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - 1 + 1} - n] + \\Theta(n^{\\lg 3}) \\\\ & = O(n^{\\lg 3}). \\end{aligned} $$ We guess $T(n) \\le cn^{\\lg 3} - dn$, $$ \\begin{aligned} T(n) & = 3T(\\lfloor n / 2 \\rfloor) + n \\\\ & \\le 3 \\cdot (c(n / 2)^{\\lg 3} - d(n / 2)) + n \\\\ & = (3 / 2^{\\lg 3})cn^{\\lg 3} - (3d / 2)n + n \\\\ & = cn^{\\lg 3} + (1 - 3d / 2)n, \\end{aligned} $$ where the last step holds for $d \\ge 2$. 4.4-2 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n / 2) + n^2$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $1^{\\lg n} = 1$ leaf. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg{n - 1}$, is $1^i (n / 2^i)^2 = (1 / 4)^i n^2$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & < \\sum_{i = 0}^\\infty \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & = \\frac{1}{1 - (1 / 4)} n^2 + \\Theta(1) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n / 2)^2 + n^2 \\\\ & = cn^2 / 4 + n^2 \\\\ & = (c / 4 + 1)n^2 \\\\ & \\le cn^2, \\end{aligned} $$ where the last step holds for $c \\ge 4 / 3$. 4.4-3 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 4T(n / 2 + 2) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(n / 2^i + 2) = 2^i n + 2 \\cdot 4^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} (2^i n + 2 \\cdot 4^i) + \\Theta(n^2) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} 2^i n + \\sum_{i = 0}^{\\lg n - 1} 2 \\cdot 4^i + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}n + 2 \\cdot \\frac{4^{\\lg n} - 1}{4 - 1} + \\Theta(n^2) \\\\ & = (2^{\\lg n} - 1)n + \\frac{2}{3} (4^{\\lg n} - 1) + \\Theta(n^2) \\\\ & = (n - 1)n + \\frac{2}{3}(n^2 - 1) + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le c(n^2 - dn)$, $$ \\begin{aligned} T(n) & = 4T(n / 2 + 2) + n \\\\ & \\le 4c[(n / 2 + 2)^2 - d(n / 2 + 2)] + n \\\\ & = 4c(n^2 / 4 + 2n + 4 - dn / 2 - 2d) + n \\\\ & = cn^2 + 8cn + 16c - 2cdn - 8cd + n \\\\ & = cn^2 - cdn + 8cn + 16c - cdn - 8cd + n \\\\ & = c(n^2 - dn) - (cd - 8c - 1)n - (d - 2) \\cdot 8c \\\\ & \\le c(n^2 - dn), \\end{aligned} $$ where the last step holds for $cd - 8c - 1 \\ge 0$. 4.4-4 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 2T(n - 1) + 1$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n - i$. Thus, the tree has $n$ levels and $2^{n - 1}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n - 1$, is $2^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n - 1} 2^i \\\\ & = \\frac{2^n - 1}{2 - 1} \\\\ & = 2^n - 1 \\\\ & = \\Theta(2^n). \\end{aligned} $$ We guess $T(n) \\le c2^n + n$, $$ \\begin{aligned} T(n) & \\le 2 \\cdot c2^{n - 1} + (n - 1) + 1 \\\\ & = c2^n + n \\\\ & = O(2^n). \\end{aligned} $$ 4.4-5 Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n - 1) + T(n / 2) + n$. Use the substitution method to verify your answer. This is a curious one. The tree makes it look like it is exponential in the worst case. The tree is not full (not a complete binary tree of height $n$), but it is not polynomial either. It's easy to show $O(2^n)$ and $\\Omega(n^2)$. To justify that this is a pretty tight upper bound, we'll show that we can't have any other choice. If we have that $T(n) \\le cn^k$, when we substitue into the recurrence, the new coefficient for $n^k$ can be as high as $c(1 + \\frac{1}{2^k})$ which is bigger than $c$ regardless of how we choose the value $c$. We guess $T(n) \\le c2^n - 4n$, $$ \\begin{aligned} T(n) & \\le c2^{n - 1} - 4(n - 1) + c2^{n / 2} - 4n / 2 + n \\\\ & = c(2^{n - 1} + 2^{n / 2}) - 5n + 4 & (n \\ge 1 / 4) \\\\ & \\le c(2^{n - 1} + 2^{n / 2}) - 4n & (n \\ge 2)\\\\ & = c(2^{n - 1} + 2^{n - 1}) - 4n \\\\ & \\le c2^n - 4n \\\\ & = O(2^n). \\end{aligned} $$ We guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - 1)^2 + c(n / 2)^2 + n \\\\ & = cn^2 - 2cn + c + cn^2 / 4 + n \\\\ & = (5 / 4)cn^2 + (1 - 2c)n + c \\\\ & \\ge cn^2 + (1 - 2c)n + c & (c \\le 1 / 2) \\\\ & \\ge cn^2 \\\\ & = \\Omega(n^2). \\end{aligned} $$ 4.4-6 Argue that the solution to the recurrence $T(n) = T(n / 3) + T(2n / 3) + cn$, where $c$ is a constant, is $\\Omega(n\\lg n)$ by appealing to the recursion tree. We know that the cost at each level of the tree is $cn$ by examining the tree in figure 4.6. To find a lower bound on the cost of the algorithm, we need a lower bound on the height of the tree. The shortest simple path from root to leaf is found by following the leftest child at each node. Since we divide by $3$ at each step, we see that this path has length $\\log_3 n$. Therefore, the cost of the algorithm is $$cn(\\log_3 n + 1) \\ge cn\\log_3 n = \\frac{c}{\\log_3} n\\log n = \\Omega(n\\log n).$$ 4.4-7 Draw the recursion tree for $T(n) = 4T(\\lfloor n / 2 \\rfloor) + cn$, where $c$ is a constant, and provide a tight asymptotic bound on its solution. Verify your answer with the substitution method. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^{\\lg 4} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(cn / 2^i) = 2^icn$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} 2^icn + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}cn + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le dn^2 - cn$, $$ \\begin{aligned} T(n) & \\le 4d(n / 2)^2 - 4c(n / 2) + cn \\\\ & = dn^2 - cn. \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge dn^2 - cn$, $$ \\begin{aligned} T(n) & \\ge 4d(n / 2)^2 - 4c(n / 2) + cn \\\\ & = dn^2 - cn. \\end{aligned} $$ 4.4-8 Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(n - a) + T(a) + cn$, where $a \\ge 1$ and $c > 0$ are constants. The tree has $n / a + 1$ levels. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n / a - 1$, is $c(n - ia)$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n / a} c(n - ia) + (n / a)ca \\\\ & = \\sum_{i = 0}^{n / a} cn - \\sum_{i = 0}^{n / a} cia + (n / a)ca \\\\ & = cn^2/a - \\Theta(n) + \\Theta(n) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n - a)^2 + ca + cn \\\\ & \\le cn^2 - 2can + ca + cn \\\\ & \\le cn^2 - c(2an - a - n) & (a > 1 / 2, n > 2a) \\\\ & \\le cn^2 - cn \\\\ & \\le cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - a)^2 + ca + cn \\\\ & \\ge cn^2 - 2acn + ca + cn \\\\ & \\ge cn^2 - c(2an - a - n) & (a < 1 / 2, n > 2a) \\\\ & \\ge cn^2 + cn \\\\ & \\ge cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$ 4.4-9 Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(\\alpha n) + T((1 - \\alpha)n) + cn$, where $\\alpha$ is a constant in the range $0 < \\alpha < 1$, and $c > 0$ is also a constant. We can assume that $0 < \\alpha \\le 1 / 2$, since otherwise we can let $\\beta = 1 \u2212 \\alpha$ and solve it for $\\beta$. Thus, the depth of the tree is $\\log_{1 / \\alpha} n$ and each level costs $cn$. And let's guess that the leaves are $\\Theta(n)$, $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\log_{1 / \\alpha} n} cn + \\Theta(n) \\\\ & = cn\\log_{1 / \\alpha} n + \\Theta(n) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ We can also show $T(n) = \\Theta(n\\lg n)$ by substitution. To prove the upper bound, we guess that $T(n) \\le dn\\lg n$ for a constant $d > 0$, $$ \\begin{aligned} T(n) & = T(\\alpha n) + T((1 - \\alpha)n) + cn \\\\ & \\le d\\alpha n\\lg(\\alpha n) + d(1 - \\alpha)n\\lg((1 - \\alpha)n) + cn \\\\ & = d\\alpha n\\lg\\alpha + d\\alpha n\\lg n + d(1 - \\alpha)n\\lg(1 - \\alpha) + d(1 - \\alpha)n\\lg n + cn \\\\ & = dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\\\ & \\le dn\\lg n, \\end{aligned} $$ where the last step holds when $d \\ge \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}$. We can achieve this result by solving the inequality $$ \\begin{aligned} dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\le dn\\lg n \\\\ \\implies dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\le 0 \\\\ \\implies d(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) & \\le -c \\\\ \\implies d & \\ge \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}, \\end{aligned} $$ To prove the lower bound, we guess that $T(n) \\ge dn\\lg n$ for a constant $d > 0$, $$ \\begin{aligned} T(n) & = T(\\alpha n) + T((1 - \\alpha)n) + cn \\\\ & \\ge d\\alpha n\\lg(\\alpha n) + d(1 - \\alpha)n\\lg((1 - \\alpha)n) + cn \\\\ & = d\\alpha n\\lg\\alpha + d\\alpha n\\lg n + d(1 - \\alpha)n\\lg(1 - \\alpha) + d(1 - \\alpha)n\\lg n + cn \\\\ & = dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\\\ & \\ge dn\\lg n, \\end{aligned} $$ where the last step holds when $0 < d \\le \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}$. We can achieve this result by solving the inequality $$ \\begin{aligned} dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\ge dn\\lg n \\\\ \\implies dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\ge 0 \\\\ \\implies d(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) & \\ge -c \\\\ \\implies 0 < d & \\le \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}, \\end{aligned} $$ Therefore, $T(n) = \\Theta(n\\lg n)$.","title":"4.4 The recursion-tree method for solving recurrences"},{"location":"Chap04/4.4/#44-1","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 3T(\\lfloor n / 2 \\rfloor) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $3^{\\lg n} = n^{\\lg 3}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $3^i(n / 2^i) = (3 / 2)^i n$. $$ \\begin{aligned} T(n) & = n + \\frac{3}{2}n + \\Big(\\frac{3}{2}\\Big)^2 n + \\cdots + \\Big(\\frac{3}{2}\\Big)^{\\lg n - 1} n + \\Theta(n^{\\lg 3}) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{3}{2}\\Big)^i n + \\Theta(n^{\\lg 3}) \\\\ & = \\frac{(3 / 2)^{\\lg n} - 1}{(3 / 2) - 1}n + \\Theta(n^{\\lg 3}) \\\\ & = 2[(3 / 2)^{\\lg n} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg(3 / 2)} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - \\lg 2} - 1]n + \\Theta(n^{\\lg 3}) \\\\ & = 2[n^{\\lg 3 - 1 + 1} - n] + \\Theta(n^{\\lg 3}) \\\\ & = O(n^{\\lg 3}). \\end{aligned} $$ We guess $T(n) \\le cn^{\\lg 3} - dn$, $$ \\begin{aligned} T(n) & = 3T(\\lfloor n / 2 \\rfloor) + n \\\\ & \\le 3 \\cdot (c(n / 2)^{\\lg 3} - d(n / 2)) + n \\\\ & = (3 / 2^{\\lg 3})cn^{\\lg 3} - (3d / 2)n + n \\\\ & = cn^{\\lg 3} + (1 - 3d / 2)n, \\end{aligned} $$ where the last step holds for $d \\ge 2$.","title":"4.4-1"},{"location":"Chap04/4.4/#44-2","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n / 2) + n^2$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $1^{\\lg n} = 1$ leaf. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg{n - 1}$, is $1^i (n / 2^i)^2 = (1 / 4)^i n^2$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & < \\sum_{i = 0}^\\infty \\Big(\\frac{1}{4}\\Big)^i n^2 + \\Theta(1) \\\\ & = \\frac{1}{1 - (1 / 4)} n^2 + \\Theta(1) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n / 2)^2 + n^2 \\\\ & = cn^2 / 4 + n^2 \\\\ & = (c / 4 + 1)n^2 \\\\ & \\le cn^2, \\end{aligned} $$ where the last step holds for $c \\ge 4 / 3$.","title":"4.4-2"},{"location":"Chap04/4.4/#44-3","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 4T(n / 2 + 2) + n$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(n / 2^i + 2) = 2^i n + 2 \\cdot 4^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} (2^i n + 2 \\cdot 4^i) + \\Theta(n^2) \\\\ & = \\sum_{i = 0}^{\\lg n - 1} 2^i n + \\sum_{i = 0}^{\\lg n - 1} 2 \\cdot 4^i + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}n + 2 \\cdot \\frac{4^{\\lg n} - 1}{4 - 1} + \\Theta(n^2) \\\\ & = (2^{\\lg n} - 1)n + \\frac{2}{3} (4^{\\lg n} - 1) + \\Theta(n^2) \\\\ & = (n - 1)n + \\frac{2}{3}(n^2 - 1) + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ We guess $T(n) \\le c(n^2 - dn)$, $$ \\begin{aligned} T(n) & = 4T(n / 2 + 2) + n \\\\ & \\le 4c[(n / 2 + 2)^2 - d(n / 2 + 2)] + n \\\\ & = 4c(n^2 / 4 + 2n + 4 - dn / 2 - 2d) + n \\\\ & = cn^2 + 8cn + 16c - 2cdn - 8cd + n \\\\ & = cn^2 - cdn + 8cn + 16c - cdn - 8cd + n \\\\ & = c(n^2 - dn) - (cd - 8c - 1)n - (d - 2) \\cdot 8c \\\\ & \\le c(n^2 - dn), \\end{aligned} $$ where the last step holds for $cd - 8c - 1 \\ge 0$.","title":"4.4-3"},{"location":"Chap04/4.4/#44-4","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = 2T(n - 1) + 1$. Use the substitution method to verify your answer. The subproblem size for a node at depth $i$ is $n - i$. Thus, the tree has $n$ levels and $2^{n - 1}$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n - 1$, is $2^i$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n - 1} 2^i \\\\ & = \\frac{2^n - 1}{2 - 1} \\\\ & = 2^n - 1 \\\\ & = \\Theta(2^n). \\end{aligned} $$ We guess $T(n) \\le c2^n + n$, $$ \\begin{aligned} T(n) & \\le 2 \\cdot c2^{n - 1} + (n - 1) + 1 \\\\ & = c2^n + n \\\\ & = O(2^n). \\end{aligned} $$","title":"4.4-4"},{"location":"Chap04/4.4/#44-5","text":"Use a recursion tree to determine a good asymptotic upper bound on the recurrence $T(n) = T(n - 1) + T(n / 2) + n$. Use the substitution method to verify your answer. This is a curious one. The tree makes it look like it is exponential in the worst case. The tree is not full (not a complete binary tree of height $n$), but it is not polynomial either. It's easy to show $O(2^n)$ and $\\Omega(n^2)$. To justify that this is a pretty tight upper bound, we'll show that we can't have any other choice. If we have that $T(n) \\le cn^k$, when we substitue into the recurrence, the new coefficient for $n^k$ can be as high as $c(1 + \\frac{1}{2^k})$ which is bigger than $c$ regardless of how we choose the value $c$. We guess $T(n) \\le c2^n - 4n$, $$ \\begin{aligned} T(n) & \\le c2^{n - 1} - 4(n - 1) + c2^{n / 2} - 4n / 2 + n \\\\ & = c(2^{n - 1} + 2^{n / 2}) - 5n + 4 & (n \\ge 1 / 4) \\\\ & \\le c(2^{n - 1} + 2^{n / 2}) - 4n & (n \\ge 2)\\\\ & = c(2^{n - 1} + 2^{n - 1}) - 4n \\\\ & \\le c2^n - 4n \\\\ & = O(2^n). \\end{aligned} $$ We guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - 1)^2 + c(n / 2)^2 + n \\\\ & = cn^2 - 2cn + c + cn^2 / 4 + n \\\\ & = (5 / 4)cn^2 + (1 - 2c)n + c \\\\ & \\ge cn^2 + (1 - 2c)n + c & (c \\le 1 / 2) \\\\ & \\ge cn^2 \\\\ & = \\Omega(n^2). \\end{aligned} $$","title":"4.4-5"},{"location":"Chap04/4.4/#44-6","text":"Argue that the solution to the recurrence $T(n) = T(n / 3) + T(2n / 3) + cn$, where $c$ is a constant, is $\\Omega(n\\lg n)$ by appealing to the recursion tree. We know that the cost at each level of the tree is $cn$ by examining the tree in figure 4.6. To find a lower bound on the cost of the algorithm, we need a lower bound on the height of the tree. The shortest simple path from root to leaf is found by following the leftest child at each node. Since we divide by $3$ at each step, we see that this path has length $\\log_3 n$. Therefore, the cost of the algorithm is $$cn(\\log_3 n + 1) \\ge cn\\log_3 n = \\frac{c}{\\log_3} n\\log n = \\Omega(n\\log n).$$","title":"4.4-6"},{"location":"Chap04/4.4/#44-7","text":"Draw the recursion tree for $T(n) = 4T(\\lfloor n / 2 \\rfloor) + cn$, where $c$ is a constant, and provide a tight asymptotic bound on its solution. Verify your answer with the substitution method. The subproblem size for a node at depth $i$ is $n / 2^i$. Thus, the tree has $\\lg n + 1$ levels and $4^{\\lg n} = n^{\\lg 4} = n^2$ leaves. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, \\lg n - 1$, is $4^i(cn / 2^i) = 2^icn$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\lg n - 1} 2^icn + \\Theta(n^2) \\\\ & = \\frac{2^{\\lg n} - 1}{2 - 1}cn + \\Theta(n^2) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le dn^2 - cn$, $$ \\begin{aligned} T(n) & \\le 4d(n / 2)^2 - 4c(n / 2) + cn \\\\ & = dn^2 - cn. \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge dn^2 - cn$, $$ \\begin{aligned} T(n) & \\ge 4d(n / 2)^2 - 4c(n / 2) + cn \\\\ & = dn^2 - cn. \\end{aligned} $$","title":"4.4-7"},{"location":"Chap04/4.4/#44-8","text":"Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(n - a) + T(a) + cn$, where $a \\ge 1$ and $c > 0$ are constants. The tree has $n / a + 1$ levels. The total cost over all nodes at depth $i$, for $i = 0, 1, 2, \\ldots, n / a - 1$, is $c(n - ia)$. $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{n / a} c(n - ia) + (n / a)ca \\\\ & = \\sum_{i = 0}^{n / a} cn - \\sum_{i = 0}^{n / a} cia + (n / a)ca \\\\ & = cn^2/a - \\Theta(n) + \\Theta(n) \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $O(n^2)$, we guess $T(n) \\le cn^2$, $$ \\begin{aligned} T(n) & \\le c(n - a)^2 + ca + cn \\\\ & \\le cn^2 - 2can + ca + cn \\\\ & \\le cn^2 - c(2an - a - n) & (a > 1 / 2, n > 2a) \\\\ & \\le cn^2 - cn \\\\ & \\le cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$ For $\\Omega(n^2)$, we guess $T(n) \\ge cn^2$, $$ \\begin{aligned} T(n) & \\ge c(n - a)^2 + ca + cn \\\\ & \\ge cn^2 - 2acn + ca + cn \\\\ & \\ge cn^2 - c(2an - a - n) & (a < 1 / 2, n > 2a) \\\\ & \\ge cn^2 + cn \\\\ & \\ge cn^2 \\\\ & = \\Theta(n^2). \\end{aligned} $$","title":"4.4-8"},{"location":"Chap04/4.4/#44-9","text":"Use a recursion tree to give an asymptotically tight solution to the recurrence $T(n) = T(\\alpha n) + T((1 - \\alpha)n) + cn$, where $\\alpha$ is a constant in the range $0 < \\alpha < 1$, and $c > 0$ is also a constant. We can assume that $0 < \\alpha \\le 1 / 2$, since otherwise we can let $\\beta = 1 \u2212 \\alpha$ and solve it for $\\beta$. Thus, the depth of the tree is $\\log_{1 / \\alpha} n$ and each level costs $cn$. And let's guess that the leaves are $\\Theta(n)$, $$ \\begin{aligned} T(n) & = \\sum_{i = 0}^{\\log_{1 / \\alpha} n} cn + \\Theta(n) \\\\ & = cn\\log_{1 / \\alpha} n + \\Theta(n) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ We can also show $T(n) = \\Theta(n\\lg n)$ by substitution. To prove the upper bound, we guess that $T(n) \\le dn\\lg n$ for a constant $d > 0$, $$ \\begin{aligned} T(n) & = T(\\alpha n) + T((1 - \\alpha)n) + cn \\\\ & \\le d\\alpha n\\lg(\\alpha n) + d(1 - \\alpha)n\\lg((1 - \\alpha)n) + cn \\\\ & = d\\alpha n\\lg\\alpha + d\\alpha n\\lg n + d(1 - \\alpha)n\\lg(1 - \\alpha) + d(1 - \\alpha)n\\lg n + cn \\\\ & = dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\\\ & \\le dn\\lg n, \\end{aligned} $$ where the last step holds when $d \\ge \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}$. We can achieve this result by solving the inequality $$ \\begin{aligned} dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\le dn\\lg n \\\\ \\implies dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\le 0 \\\\ \\implies d(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) & \\le -c \\\\ \\implies d & \\ge \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}, \\end{aligned} $$ To prove the lower bound, we guess that $T(n) \\ge dn\\lg n$ for a constant $d > 0$, $$ \\begin{aligned} T(n) & = T(\\alpha n) + T((1 - \\alpha)n) + cn \\\\ & \\ge d\\alpha n\\lg(\\alpha n) + d(1 - \\alpha)n\\lg((1 - \\alpha)n) + cn \\\\ & = d\\alpha n\\lg\\alpha + d\\alpha n\\lg n + d(1 - \\alpha)n\\lg(1 - \\alpha) + d(1 - \\alpha)n\\lg n + cn \\\\ & = dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn \\\\ & \\ge dn\\lg n, \\end{aligned} $$ where the last step holds when $0 < d \\le \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}$. We can achieve this result by solving the inequality $$ \\begin{aligned} dn\\lg n + dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\ge dn\\lg n \\\\ \\implies dn(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) + cn & \\ge 0 \\\\ \\implies d(\\alpha \\lg\\alpha + (1 - \\alpha) \\lg(1 - \\alpha)) & \\ge -c \\\\ \\implies 0 < d & \\le \\frac{-c}{\\alpha\\lg\\alpha + (1 - \\alpha)\\lg(1 - \\alpha)}, \\end{aligned} $$ Therefore, $T(n) = \\Theta(n\\lg n)$.","title":"4.4-9"},{"location":"Chap04/4.5/","text":"4.5-1 Use the master method to give tight asymptotic bounds for the following recurrences: a. $T(n) = 2T(n / 4) + 1$. b. $T(n) = 2T(n / 4) + \\sqrt n$. c. $T(n) = 2T(n / 4) + n$. d. $T(n) = 2T(n / 4) + n^2$. a. $\\Theta(n^{\\log_4 2}) = \\Theta(\\sqrt n)$. b. $\\Theta(n^{\\log_4 2}\\lg n) = \\Theta(\\sqrt n\\lg n)$. c. $\\Theta(n)$. d. $\\Theta(n^2)$. 4.5-2 Professor Caesar wishes to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen's algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into pieces of size $n / 4 \\times n / 4$, and the divide and combine steps together will take $\\Theta(n^2)$ time. He needs to determine how many subproblems his algorithm has to create in order to beat Strassen's algorithm. If his algorithm creates $a$ subproblems, then the recurrence for the running time $T(n)$ becomes $T(n) = aT(n / 4) + \\Theta(n^2)$. What is the largest integer value of $a$ for which Professor Caesar's algorithm would be asymptotically faster than Strassen's algorithm? Strassen's algorithm has running time of $\\Theta(n^{\\lg 7})$. The largest integer $a$ such that $\\log_4 a < \\lg 7$ is $a = 48$. 4.5-3 Use the master method to show that the solution to the binary-search recurrence $T(n) = T(n / 2) + \\Theta(1)$ is $T(n) = \\Theta(\\lg n)$. (See exercise 2.3-5 for a description of binary search.) $$ \\begin{aligned} a & = 1, b = 2, \\\\ f(n) & = \\Theta(n^{\\lg 1}) = \\Theta(1), \\\\ T(n) & = \\Theta(\\lg n). \\end{aligned} $$ 4.5-4 Can the master method be applied to the recurrence $T(n) = 4T(n / 2) + n^2\\lg n$? Why or why not? Give an asymptotic upper bound for this recurrence. With $a = 4$, $b = 2$, we have $f(n) = n^2\\lg n \\ne O(n^{2 - \\epsilon}) \\ne \\Omega(n^{2 + \\epsilon})$, so we cannot apply the master method. We guess $T(n) \\le cn^2\\lg^2 n$, subsituting $T(n/2) \\le c(n/2)^2\\lg^2 (n/2)$ into the recurrence yields $$ \\begin{aligned} T(n) & = 4T(n / 2) + n^2\\lg n \\\\ & \\le 4c(n / 2)^2\\lg^2(n / 2) + n^2\\lg n \\\\ & = cn^2\\lg(n / 2)\\lg n - cn^2\\lg(n / 2)\\lg 2 + n^2\\lg n \\\\ & = cn^2\\lg^2 n - cn^2\\lg n\\lg 2 - cn^2\\lg(n / 2)\\lg 2 + n^2\\lg n \\\\ & = cn^2\\lg^2 n + (1 - c\\lg 2)n^2\\lg n - cn^2\\lg(n / 2)\\lg 2 & (c \\ge 1/\\lg 2) \\\\ & \\le cn^2\\lg^2 n - cn^2\\lg(n / 2)\\lg 2 \\\\ & \\le cn^2\\lg^2 n. \\end{aligned} $$ Exercise 4.6-2 is the general case for this. 4.5-5 $\\star$ Consider the regularity condition $af(n / b) \\le cf(n)$ for some constant $c < 1$, which is part of case 3 of the master theorem. Give an example of constants $a \\ge 1$ and $b > 1$ and a function $f(n)$ that satisfies all the conditions in case 3 of the master theorem, except the regularity condition. $a = 1$, $b = 2$ and $f(n) = n(2 - \\cos n)$. If we try to prove it, $$ \\begin{aligned} \\frac{n}{2}(2 - \\cos\\frac{n}{2}) & < cn \\\\ \\frac{1 - cos(n / 2)}{2} & < c \\\\ 1 - \\frac{cos(n / 2)}{2} & \\le c. \\end{aligned} $$ Since $\\min\\cos(n / 2) = -1$, this implies that $c \\ge 3 / 2$. But $c < 1$.","title":"4.5 The master method for solving recurrences"},{"location":"Chap04/4.5/#45-1","text":"Use the master method to give tight asymptotic bounds for the following recurrences: a. $T(n) = 2T(n / 4) + 1$. b. $T(n) = 2T(n / 4) + \\sqrt n$. c. $T(n) = 2T(n / 4) + n$. d. $T(n) = 2T(n / 4) + n^2$. a. $\\Theta(n^{\\log_4 2}) = \\Theta(\\sqrt n)$. b. $\\Theta(n^{\\log_4 2}\\lg n) = \\Theta(\\sqrt n\\lg n)$. c. $\\Theta(n)$. d. $\\Theta(n^2)$.","title":"4.5-1"},{"location":"Chap04/4.5/#45-2","text":"Professor Caesar wishes to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen's algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into pieces of size $n / 4 \\times n / 4$, and the divide and combine steps together will take $\\Theta(n^2)$ time. He needs to determine how many subproblems his algorithm has to create in order to beat Strassen's algorithm. If his algorithm creates $a$ subproblems, then the recurrence for the running time $T(n)$ becomes $T(n) = aT(n / 4) + \\Theta(n^2)$. What is the largest integer value of $a$ for which Professor Caesar's algorithm would be asymptotically faster than Strassen's algorithm? Strassen's algorithm has running time of $\\Theta(n^{\\lg 7})$. The largest integer $a$ such that $\\log_4 a < \\lg 7$ is $a = 48$.","title":"4.5-2"},{"location":"Chap04/4.5/#45-3","text":"Use the master method to show that the solution to the binary-search recurrence $T(n) = T(n / 2) + \\Theta(1)$ is $T(n) = \\Theta(\\lg n)$. (See exercise 2.3-5 for a description of binary search.) $$ \\begin{aligned} a & = 1, b = 2, \\\\ f(n) & = \\Theta(n^{\\lg 1}) = \\Theta(1), \\\\ T(n) & = \\Theta(\\lg n). \\end{aligned} $$","title":"4.5-3"},{"location":"Chap04/4.5/#45-4","text":"Can the master method be applied to the recurrence $T(n) = 4T(n / 2) + n^2\\lg n$? Why or why not? Give an asymptotic upper bound for this recurrence. With $a = 4$, $b = 2$, we have $f(n) = n^2\\lg n \\ne O(n^{2 - \\epsilon}) \\ne \\Omega(n^{2 + \\epsilon})$, so we cannot apply the master method. We guess $T(n) \\le cn^2\\lg^2 n$, subsituting $T(n/2) \\le c(n/2)^2\\lg^2 (n/2)$ into the recurrence yields $$ \\begin{aligned} T(n) & = 4T(n / 2) + n^2\\lg n \\\\ & \\le 4c(n / 2)^2\\lg^2(n / 2) + n^2\\lg n \\\\ & = cn^2\\lg(n / 2)\\lg n - cn^2\\lg(n / 2)\\lg 2 + n^2\\lg n \\\\ & = cn^2\\lg^2 n - cn^2\\lg n\\lg 2 - cn^2\\lg(n / 2)\\lg 2 + n^2\\lg n \\\\ & = cn^2\\lg^2 n + (1 - c\\lg 2)n^2\\lg n - cn^2\\lg(n / 2)\\lg 2 & (c \\ge 1/\\lg 2) \\\\ & \\le cn^2\\lg^2 n - cn^2\\lg(n / 2)\\lg 2 \\\\ & \\le cn^2\\lg^2 n. \\end{aligned} $$ Exercise 4.6-2 is the general case for this.","title":"4.5-4"},{"location":"Chap04/4.5/#45-5-star","text":"Consider the regularity condition $af(n / b) \\le cf(n)$ for some constant $c < 1$, which is part of case 3 of the master theorem. Give an example of constants $a \\ge 1$ and $b > 1$ and a function $f(n)$ that satisfies all the conditions in case 3 of the master theorem, except the regularity condition. $a = 1$, $b = 2$ and $f(n) = n(2 - \\cos n)$. If we try to prove it, $$ \\begin{aligned} \\frac{n}{2}(2 - \\cos\\frac{n}{2}) & < cn \\\\ \\frac{1 - cos(n / 2)}{2} & < c \\\\ 1 - \\frac{cos(n / 2)}{2} & \\le c. \\end{aligned} $$ Since $\\min\\cos(n / 2) = -1$, this implies that $c \\ge 3 / 2$. But $c < 1$.","title":"4.5-5 $\\star$"},{"location":"Chap04/4.6/","text":"4.6-1 $\\star$ Give a simple and exact expression for $n_j$ in equation $\\text{(4.27)}$ for the case in which $b$ is a positive integer instead of an arbitrary real number. We state that $\\forall{j \\ge 0}, n_j = \\left \\lceil \\frac{n}{b^j} \\right \\rceil$. Indeed, for $j = 0$ we have from the recurrence's base case that $n_0 = n = \\left \\lceil \\frac{n}{b^0} \\right \\rceil$. Now, suppose $n_{j - 1} = \\left \\lceil \\frac{n}{b^{j - 1}} \\right \\rceil$ for some $j > 0$. By definition, $n_j = \\left \\lceil \\frac{n_{j - 1}}{b} \\right \\rceil$. It follows from the induction hypothesis that $n_j = \\left \\lceil \\frac{\\left \\lceil \\frac{n}{b^{j - 1}} \\right \\rceil}{b} \\right \\rceil$. Since $b$ is a positive integer, equation $\\text{(3.4)}$ implies that $\\left \\lceil \\frac{\\left \\lceil \\frac{n}{b^{j - 1}} \\right \\rceil}{b} \\right \\rceil = \\left \\lceil \\frac{n}{b^j} \\right \\rceil$. Therefore, $n_j = \\left \\lceil \\frac{n}{b^j} \\right \\rceil$. P.S. $n_j$ is obtained by shifting the base $b$ representation $j$ positions to the right, and adding $1$ if any of the $j$ least significant positions are non-zero. 4.6-2 $\\star$ Show that if $f(n) = \\Theta(n^{\\log_b a}\\lg^k{n})$, where $k \\ge 0$, then the master recurrence has solution $T(n) = \\Theta(n^{\\log_b a}\\lg^{k + 1}n)$. For simplicity, confine your analysis to exact powers of $b$. $$ \\begin{aligned} g(n) & = \\sum_{j = 0}^{\\log_b n - 1} a^j f(n / b^j) \\\\ f(n / b^j) & = \\Theta\\Big((n / b^j)^{\\log_b a} \\lg^k(n / b^j) \\Big) \\\\ g(n) & = \\Theta\\Big(\\sum_{j = 0}^{\\log_b n - 1}a^j\\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\big(\\frac{n}{b^j}\\big)\\Big) \\\\ & = \\Theta(A) \\\\ A & = \\sum_{j = 0}^{\\log_b n - 1} a^j \\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\frac{a}{b^{\\log_b a}}\\Big)^j\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a}\\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} B \\\\ \\lg^k\\frac{n}{d} & = (\\lg n - \\lg d)^k = \\lg^k{n} + o(\\lg^k{n}) \\\\ B & = \\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\lg^k{n} - o(\\lg^k{n})\\Big) \\\\ & = \\log_b n\\lg^k{n} + \\log_b n \\cdot o(\\lg^k{n}) \\\\ & = \\Theta(\\log_b n\\lg^k{n}) \\\\ & = \\Theta(\\lg^{k + 1}{n}) \\\\ g(n) & = \\Theta(A) \\\\ & = \\Theta(n^{\\log_b a}B) \\\\ & = \\Theta(n^{\\log_b a}\\lg^{k + 1}{n}). \\end{aligned} $$ 4.6-3 $\\star$ Show that case 3 of the master method is overstated, in the sense that the regularity condition $af(n / b) \\le cf(n)$ for some constant $c < 1$ implies that there exists a constant $\\epsilon > 0$ such that $f(n) = \\Omega(n^{\\log_b a + \\epsilon})$. $$ \\begin{aligned} af(n / b) & \\le cf(n) \\\\ \\Rightarrow f(n / b) & \\le \\frac{c}{a} f(n) \\\\ \\Rightarrow f(n) & \\le \\frac{c}{a} f(bn) \\\\ & = \\frac{c}{a} \\left(\\frac{c}{a} f(b^2n)\\right) \\\\ & = \\frac{c}{a} \\left(\\frac{c}{a}\\left(\\frac{c}{a} f(b^3n)\\right)\\right) \\\\ & = \\left(\\frac{c}{a}\\right)^i f(b^i n) \\\\ \\Rightarrow f(b^i n) & \\ge \\left(\\frac{a}{c}\\right)^i f(n). \\end{aligned} $$ Let $n = 1$, then we have $$f(b^i) \\ge \\left(\\frac{a}{c}\\right)^i f(1) \\quad (*).$$ Let $b^i = n \\Rightarrow i = \\log_b n$, then substitue back to equation $(*)$, $$ \\begin{aligned} f(n) & \\ge \\left(\\frac{a}{c}\\right)^{\\log_b n} f(1) \\\\ & \\ge n^{\\log_b \\frac{a}{c}} f(1) \\\\ & \\ge n^{\\log_b a + \\epsilon} & \\text{ where $\\epsilon > 0$ because $\\frac{a}{c} > a$ (recall that $c < 1$)} \\\\ & = \\Omega(n^{\\log_b a + \\epsilon}). \\end{aligned} $$","title":"4.6 Proof of the master theorem"},{"location":"Chap04/4.6/#46-1-star","text":"Give a simple and exact expression for $n_j$ in equation $\\text{(4.27)}$ for the case in which $b$ is a positive integer instead of an arbitrary real number. We state that $\\forall{j \\ge 0}, n_j = \\left \\lceil \\frac{n}{b^j} \\right \\rceil$. Indeed, for $j = 0$ we have from the recurrence's base case that $n_0 = n = \\left \\lceil \\frac{n}{b^0} \\right \\rceil$. Now, suppose $n_{j - 1} = \\left \\lceil \\frac{n}{b^{j - 1}} \\right \\rceil$ for some $j > 0$. By definition, $n_j = \\left \\lceil \\frac{n_{j - 1}}{b} \\right \\rceil$. It follows from the induction hypothesis that $n_j = \\left \\lceil \\frac{\\left \\lceil \\frac{n}{b^{j - 1}} \\right \\rceil}{b} \\right \\rceil$. Since $b$ is a positive integer, equation $\\text{(3.4)}$ implies that $\\left \\lceil \\frac{\\left \\lceil \\frac{n}{b^{j - 1}} \\right \\rceil}{b} \\right \\rceil = \\left \\lceil \\frac{n}{b^j} \\right \\rceil$. Therefore, $n_j = \\left \\lceil \\frac{n}{b^j} \\right \\rceil$. P.S. $n_j$ is obtained by shifting the base $b$ representation $j$ positions to the right, and adding $1$ if any of the $j$ least significant positions are non-zero.","title":"4.6-1 $\\star$"},{"location":"Chap04/4.6/#46-2-star","text":"Show that if $f(n) = \\Theta(n^{\\log_b a}\\lg^k{n})$, where $k \\ge 0$, then the master recurrence has solution $T(n) = \\Theta(n^{\\log_b a}\\lg^{k + 1}n)$. For simplicity, confine your analysis to exact powers of $b$. $$ \\begin{aligned} g(n) & = \\sum_{j = 0}^{\\log_b n - 1} a^j f(n / b^j) \\\\ f(n / b^j) & = \\Theta\\Big((n / b^j)^{\\log_b a} \\lg^k(n / b^j) \\Big) \\\\ g(n) & = \\Theta\\Big(\\sum_{j = 0}^{\\log_b n - 1}a^j\\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\big(\\frac{n}{b^j}\\big)\\Big) \\\\ & = \\Theta(A) \\\\ A & = \\sum_{j = 0}^{\\log_b n - 1} a^j \\big(\\frac{n}{b^j}\\big)^{\\log_b a}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\frac{a}{b^{\\log_b a}}\\Big)^j\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a}\\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = n^{\\log_b a} B \\\\ \\lg^k\\frac{n}{d} & = (\\lg n - \\lg d)^k = \\lg^k{n} + o(\\lg^k{n}) \\\\ B & = \\sum_{j = 0}^{\\log_b n - 1}\\lg^k\\frac{n}{b^j} \\\\ & = \\sum_{j = 0}^{\\log_b n - 1}\\Big(\\lg^k{n} - o(\\lg^k{n})\\Big) \\\\ & = \\log_b n\\lg^k{n} + \\log_b n \\cdot o(\\lg^k{n}) \\\\ & = \\Theta(\\log_b n\\lg^k{n}) \\\\ & = \\Theta(\\lg^{k + 1}{n}) \\\\ g(n) & = \\Theta(A) \\\\ & = \\Theta(n^{\\log_b a}B) \\\\ & = \\Theta(n^{\\log_b a}\\lg^{k + 1}{n}). \\end{aligned} $$","title":"4.6-2 $\\star$"},{"location":"Chap04/4.6/#46-3-star","text":"Show that case 3 of the master method is overstated, in the sense that the regularity condition $af(n / b) \\le cf(n)$ for some constant $c < 1$ implies that there exists a constant $\\epsilon > 0$ such that $f(n) = \\Omega(n^{\\log_b a + \\epsilon})$. $$ \\begin{aligned} af(n / b) & \\le cf(n) \\\\ \\Rightarrow f(n / b) & \\le \\frac{c}{a} f(n) \\\\ \\Rightarrow f(n) & \\le \\frac{c}{a} f(bn) \\\\ & = \\frac{c}{a} \\left(\\frac{c}{a} f(b^2n)\\right) \\\\ & = \\frac{c}{a} \\left(\\frac{c}{a}\\left(\\frac{c}{a} f(b^3n)\\right)\\right) \\\\ & = \\left(\\frac{c}{a}\\right)^i f(b^i n) \\\\ \\Rightarrow f(b^i n) & \\ge \\left(\\frac{a}{c}\\right)^i f(n). \\end{aligned} $$ Let $n = 1$, then we have $$f(b^i) \\ge \\left(\\frac{a}{c}\\right)^i f(1) \\quad (*).$$ Let $b^i = n \\Rightarrow i = \\log_b n$, then substitue back to equation $(*)$, $$ \\begin{aligned} f(n) & \\ge \\left(\\frac{a}{c}\\right)^{\\log_b n} f(1) \\\\ & \\ge n^{\\log_b \\frac{a}{c}} f(1) \\\\ & \\ge n^{\\log_b a + \\epsilon} & \\text{ where $\\epsilon > 0$ because $\\frac{a}{c} > a$ (recall that $c < 1$)} \\\\ & = \\Omega(n^{\\log_b a + \\epsilon}). \\end{aligned} $$","title":"4.6-3 $\\star$"},{"location":"Chap04/Problems/4-1/","text":"Give asymptotic upper and lower bound for $T(n)$ in each of the following recurrences. Assume that $T(n)$ is constant for $n \\le 2$. Make your bounds as tight as possible, and justify your answers. a. $T(n) = 2T(n / 2) + n^4$. b. $T(n) = T(7n / 10) + n$. c. $T(n) = 16T(n / 4) + n^2$. d. $T(n) = 7T(n / 3) + n^2$. e. $T(n) = 7T(n / 2) + n^2$. f. $T(n) = 2T(n / 4) + \\sqrt n$. g. $T(n) = T(n - 2) + n^2$. a. By master theorem, $T(n) = \\Theta(n^4)$. b. By master theorem, $T(n) = \\Theta(n)$. c. By master theorem, $T(n) = \\Theta(n^2\\lg n)$. d. By master theorem, $T(n) = \\Theta(n^2)$. e. By master theorem, $T(n) = \\Theta(n^{\\lg 7})$. f. By master theorem, $T(n) = \\Theta(\\sqrt n \\lg n)$. g. Let $d = m \\mod 2$, $$ \\begin{aligned} T(n) & = \\sum_{j = 1}^{j = n / 2} (2j + d)^2 \\\\ & = \\sum_{j = 1}^{n / 2} 4j^2 + 4jd + d^2 \\\\ & = \\frac{n(n + 2)(n + 1)}{6} + \\frac{n(n + 2)d}{2} + \\frac{d^2n}{2} \\\\ & = \\Theta(n^3). \\end{aligned} $$","title":"4-1 Recurrence examples"},{"location":"Chap04/Problems/4-2/","text":"Throughout this book, we assume that parameter passing during procedure calls takes constant time, even if an $N$-element array is being passed. This assumption is valid in most systems because a pointer to the array is passed, not the array itself. This problem examines the implications of three parameter-passing strategies: An array is passed by pointer. Time $= \\Theta(1)$. An array is passed by copying. Time $= \\Theta(N)$, where $N$ is the size of the array. An array is passed by copying only the subrage that might be accessed by the called procedure. Time $= \\Theta(q - p + 1)$ if the subarray $A[p..q]$ is passed. a. Consider the recursive binary search algorithm for finding a number in a sorted array (see Exercise 2.3-5). Give recurrences for the worst-case running times of binary search when arrays are passed using each of the three methods above, and give good upper bounds on the solutions of the recurrences. Let $N$ be the size of the original problems and $n$ be the size of a subproblem. b. Redo part (a) for the $\\text{MERGE-SORT}$ algorithm from Section 2.3.1. a. $T(n) = T(n / 2) + c = \\Theta(\\lg n)$. (master method) $\\Theta(n\\lg n)$. $$ \\begin{aligned} T(n) & = T(n / 2) + cN \\\\ & = 2cN + T(n / 4) \\\\ & = 3cN + T(n / 8) \\\\ & = \\sum_{i = 0}^{\\lg n - 1}(2^icN / 2^i) \\\\ & = cN\\lg n \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ $T(n) = T(n / 2) + cn = \\Theta(n)$. (master method) b. $T(n) = 2T(n / 2) + cn = \\Theta(n\\lg n)$. (master method) $\\Theta(n^2)$. $$ \\begin{aligned} T(n) & = 2T(n / 2) + cn + 2N = 4N + cn + 2c(n / 2) + 4T(n / 4) \\\\ & = 8N + 2cn + 4c(n / 4) + 8T(n / 8) \\\\ & = \\sum_{i = 0}^{\\lg n - 1}(cn + 2^iN) \\\\ & = \\sum_{i = 0}^{\\lg n - 1}cn + N\\sum_{i = 0}^{\\lg n - 1}2^i \\\\ & = cn\\lg n + N\\frac{2^{\\lg n} - 1}{2 - 1} \\\\ & = cn\\lg n + nN - N = \\Theta(nN) \\\\ & = \\Theta(n^2). \\end{aligned} $$ $\\Theta(n\\lg n)$. $$ \\begin{aligned} T(n) & = 2T(n / 2) + cn + 2n / 2 \\\\ & = 2T(n / 2) + (c + 1)n \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$","title":"4-2 Parameter-passing costs"},{"location":"Chap04/Problems/4-3/","text":"Give asymptotic upper and lower bounds for $T(n)$ in each of the following recurrences. Assume that $T(n)$ is constant for sufficiently small $n$. Make your bounds as tight as possible, and justify your answers. a. $T(n) = 4T(n / 3) + n\\lg n$. b. $T(n) = 3T(n / 3) + n / \\lg n$. c. $T(n) = 4T(n / 2) + n^2\\sqrt n$. d. $T(n) = 3T(n / 3 - 2) + n / 2$. e. $T(n) = 2T(n / 2) + n / \\lg n$. f. $T(n) = T(n / 2) + T(n / 4) + T(n / 8) + n$. g. $T(n) = T(n - 1) + 1 / n$. h. $T(n) = T(n - 1) + \\lg n$. i. $T(n) = T(n - 2) + 1 / \\lg n$. j. $T(n) = \\sqrt nT(\\sqrt n) + n$ a. By master theorem, $T(n) = \\Theta(n^{\\log_3 4})$. b. By the recursion-tree method, we can guess that $T(n) = \\Theta(n\\log_3\\log_3 n)$. We start by proving the upper bound. Suppose $k < n \\implies T(k) \\le ck \\log_3\\log_3 k - k$, where we subtract a lower order term to strengthen our induction hypothesis. It follows that $$ \\begin{aligned} T(n) & \\le 3 (c \\frac{n}{3} \\log_3\\log_3 \\frac{n}{3} - \\frac{n}{3}) + \\frac{n}{\\lg n} \\\\ & \\le c n \\log_3\\log_3 n - n + \\frac{n}{\\lg n} \\\\ & \\le c n \\log_3\\log_3 n, \\end{aligned} $$ if $n$ is sufficiently large. The lower bound can proved analogously. c. By master theorem, $T(n) = \\Theta(n^{2.5})$. d. It is $\\Theta(n\\lg n)$. The subtraction occurring inside the argument to $T$ won't change the asymptotics of the solution, that is, for large $n$ the division is so much more of a change than the subtraction that it is the only part that matters. once we drop that subtraction, the solution comes by the master theorem. e. By the same reasoning as part (b), the function is $O(n\\lg n)$ and $\\Omega(n^{1 - \\epsilon})$ for every \u000f$\\epsilon$ and so is $\\tilde O(n)$, see Problem 3-5 . f. We guess $T(n) \\le cn$, $$ \\begin{aligned} T(n) & = T(n / 2) + T(n / 4) + T(n / 8) + n \\\\ & \\le \\frac{7}{8}cn + n \\le cn. \\\\ \\end{aligned} $$ where the last step holds for $c \\ge 8$. g. Recall that $\\chi_A$ denotes the indicator function of $A$. We see that the sum is $$T(0) + \\sum_{j = 1}^n \\frac{1}{j} = T(0) + \\int_1^{n + 1}\\sum_{j = 1}^{n + 1} \\frac{\\chi_{j, j + 1}(x)}{j}dx.$$ Since $\\frac{1}{x}$ is monatonically decreasing, we have that for every $i \\in \\mathbb Z^+$, $$\\text{sup}_{x \\in (i, i + 1)} \\sum_{j = 1}^{n + 1} \\frac{\\chi_{j, j + 1}(x)}{j} - \\frac{1}{x} = \\frac{1}{i} - \\frac{1}{i + 1} = \\frac{1}{i(i + 1)}.$$ Our expression for $T(n)$ becomes $$T(N) = T(0) + \\int_1^{n + 1} \\Big(\\frac{1}{x} + O(\\frac{1}{\\lfloor x \\rfloor(\\lfloor x \\rfloor + 1)})\\Big)dx.$$ We deal with the error term by first chopping out the constant amount between 1 and 2 and then bound the error term by $O(\\frac{1}{x(x - 1)})$ which has an anti-derivative (by method of partial fractions) that is $O(\\frac{1}{n})$, $$ \\begin{aligned} T(N) & = \\int_1^{n + 1} \\frac{dx}{x} + O(\\frac{1}{n}) \\\\ & = \\lg n + T(0) + \\frac{1}{2} + O(\\frac{1}{n}). \\end{aligned} $$ This gets us our final answer of $T(n) = \\Theta(\\lg n)$. h. We see that we explicity have $$ \\begin{aligned} T(n) & = T(0) + \\sum_{j = 1}^n \\lg j \\\\ & = T(0) + \\int_1^{n + 1} \\sum_{j = 1}^{n + 1} \\chi_{(j, j + 1)}(x) \\lg j dx. \\end{aligned} $$ Similarly to above, we will relate this sum to the integral of $\\lg x$. $$\\text{sup}_{x \\in (i, i + 1)} \\sum_{j = 1}^{n + 1} \\chi_{(j, j + 1)}(x) \\lg j - \\lg x = \\lg(j + 1) - \\lg j = \\lg \\Big(\\frac{j + 1}{j}\\Big).$$ Therefore, $$ \\begin{aligned} T(n) & \\le \\int_i^n \\lg(x + 2) + \\lg x - \\lg(x + 1)dx \\\\ & (1 + O(\\frac{1}{\\lg n})) \\Theta(n\\lg n). \\end{aligned} $$ i. See the approach used in the previous two parts, we will get $T(n) = \\Theta(\\frac{n}{\\lg n})$. j. Let $i$ be the smallest $i$ so that $n^{\\frac{1}{2^i}} < 2$. We recall from a previous problem (3-6.e) that this is $\\lg\\lg n$ Expanding the recurrence, we have that it is $$ \\begin{aligned} T(n) & = n^{1 - \\frac{1}{2^i}}T(2) + n + n\\sum_{j = 1}^i \\\\ & = \\Theta(n\\lg\\lg n). \\end{aligned} $$","title":"4-3 More recurrence examples"},{"location":"Chap04/Problems/4-4/","text":"This problem develops properties of the Fibonacci numbers, which are defined by recurrence $\\text{(3.22)}$. We shall use the technique of generating functions to solve the Fibonacci recurrence. Define the generating function (or formal power series ) $\\mathcal F$ as $$ \\begin{aligned} \\mathcal F(z) & = \\sum_{i = 0}^{\\infty} F_iz^i \\\\ & = 0 + z + z^2 + 2z^3 + 3z^4 + 5z^5 + 8z^6 + 13z^7 + 21z^8 + \\cdots, \\end{aligned} $$ where $F_i$ is the $i$th Fibonacci number. a. Show that $\\mathcal F(z) = z + z\\mathcal F(z) + z^2\\mathcal F$. b. Show that $$ \\begin{aligned} \\mathcal F(z) & = \\frac{z}{1 - z - z^2} \\\\ & = \\frac{z}{(1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{1}{\\sqrt 5}\\Big(\\frac{1}{1 - \\phi z} - \\frac{1}{1 - \\hat{\\phi} z}\\Big), \\end{aligned} $$ where $\\phi = \\frac{1 + \\sqrt 5}{2} = 1.61803\\ldots$ and $\\hat\\phi = \\frac{1 - \\sqrt 5}{2} = -0.61803\\ldots$ c. Show that $$\\mathcal F(z) = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt 5}(\\phi^i - \\hat{\\phi}^i)z^i.$$ d. Use part (c) to prove that $F_i = \\phi^i / \\sqrt 5$ for $i > 0$, rounded to the nearest integer. ($\\textit{Hint:}$ Observe that $|\\hat{\\phi}| < 1$.) a. $$ \\begin{aligned} z + z\\mathcal F(z) + z^2\\mathcal F(Z) & = z + z\\sum_{i = 0}^{\\infty} F_iz^i + z^2\\sum_{i = 0}^{\\infty}F_i z^i \\\\ & = z + \\sum_{i = 1}^{\\infty} F_{i - 1}z^i + \\sum_{i = 2}^{\\infty}F_{i - 2} z^i \\\\ & = z + F_1z + \\sum_{i = 2}^{\\infty}(F_{i - 1} + F_{i - 2})z^i \\\\ & = z + F_1z + \\sum_{i = 2}^{\\infty}F_iz^i \\\\ & = \\mathcal F(z). \\end{aligned} $$ b. Note that $\\phi - \\hat\\phi = \\sqrt 5$, $\\phi + \\hat\\phi = 1$ and $\\phi\\hat\\phi = - 1$. $$ \\begin{aligned} \\mathcal F(z) & = \\frac{\\mathcal F(z)(1 - z - z^2)}{1 - z - z^2} \\\\ & = \\frac{\\mathcal F(z) - z\\mathcal F(z) - z^2\\mathcal F(z) - z + z}{1 - z - z^2} \\\\ & = \\frac{\\mathcal F(z) - \\mathcal F(z) + z}{1 - z - z^2} \\\\ & = \\frac{z}{1 - z - z^2} \\\\ & = \\frac{z}{1 - (\\phi + \\hat\\phi)z + \\phi\\hat\\phi z^2} \\\\ & = \\frac{z}{(1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{\\sqrt 5 z}{\\sqrt 5 (1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{(\\phi - \\hat\\phi)z + 1 - 1}{\\sqrt 5 (1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{(1 - \\hat\\phi z) - (1 - \\phi z)}{\\sqrt 5 (1 - \\phi z)(1 - \\hat\\phi z)} \\\\ & = \\frac{1}{\\sqrt 5}\\Big(\\frac{1}{1 - \\phi z} - \\frac{1}{1 - \\hat\\phi z}\\Big). \\end{aligned} $$ c. We have $\\frac{1}{1 - x} = \\sum_{k = 0}^{\\infty}x^k$, when $|x| < 1$, thus $$ \\begin{aligned} \\mathcal F(n) & = \\frac{1}{\\sqrt 5}\\Big(\\frac{1}{1 - \\phi z} - \\frac{1}{1 - \\hat\\phi z}\\Big) \\\\ & = \\frac{1}{\\sqrt 5}\\Big(\\sum_{i = 0}^{\\infty}\\phi^i z^i - \\sum_{i = 0}^{\\infty}\\hat{\\phi}^i z^i\\Big) \\\\ & = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt 5}(\\phi^i - \\hat{\\phi}^i) z^i. \\end{aligned} $$ d. $\\mathcal F(z) = \\sum_{i = 0}^{\\infty}\\alpha_i z^i$ where $\\alpha_i = \\frac{\\phi^i - \\hat{\\phi}^i}{\\sqrt 5}$. From this follows that $\\alpha_i = F_i$, that is $$F_i = \\frac{\\phi^i - \\hat{\\phi}^i}{\\sqrt 5} = \\frac{\\phi^i}{\\sqrt 5} - \\frac{\\hat{\\phi}^i}{\\sqrt 5},$$ For $i = 1$, $\\phi / \\sqrt 5 = (\\sqrt 5 + 5) / 10 > 0.5$. For $i > 2$, $|\\hat{\\phi}^i| < 0.5$.","title":"4-4 Fibonacci numbers"},{"location":"Chap04/Problems/4-5/","text":"Professor Diogenes has $n$ supposedly identical integrated-circuit chips that in principle are capable of testing each other. The professor's test jig accomodates two chips at a time. When the jig is loaded, each chip tests the other and reports whether it is good or bad. A good chip always reports accurately whether the other chip is good or bad, but the professor cannot trust the answer of a bad chip. Thus, the four possible outcomes of a test are as follows: $$ \\begin{array}{lll} \\text{Chip $A$ says} & \\text{Chip $B$ says} & \\text{Conclusion} \\\\ \\hline \\text{$B$ is good} & \\text{$A$ is good} & \\text{both are good, or both are bad} \\\\ \\text{$B$ is good} & \\text{$A$ is bad} & \\text{at least one is bad} \\\\ \\text{$B$ is bad} & \\text{$A$ is good} & \\text{at least one is bad} \\\\ \\text{$B$ is bad} & \\text{$A$ is bad} & \\text{at least one is bad} \\end{array} $$ a. Show that if more than $n / 2$ chips are bad, the professor cannot necessarily determine which chips are good using any strategy based on this kind of pairwise test. Assume that the bad chips can conspire to fool the professor. b. Consider the problem of finding a single good chip from among $n$ chips, assuming that more than $n / 2$ of the chips are good. Show that $\\lfloor n / 2 \\rfloor$ pairwise tests are sufficient to reduce the problem to one of nearly half the size. c. Show that the good chips can be identified with $\\Theta(n)$ pairwise tests, assuming that more than $n / 2$ chips are good. Give and solve the recurrence that describes the number of tests. a. Lets say that there are $g < n / 2$ good chips and $n - g$ bad chips. From this assumption, we can always find a set of good chips $G$ and a set of bad chips $B$ of equal size $g$ since $n - g \\ge g$. Now, assume that chips in $B$ always conspire to fool the professor in the following: \"for any test made by the professor, chips in $B$ declare chips in $B$ as 'good' and chips in $G$ as 'bad'.\" Since the chips in $G$ always report correct answers thus there exists symmetric behaviors, it is not possible to distinguish bad chips from good ones. b. Generalize the original problem to: \"Assume there are more good chips than bad chips.\" Algorithm: Pairwise test them, and leave the last one alone if the number of chips is odd. If the report says at least one of them is bad, throw both chips away; otherwise, throw one away from each pair. Recursively find one good chip among the remaining chips. The recursion ends when the number of remaining chips is $1$ or $2$. If there is only $1$ chip left, then it is the good chip we desire. If there are $2$ chips left, we make a pairwise test between them. If the report says both are good, we can conclude that both are good chips. Otherwise, one is good and the other is bad and we throw both away. The chip we left alone at step $1$ is a good chip. Explanation: If the number of chips is odd, from assumption we know the number of good chips must be greater than the number of bad chips. We randomly leave one chip alone from the chips, in which good chips are not less than bad chips. Chip pairs that do not say each other is good either have one bad chip or have two bad chips, throwing them away doesn't change the fact that good chips are not less than bad chips. The remaining chip pairs are either both good chips or bad chips, after throwing one chip away in every those pairs, we have reduced the size of the problem to at most half of the original problem size. If the number of good chips is $n$ ($n > 1$) more than that of bad chips, we just throw away the chip we left alone when the number of chips is odd. In this case, the number of good chips is at least one more than that of bad chips, and we can eventually find a good chip as our algorithm claims. If the number of good chips is exactly one more than that of bad chips, there are $2$ cases. We left alone the good chip, and remaining chips are one half good and one half bad. In this case, all the chips will be thrown away eventually. And the chip left alone is the one we desire. We left alone the bad chip, there are more good chips than bad chips in the remaining chips. In this case, we can recursively find a good chip in the remaining chips and the left bad chip will be thrown away at the end. c. As the solution provided in (b), we can find one good chip in $$T(n) \\le T(\\lceil n / 2 \\rceil) + \\lfloor n / 2 \\rfloor.$$ By the master theorem, we have $T(n) = O(n)$. After finding a good chip, we can identify all good chips with that good chip we just found in $n - 1$ tests, so the total number of tests is $$O(n) + n - 1 = \\Theta(n).$$","title":"4-5 Chip testing"},{"location":"Chap04/Problems/4-6/","text":"An $m \\times n$ array $A$ of real numbers is a Monge array if for all $i$, $j$, $k$, and $l$ such that $1 \\le i < k \\le m$ and $1 \\le j < l \\le n$, we have $$A[i, j] + A[k, l] \\le A[i, l] + A[k, j].$$ In other words, whenever we pick two rows and two columns of a Monge array and consider the four elements at the intersections of the rows and columns, the sum of the upper-left and lower-right elements is less than or equal to the sum of the lower-left and upper-right elements. For example, the following array is Monge: $$ \\begin{matrix} 10 & 17 & 13 & 28 & 23 \\\\ 17 & 22 & 16 & 29 & 23 \\\\ 24 & 28 & 22 & 34 & 24 \\\\ 11 & 13 & 6 & 17 & 7 \\\\ 45 & 44 & 32 & 37 & 23 \\\\ 36 & 33 & 19 & 21 & 6 \\\\ 75 & 66 & 51 & 53 & 34 \\end{matrix} $$ a. Prove that an array is Monge if and only if for all $i = 1, 2, \\ldots, m - 1$, and $j = 1, 2, \\ldots, n - 1$ we have $$A[i, j] + A[i + 1,j + 1] \\le A[i, j + 1] + A[i + 1, j].$$ ($\\textit{Hint:}$ For the \"if\" part, use induction seperately on rows and columns.) b. The following array is not Monge. Change one element in order to make it Monge. ($\\textit{Hint:}$ Use part (a).) $$ \\begin{matrix} 37 & 23 & 22 & 32 \\\\ 21 & 6 & 7 & 10 \\\\ 53 & 34 & 30 & 31 \\\\ 32 & 13 & 9 & 6 \\\\ 43 & 21 & 15 & 8 \\end{matrix} $$ c. Let $f(i)$ be the index of the column containing the leftmost minimum element of row $i$. Prove that $f(1) \\le f(2) \\le \\cdots \\le f(m)$ for any $m \\times n$ Monge array. d. Here is a description of a divide-and-conquer algorithm that computes the leftmost minimum element in each row of an $m \\times n$ Monge array $A$: Construct a submatrix $A'$ of $A$ consisting of the even-numbered rows of $A$. Recursively determine the leftmost minimum for each row in $A'$. Then compute the leftmost minimum in the odd-numbered rows of $A$. Explain how to compute the leftmost minimum in the odd-numbered rows of $A$ (given that the leftmost minimum of the even-numbered rows is known) in $O(m + n)$ time. e. Write the recurrence describing the running time of the algorithm described in part (d). Show that its solution is $O(m + n\\log m)$. a. The \"only if\" part is trivial, it follows form the definition of Monge array. As for the \"if\" part, let's first prove that $$ \\begin{aligned} A[i, j] + A[i + 1, j + 1] & \\le A[i, j + 1] + A[i + 1, j] \\\\ \\Rightarrow A[i, j] + A[k, j + 1] & \\le A[i, j + 1] + A[k, j], \\end{aligned} $$ where $i < k$. Let's prove it by induction. The base case of $k = i + 1$ is given. As for the inductive step, we assume it holds for $k = i + n$ and we want to prove it for $k + 1 = i + n + 1$. If we add the given to the assumption, we get $$ \\begin{aligned} A[i, j] + A[k, j + 1] & \\le A[i, j + 1] + A[k, j] & \\text{(assumption)} \\\\ A[k, j] + A[k + 1, j + 1] & \\le A[k, j + 1] + A[k + 1, j] & \\text{(given)} \\\\ \\Rightarrow A[i, j] + A[k, j + 1] + A[k, j] + A[k + 1, j + 1] & \\le A[i, j + 1] + A[k, j] + A[k, j + 1] + A[k + 1, j] \\\\ \\Rightarrow A[i, j] + A[k + 1, j + 1] & \\le A[i, j + 1] + A[k + 1, j] \\end{aligned} $$ b. $$ \\begin{matrix} 37 & 23 & \\mathbf{24} & 32 \\\\ 21 & 6 & 7 & 10 \\\\ 53 & 34 & 30 & 31 \\\\ 32 & 13 & 9 & 6 \\\\ 43 & 21 & 15 & 8 \\\\ \\end{matrix} $$ c. Let $a_i$ and $b_j$ be the leftmost minimal elements on rows $a$ and $b$ and let's assume that $i > j$. Then we have $$A[j, a] + A[i, b] \\le A[i, a] + A[j, b].$$ But $$ \\begin{aligned} A[j, a] \\ge A[i, a] & (a_i \\text{ is minimal}) \\\\ A[i, b] \\ge A[j, b] & (b_j \\text{ is minimal}) \\\\ \\end{aligned} $$ Which implies that $$ \\begin{aligned} A[j, a] + A[i, b] & \\ge A[i, a] + A[j, b] \\\\ A[j, a] + A[i, b] & = A[i, a] + A[j, b] \\end{aligned} $$ Which in turn implies that either: $$ \\begin{aligned} A[j, b] < A[i, b] & \\Rightarrow A[i, a] > A[j, a] \\Rightarrow a_i \\text{ is not minimal} \\\\ A[j, b] = A[i, b] & \\Rightarrow b_j \\text{ is not the leftmost minimal} \\end{aligned} $$ d. If $\\mu_i$ is the index of the $i$-th row's leftmost minimum, then we have $$\\mu_{i - 1} \\le \\mu_i \\le \\mu_{i + 1}.$$ For $i = 2k + 1$, $k \\ge 0$, finding $\\mu_i$ takes $\\mu_{i + 1} - \\mu_{i - 1} + 1$ steps at most, since we only need to compare with those numbers. Thus $$ \\begin{aligned} T(m, n) & = \\sum_{i = 0}^{m / 2 - 1} (\\mu_{2i + 2} - \\mu_{2i} + 1) \\\\ & = \\sum_{i = 0}^{m / 2 - 1} \\mu_{2i + 2} - \\sum_{i = 0}^{m / 2 - 1}\\mu_{2i} + m / 2 \\\\ & = \\sum_{i = 1}^{m / 2} \\mu_{2i} - \\sum_{i = 0}^{m / 2 - 1}\\mu_{2i} + m / 2 \\\\ &= \\mu_m - \\mu_0 + m / 2 \\\\ & = n + m / 2 \\\\ & = O(m + n). \\end{aligned} $$ e. The divide time is $O(1)$, the conquer part is $T(m / 2)$ and the merge part is $O(m + n)$. Thus, $$ \\begin{aligned} T(m) & = T(m / 2) + cn + dm \\\\ & = cn + dm + cn + dm / 2 + cn + dm / 4 + \\cdots \\\\ & = \\sum_{i = 0}^{\\lg m - 1}cn + \\sum_{i = 0}^{\\lg m - 1}\\frac{dm}{2^i} \\\\ & = cn\\lg m + dm\\sum_{i = 0}^{\\lg m - 1} \\\\ & < cn\\lg m + 2dm \\\\ & = O(n\\lg m + m). \\end{aligned} $$","title":"4-6 Monge arrays"},{"location":"Chap05/5.1/","text":"5.1-1 Show that the assumption that we are always able to determine which candidate is best in line 4 of procedure $\\text{HIRE-ASSISTANT}$ implies that we know a total order on the ranks of the candidates. A total order is a partial order that is a total relation $(\\forall a, b \\in A:aRb \\text{ or } bRa)$. A relation is a partial order if it is reflexive, antisymmetric and transitive. Assume that the relation is good or better. Reflexive: This is a bit trivial, but everybody is as good or better as themselves. Transitive: If $A$ is better than $B$ and $B$ is better than $C$, then $A$ is better than $C$. Antisymmetric: If $A$ is better than $B$, then $B$ is not better than $A$. So far we have a partial order. Since we assume we can compare any two candidates, then comparison must be a total relation and thus we have a total order. 5.1-2 $\\star$ Describe an implementation of the procedure $\\text{RANDOM}(a, b)$ that only makes calls to $\\text{RANDOM}(0, 1)$. What is the expected running time of your procedure, as a function of $a$ and $b$? As $(b - a)$ could be any number, we need at least $\\lceil \\lg(b - a) \\rceil$ bits to represent the number. We set $\\lceil \\lg(b - a) \\rceil$ as $k$. Basically, we need to call $\\text{RANDOM}(0, 1)$ $k$ times. If the number represented by binary is bigger than $b - a$, it's not valid number and we give it another try, otherwise we return that number. RANDOM ( a , b ) range = b - a bits = ceil ( log ( 2 , range )) result = 0 for i = 0 to bits - 1 r = RANDOM ( 0 , 1 ) result = result + r << i if result > range return RANDOM ( a , b ) else return a + result The expectation of times of calling procedure $\\text{RANDOM}(a, b)$ is $\\frac{2^k}{b - a}$. $\\text{RANDOM}(0, 1)$ will be called $k$ times in that procedure. The expected running time is $\\Theta(\\frac{2^k}{b - a} \\cdot k)$, $k$ is $\\lceil \\lg(b - a) \\rceil$. Considering $2^k$ is less than $2 \\cdot (b - a)$, so the running time is $O(k)$. 5.1-3 $\\star$ Suppose that you want to output $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. At your disposal is a procedure $\\text{BIASED-RANDOM}$, that outputs either $0$ or $1$. It outputs $1$ with some probability $p$ and $0$ with probability $1 - p$, where $0 < p < 1$, but you do not know what $p$ is. Give an algorithm that uses $\\text{BIASED-RANDOM}$ as a subroutine, and returns an unbiased answer, returning $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. What is the expected running time of your algorithm as a function of $p$? There are 4 outcomes when we call $\\text{BIASED-RANDOM}$ twice, i.e., $00$, $01$, $10$, $11$. The strategy is as following: $00$ or $11$: call $\\text{BIASED-RANDOM}$ twice again $01$: output $0$ $10$: output $1$ We can calculate the probability of each outcome: $\\Pr\\{00 | 11\\} = p^2 + (1 - p)^2$ $\\Pr\\{01\\} = (1 - p)p$ $\\Pr\\{10\\} = p(1 - p)$ Since there's no other way to return a value, it returns $0$ and $1$ both with probability $1 / 2$. The pseudo code is as follow: UNBIASED - RANDOM while true x = BIASED - RANDOM y = BIASED - RANDOM if x != y return x This algorithm actually uses the equivalence of the probability of occurrence of $01$ and $10$, and subtly converts the unequal $00$ and $11$ to $01$ and $10$, thus eliminating the probability that its probability is not equivalent. Each iteration is a Bernoulli trial, where \"success\" means that the iteration does return a value. We can view each iteration as a Bernoulli trial, where \"success\" means that the iteration returns a value. $$ \\begin{aligned} \\Pr\\{\\text{success}\\} & = \\Pr\\{0\\text{ is returned}\\} + \\Pr\\{1\\text{ is returned}\\} \\\\ & = 2p(1 - p). \\end{aligned} $$ The expected number of trials for this scenario is $1 / (2p(1 - p))$. Thus, the expected running time of $\\text{UNBIASED-RANDOM}$ is $\\Theta(1 / (2p(1 - p))$.","title":"5.1 The hiring problem"},{"location":"Chap05/5.1/#51-1","text":"Show that the assumption that we are always able to determine which candidate is best in line 4 of procedure $\\text{HIRE-ASSISTANT}$ implies that we know a total order on the ranks of the candidates. A total order is a partial order that is a total relation $(\\forall a, b \\in A:aRb \\text{ or } bRa)$. A relation is a partial order if it is reflexive, antisymmetric and transitive. Assume that the relation is good or better. Reflexive: This is a bit trivial, but everybody is as good or better as themselves. Transitive: If $A$ is better than $B$ and $B$ is better than $C$, then $A$ is better than $C$. Antisymmetric: If $A$ is better than $B$, then $B$ is not better than $A$. So far we have a partial order. Since we assume we can compare any two candidates, then comparison must be a total relation and thus we have a total order.","title":"5.1-1"},{"location":"Chap05/5.1/#51-2-star","text":"Describe an implementation of the procedure $\\text{RANDOM}(a, b)$ that only makes calls to $\\text{RANDOM}(0, 1)$. What is the expected running time of your procedure, as a function of $a$ and $b$? As $(b - a)$ could be any number, we need at least $\\lceil \\lg(b - a) \\rceil$ bits to represent the number. We set $\\lceil \\lg(b - a) \\rceil$ as $k$. Basically, we need to call $\\text{RANDOM}(0, 1)$ $k$ times. If the number represented by binary is bigger than $b - a$, it's not valid number and we give it another try, otherwise we return that number. RANDOM ( a , b ) range = b - a bits = ceil ( log ( 2 , range )) result = 0 for i = 0 to bits - 1 r = RANDOM ( 0 , 1 ) result = result + r << i if result > range return RANDOM ( a , b ) else return a + result The expectation of times of calling procedure $\\text{RANDOM}(a, b)$ is $\\frac{2^k}{b - a}$. $\\text{RANDOM}(0, 1)$ will be called $k$ times in that procedure. The expected running time is $\\Theta(\\frac{2^k}{b - a} \\cdot k)$, $k$ is $\\lceil \\lg(b - a) \\rceil$. Considering $2^k$ is less than $2 \\cdot (b - a)$, so the running time is $O(k)$.","title":"5.1-2 $\\star$"},{"location":"Chap05/5.1/#51-3-star","text":"Suppose that you want to output $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. At your disposal is a procedure $\\text{BIASED-RANDOM}$, that outputs either $0$ or $1$. It outputs $1$ with some probability $p$ and $0$ with probability $1 - p$, where $0 < p < 1$, but you do not know what $p$ is. Give an algorithm that uses $\\text{BIASED-RANDOM}$ as a subroutine, and returns an unbiased answer, returning $0$ with probability $1 / 2$ and $1$ with probability $1 / 2$. What is the expected running time of your algorithm as a function of $p$? There are 4 outcomes when we call $\\text{BIASED-RANDOM}$ twice, i.e., $00$, $01$, $10$, $11$. The strategy is as following: $00$ or $11$: call $\\text{BIASED-RANDOM}$ twice again $01$: output $0$ $10$: output $1$ We can calculate the probability of each outcome: $\\Pr\\{00 | 11\\} = p^2 + (1 - p)^2$ $\\Pr\\{01\\} = (1 - p)p$ $\\Pr\\{10\\} = p(1 - p)$ Since there's no other way to return a value, it returns $0$ and $1$ both with probability $1 / 2$. The pseudo code is as follow: UNBIASED - RANDOM while true x = BIASED - RANDOM y = BIASED - RANDOM if x != y return x This algorithm actually uses the equivalence of the probability of occurrence of $01$ and $10$, and subtly converts the unequal $00$ and $11$ to $01$ and $10$, thus eliminating the probability that its probability is not equivalent. Each iteration is a Bernoulli trial, where \"success\" means that the iteration does return a value. We can view each iteration as a Bernoulli trial, where \"success\" means that the iteration returns a value. $$ \\begin{aligned} \\Pr\\{\\text{success}\\} & = \\Pr\\{0\\text{ is returned}\\} + \\Pr\\{1\\text{ is returned}\\} \\\\ & = 2p(1 - p). \\end{aligned} $$ The expected number of trials for this scenario is $1 / (2p(1 - p))$. Thus, the expected running time of $\\text{UNBIASED-RANDOM}$ is $\\Theta(1 / (2p(1 - p))$.","title":"5.1-3 $\\star$"},{"location":"Chap05/5.2/","text":"5.2-1 In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly one time? What is the probability you hire exactly $n$ times? You will hire exactly one time if the best candidate is at first. There are $(n \u2212 1)!$ orderings with the best candidate being at first, so the probability that you hire exactly one time is $\\frac{(n - 1)!}{n!} = \\frac{1}{n}$. You will hire exactly $n$ times if the candidates are presented in increasing order. There is only an ordering for this situation, so the probability that you hire exactly $n$ times is $\\frac{1}{n!}$. 5.2-2 In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly twice? Note that Candidate $1$ is always hired The best candidate (candidate whose rank is $n$) is always hired If the best candidate is candidate $1$, then that's the only candidate hired. In order for $\\text{HIRE-ASSISTANT}$ to hire exactly twice, candidate $1$ should have rank $i$, where $1 \\le i \\le n - 1$, and all candidates whose ranks are $i + 1, i + 2, \\dots, n - 1$ should be interviewed after the candidate whose rank is $n$ (the best candidate). Let $E_i$ be the event in which candidate $1$ have rank $i$, so we have $P(E_i) = 1 / n$ for $1 \\le i \\le n$. Our goal is to find for $1 \\le i \\le n - 1$, given $E_i$ occurs, i.e., candidate $1$ has rank $i$, the candidate whose rank is $n$ (the best candidate) is the first one interviewed out of the $n - i$ candidates whose ranks are $i + 1, i + 2, \\dots, n$. So, $$\\sum_{i = 1}^{n - 1} P(E_i) \\cdot \\frac{1}{n - i} = \\sum_{i = 1}^{n - 1} \\frac{1}{n} \\cdot \\frac{1}{n - i}.$$ 5.2-3 Use indicator random variables to compute the expected value of the sum of $n$ dice. Expectation of a single dice $X_i$ is $$ \\begin{aligned} \\text E[X_k] & = \\sum_{i = 1}^6 i \\Pr\\{X_k = i\\} \\\\ & = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} \\\\ & = \\frac{21}{6} \\\\ & = 3.5. \\end{aligned} $$ As for multiple dices, $$ \\begin{aligned} \\text E[X] & = \\text E\\Bigg[\\sum_{i = 1}^n X_i \\Bigg] \\\\ & = \\sum_{i = 1}^n \\text E[X_i] \\\\ & = \\sum_{i = 1}^n 3.5 \\\\ & = 3.5 \\cdot n. \\end{aligned} $$ 5.2-4 Use indicator random variables to solve the following problem, which is known as the hat-check problem . Each of $n$ customers gives a hat to a hat-check person at a restaurant. The hat-check person gives the hats back to the customers in a random order. What is the expected number of customers who get back their hat? Let $X$ be the number of customers who get back their own hat and $X_i$ be the indicator random variable that customer $i$ gets his hat back. The probability that an individual gets his hat back is $\\frac{1}{n}$. Thus we have $$E[X] = E\\Bigg[\\sum_{i = 1}^n X_i\\Bigg] = \\sum_{i = 1}^n E[X_i] = \\sum_{i = 1}^n \\frac{1}{n} = 1.$$ 5.2-5 Let $A[1..n]$ be an array of $n$ distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair $(i, j)$ is called an inversion of $A$. (See Problem 2-4 for more on inversions.) Suppose that the elements of $A$ form a uniform random permutation of $\\langle 1, 2, \\ldots, n \\rangle$. Use indicator random variables to compute the expected number of inversions. Let $X_{i, j}$ for $i < j$ be the indicator of $A[i] > A[j]$. We have that the expected number of inversions $$ \\begin{aligned} \\text E\\Bigg[\\sum_{i < j} X_{i, j}\\Bigg] & = \\sum_{i < j} E[X_{i, j}] \\\\ & = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\Pr\\{A[i] > A[j]\\} \\\\ & = \\frac{1}{2} \\sum_{i = 1}^{n - 1} n - i \\\\ & = \\frac{n(n - 1)}{2} - \\frac{n(n - 1)}{4} \\\\ & = \\frac{n(n - 1)}{4}. \\end{aligned} $$","title":"5.2 Indicator random variables"},{"location":"Chap05/5.2/#52-1","text":"In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly one time? What is the probability you hire exactly $n$ times? You will hire exactly one time if the best candidate is at first. There are $(n \u2212 1)!$ orderings with the best candidate being at first, so the probability that you hire exactly one time is $\\frac{(n - 1)!}{n!} = \\frac{1}{n}$. You will hire exactly $n$ times if the candidates are presented in increasing order. There is only an ordering for this situation, so the probability that you hire exactly $n$ times is $\\frac{1}{n!}$.","title":"5.2-1"},{"location":"Chap05/5.2/#52-2","text":"In $\\text{HIRE-ASSISTANT}$, assuming that the candidates are presented in a random order, what is the probability that you hire exactly twice? Note that Candidate $1$ is always hired The best candidate (candidate whose rank is $n$) is always hired If the best candidate is candidate $1$, then that's the only candidate hired. In order for $\\text{HIRE-ASSISTANT}$ to hire exactly twice, candidate $1$ should have rank $i$, where $1 \\le i \\le n - 1$, and all candidates whose ranks are $i + 1, i + 2, \\dots, n - 1$ should be interviewed after the candidate whose rank is $n$ (the best candidate). Let $E_i$ be the event in which candidate $1$ have rank $i$, so we have $P(E_i) = 1 / n$ for $1 \\le i \\le n$. Our goal is to find for $1 \\le i \\le n - 1$, given $E_i$ occurs, i.e., candidate $1$ has rank $i$, the candidate whose rank is $n$ (the best candidate) is the first one interviewed out of the $n - i$ candidates whose ranks are $i + 1, i + 2, \\dots, n$. So, $$\\sum_{i = 1}^{n - 1} P(E_i) \\cdot \\frac{1}{n - i} = \\sum_{i = 1}^{n - 1} \\frac{1}{n} \\cdot \\frac{1}{n - i}.$$","title":"5.2-2"},{"location":"Chap05/5.2/#52-3","text":"Use indicator random variables to compute the expected value of the sum of $n$ dice. Expectation of a single dice $X_i$ is $$ \\begin{aligned} \\text E[X_k] & = \\sum_{i = 1}^6 i \\Pr\\{X_k = i\\} \\\\ & = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} \\\\ & = \\frac{21}{6} \\\\ & = 3.5. \\end{aligned} $$ As for multiple dices, $$ \\begin{aligned} \\text E[X] & = \\text E\\Bigg[\\sum_{i = 1}^n X_i \\Bigg] \\\\ & = \\sum_{i = 1}^n \\text E[X_i] \\\\ & = \\sum_{i = 1}^n 3.5 \\\\ & = 3.5 \\cdot n. \\end{aligned} $$","title":"5.2-3"},{"location":"Chap05/5.2/#52-4","text":"Use indicator random variables to solve the following problem, which is known as the hat-check problem . Each of $n$ customers gives a hat to a hat-check person at a restaurant. The hat-check person gives the hats back to the customers in a random order. What is the expected number of customers who get back their hat? Let $X$ be the number of customers who get back their own hat and $X_i$ be the indicator random variable that customer $i$ gets his hat back. The probability that an individual gets his hat back is $\\frac{1}{n}$. Thus we have $$E[X] = E\\Bigg[\\sum_{i = 1}^n X_i\\Bigg] = \\sum_{i = 1}^n E[X_i] = \\sum_{i = 1}^n \\frac{1}{n} = 1.$$","title":"5.2-4"},{"location":"Chap05/5.2/#52-5","text":"Let $A[1..n]$ be an array of $n$ distinct numbers. If $i < j$ and $A[i] > A[j]$, then the pair $(i, j)$ is called an inversion of $A$. (See Problem 2-4 for more on inversions.) Suppose that the elements of $A$ form a uniform random permutation of $\\langle 1, 2, \\ldots, n \\rangle$. Use indicator random variables to compute the expected number of inversions. Let $X_{i, j}$ for $i < j$ be the indicator of $A[i] > A[j]$. We have that the expected number of inversions $$ \\begin{aligned} \\text E\\Bigg[\\sum_{i < j} X_{i, j}\\Bigg] & = \\sum_{i < j} E[X_{i, j}] \\\\ & = \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\Pr\\{A[i] > A[j]\\} \\\\ & = \\frac{1}{2} \\sum_{i = 1}^{n - 1} n - i \\\\ & = \\frac{n(n - 1)}{2} - \\frac{n(n - 1)}{4} \\\\ & = \\frac{n(n - 1)}{4}. \\end{aligned} $$","title":"5.2-5"},{"location":"Chap05/5.3/","text":"5.3-1 Professor Marceau objects to the loop invariant used in the proof of Lemma 5.5. He questions whether it is true prior to the first iteration. He reasons that we could just as easily declare that an empty subarray contains no $0$-permutations. Therefore, the probability that an empty subarray contains a $0$-permutation should be $0$, thus invalidating the loop invariant prior to the first iteration. Rewrite the procedure $\\text{RANDOMIZE-IN-PLACE}$ so that its associated loop invariant applies to a nonempty subarray prior to the first iteration, and modify the proof of Lemma 5.5 for your procedure. Modify the algorithm by unrolling the $i = 1$ case. swap ( A [ 1 ], A [ RANDOM ( 1 , n )]) for i = 2 to n swap ( A [ i ], A [ RANDOM ( i , n )]) Modify the proof of Lemma 5.5 by starting with $i = 2$ instead of $i = 1$. This resolves the issue of $0$-permutations. 5.3-2 Professor Kelp decides to write a procedure that produces at random any permutation besides the identity permutation. He proposes the following procedure: PERMUTE - WITHOUT - IDENTITY ( A ) n = A . length for i = 1 to n - 1 swap A [ i ] with A [ RANDOM ( i + 1 , n )] Does this code do what Professor Kelp intends? The code does not do what he intends. Suppose $A = [1, 2, 3]$. If the algorithm worked as proposed, then with nonzero probability the algorithm should output $[3, 2, 1]$. On the first iteration we swap $A[1]$ with either $A[2]$ or $A[3]$. Since we want $[3, 2, 1]$ and will never again alter $A[1]$, we must necessarily swap with $A[3]$. Now the current array is $[3, 2, 1]$. On the second (and final) iteration, we have no choice but to swap $A[2]$ with $A[3]$, so the resulting array is $[3, 1, 2]$. Thus, the procedure cannot possibly be producing random non-identity permutations. 5.3-3 Suppose that instead of swapping element $A[i]$ with a random element from the subarray $A[i..n]$, we swapped it with a random element from anywhere in the array: PERMUTE - WITH - ALL ( A ) n = A . length for i = 1 to n swap A [ i ] with A [ RANDOM ( 1 , n )] Does this code produce a uniform random permutation? Why or why not? Consider the case of $n = 3$ in running the algorithm, three IID choices will be made, and so you'll end up having $27$ possible end states each with equal probability. There are $3! = 6$ possible orderings, these should appear equally often, but this can't happen because $6$ does not divide $27$. 5.3-4 Professor Armstrong suggests the following procedure for generating a uniform random permutation: PERMUTE - BY - CYCLIC ( A ) n = A . length let B [ 1. . n ] be a new array offset = RANDOM ( 1 , n ) for i = 1 to n dest = i + offset if dest > n dest = dest - n B [ dest ] = A [ i ] return B Show that each element $A[i]$ has a $1 / n$ probability of winding up in any particular position in $B$. Then show that Professor Armstrong is mistaken by showing that the resulting permutation is not uniformly random. Fix a position $j$ and an index $i$. We'll show that the probability that $A[i]$ winds up in position $j$ is $1 / n$. The probability $B[j] = A[i]$ is the probability that $dest = j$, which is the probability that $i + offset$ or $i + offset \u2212 n$ is equal to $j$, which is $1 / n$. This algorithm can't possibly return a random permutation because it doesn't change the relative positions of the elements; it merely cyclically permutes the whole permutation. For instance, suppose $A = [1, 2, 3]$, if $offset = 1$, $B = [3, 1, 2]$, if $offset = 2$, $B = [2, 3, 1]$, if $v = 3$, $B = [1, 2, 3]$. Thus, the algorithm will never produce $B = [1, 3, 2]$, so the resulting permutation cannot be uniformly random. 5.3-5 $\\star$ Prove that in the array $P$ in procedure $\\text{PERMUTE-BY-SORTING}$, the probability that all elements are unique is at least $1 - 1 / n$. Let $\\Pr\\{j\\}$ be the probability that the element with index $j$ is unique. If there are $n^3$ elements, then the $\\Pr\\{j\\} = 1 - \\frac{j - 1}{n^3}$. $$ \\begin{aligned} \\Pr\\{1 \\cap 2 \\cap 3 \\cap \\ldots\\} & = \\Pr\\{1\\} \\cdot \\Pr\\{2 \\mid 1\\} \\cdot \\Pr\\{3 \\mid 1 \\cap 2\\} \\cdots \\\\ & = 1 (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})(1 - \\frac{3}{n^3}) \\cdots \\\\ & \\ge 1 (1 - \\frac{n}{n^3}) (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3}) \\cdots \\\\ & \\ge (1 - \\frac{1}{n^2})^n \\\\ & \\ge 1 - \\frac{1}{n}, \\\\ \\end{aligned} $$ where the last step holds for $(1 - x)^n \\ge 1 - nx$. 5.3-6 Explain how to implement the algorithm $\\text{PERMUTE-BY-SORTING}$ to handle the case in which two or more priorities are identical. That is, your algorithm should produce a uniform random permutation, even if two or more priorities are identical. PERMUTE - BY - SORTING ( A ) let P [ 1. . n ] be a new array for i = 1 to n P [ i ] = i for i = 1 to n swap P [ i ] with P [ RANDOM ( i , n )] 5.3-7 Suppose we want to create a random sample of the set $\\{1, 2, 3, \\ldots, n\\}$, that is, an $m$-element subset $S$, where $0 \\le m \\le n$, such that each $m$-subset is equally likely to be created. One way would be to set $A[i] = i$ for $i = 1, 2, 3, \\ldots, n$, call $\\text{RANDOMIZE-IN-PLACE}(A)$, and then take just the first $m$ array elements. This method would make $n$ calls to the $\\text{RANDOM}$ procedure. If $n$ is much larger than $m$, we can create a random sample with fewer calls to $\\text{RANDOM}$. Show that the following recursive procedure returns a random $m$-subset $S$ of $\\{1, 2, 3, \\ldots, n\\}$, in which each $m$-subset is equally likely, while making only $m$ calls to $\\text{RANDOM}$: RANDOM - SAMPLE ( m , n ) if m == 0 return \u00d8 else S = RANDOM - SAMPLE ( m - 1 , n - 1 ) i = RANDOM ( 1 , n ) if i \u2208 S S = S \u222a { n } else S = S \u222a { i } return S We prove that it produces a random $m$ subset by induction on $m$. It is obviously true if $m = 0$ as there is only one size $m$ subset of $[n]$. Suppose $S$ is a uniform $m \u2212 1$ subset of $n \u2212 1$, that is, $\\forall j \\in [n - 1]$, $\\Pr[j \\in S] = \\frac{m - 1}{n - 1}$. If we let $S'$ denote the returned set, suppose first $j \\in [n \u2212 1]$, $$ \\begin{aligned} \\Pr[j \\in S'] & = \\Pr[j \\in S] + \\Pr[j \\notin S \\wedge i = j] \\\\ & = \\frac{m - 1}{n - 1} + \\Pr[j \\notin S]\\Pr[i = j] \\\\ & = \\frac{m - 1}{n - 1} + \\left(1 - \\frac{m - 1}{n - 1}\\right) \\frac{1}{n} \\\\ & = \\frac{n(m - 1) + n - m}{(n - 1)n} \\\\ & = \\frac{nm - m}{(n - 1)n} = \\frac{m}{n}. \\end{aligned} $$ Since the constructed subset contains each of $[n \u2212 1]$ with the correct probability, it must also contain $n$ with the correct probability because the probabilities sum to $1$.","title":"5.3 Randomized algorithms"},{"location":"Chap05/5.3/#53-1","text":"Professor Marceau objects to the loop invariant used in the proof of Lemma 5.5. He questions whether it is true prior to the first iteration. He reasons that we could just as easily declare that an empty subarray contains no $0$-permutations. Therefore, the probability that an empty subarray contains a $0$-permutation should be $0$, thus invalidating the loop invariant prior to the first iteration. Rewrite the procedure $\\text{RANDOMIZE-IN-PLACE}$ so that its associated loop invariant applies to a nonempty subarray prior to the first iteration, and modify the proof of Lemma 5.5 for your procedure. Modify the algorithm by unrolling the $i = 1$ case. swap ( A [ 1 ], A [ RANDOM ( 1 , n )]) for i = 2 to n swap ( A [ i ], A [ RANDOM ( i , n )]) Modify the proof of Lemma 5.5 by starting with $i = 2$ instead of $i = 1$. This resolves the issue of $0$-permutations.","title":"5.3-1"},{"location":"Chap05/5.3/#53-2","text":"Professor Kelp decides to write a procedure that produces at random any permutation besides the identity permutation. He proposes the following procedure: PERMUTE - WITHOUT - IDENTITY ( A ) n = A . length for i = 1 to n - 1 swap A [ i ] with A [ RANDOM ( i + 1 , n )] Does this code do what Professor Kelp intends? The code does not do what he intends. Suppose $A = [1, 2, 3]$. If the algorithm worked as proposed, then with nonzero probability the algorithm should output $[3, 2, 1]$. On the first iteration we swap $A[1]$ with either $A[2]$ or $A[3]$. Since we want $[3, 2, 1]$ and will never again alter $A[1]$, we must necessarily swap with $A[3]$. Now the current array is $[3, 2, 1]$. On the second (and final) iteration, we have no choice but to swap $A[2]$ with $A[3]$, so the resulting array is $[3, 1, 2]$. Thus, the procedure cannot possibly be producing random non-identity permutations.","title":"5.3-2"},{"location":"Chap05/5.3/#53-3","text":"Suppose that instead of swapping element $A[i]$ with a random element from the subarray $A[i..n]$, we swapped it with a random element from anywhere in the array: PERMUTE - WITH - ALL ( A ) n = A . length for i = 1 to n swap A [ i ] with A [ RANDOM ( 1 , n )] Does this code produce a uniform random permutation? Why or why not? Consider the case of $n = 3$ in running the algorithm, three IID choices will be made, and so you'll end up having $27$ possible end states each with equal probability. There are $3! = 6$ possible orderings, these should appear equally often, but this can't happen because $6$ does not divide $27$.","title":"5.3-3"},{"location":"Chap05/5.3/#53-4","text":"Professor Armstrong suggests the following procedure for generating a uniform random permutation: PERMUTE - BY - CYCLIC ( A ) n = A . length let B [ 1. . n ] be a new array offset = RANDOM ( 1 , n ) for i = 1 to n dest = i + offset if dest > n dest = dest - n B [ dest ] = A [ i ] return B Show that each element $A[i]$ has a $1 / n$ probability of winding up in any particular position in $B$. Then show that Professor Armstrong is mistaken by showing that the resulting permutation is not uniformly random. Fix a position $j$ and an index $i$. We'll show that the probability that $A[i]$ winds up in position $j$ is $1 / n$. The probability $B[j] = A[i]$ is the probability that $dest = j$, which is the probability that $i + offset$ or $i + offset \u2212 n$ is equal to $j$, which is $1 / n$. This algorithm can't possibly return a random permutation because it doesn't change the relative positions of the elements; it merely cyclically permutes the whole permutation. For instance, suppose $A = [1, 2, 3]$, if $offset = 1$, $B = [3, 1, 2]$, if $offset = 2$, $B = [2, 3, 1]$, if $v = 3$, $B = [1, 2, 3]$. Thus, the algorithm will never produce $B = [1, 3, 2]$, so the resulting permutation cannot be uniformly random.","title":"5.3-4"},{"location":"Chap05/5.3/#53-5-star","text":"Prove that in the array $P$ in procedure $\\text{PERMUTE-BY-SORTING}$, the probability that all elements are unique is at least $1 - 1 / n$. Let $\\Pr\\{j\\}$ be the probability that the element with index $j$ is unique. If there are $n^3$ elements, then the $\\Pr\\{j\\} = 1 - \\frac{j - 1}{n^3}$. $$ \\begin{aligned} \\Pr\\{1 \\cap 2 \\cap 3 \\cap \\ldots\\} & = \\Pr\\{1\\} \\cdot \\Pr\\{2 \\mid 1\\} \\cdot \\Pr\\{3 \\mid 1 \\cap 2\\} \\cdots \\\\ & = 1 (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})(1 - \\frac{3}{n^3}) \\cdots \\\\ & \\ge 1 (1 - \\frac{n}{n^3}) (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3}) \\cdots \\\\ & \\ge (1 - \\frac{1}{n^2})^n \\\\ & \\ge 1 - \\frac{1}{n}, \\\\ \\end{aligned} $$ where the last step holds for $(1 - x)^n \\ge 1 - nx$.","title":"5.3-5 $\\star$"},{"location":"Chap05/5.3/#53-6","text":"Explain how to implement the algorithm $\\text{PERMUTE-BY-SORTING}$ to handle the case in which two or more priorities are identical. That is, your algorithm should produce a uniform random permutation, even if two or more priorities are identical. PERMUTE - BY - SORTING ( A ) let P [ 1. . n ] be a new array for i = 1 to n P [ i ] = i for i = 1 to n swap P [ i ] with P [ RANDOM ( i , n )]","title":"5.3-6"},{"location":"Chap05/5.3/#53-7","text":"Suppose we want to create a random sample of the set $\\{1, 2, 3, \\ldots, n\\}$, that is, an $m$-element subset $S$, where $0 \\le m \\le n$, such that each $m$-subset is equally likely to be created. One way would be to set $A[i] = i$ for $i = 1, 2, 3, \\ldots, n$, call $\\text{RANDOMIZE-IN-PLACE}(A)$, and then take just the first $m$ array elements. This method would make $n$ calls to the $\\text{RANDOM}$ procedure. If $n$ is much larger than $m$, we can create a random sample with fewer calls to $\\text{RANDOM}$. Show that the following recursive procedure returns a random $m$-subset $S$ of $\\{1, 2, 3, \\ldots, n\\}$, in which each $m$-subset is equally likely, while making only $m$ calls to $\\text{RANDOM}$: RANDOM - SAMPLE ( m , n ) if m == 0 return \u00d8 else S = RANDOM - SAMPLE ( m - 1 , n - 1 ) i = RANDOM ( 1 , n ) if i \u2208 S S = S \u222a { n } else S = S \u222a { i } return S We prove that it produces a random $m$ subset by induction on $m$. It is obviously true if $m = 0$ as there is only one size $m$ subset of $[n]$. Suppose $S$ is a uniform $m \u2212 1$ subset of $n \u2212 1$, that is, $\\forall j \\in [n - 1]$, $\\Pr[j \\in S] = \\frac{m - 1}{n - 1}$. If we let $S'$ denote the returned set, suppose first $j \\in [n \u2212 1]$, $$ \\begin{aligned} \\Pr[j \\in S'] & = \\Pr[j \\in S] + \\Pr[j \\notin S \\wedge i = j] \\\\ & = \\frac{m - 1}{n - 1} + \\Pr[j \\notin S]\\Pr[i = j] \\\\ & = \\frac{m - 1}{n - 1} + \\left(1 - \\frac{m - 1}{n - 1}\\right) \\frac{1}{n} \\\\ & = \\frac{n(m - 1) + n - m}{(n - 1)n} \\\\ & = \\frac{nm - m}{(n - 1)n} = \\frac{m}{n}. \\end{aligned} $$ Since the constructed subset contains each of $[n \u2212 1]$ with the correct probability, it must also contain $n$ with the correct probability because the probabilities sum to $1$.","title":"5.3-7"},{"location":"Chap05/5.4/","text":"5.4-1 How many people must there be in a room before the probability that someone has the same birthday as you do is at least $1 / 2$? How many people must there be before the probability that at least two people have a birthday on July 4 is greater than $1 / 2$? The probability of a person not having the same birthday as me is $(n - 1) / n$. The probability of $k$ people not having the same birthday as me is that, squared. We apply the same approach as the text - we take the complementary event and solve it for $k$, $$ \\begin{aligned} 1 - \\big(\\frac{n - 1}{k}\\big)^k & \\ge \\frac{1}{2} \\\\ \\big(\\frac{n - 1}{k}\\big)^k & \\le \\frac{1}{2} \\\\ k\\lg\\big(\\frac{n - 1}{n}\\big) & \\ge \\lg\\frac{1}{2} \\\\ k = \\frac{\\log(1 / 2)}{\\log(364 / 365)} & \\approx 253. \\end{aligned} $$ As for the other question, $$ \\begin{aligned} \\Pr\\{\\text{2 born on Jul 4}\\} & = 1 - \\Pr\\{\\text{1 born on Jul 4}\\} - \\Pr\\{\\text{0 born on Jul 4}\\} \\\\ & = 1 - \\frac{k}{n}\\big(\\frac{n - 1}{n}\\big)^{k - 1} - \\big(\\frac{n - 1}{n}\\big)^k \\\\ & = 1 - \\big(\\frac{n - 1}{n}\\big)^{k - 1}\\big(\\frac{n + k - 1}{n}\\big). \\end{aligned} $$ Writing a Ruby programme to find the closest integer, we get $115$. 5.4-2 Suppose that we toss balls into $b$ bins until some bin contains two balls. Each toss is independent, and each ball is equally likely to end up in any bin. What is the expected number of ball tosses? This is just a restatement of the birthday problem. I consider this all that needs to be said on this subject. 5.4-3 $\\star$ For the analysis of the birthday paradox, is it important that the birthdays be mutually independent, or is pairwise independence sufficient? Justify your answer. Pairwise independence is enough. It's sufficient for the derivation after $\\text{(5.6)}$. 5.4-4 $\\star$ How many people should be invited to a party in order to make it likely that there are $three$ people with the same birthday? The answer is $88$. I reached it by trial and error. But let's analyze it with indicator random variables. Let $X_{ijk}$ be the indicator random variable for the event of the people with indices $i$, $j$ and $k$ have the same birthday. The probability is $1 / n^2$. Then, $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n X_{ijk} \\\\ & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n \\frac{1}{n^2} \\\\ & = \\binom{n}{3}\\frac{1}{n^2} \\\\ & = \\frac{k(k - 1)(k - 2)}{6n^2}. \\end{aligned} $$ Solving this yields $94$. It's a bit more, but again, indicator random variables are approximate. Finding more commentary online is tricky. 5.4-5 $\\star$ What is the probability that a $k$-string over a set of size $n$ forms a $k$-permutation? How does this question relate to the birthday paradox? $$ \\begin{aligned} \\Pr\\{k\\text{-perm in }n\\} & = 1 \\cdot \\frac{n - 1}{n} \\cdot \\frac{n - 2}{n} \\cdots \\frac{n - k + 1}{n} \\\\ & = \\frac{(n - 1)!}{(n - k)!n^k}. \\end{aligned} $$ This is the complementary event to the birthday problem, that is, the chance of $k$ people have distinct birthdays. 5.4-6 $\\star$ Suppose that $n$ balls are tossed into $n$ bins, where each toss is independent and the ball is equally likely to end up in any bin. What is the expected number of empty bins? What is the expected number of bins with exactly one ball? Let $X_i$ be the indicator variable that bin $i$ is empty after all balls are tossed and $X$ be the random variable that gives the number of empty bins. Thus we have $$E[X] = \\sum_{i = 1}^n E[X_i] = \\sum_{i = 1}^n \\bigg(\\frac{n - 1}{n}\\bigg)^n = n\\bigg(\\frac{n - 1}{n}\\bigg)^n.$$ Let $X_i$ be the indicator variable that bin $i$ contains exactly $1$ ball after all balls are tossed and $X$ be the random variable that gives the number of bins containing exactly $1$ ball. Thus we have $$E[X] = \\sum_{i = 1}^n E[X_i] = \\sum_{i = 1}^n \\binom{n}{1}\\bigg(\\frac{n - 1}{n}\\bigg)^{n - 1} \\frac{1}{n} = n\\bigg(\\frac{n - 1}{n}\\bigg)^{n - 1},$$ because we need to choose which toss will go into bin $i$, then multiply by the probability that that toss goes into that bin and the remaining $n \u2212 1$ tosses avoid it. 5.4-7 $\\star$ Sharpen the lower bound on streak length by showing that in $n$ flips of a fair coin, the probability is less than $1 / n$ that no streak longer than $\\lg n - 2\\lg\\lg n$ consecutive heads occurs. We split up the n flips into $n / s$ groups where we pick $s = \\lg(n) - 2 \\lg(\\lg(n))$. We will show that at least one of these groups comes up all heads with probability at least $\\frac{n - 1}{n}$. So, the probability the group starting in position $i$ comes up all heads is $$\\Pr(A_{i,\\lg n - 2\\lg(\\lg n)}) = \\frac{1}{2^{\\lg n - 2\\lg(\\lg n)}} = \\frac{\\lg n^2}{n}.$$ Since the groups are based of of disjoint sets of IID coin flips, these probabilities are independent. so, $$ \\begin{aligned} \\Pr(\\bigwedge\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) & = \\prod_i\\Pr(\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) \\\\ & = \\Big(1-\\frac{\\lg n^2}{n}\\Big)^{\\frac{n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & \\le e^{-\\frac{\\lg n^2}{\\lg n - 2\\lg(\\lg n)}} \\\\ &= \\frac{1}{n} e^{\\frac{-2\\lg(\\lg n)\\lg n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & = n^{-1-\\frac{2\\lg(\\lg n)}{\\lg n - 2\\lg(\\lg n)}} \\\\ & < n^{-1}. \\end{aligned} $$ Showing that the probability that there is no run of length at least $\\lg n - 2\\lg(\\lg n)$ to be $< \\frac{1}{n}$.","title":"5.4 Probabilistic analysis and further uses of indicator random variables"},{"location":"Chap05/5.4/#54-1","text":"How many people must there be in a room before the probability that someone has the same birthday as you do is at least $1 / 2$? How many people must there be before the probability that at least two people have a birthday on July 4 is greater than $1 / 2$? The probability of a person not having the same birthday as me is $(n - 1) / n$. The probability of $k$ people not having the same birthday as me is that, squared. We apply the same approach as the text - we take the complementary event and solve it for $k$, $$ \\begin{aligned} 1 - \\big(\\frac{n - 1}{k}\\big)^k & \\ge \\frac{1}{2} \\\\ \\big(\\frac{n - 1}{k}\\big)^k & \\le \\frac{1}{2} \\\\ k\\lg\\big(\\frac{n - 1}{n}\\big) & \\ge \\lg\\frac{1}{2} \\\\ k = \\frac{\\log(1 / 2)}{\\log(364 / 365)} & \\approx 253. \\end{aligned} $$ As for the other question, $$ \\begin{aligned} \\Pr\\{\\text{2 born on Jul 4}\\} & = 1 - \\Pr\\{\\text{1 born on Jul 4}\\} - \\Pr\\{\\text{0 born on Jul 4}\\} \\\\ & = 1 - \\frac{k}{n}\\big(\\frac{n - 1}{n}\\big)^{k - 1} - \\big(\\frac{n - 1}{n}\\big)^k \\\\ & = 1 - \\big(\\frac{n - 1}{n}\\big)^{k - 1}\\big(\\frac{n + k - 1}{n}\\big). \\end{aligned} $$ Writing a Ruby programme to find the closest integer, we get $115$.","title":"5.4-1"},{"location":"Chap05/5.4/#54-2","text":"Suppose that we toss balls into $b$ bins until some bin contains two balls. Each toss is independent, and each ball is equally likely to end up in any bin. What is the expected number of ball tosses? This is just a restatement of the birthday problem. I consider this all that needs to be said on this subject.","title":"5.4-2"},{"location":"Chap05/5.4/#54-3-star","text":"For the analysis of the birthday paradox, is it important that the birthdays be mutually independent, or is pairwise independence sufficient? Justify your answer. Pairwise independence is enough. It's sufficient for the derivation after $\\text{(5.6)}$.","title":"5.4-3 $\\star$"},{"location":"Chap05/5.4/#54-4-star","text":"How many people should be invited to a party in order to make it likely that there are $three$ people with the same birthday? The answer is $88$. I reached it by trial and error. But let's analyze it with indicator random variables. Let $X_{ijk}$ be the indicator random variable for the event of the people with indices $i$, $j$ and $k$ have the same birthday. The probability is $1 / n^2$. Then, $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n X_{ijk} \\\\ & = \\sum_{i = 1}^n\\sum_{j = i + 1}^n \\sum_{k = j + 1}^n \\frac{1}{n^2} \\\\ & = \\binom{n}{3}\\frac{1}{n^2} \\\\ & = \\frac{k(k - 1)(k - 2)}{6n^2}. \\end{aligned} $$ Solving this yields $94$. It's a bit more, but again, indicator random variables are approximate. Finding more commentary online is tricky.","title":"5.4-4 $\\star$"},{"location":"Chap05/5.4/#54-5-star","text":"What is the probability that a $k$-string over a set of size $n$ forms a $k$-permutation? How does this question relate to the birthday paradox? $$ \\begin{aligned} \\Pr\\{k\\text{-perm in }n\\} & = 1 \\cdot \\frac{n - 1}{n} \\cdot \\frac{n - 2}{n} \\cdots \\frac{n - k + 1}{n} \\\\ & = \\frac{(n - 1)!}{(n - k)!n^k}. \\end{aligned} $$ This is the complementary event to the birthday problem, that is, the chance of $k$ people have distinct birthdays.","title":"5.4-5 $\\star$"},{"location":"Chap05/5.4/#54-6-star","text":"Suppose that $n$ balls are tossed into $n$ bins, where each toss is independent and the ball is equally likely to end up in any bin. What is the expected number of empty bins? What is the expected number of bins with exactly one ball? Let $X_i$ be the indicator variable that bin $i$ is empty after all balls are tossed and $X$ be the random variable that gives the number of empty bins. Thus we have $$E[X] = \\sum_{i = 1}^n E[X_i] = \\sum_{i = 1}^n \\bigg(\\frac{n - 1}{n}\\bigg)^n = n\\bigg(\\frac{n - 1}{n}\\bigg)^n.$$ Let $X_i$ be the indicator variable that bin $i$ contains exactly $1$ ball after all balls are tossed and $X$ be the random variable that gives the number of bins containing exactly $1$ ball. Thus we have $$E[X] = \\sum_{i = 1}^n E[X_i] = \\sum_{i = 1}^n \\binom{n}{1}\\bigg(\\frac{n - 1}{n}\\bigg)^{n - 1} \\frac{1}{n} = n\\bigg(\\frac{n - 1}{n}\\bigg)^{n - 1},$$ because we need to choose which toss will go into bin $i$, then multiply by the probability that that toss goes into that bin and the remaining $n \u2212 1$ tosses avoid it.","title":"5.4-6 $\\star$"},{"location":"Chap05/5.4/#54-7-star","text":"Sharpen the lower bound on streak length by showing that in $n$ flips of a fair coin, the probability is less than $1 / n$ that no streak longer than $\\lg n - 2\\lg\\lg n$ consecutive heads occurs. We split up the n flips into $n / s$ groups where we pick $s = \\lg(n) - 2 \\lg(\\lg(n))$. We will show that at least one of these groups comes up all heads with probability at least $\\frac{n - 1}{n}$. So, the probability the group starting in position $i$ comes up all heads is $$\\Pr(A_{i,\\lg n - 2\\lg(\\lg n)}) = \\frac{1}{2^{\\lg n - 2\\lg(\\lg n)}} = \\frac{\\lg n^2}{n}.$$ Since the groups are based of of disjoint sets of IID coin flips, these probabilities are independent. so, $$ \\begin{aligned} \\Pr(\\bigwedge\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) & = \\prod_i\\Pr(\\neg A_{i,\\lg n - 2\\lg(\\lg n)}) \\\\ & = \\Big(1-\\frac{\\lg n^2}{n}\\Big)^{\\frac{n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & \\le e^{-\\frac{\\lg n^2}{\\lg n - 2\\lg(\\lg n)}} \\\\ &= \\frac{1}{n} e^{\\frac{-2\\lg(\\lg n)\\lg n}{\\lg n - 2\\lg(\\lg n)}} \\\\ & = n^{-1-\\frac{2\\lg(\\lg n)}{\\lg n - 2\\lg(\\lg n)}} \\\\ & < n^{-1}. \\end{aligned} $$ Showing that the probability that there is no run of length at least $\\lg n - 2\\lg(\\lg n)$ to be $< \\frac{1}{n}$.","title":"5.4-7 $\\star$"},{"location":"Chap05/Problems/5-1/","text":"With a $b$-bit counter, we can ordinarily only count up to $2^b - 1$. With R. Morris's probabilistic counting , we can count up to a much larger value at the expense of some loss of precision. We let a counter value of $i$ represent that a count of $n_i$ for $i = 0, 1, \\ldots, 2^b - 1$, where the $n_i$ form an increasing sequence of nonnegative values. We assume that the initial value of the counter is $0$, representing a count of $n_0 = 0$. The $\\text{INCREMENT}$ operation works on a counter containing the value $i$ in a probabilistic manner. If $i = 2^b - 1$, then the operation reports an overflow error. Otherwise, the $\\text{INCREMENT}$ operation increases the counter by $1$ with probability $1 / (n_{i + 1} - n_i)$, and it leaves the counter unchanged with probability $1 - 1 / (n_{i + 1} - n_i)$. If we select $n_i = i$ for all $i \\ge 0$, then the counter is an ordinary one. More interesting situations arise if we select, say, $n_i = 2^{i - 1}$ for $i > 0$ or $n_i = F_i$ (the $i$th Fibonacci number\u2014see Section 3.2). For this problem, assume that $n_{2^b - 1}$ is large enough that the probability of an overflow error is negligible. a. Show that the expected value represented by the counter after $n$ $\\text{INCREMENT}$ operations have been performed is exactly $n$. b. The analysis of the variance of the count represented by the counter depends on the sequence of the $n_i$. Let us consider a simple case: $n_i = 100i$ for all $i \\ge 0$. Estimate the variance in the value represented by the register after $n$ $\\text{INCREMENT}$ operations have been performed. a. To show that the expected value represented by the counter after $n$ $\\text{INCREMENT}$ operations have been performed is exactly $n$, we can show that each expected increment represented by the counter is $1$. Assume the initial value of the counter is $i$, increasing the number represented from $n_i$ to $n_{i + 1}$ is with a probability of $\\frac{1}{n_{i + 1} - n_i}$ and leaving the value not changed otherwise. The expected increase: $$ \\frac{n_{i + 1} - n_i}{n_{i + 1} - n_i} = 1. $$ b. For this choice of $n_i$ , we have that at each increment operation, the probability that we change the value of the counter is $\\frac{1}{100}$. Since this is a constant with respect to the current value of the counter $i$, we can view the final result as a binomial distribution with a $p$ value of $0.01$. Since the variance of a binomial distribution is $np(1 \u2212 p)$, and we have that each success is worth $100$ instead, the variance is going to be equal to $0.99n$.","title":"5-1 Probabilstic counting"},{"location":"Chap05/Problems/5-2/","text":"The problem examines three algorithms for searching for a value $x$ in an unsorted array $A$ consisting for $n$ elements. Consider the following randomized strategy: pick a random index $i$ into $A$. If $A[i] = x$, then we terminate; otherwise, we continue the search by picking a new random index into $A$. We continue picking random indices into $A$ until we find an index $j$ such that $A[j] = x$ or until we have checked every element of $A$. Note that we pick from the whole set of indices each time, so that we may examine a given element more than once. a. Write pseudocode for a procedure $\\text{RANDOM-SEARCH}$ to implement the strategy above. Be sure that your algorithm terminates when all indices into $A$ have been picked. b. Suppose that there is exactly one index $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that we must pick before we find $x$ and $\\text{RANDOM-SEARCH}$ terminates? c. Generalizing your solution to part (b), suppose that there are $k \\ge 1$ indices $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that we must pick before we find $x$ and $\\text{RANDOM-SEARCH}$ terminates? Your answer should be a function of $n$ and $k$. d. Suppose that there are no indices $i$ such that $A[i] = x$. What is the expected number of indices into $A$ that we must pick before we have checked all elements of $A$ and $\\text{RANDOM-SEARCH}$ terminates? Now consider a deterministic linear search algorithm, which we refer to as $\\text{DETERMINISTIC-SEARCH}$. Specifically, the algorithm searches $A$ for $x$ in order, considering $A[1], A[2], A[3], \\ldots, A[n]$ until either it finds $A[i] = x$ or it reaches the end of the array. Assume that possible permutations of the input array are equally likely. e. Suppose that there is exactly one index $i$ such that $A[i] = x$. What is the average-case running time of $\\text{DETERMINISTIC-SEARCH}$? What is the worst-case running time of $\\text{DETERMINISTIC-SEARCH}$? f. Generalizing your solution to part (e), suppose that there are $k \\ge 1$ indices $i$ such that $A[i] = x$. What is the average-case running time of $\\text{DETERMINISTIC-SEARCH}$? What is the worst-case running time of $\\text{DETERMINISTIC-SEARCH}$? Your answer should be a function of $n$ and $k$. g. Suppose that there are no indices $i$ such that $A[i] = x$. What is the average-case running time of $\\text{DETERMINISTIC-SEARCH}$? What is the worst-case running time of $\\text{DETERMINISTIC-SEARCH}$? Finally, consider a randomized algorithm $\\text{SCRAMBLE-SEARCH}$ that works by first randomly permuting the input array and then running the deterministic linear search given above on the resulting permuting array. h. Letting $k$ be the number of indices $i$ such that $A[i] = x$, give the worst-case and expected running time of $\\text{SCRAMBLE-SEARCH}$ for the cases in which $k = 0$ and $k = 1$. Generalizing your solution to handle the case in which $k \\ge 1$. i. Which of the three searching algorithms would you use? Explain your answer. a. RANDOM - SEARCH ( x , A , n ) v = \u00d8 while | \u00d8 | != n i = RANDOM ( 1 , n ) if A [ i ] = x return i else v = v \u2229 i return NIL $v$ can be implemented in multiple ways: a hash table, a tree or a bitmap. The last one would probabily perform best and consume the least space. b. $\\text{RANDOM-SEARCH}$ is well-modelled by Bernoulli trials. The expected number of picks is $n$. c. In similar fashion, the expected number of picks is $n / k$. d. This is modelled by the balls and bins problem, explored in section 5.4.2. The answer is $n(\\ln n + O(1))$. e. The worst-case running time is $n$. The average-case is $(n + 1) / 2$ (obviously). f. The worst-case running time is $n - k + 1$. The average-case running time is $(n + 1) / (k + 1)$. Let $X_i$ be an indicator random variable that the $i$th element is a match. $\\Pr\\{X_i\\} = 1 / (k + 1)$. Let $Y$ be an indicator random variable that we have found a match after the first $n - k + 1$ elements ($\\Pr\\{Y\\} = 1$). Thus, $$ \\begin{aligned} \\text E[X] & = \\text E[X_1 + X_2 + \\ldots + X_{n - k} + Y] \\\\ & = 1 + \\sum_{i = 1}^{n - k}\\text E[X_i] = 1 + \\frac{n - k}{k + 1} \\\\ & = \\frac{n + 1}{k + 1}. \\end{aligned} $$ g. Both the worst-case and average case is $n$. h. It's the same as $\\text{DETERMINISTIC-SEARCH}$, only we replace \"average-case\" with \"expected\". i. Definitelly $\\text{DETERMINISTIC-SEARCH}$. $\\text{SCRAMBLE-SEARCH}$ gives better expected results, but for the cost of randomly permuting the array, which is a linear operation. In the same time we could have scanned the full array and reported a result.","title":"5-2 Searching an unsorted array"},{"location":"Chap06/6.1/","text":"6.1-1 What are the minimum and maximum numbers of elements in a heap of height $h$? At least $2^h$ and at most $2^{h + 1} \u2212 1$. Can be seen because a complete binary tree of depth $h \u2212 1$ has $\\sum_{i = 0}^{h - 1} 2^i = 2^h - 1$ elements, and the number of elements in a heap of depth $h$ is between the number for a complete binary tree of depth $h \u2212 1$ exclusive and the number in a complete binary tree of depth $h$ inclusive. 6.1-2 Show that an $n$-element heap has height $\\lfloor \\lg n \\rfloor$. Write $n = 2^m \u2212 1 + k$ where $m$ is as large as possible. Then the heap consists of a complete binary tree of height $m \u2212 1$, along with $k$ additional leaves along the bottom. The height of the root is the length of the longest simple path to one of these $k$ leaves, which must have length $m$. It is clear from the way we defined $m$ that $m = \\lfloor \\lg n\\rfloor$. 6.1-3 Show that in any subtree of a max-heap, the root of the subtree contains the largest value occuring anywhere in the subtree. If the largest element in the subtree were somewhere other than the root, it has a parent that is in the subtree. So, it is larger than it's parent, so, the heap property is violated at the parent of the maximum element in the subtree. 6.1-4 Where in a max-heap might the smallest element reside, assuming that all elements are distinct? In any of the leaves, that is, elements with index $\\lfloor n / 2 \\rfloor + k$, where $k \\geq 1$ (see exercise 6.1-7), that is, in the second half of the heap array. 6.1-5 Is an array that is in sorted order a min-heap? Yes. For any index $i$, both $\\text{LEFT}(i)$ and $\\text{RIGHT}(i)$ are larger and thus the elements indexed by them are greater or equal to $A[i]$ (because the array is sorted.) 6.1-6 Is the array with values $\\langle 23, 17, 14, 6, 13, 10, 1, 5, 7, 12 \\rangle$ a max-heap? No. Since $\\text{PARENT}(7)$ is $6$ in the array. This violates the max-heap property. 6.1-7 Show that, with the array representation for sorting an $n$-element heap, the leaves are the nodes indexed by $\\lfloor n / 2 \\rfloor + 1, \\lfloor n / 2 \\rfloor + 2, \\ldots, n$. Let's take the left child of the node indexed by $\\lfloor n / 2 \\rfloor + 1$. $$ \\begin{aligned} \\text{LEFT}(\\lfloor n / 2 \\rfloor + 1) & = 2(\\lfloor n / 2 \\rfloor + 1) \\\\ & > 2(n / 2 - 1) + 2 \\\\ & = n - 2 + 2 \\\\ & = n. \\end{aligned} $$ Since the index of the left child is larger than the number of elements in the heap, the node doesn't have childrens and thus is a leaf. Same goes for all nodes with larger indices. Note that if we take element indexed by $\\lfloor n / 2 \\rfloor$, it will not be a leaf. In case of even number of nodes, it will have a left child with index $n$ and in the case of odd number of nodes, it will have a left child with index $n - 1$ and a right child with index $n$. This makes the number of leaves in a heap of size $n$ equal to $\\lceil n / 2 \\rceil$.","title":"6.1 Heaps"},{"location":"Chap06/6.1/#61-1","text":"What are the minimum and maximum numbers of elements in a heap of height $h$? At least $2^h$ and at most $2^{h + 1} \u2212 1$. Can be seen because a complete binary tree of depth $h \u2212 1$ has $\\sum_{i = 0}^{h - 1} 2^i = 2^h - 1$ elements, and the number of elements in a heap of depth $h$ is between the number for a complete binary tree of depth $h \u2212 1$ exclusive and the number in a complete binary tree of depth $h$ inclusive.","title":"6.1-1"},{"location":"Chap06/6.1/#61-2","text":"Show that an $n$-element heap has height $\\lfloor \\lg n \\rfloor$. Write $n = 2^m \u2212 1 + k$ where $m$ is as large as possible. Then the heap consists of a complete binary tree of height $m \u2212 1$, along with $k$ additional leaves along the bottom. The height of the root is the length of the longest simple path to one of these $k$ leaves, which must have length $m$. It is clear from the way we defined $m$ that $m = \\lfloor \\lg n\\rfloor$.","title":"6.1-2"},{"location":"Chap06/6.1/#61-3","text":"Show that in any subtree of a max-heap, the root of the subtree contains the largest value occuring anywhere in the subtree. If the largest element in the subtree were somewhere other than the root, it has a parent that is in the subtree. So, it is larger than it's parent, so, the heap property is violated at the parent of the maximum element in the subtree.","title":"6.1-3"},{"location":"Chap06/6.1/#61-4","text":"Where in a max-heap might the smallest element reside, assuming that all elements are distinct? In any of the leaves, that is, elements with index $\\lfloor n / 2 \\rfloor + k$, where $k \\geq 1$ (see exercise 6.1-7), that is, in the second half of the heap array.","title":"6.1-4"},{"location":"Chap06/6.1/#61-5","text":"Is an array that is in sorted order a min-heap? Yes. For any index $i$, both $\\text{LEFT}(i)$ and $\\text{RIGHT}(i)$ are larger and thus the elements indexed by them are greater or equal to $A[i]$ (because the array is sorted.)","title":"6.1-5"},{"location":"Chap06/6.1/#61-6","text":"Is the array with values $\\langle 23, 17, 14, 6, 13, 10, 1, 5, 7, 12 \\rangle$ a max-heap? No. Since $\\text{PARENT}(7)$ is $6$ in the array. This violates the max-heap property.","title":"6.1-6"},{"location":"Chap06/6.1/#61-7","text":"Show that, with the array representation for sorting an $n$-element heap, the leaves are the nodes indexed by $\\lfloor n / 2 \\rfloor + 1, \\lfloor n / 2 \\rfloor + 2, \\ldots, n$. Let's take the left child of the node indexed by $\\lfloor n / 2 \\rfloor + 1$. $$ \\begin{aligned} \\text{LEFT}(\\lfloor n / 2 \\rfloor + 1) & = 2(\\lfloor n / 2 \\rfloor + 1) \\\\ & > 2(n / 2 - 1) + 2 \\\\ & = n - 2 + 2 \\\\ & = n. \\end{aligned} $$ Since the index of the left child is larger than the number of elements in the heap, the node doesn't have childrens and thus is a leaf. Same goes for all nodes with larger indices. Note that if we take element indexed by $\\lfloor n / 2 \\rfloor$, it will not be a leaf. In case of even number of nodes, it will have a left child with index $n$ and in the case of odd number of nodes, it will have a left child with index $n - 1$ and a right child with index $n$. This makes the number of leaves in a heap of size $n$ equal to $\\lceil n / 2 \\rceil$.","title":"6.1-7"},{"location":"Chap06/6.2/","text":"6.2-1 Using figure 6.2 as a model, illustrate the operation of $\\text{MAX-HEAPIFY}(A, 3)$ on the array $A = \\langle 27, 17, 3, 16, 13, 10, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle$. $$ \\begin{aligned} \\langle 27, 17, 3, 16, 13, 10,1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 3, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 9, 1, 5, 7, 12, 4, 8, 3, 0 \\rangle \\\\ \\end{aligned} $$ 6.2-2 Starting with the procedure $\\text{MAX-HEAPIFY}$, write pseudocode for the procedure $\\text{MIN-HEAPIFY}(A, i)$, which performs the corresponding manipulation on a min-heap. How does the running time of $\\text{MIN-HEAPIFY}$ compare to that of $\\text{MAX-HEAPIFY}$? MIN - HEAPIFY ( A , i ) l = LEFT ( i ) r = RIGHT ( i ) if l \u2264 A . heap - size and A [ l ] < A [ i ] smallest = l else smallest = i if r \u2264 A . heap - size and A [ r ] < A [ smallest ] smallest = r if smallest != i exchange A [ i ] with A [ smallest ] MIN - HEAPIFY ( A , smallest ) The running time is the same. Actually, the algorithm is the same with the exceptions of two comparisons and some names. 6.2-3 What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ when the element $A[i]$ is larger than its children? No effect. The comparisons are carried out, $A[i]$ is found to be largest and the procedure just returns. 6.2-4 What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ for $i > A.heap\\text-size / 2$? No effect. In that case, it is a leaf. Both $\\text{LEFT}$ and $\\text{RIGHT}$ return values that fail the comparison with the heap size and $i$ is stored in largest. Afterwards the procedure just returns. 6.2-5 The code for $\\text{MAX-HEAPIFY}$ is quite efficient in terms of constant factors, except possibly for the recursive call in line 10, which might cause some compilers to produce inefficient code. Write an efficient $\\text{MAX-HEAPIFY}$ that uses an iterative control construct (a loop) instead of recursion. MAX - HEAPIFY ( A , i ) while true l = LEFT ( i ) r = RIGHT ( i ) if l \u2264 A . heap - size and A [ l ] > A [ i ] largest = l else largest = i if r \u2264 A . heap - size and A [ r ] > A [ largest ] largest = r if largest == i return exchange A [ i ] with A [ largest ] i = largest 6.2-6 Show that the worst-case running time of $\\text{MAX-HEAPIFY}$ on a heap of size $n$ is $\\Omega(\\lg n)$. ($\\textit{Hint:}$ For a heap with $n$ nodes, give node values that cause $\\text{MAX-HEAPIFY}$ to be called recursively at every node on a simple path from the root down to a leaf.) Consider the heap resulting from $A$ where $A[1] = 1$ and $A[i] = 2$ for $2 \\le i \\le n$. Since $1$ is the smallest element of the heap, it must be swapped through each level of the heap until it is a leaf node. Since the heap has height $\\lfloor \\lg n\\rfloor$, $\\text{MAX-HEAPIFY}$ has worst-case time $\\Omega(\\lg n)$.","title":"6.2 Maintaining the heap property"},{"location":"Chap06/6.2/#62-1","text":"Using figure 6.2 as a model, illustrate the operation of $\\text{MAX-HEAPIFY}(A, 3)$ on the array $A = \\langle 27, 17, 3, 16, 13, 10, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle$. $$ \\begin{aligned} \\langle 27, 17, 3, 16, 13, 10,1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 3, 1, 5, 7, 12, 4, 8, 9, 0 \\rangle \\\\ \\langle 27, 17, 10, 16, 13, 9, 1, 5, 7, 12, 4, 8, 3, 0 \\rangle \\\\ \\end{aligned} $$","title":"6.2-1"},{"location":"Chap06/6.2/#62-2","text":"Starting with the procedure $\\text{MAX-HEAPIFY}$, write pseudocode for the procedure $\\text{MIN-HEAPIFY}(A, i)$, which performs the corresponding manipulation on a min-heap. How does the running time of $\\text{MIN-HEAPIFY}$ compare to that of $\\text{MAX-HEAPIFY}$? MIN - HEAPIFY ( A , i ) l = LEFT ( i ) r = RIGHT ( i ) if l \u2264 A . heap - size and A [ l ] < A [ i ] smallest = l else smallest = i if r \u2264 A . heap - size and A [ r ] < A [ smallest ] smallest = r if smallest != i exchange A [ i ] with A [ smallest ] MIN - HEAPIFY ( A , smallest ) The running time is the same. Actually, the algorithm is the same with the exceptions of two comparisons and some names.","title":"6.2-2"},{"location":"Chap06/6.2/#62-3","text":"What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ when the element $A[i]$ is larger than its children? No effect. The comparisons are carried out, $A[i]$ is found to be largest and the procedure just returns.","title":"6.2-3"},{"location":"Chap06/6.2/#62-4","text":"What is the effect of calling $\\text{MAX-HEAPIFY}(A, i)$ for $i > A.heap\\text-size / 2$? No effect. In that case, it is a leaf. Both $\\text{LEFT}$ and $\\text{RIGHT}$ return values that fail the comparison with the heap size and $i$ is stored in largest. Afterwards the procedure just returns.","title":"6.2-4"},{"location":"Chap06/6.2/#62-5","text":"The code for $\\text{MAX-HEAPIFY}$ is quite efficient in terms of constant factors, except possibly for the recursive call in line 10, which might cause some compilers to produce inefficient code. Write an efficient $\\text{MAX-HEAPIFY}$ that uses an iterative control construct (a loop) instead of recursion. MAX - HEAPIFY ( A , i ) while true l = LEFT ( i ) r = RIGHT ( i ) if l \u2264 A . heap - size and A [ l ] > A [ i ] largest = l else largest = i if r \u2264 A . heap - size and A [ r ] > A [ largest ] largest = r if largest == i return exchange A [ i ] with A [ largest ] i = largest","title":"6.2-5"},{"location":"Chap06/6.2/#62-6","text":"Show that the worst-case running time of $\\text{MAX-HEAPIFY}$ on a heap of size $n$ is $\\Omega(\\lg n)$. ($\\textit{Hint:}$ For a heap with $n$ nodes, give node values that cause $\\text{MAX-HEAPIFY}$ to be called recursively at every node on a simple path from the root down to a leaf.) Consider the heap resulting from $A$ where $A[1] = 1$ and $A[i] = 2$ for $2 \\le i \\le n$. Since $1$ is the smallest element of the heap, it must be swapped through each level of the heap until it is a leaf node. Since the heap has height $\\lfloor \\lg n\\rfloor$, $\\text{MAX-HEAPIFY}$ has worst-case time $\\Omega(\\lg n)$.","title":"6.2-6"},{"location":"Chap06/6.3/","text":"6.3-1 Using figure 6.3 as a model, illustrate the operation of $\\text{BUILD-MAX-HEAP}$ on the array $A = \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle$. $$ \\begin{aligned} \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle \\\\ \\langle 5, 3, 17, 22, 84, 19, 6, 10, 9 \\rangle \\\\ \\langle 5, 3, 19, 22, 84, 17, 6, 10, 9 \\rangle \\\\ \\langle 5, 84, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 5, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 5, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 10, 3, 17, 6, 5, 9 \\rangle \\\\ \\end{aligned} $$ 6.3-2 Why do we want the loop index $i$ in line 2 of $\\text{BUILD-MAX-HEAP}$ to decrease from $\\lfloor A.length / 2 \\rfloor$ to $1$ rather than increase from $1$ to $\\lfloor A.length/2 \\rfloor$? Otherwise we won't be allowed to call $\\text{MAX-HEAPIFY}$, since it will fail the condition of having the subtrees be max-heaps. That is, if we start with $1$, there is no guarantee that $A[2]$ and $A[3]$ are roots of max-heaps. 6.3-3 Show that there are at most $\\lceil n / 2^{h + 1} \\rceil$ nodes of height $h$ in any $n$-element heap. From 6.1-7, we know that the leaves of a heap are the nodes indexed by $$\\left\\lfloor n / 2 \\right\\rfloor + 1, \\left\\lfloor n / 2 \\right\\rfloor + 2, \\dots, n.$$ Note that those elements corresponds to the second half of the heap array (plus the middle element if $n$ is odd). Thus, the number of leaves in any heap of size $n$ is $\\left\\lceil n / 2 \\right\\rceil$. Let's prove by induction. Let $n_h$ denote the number of nodes at height $h$. The upper bound holds for the base since $n_0 = \\left\\lceil n / 2 \\right\\rceil$ is exactly the number of leaves in a heap of size $n$. Now assume it holds for $h \u2212 1$. We have prove that it also holds for $h$. Note that if $n_{h - 1}$ is even each node at height $h$ has exactly two children, which implies $n_h = n_{h - 1} / 2 = \\left\\lfloor n_{h - 1} / 2 \\right\\rfloor$. If $n_{h - 1}$ is odd, one node at height $h$ has one child and the remaining has two children, which also implies $n_h = \\left\\lfloor n_{h - 1} / 2 \\right\\rfloor + 1 = \\left\\lceil n_{h - 1} / 2 \\right\\rceil$. Thus, we have $$ \\begin{aligned} n_h & = \\left\\lceil \\frac{n_{h - 1}}{2} \\right\\rceil \\\\ & \\le \\left\\lceil \\frac{1}{2} \\cdot \\left\\lceil \\frac{n}{2^{(h - 1) + 1}} \\right\\rceil \\right\\rceil \\\\ & = \\left\\lceil \\frac{1}{2} \\cdot \\left\\lceil \\frac{n}{2^h} \\right\\rceil \\right\\rceil \\\\ & = \\left\\lceil \\frac{n}{2^{h + 1}} \\right\\rceil, \\end{aligned} $$ which implies that it holds for $h$.","title":"6.3 Building a heap"},{"location":"Chap06/6.3/#63-1","text":"Using figure 6.3 as a model, illustrate the operation of $\\text{BUILD-MAX-HEAP}$ on the array $A = \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle$. $$ \\begin{aligned} \\langle 5, 3, 17, 10, 84, 19, 6, 22, 9 \\rangle \\\\ \\langle 5, 3, 17, 22, 84, 19, 6, 10, 9 \\rangle \\\\ \\langle 5, 3, 19, 22, 84, 17, 6, 10, 9 \\rangle \\\\ \\langle 5, 84, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 5, 19, 22, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 5, 3, 17, 6, 10, 9 \\rangle \\\\ \\langle 84, 22, 19, 10, 3, 17, 6, 5, 9 \\rangle \\\\ \\end{aligned} $$","title":"6.3-1"},{"location":"Chap06/6.3/#63-2","text":"Why do we want the loop index $i$ in line 2 of $\\text{BUILD-MAX-HEAP}$ to decrease from $\\lfloor A.length / 2 \\rfloor$ to $1$ rather than increase from $1$ to $\\lfloor A.length/2 \\rfloor$? Otherwise we won't be allowed to call $\\text{MAX-HEAPIFY}$, since it will fail the condition of having the subtrees be max-heaps. That is, if we start with $1$, there is no guarantee that $A[2]$ and $A[3]$ are roots of max-heaps.","title":"6.3-2"},{"location":"Chap06/6.3/#63-3","text":"Show that there are at most $\\lceil n / 2^{h + 1} \\rceil$ nodes of height $h$ in any $n$-element heap. From 6.1-7, we know that the leaves of a heap are the nodes indexed by $$\\left\\lfloor n / 2 \\right\\rfloor + 1, \\left\\lfloor n / 2 \\right\\rfloor + 2, \\dots, n.$$ Note that those elements corresponds to the second half of the heap array (plus the middle element if $n$ is odd). Thus, the number of leaves in any heap of size $n$ is $\\left\\lceil n / 2 \\right\\rceil$. Let's prove by induction. Let $n_h$ denote the number of nodes at height $h$. The upper bound holds for the base since $n_0 = \\left\\lceil n / 2 \\right\\rceil$ is exactly the number of leaves in a heap of size $n$. Now assume it holds for $h \u2212 1$. We have prove that it also holds for $h$. Note that if $n_{h - 1}$ is even each node at height $h$ has exactly two children, which implies $n_h = n_{h - 1} / 2 = \\left\\lfloor n_{h - 1} / 2 \\right\\rfloor$. If $n_{h - 1}$ is odd, one node at height $h$ has one child and the remaining has two children, which also implies $n_h = \\left\\lfloor n_{h - 1} / 2 \\right\\rfloor + 1 = \\left\\lceil n_{h - 1} / 2 \\right\\rceil$. Thus, we have $$ \\begin{aligned} n_h & = \\left\\lceil \\frac{n_{h - 1}}{2} \\right\\rceil \\\\ & \\le \\left\\lceil \\frac{1}{2} \\cdot \\left\\lceil \\frac{n}{2^{(h - 1) + 1}} \\right\\rceil \\right\\rceil \\\\ & = \\left\\lceil \\frac{1}{2} \\cdot \\left\\lceil \\frac{n}{2^h} \\right\\rceil \\right\\rceil \\\\ & = \\left\\lceil \\frac{n}{2^{h + 1}} \\right\\rceil, \\end{aligned} $$ which implies that it holds for $h$.","title":"6.3-3"},{"location":"Chap06/6.4/","text":"6.4-1 Using figure 6.4 as a model, illustrate the operation of $\\text{HEAPSORT}$ on the array $A = \\langle 5, 13, 2, 25, 7, 17, 20, 8, 4 \\rangle$. $$ \\begin{aligned} \\langle 5, 13, 2, 25, 7, 17, 20, 8, 4 \\rangle \\\\ \\langle 5, 13, 20, 25, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 5, 25, 20, 13, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 25, 5, 20, 13, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 25, 13, 20, 5, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 25, 13, 20, 8, 7, 17, 2, 5, 4 \\rangle \\\\ \\langle 4, 13, 20, 8, 7, 17, 2, 5, 25 \\rangle \\\\ \\langle 20, 13, 4, 8, 7, 17, 2, 5, 25 \\rangle \\\\ \\langle 20, 13, 17, 8, 7, 4, 2, 5, 25 \\rangle \\\\ \\langle 5, 13, 17, 8, 7, 4, 2, 20, 25 \\rangle \\\\ \\langle 17, 13, 5, 8, 7, 4, 2, 20, 25 \\rangle \\\\ \\langle 2, 13, 5, 8, 7, 4, 17, 20, 25 \\rangle \\\\ \\langle 13, 2, 5, 8, 7, 4, 17, 20, 25 \\rangle \\\\ \\langle 13, 8, 5, 2, 7, 4, 17, 20, 25 \\rangle \\\\ \\langle 4, 8, 5, 2, 7, 13, 17, 20, 25 \\rangle \\\\ \\langle 8, 4, 5, 2, 7, 13, 17, 20, 25 \\rangle \\\\ \\langle 8, 7, 5, 2, 4, 13, 17, 20, 25 \\rangle \\\\ \\langle 4, 7, 5, 2, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 7, 4, 5, 2, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 2, 4, 5, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 5, 4, 2, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 2, 4, 5, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 4, 2, 5, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 2, 4, 5, 7, 8, 13, 17, 20, 25 \\rangle \\end{aligned} $$ 6.4-2 Argue the correctness of $\\text{HEAPSORT}$ using the following loop invariant: At the start of each iteration of the for loop of lines 2-5, the subarray $A[1..i]$ is a max-heap containing the $i$ smallest elements of $A[1..n]$, and the subarray $A[i + 1..n]$ contains the $n - i$ largest elements of $A[1..n]$, sorted. Initialization: The subarray $A[i + 1..n]$ is empty, thus the invariant holds. Maintenance: $A[1]$ is the largest element in $A[1..i]$ and it is smaller than the elements in $A[i + 1..n]$. When we put it in the $i$th position, then $A[i..n]$ contains the largest elements, sorted. Decreasing the heap size and calling $\\text{MAX-HEAPIFY}$ turns $A[1..i - 1]$ into a max-heap. Decrementing $i$ sets up the invariant for the next iteration. Termination: After the loop $i = 1$. This means that $A[2..n]$ is sorted and $A[1]$ is the smallest element in the array, which makes the array sorted. 6.4-3 What is the running time of $\\text{HEAPSORT}$ on an array $A$ of length $n$ that is already sorted in increasing order? What about decreasing order? Both of them are $\\Theta(n\\lg n)$. If the array is sorted in increasing order, the algorithm will need to convert it to a heap that will take $O(n)$. Afterwards, however, there are $n - 1$ calls to $\\text{MAX-HEAPIFY}$ and each one will perform the full $\\lg k$ operations. Since: $$\\sum_{k = 1}^{n - 1}\\lg k = \\lg((n - 1)!) = \\Theta(n\\lg n).$$ Same goes for decreasing order. $\\text{BUILD-MAX-HEAP}$ will be faster (by a constant factor), but the computation time will be dominated by the loop in $\\text{HEAPSORT}$, which is $\\Theta(n\\lg n)$. 6.4-4 Show that the worst-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This is essentially the first part of exercise 6.4-3. Whenever we have an array that is already sorted, we take linear time to convert it to a max-heap and then $n\\lg n$ time to sort it. 6.4-5 $\\star$ Show that when all elements are distinct, the best-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This proved to be quite tricky. My initial solution was wrong. Also, heapsort appeared in 1964, but the lower bound was proved by Schaffer and Sedgewick in 1992. It's evil to put this an exercise. Let's assume that the heap is a full binary tree with $n = 2^k - 1$. There are $2^{k - 1}$ leaves and $2^{k - 1} - 1$ inner nodes. Let's look at sorting the first $2^{k - 1}$ elements of the heap. Let's consider their arrangement in the heap and color the leaves to be red and the inner nodes to be blue. The colored nodes are a subtree of the heap (otherwise there would be a contradiction). Since there are $2^{k - 1}$ colored nodes, at most $2^{k - 2}$ are red, which means that at least $2^{k - 2} - 1$ are blue. While the red nodes can jump directly to the root, the blue nodes need to travel up before they get removed. Let's count the number of swaps to move the blue nodes to the root. The minimal case of swaps is when there are $2^{k - 2} - 1$ blue nodes and they are arranged in a binary tree. If there are $d$ such blue nodes, then there would be $i = \\lg d$ levels, each containing $2^i$ nodes with length $i$. Thus the number of swaps is, $$\\sum_{i = 0}^{\\lg d}i2^i = 2 + (\\lg d - 2)2^{\\lg d} = \\Omega(d\\lg d).$$ And now for a lazy (but cute) trick. We've figured out a tight bound on sorting half of the heap. We have the following recurrence: $$T(n) = T(n / 2) + \\Omega(n\\lg n).$$ Applying the master method, we get that $T(n) = \\Omega(n\\lg n)$.","title":"6.4 The heapsort algorithm"},{"location":"Chap06/6.4/#64-1","text":"Using figure 6.4 as a model, illustrate the operation of $\\text{HEAPSORT}$ on the array $A = \\langle 5, 13, 2, 25, 7, 17, 20, 8, 4 \\rangle$. $$ \\begin{aligned} \\langle 5, 13, 2, 25, 7, 17, 20, 8, 4 \\rangle \\\\ \\langle 5, 13, 20, 25, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 5, 25, 20, 13, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 25, 5, 20, 13, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 25, 13, 20, 5, 7, 17, 2, 8, 4 \\rangle \\\\ \\langle 25, 13, 20, 8, 7, 17, 2, 5, 4 \\rangle \\\\ \\langle 4, 13, 20, 8, 7, 17, 2, 5, 25 \\rangle \\\\ \\langle 20, 13, 4, 8, 7, 17, 2, 5, 25 \\rangle \\\\ \\langle 20, 13, 17, 8, 7, 4, 2, 5, 25 \\rangle \\\\ \\langle 5, 13, 17, 8, 7, 4, 2, 20, 25 \\rangle \\\\ \\langle 17, 13, 5, 8, 7, 4, 2, 20, 25 \\rangle \\\\ \\langle 2, 13, 5, 8, 7, 4, 17, 20, 25 \\rangle \\\\ \\langle 13, 2, 5, 8, 7, 4, 17, 20, 25 \\rangle \\\\ \\langle 13, 8, 5, 2, 7, 4, 17, 20, 25 \\rangle \\\\ \\langle 4, 8, 5, 2, 7, 13, 17, 20, 25 \\rangle \\\\ \\langle 8, 4, 5, 2, 7, 13, 17, 20, 25 \\rangle \\\\ \\langle 8, 7, 5, 2, 4, 13, 17, 20, 25 \\rangle \\\\ \\langle 4, 7, 5, 2, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 7, 4, 5, 2, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 2, 4, 5, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 5, 4, 2, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 2, 4, 5, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 4, 2, 5, 7, 8, 13, 17, 20, 25 \\rangle \\\\ \\langle 2, 4, 5, 7, 8, 13, 17, 20, 25 \\rangle \\end{aligned} $$","title":"6.4-1"},{"location":"Chap06/6.4/#64-2","text":"Argue the correctness of $\\text{HEAPSORT}$ using the following loop invariant: At the start of each iteration of the for loop of lines 2-5, the subarray $A[1..i]$ is a max-heap containing the $i$ smallest elements of $A[1..n]$, and the subarray $A[i + 1..n]$ contains the $n - i$ largest elements of $A[1..n]$, sorted. Initialization: The subarray $A[i + 1..n]$ is empty, thus the invariant holds. Maintenance: $A[1]$ is the largest element in $A[1..i]$ and it is smaller than the elements in $A[i + 1..n]$. When we put it in the $i$th position, then $A[i..n]$ contains the largest elements, sorted. Decreasing the heap size and calling $\\text{MAX-HEAPIFY}$ turns $A[1..i - 1]$ into a max-heap. Decrementing $i$ sets up the invariant for the next iteration. Termination: After the loop $i = 1$. This means that $A[2..n]$ is sorted and $A[1]$ is the smallest element in the array, which makes the array sorted.","title":"6.4-2"},{"location":"Chap06/6.4/#64-3","text":"What is the running time of $\\text{HEAPSORT}$ on an array $A$ of length $n$ that is already sorted in increasing order? What about decreasing order? Both of them are $\\Theta(n\\lg n)$. If the array is sorted in increasing order, the algorithm will need to convert it to a heap that will take $O(n)$. Afterwards, however, there are $n - 1$ calls to $\\text{MAX-HEAPIFY}$ and each one will perform the full $\\lg k$ operations. Since: $$\\sum_{k = 1}^{n - 1}\\lg k = \\lg((n - 1)!) = \\Theta(n\\lg n).$$ Same goes for decreasing order. $\\text{BUILD-MAX-HEAP}$ will be faster (by a constant factor), but the computation time will be dominated by the loop in $\\text{HEAPSORT}$, which is $\\Theta(n\\lg n)$.","title":"6.4-3"},{"location":"Chap06/6.4/#64-4","text":"Show that the worst-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This is essentially the first part of exercise 6.4-3. Whenever we have an array that is already sorted, we take linear time to convert it to a max-heap and then $n\\lg n$ time to sort it.","title":"6.4-4"},{"location":"Chap06/6.4/#64-5-star","text":"Show that when all elements are distinct, the best-case running time of $\\text{HEAPSORT}$ is $\\Omega(n\\lg n)$. This proved to be quite tricky. My initial solution was wrong. Also, heapsort appeared in 1964, but the lower bound was proved by Schaffer and Sedgewick in 1992. It's evil to put this an exercise. Let's assume that the heap is a full binary tree with $n = 2^k - 1$. There are $2^{k - 1}$ leaves and $2^{k - 1} - 1$ inner nodes. Let's look at sorting the first $2^{k - 1}$ elements of the heap. Let's consider their arrangement in the heap and color the leaves to be red and the inner nodes to be blue. The colored nodes are a subtree of the heap (otherwise there would be a contradiction). Since there are $2^{k - 1}$ colored nodes, at most $2^{k - 2}$ are red, which means that at least $2^{k - 2} - 1$ are blue. While the red nodes can jump directly to the root, the blue nodes need to travel up before they get removed. Let's count the number of swaps to move the blue nodes to the root. The minimal case of swaps is when there are $2^{k - 2} - 1$ blue nodes and they are arranged in a binary tree. If there are $d$ such blue nodes, then there would be $i = \\lg d$ levels, each containing $2^i$ nodes with length $i$. Thus the number of swaps is, $$\\sum_{i = 0}^{\\lg d}i2^i = 2 + (\\lg d - 2)2^{\\lg d} = \\Omega(d\\lg d).$$ And now for a lazy (but cute) trick. We've figured out a tight bound on sorting half of the heap. We have the following recurrence: $$T(n) = T(n / 2) + \\Omega(n\\lg n).$$ Applying the master method, we get that $T(n) = \\Omega(n\\lg n)$.","title":"6.4-5 $\\star$"},{"location":"Chap06/6.5/","text":"6.5-1 Illustrate the operation $\\text{HEAP-EXTRACT-MAX}$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Extract the max node $15$, then move $1$ to the top of the heap. Since $13 > 9 > 1$, swap $1$ and $13$. Since $12 > 5 > 1$, swap $1$ and $12$. Since $6 > 2 > 1$, swap $1$ and $6$. 6.5-2 Illustrate the operation of $\\text{MAX-HEAP-INSERT}(A, 10)$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Since $\\text{MAX-HEAP-INSERT}(A, 10)$ is called, we append a node assigned value $-\\infty$. Update the $key$ value of the new node. Since the parent $key$ is smaller than $10$, the nodes are swapped. Since the parent $key$ is smaller than $10$, the nodes are swapped. 6.5-3 Write pseudocode for the procedures $\\text{HEAP-MINIMUM}$, $\\text{HEAP-EXTRACT-MIN}$, $\\text{HEAP-DECREASE-KEY}$, and $\\text{MIN-HEAP-INSERT}$ that implement a min-priority queue with a min-heap. HEAP - MINIMUM ( A ) return A [ 1 ] HEAP - EXTRACT - MIN ( A ) if A . heap - size < 1 error \"heap underflow\" min = A [ 1 ] A [ 1 ] = A [ A . heap - size ] A . heap - size = A . heap - size - 1 MIN - HEAPIFY ( A , 1 ) return min HEAP - DECREASE - KEY ( A , i , key ) if key > A [ i ] error \"new key is larger than current key\" A [ i ] = key while i > 1 and A [ PAREANT ( i )] > A [ i ] exchange A [ i ] with A [ PARENT ( i )] i = PARENT ( i ) MIN - HEAP - INSERT ( A , key ) A . heap - size = A . heap - size + 1 A [ A . heap - size ] = \u221e HEAP - DECREASE - KEY ( A , A . heap - size , key ) 6.5-4 Why do we bother setting the key of the inserted node to $-\\infty$ in line 2 of $\\text{MAX-HEAP-INSERT}$ when the next thing we do is increase its key to the desired value? In order to pass the guard clause. Otherwise we have to drop the check if $key < A[i]$. 6.5-5 Argue the correctness of $\\text{HEAP-INCREASE-KEY}$ using the following loop invariant: At the start of each iteration of the while loop of lines 4-6, the subarray $A[1 ..A.heap\\text-size]$ satisfies the max-heap property, except that there may be one violation: $A[i]$ may be larger than $A[\\text{PARENT}(i)]$. You may assume that the subarray $A[1..A.heap\\text-size]$ satisfies the max-heap property at the time $\\text{HEAP-INCREASE-KEY}$ is called. Initialization: $A$ is a heap except that $A[i]$ might be larger that it's parent, because it has been modified. $A[i]$ is larger than its children, because otherwise the guard clause would fail and the loop will not be entered (the new value is larger than the old value and the old value is larger than the children). Maintenance: When we exchange $A[i]$ with its parent, the max-heap property is satisfied except that now $A[\\text{PARENT}(i)]$ might be larger than its parent. Changing $i$ to its parent maintains the invariant. Termination: The loop terminates whenever the heap is exhausted or the max-heap property for $A[i]$ and its parent is preserved. At the loop termination, $A$ is a max-heap. 6.5-6 Each exchange operation on line 5 of $\\text{HEAP-INCREASE-KEY}$ typically requires three assignments. Show how to use the idea of the inner loop of $\\text{INSERTION-SORT}$ to reduce the three assignments down to just one assignment. HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] error \"new key is smaller than current key\" while i > 1 and A [ PARENT ( i )] < key A [ i ] = A [ PARENT ( i )] i = PARENT ( i ) A [ i ] = key 6.5-7 Show how to implement a first-in, first-out queue with a priority queue. Show how to implement a stack with a priority queue. (Queues and stacks are defined in section 10.1). Both are simple. For a stack we keep adding elements in increasing priority, while in a queue we add them in decreasing priority. For the stack we can set the new priority to $\\text{HEAP-MAXIMUM}(A) + 1$. For the queue we need to keep track of it and decrease it on every insertion. Both are not very efficient. Furthermore, if the priority can overflow or underflow, so will eventually need to reassign priorities. 6.5-8 The operation $\\text{HEAP-DELETE}(A, i)$ deletes the item in node $i$ from heap $A$. Give an implementation of $\\text{HEAP-DELETE}$ that runs in $O(\\lg n)$ time for an $n$-element max-heap. HEAP - DELETE ( A , i ) if A [ i ] > A [ A . heap - size ] A [ i ] = A [ A . heap - size ] MAX - HEAPIFY ( A , i ) else HEAP - INCREASE - KEY ( A , i , A [ A . heap - size ]) A . heap - size = A . heap - size - 1 Note: The following algorithm is wrong. For example, given an array $A = [15, 7, 9, 1, 2, 3, 8]$ which is a max-heap, and if we delete $A[5] = 2$, then it will fail. HEAP - DELETE ( A , i ) A [ i ] = A [ A . heap - size ] A . heap - size = A . heap - size - 1 MAX - HEAPIFY ( A , i ) before: 15 / \\ 7 9 / \\ / \\ 1 2 3 8 after (which is wrong since $8 > 7$ violates the max-heap property): 15 / \\ 7 9 / \\ / 1 8 3 6.5-9 Give an $O(n\\lg k)$-time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. ($\\textit{Hint:}$ Use a min-heap for $k$-way merging.) We take one element of each list and put it in a min-heap. Along with each element we have to track which list we took it from. When merging, we take the minimum element from the heap and insert another element off the list it came from (unless the list is empty). We continue until we empty the heap. We have $n$ steps and at each step we're doing an insertion into the heap, which is $\\lg k$.","title":"6.5 Priority queues"},{"location":"Chap06/6.5/#65-1","text":"Illustrate the operation $\\text{HEAP-EXTRACT-MAX}$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Extract the max node $15$, then move $1$ to the top of the heap. Since $13 > 9 > 1$, swap $1$ and $13$. Since $12 > 5 > 1$, swap $1$ and $12$. Since $6 > 2 > 1$, swap $1$ and $6$.","title":"6.5-1"},{"location":"Chap06/6.5/#65-2","text":"Illustrate the operation of $\\text{MAX-HEAP-INSERT}(A, 10)$ on the heap $A = \\langle 15, 13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1 \\rangle$. Original heap. Since $\\text{MAX-HEAP-INSERT}(A, 10)$ is called, we append a node assigned value $-\\infty$. Update the $key$ value of the new node. Since the parent $key$ is smaller than $10$, the nodes are swapped. Since the parent $key$ is smaller than $10$, the nodes are swapped.","title":"6.5-2"},{"location":"Chap06/6.5/#65-3","text":"Write pseudocode for the procedures $\\text{HEAP-MINIMUM}$, $\\text{HEAP-EXTRACT-MIN}$, $\\text{HEAP-DECREASE-KEY}$, and $\\text{MIN-HEAP-INSERT}$ that implement a min-priority queue with a min-heap. HEAP - MINIMUM ( A ) return A [ 1 ] HEAP - EXTRACT - MIN ( A ) if A . heap - size < 1 error \"heap underflow\" min = A [ 1 ] A [ 1 ] = A [ A . heap - size ] A . heap - size = A . heap - size - 1 MIN - HEAPIFY ( A , 1 ) return min HEAP - DECREASE - KEY ( A , i , key ) if key > A [ i ] error \"new key is larger than current key\" A [ i ] = key while i > 1 and A [ PAREANT ( i )] > A [ i ] exchange A [ i ] with A [ PARENT ( i )] i = PARENT ( i ) MIN - HEAP - INSERT ( A , key ) A . heap - size = A . heap - size + 1 A [ A . heap - size ] = \u221e HEAP - DECREASE - KEY ( A , A . heap - size , key )","title":"6.5-3"},{"location":"Chap06/6.5/#65-4","text":"Why do we bother setting the key of the inserted node to $-\\infty$ in line 2 of $\\text{MAX-HEAP-INSERT}$ when the next thing we do is increase its key to the desired value? In order to pass the guard clause. Otherwise we have to drop the check if $key < A[i]$.","title":"6.5-4"},{"location":"Chap06/6.5/#65-5","text":"Argue the correctness of $\\text{HEAP-INCREASE-KEY}$ using the following loop invariant: At the start of each iteration of the while loop of lines 4-6, the subarray $A[1 ..A.heap\\text-size]$ satisfies the max-heap property, except that there may be one violation: $A[i]$ may be larger than $A[\\text{PARENT}(i)]$. You may assume that the subarray $A[1..A.heap\\text-size]$ satisfies the max-heap property at the time $\\text{HEAP-INCREASE-KEY}$ is called. Initialization: $A$ is a heap except that $A[i]$ might be larger that it's parent, because it has been modified. $A[i]$ is larger than its children, because otherwise the guard clause would fail and the loop will not be entered (the new value is larger than the old value and the old value is larger than the children). Maintenance: When we exchange $A[i]$ with its parent, the max-heap property is satisfied except that now $A[\\text{PARENT}(i)]$ might be larger than its parent. Changing $i$ to its parent maintains the invariant. Termination: The loop terminates whenever the heap is exhausted or the max-heap property for $A[i]$ and its parent is preserved. At the loop termination, $A$ is a max-heap.","title":"6.5-5"},{"location":"Chap06/6.5/#65-6","text":"Each exchange operation on line 5 of $\\text{HEAP-INCREASE-KEY}$ typically requires three assignments. Show how to use the idea of the inner loop of $\\text{INSERTION-SORT}$ to reduce the three assignments down to just one assignment. HEAP - INCREASE - KEY ( A , i , key ) if key < A [ i ] error \"new key is smaller than current key\" while i > 1 and A [ PARENT ( i )] < key A [ i ] = A [ PARENT ( i )] i = PARENT ( i ) A [ i ] = key","title":"6.5-6"},{"location":"Chap06/6.5/#65-7","text":"Show how to implement a first-in, first-out queue with a priority queue. Show how to implement a stack with a priority queue. (Queues and stacks are defined in section 10.1). Both are simple. For a stack we keep adding elements in increasing priority, while in a queue we add them in decreasing priority. For the stack we can set the new priority to $\\text{HEAP-MAXIMUM}(A) + 1$. For the queue we need to keep track of it and decrease it on every insertion. Both are not very efficient. Furthermore, if the priority can overflow or underflow, so will eventually need to reassign priorities.","title":"6.5-7"},{"location":"Chap06/6.5/#65-8","text":"The operation $\\text{HEAP-DELETE}(A, i)$ deletes the item in node $i$ from heap $A$. Give an implementation of $\\text{HEAP-DELETE}$ that runs in $O(\\lg n)$ time for an $n$-element max-heap. HEAP - DELETE ( A , i ) if A [ i ] > A [ A . heap - size ] A [ i ] = A [ A . heap - size ] MAX - HEAPIFY ( A , i ) else HEAP - INCREASE - KEY ( A , i , A [ A . heap - size ]) A . heap - size = A . heap - size - 1 Note: The following algorithm is wrong. For example, given an array $A = [15, 7, 9, 1, 2, 3, 8]$ which is a max-heap, and if we delete $A[5] = 2$, then it will fail. HEAP - DELETE ( A , i ) A [ i ] = A [ A . heap - size ] A . heap - size = A . heap - size - 1 MAX - HEAPIFY ( A , i ) before: 15 / \\ 7 9 / \\ / \\ 1 2 3 8 after (which is wrong since $8 > 7$ violates the max-heap property): 15 / \\ 7 9 / \\ / 1 8 3","title":"6.5-8"},{"location":"Chap06/6.5/#65-9","text":"Give an $O(n\\lg k)$-time algorithm to merge $k$ sorted lists into one sorted list, where $n$ is the total number of elements in all the input lists. ($\\textit{Hint:}$ Use a min-heap for $k$-way merging.) We take one element of each list and put it in a min-heap. Along with each element we have to track which list we took it from. When merging, we take the minimum element from the heap and insert another element off the list it came from (unless the list is empty). We continue until we empty the heap. We have $n$ steps and at each step we're doing an insertion into the heap, which is $\\lg k$.","title":"6.5-9"},{"location":"Chap06/Problems/6-1/","text":"We can build a heap by repeatedly calling $\\text{MAX-HEAP-INSERT}$ to insert the elements into the heap. Consider the following variation of the $\\text{BUILD-MAX-HEAP}$ procedure: BUILD - MAX - HEAP ' ( A ) A . heap - size = 1 for i = 2 to A . length MAX - HEAP - INSERT ( A , A [ i ]) a. Do the procedures $\\text{BUILD-MAX-HEAP}$ and $\\text{BUILD-MAX-HEAP}'$ always create the same heap when run on the same input array? Prove that they do, or provide a counterexample. b. Show that in the worst case, $\\text{BUILD-MAX-HEAP}'$ requires $\\Theta(n\\lg n)$ time to build a $n$-element heap. a. Consider the following counterexample. Input array $A = \\langle 1, 2, 3 \\rangle$: $\\text{BUILD-MAX-HEAP}(A)$: $A = \\langle 3, 2, 1 \\rangle$. $\\text{BUILD-MAX-HEAP}'(A)$: $A = \\langle 3, 1, 2 \\rangle$. b. Each insert step takes at most $O(\\lg n)$, since we are doing it $n$ times, we get a bound on the runtime of $O(n\\lg n)$.","title":"6-1 Building a heap using insertion"},{"location":"Chap06/Problems/6-2/","text":"A $d$-ary heap is like a binary heap, but (with one possible exception) non-leaf nodes have $d$ children instead of $2$ children. a. How would you represent a $d$-ary heap in an array? b. What is the height of a $d$-ary heap of $n$ elements in terms of $n$ and $d$? c. Give an efficient implementation of $\\text{EXTRACT-MAX}$ in a $d$-ary max-heap. Analyze its running time in terms of $d$ and $n$. d. Give an efficient implementation of $\\text{INSERT}$ in a $d$-ary max-heap. Analyze its running time in terms of $d$ and $n$. e. Give an efficient implementation of $\\text{INCREASE-KEY}(A, i, k)$, which flags an error if $k < A[i]$, but otherwise sets $A[i] = k$ and then updates the $d$-ary max-heap structure appropriately. Analyze its running time in terms of $d$ and $n$. a. We can use those two following functions to retrieve parent of $i$-th element and $j$-th child of $i$-th element. d - ARY - PARENT ( i ) return floor (( i - 2 ) / d + 1 ) d - ARY - CHILD ( i , j ) return d ( i \u2212 1 ) + j + 1 Obviously $1 \\le j \\le d$. You can verify those functions checking that $$d\\text{-ARY-PARENT}(d\\text{-ARY-CHILD}(i, j)) = i.$$ Also easy to see is that binary heap is special type of $d$-ary heap where $d = 2$, if you substitute $d$ with $2$, then you will see that they match functions $\\text{PARENT}$, $\\text{LEFT}$ and $\\text{RIGHT}$ mentioned in book. b. Since each node has $d$ children, the height of a $d$-ary heap with $n$ nodes is $\\Theta(\\log_d n)$. c. $d\\text{-ARY-HEAP-EXTRACT-MAX}(A)$ consists of constant time operations, followed by a call to $d\\text{-ARY-MAX-HEAPIFY}(A, i)$. The number of times this recursively calls itself is bounded by the height of the $d$-ary heap, so the running time is $O(d\\log_d n)$. d - ARY - HEAP - EXTRACT - MAX ( A ) if A . heap - size < 1 error \"heap under flow\" max = A [ 1 ] A [ 1 ] = A [ A . heap - size ] A . heap - size = A . heap - size - 1 d - ARY - MAX - HEAPIFY ( A , 1 ) return max d - ARY - MAX - HEAPIFY ( A , i ) largest = i for k = 1 to d if d - ARY - CHILD ( k , i ) \u2264 A . heap - size and A [ d - ARY - CHILD ( k , i )] > A [ i ] if A [ d - ARY - CHILD ( k , i )] > largest largest = A [ d - ARY - CHILD ( k , i )] if largest != i exchange A [ i ] with A [ largest ] d - ARY - MAX - HEAPIFY ( A , largest ) d. The runtime is $O(\\log_d n)$ since the while loop runs at most as many times as the height of the $d$-ary array. d - ARY - MAX - HEAP - INSERT ( A , key ) A . heap - size = A . heap - size + 1 A [ A . heap - size ] = key i = A . heap - size while i > 1 and A [ d - ARY - PARENT ( i ) < A [ i ]] exchange A [ i ] with A [ d - ARY - PARENT ( i )] i = d - ARY - PARENT ( i ) e. The runtime is $O(\\log_d n)$ since the while loop runs at most as many times as the height of the $d$-ary array. d - ARY - INCREASE - KEY ( A , i , key ) if key < A [ i ] error \"new key is smaller than current key\" A [ i ] = key while i > 1 and A [ d - ARY - PARENT ( i ) < A [ i ]] exchange A [ i ] with A [ d - ARY - PARENT ( i )] i = d - ARY - PARENT ( i )","title":"6-2 Analysis of $d$-ary heaps"},{"location":"Chap06/Problems/6-3/","text":"An $m \\times n$ Young tableau is an $m \\times n$ matrix such that the entries of each row are in sorted order from left to right and the entries of each column are in sorted order from top to bottom. Some of the entries of a Young tableau may be $\\infty$, which we treat as nonexistent elements. Thus, a Young tableau can be used to hold $r \\le mn$ finite numbers. a. Draw $4 \\times 4$ tableau containing the elements $\\{9, 16, 3, 2, 4, 8, 5, 14, 12\\}$. b. Argue that an $m \\times n$ Young tableau $Y$ is empty if $Y[1, 1] = \\infty$. Argue that $Y$ is full (contains $mn$ elements) if $Y[m, n] < \\infty$. c. Give an algorithm to implement $\\text{EXTRACT-MIN}$ on a nonempty $m \\times n$ Young tableau that runs in $O(m + n)$ time. Your algorithm should use a recursive subroutine that solves an $m \\times n$ problem by recursively solving either an $(m - 1) \\times n$ or an $m \\times (n - 1)$ subproblem. ($\\textit{Hint:}$ Think about $\\text{MAX-HEAPIFY}$.) Define $T(p)$ where $p = m + n$, to be the maximum running time of $\\text{EXTRACT-MIN}$ on any $m \\times n$ Young tableau. Give and solve a recurrence relation for $T(p)$ that yields the $O(m + n)$ time bound. d. Show how to insert a new element into a nonfull $m \\times n$ Young tableau in $O(m + n)$ time. e. Using no other sorting method as a subroutine, show how to use an $n \\times n$ Young tableau to sort $n^2$ numbers in $O(n^3)$ time. f. Give an $O(m + n)$-time algorithm to determine whether a given number is stored in a given $m \\times n$ Young tableau. a. $$ \\begin{matrix} 2 & 3 & 12 & 14 \\\\ 4 & 8 & 16 & \\infty \\\\ 5 & 9 & \\infty & \\infty \\\\ \\infty & \\infty & \\infty & \\infty \\end{matrix} $$ b. If the top left element is $\\infty$, then all the elements on the first row need to be $\\infty$. But if this is the case, all other elements need to be $\\infty$ because they are larger than the first element on their column. If the bottom right element is smaller than $\\infty$, all the elements on the bottom row need to be smaller than $\\infty$. But so are the other elements in the tableau, because each is smaller than the bottom element of its column. c. The $A[1, 1]$ is the smallest element. We store it, so we can return it later and then replace it with $\\infty$. This breaks the Young tableau property and we need to perform a procedure, similar to $\\text{MAX-HEAPIFY}$, to restore it. We compare $A[i, j]$ with each of its neighbours and exchange it with the smallest. This restores the property for $A[i, j]$ but reduces the problem to either $A[i, j + 1]$ or $A[i + 1, j]$. We terminate when $A[i, j]$ is smaller than its neighbours. The relation in question is $$T(p) = T(p - 1) + O(1) = T(p - 2) + O(1) + O(1) = \\cdots = O(p).$$ d. The algorithm is very similar to the previous, except that we start with the bottom right element of the tableau and move it upwards and leftwards to the correct position. The asymptotic analysis is the same. e. We can sort by starting with an empty tableau and inserting all the $n^2$ elements in it. Each insertion is $O(n + n) = O(n)$. The complexity is $n^2O(n) = O(n^3)$. Afterwards we can take them one by one and put them back in the original array which has the same complexity. In total, its $O(n^3)$. We can also do it in place if we allow for \"partial\" tableaus where only a portion of the top rows (and a portion of the last of them) is in the tableau. Then we can build the tableau in place and then start putting each minimal element to the end. This would be asymptotically equal, but use constant memory. It would also sort the array in reverse. f. We start from the lower-left corner. We check the current element $current$ with the one we're looking for $key$ and move up if $current > key$ and right if $current < key$. We declare success if $current = key$ and otherwise terminate if we walk off the tableau.","title":"6-3 Young tableaus"},{"location":"Chap07/7.1/","text":"7.1-1 Using figure 7.1 as a model, illustrate the operation of $\\text{PARTITION}$ on the array $A = \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle$. $$ \\begin{aligned} \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 19, 13, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 19, 12, 13, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 12, 13, 19, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 19, 12, 21, 13, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 12, 21, 13, 19, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 \\rangle \\end{aligned} $$ 7.1-2 What value of $q$ does $\\text{PARTITION}$ return when all elements in the array $A[p..r]$ have the same value? Modify $\\text{PARTITION}$ so that $q = \\lfloor (p + r) / 2 \\rfloor$ when all elements in the array $A[p..r]$ have the same value. It returns $r$. We can modify $\\text{PARTITION}$ by counting the number of comparisons in which $A[j] = A[r]$ and then subtracting half that number from the pivot index. 7.1-3 Give a brief argument that the running time of $\\text{PARTITION}$ on a subarray of size $n$ is $\\Theta(n)$. There is a for statement whose body executes $r - 1 - p = \\Theta(n)$ times. In the worst case every time the body of the if is executed, but it takes constant time and so does the code outside of the loop. Thus the running time is $\\Theta(n)$. 7.1-4 How would you modify $\\text{QUICKSORT}$ to sort into nonincreasing order? We only need to flip the condition on line 4.","title":"7.1 Description of quicksort"},{"location":"Chap07/7.1/#71-1","text":"Using figure 7.1 as a model, illustrate the operation of $\\text{PARTITION}$ on the array $A = \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle$. $$ \\begin{aligned} \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 19, 13, 5, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 13, 19, 12, 8, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 19, 12, 13, 7, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 12, 13, 19, 4, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 13, 19, 12, 21, 2, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 19, 12, 21, 13, 6, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 12, 21, 13, 19, 11 \\rangle \\\\ \\langle 9, 5, 8, 7, 4, 2, 6, 11, 21, 13, 19, 12 \\rangle \\end{aligned} $$","title":"7.1-1"},{"location":"Chap07/7.1/#71-2","text":"What value of $q$ does $\\text{PARTITION}$ return when all elements in the array $A[p..r]$ have the same value? Modify $\\text{PARTITION}$ so that $q = \\lfloor (p + r) / 2 \\rfloor$ when all elements in the array $A[p..r]$ have the same value. It returns $r$. We can modify $\\text{PARTITION}$ by counting the number of comparisons in which $A[j] = A[r]$ and then subtracting half that number from the pivot index.","title":"7.1-2"},{"location":"Chap07/7.1/#71-3","text":"Give a brief argument that the running time of $\\text{PARTITION}$ on a subarray of size $n$ is $\\Theta(n)$. There is a for statement whose body executes $r - 1 - p = \\Theta(n)$ times. In the worst case every time the body of the if is executed, but it takes constant time and so does the code outside of the loop. Thus the running time is $\\Theta(n)$.","title":"7.1-3"},{"location":"Chap07/7.1/#71-4","text":"How would you modify $\\text{QUICKSORT}$ to sort into nonincreasing order? We only need to flip the condition on line 4.","title":"7.1-4"},{"location":"Chap07/7.2/","text":"7.2-1 Use the substitution method to prove that the recurrence $T(n) = T(n - 1) + \\Theta(n)$ has the solution $T(n) = \\Theta(n^2)$, as claimed at the beginning of section 7.2. We represent $\\Theta(n)$ as $c_2n$ and we guess that $T(n) \\le c_1n^2$, $$ \\begin{aligned} T(n) & = T(n - 1) + c_2n \\\\ & \\le c_1(n - 1)^2 + c_2n \\\\ & = c_1n^2 - 2c_1n + c_1 + c_2n & (2c_1 > c_2, n \\ge c_1 / (2c_1 - c_2)) \\\\ & \\le c_1n^2. \\end{aligned} $$ 7.2-2 What is the running time of $\\text{QUICKSORT}$ when all elements of the array $A$ have the same value? It is $\\Theta(n^2)$, since one of the partitions is always empty (see exercise 7.1-2.) 7.2-3 Show that the running time of $\\text{QUICKSORT}$ is $\\Theta(n^2)$ when the array $A$ contains distinct elements and is sorted in decreasing order. If the array is already sorted in decreasing order, then, the pivot element is less than all the other elements. The partition step takes $\\Theta(n)$ time, and then leaves you with a subproblem of size $n \u2212 1$ and a subproblem of size $0$. This gives us the recurrence considered in 7.2-1. Which we showed has a solution that is $\\Theta(n^2)$. 7.2-4 Banks often record transactions on an account in order of the times of the transactions, but many people like to receive their bank statements with checks listed in order by check numbers. People usually write checks in order by check number, and merchants usually cash the with reasonable dispatch. The problem of converting time-of-transaction ordering to check-number ordering is therefore the problem of sorting almost-sorted input. Argue that the procedure $\\text{INSERTION-SORT}$ would tend to beat the procedure $\\text{QUICKSORT}$ on this problem. The more sorted the array is, the less work insertion sort will do. Namely, $\\text{INSERTION-SORT}$ is $\\Theta(n + d)$, where $d$ is the number of inversions in the array. In the example above the number of inversions tends to be small so insertion sort will be close to linear. On the other hand, if $\\text{PARTITION}$ does pick a pivot that does not participate in an inversion, it will produce an empty partition. Since there is a small number of inversions, $\\text{QUICKSORT}$ is very likely to produce empty partitions. 7.2-5 Suppose that the splits at every level of quicksort are in proportion $1 - \\alpha$ to $\\alpha$, where $0 < \\alpha \\le 1 / 2$ is a constant. Show that the minimum depth of a leaf in the recursion tree is approximately $-\\lg n / \\lg\\alpha$ and the maximum depth is approximately $-\\lg n / \\lg(1 - \\alpha)$. (Don't worry about integer round-off.) The minimum depth corresponds to repeatedly taking the smaller subproblem, that is, the branch whose size is proportional to $\\alpha$. Then, this will fall to $1$ in $k$ steps where $1 \\approx \\alpha^kn$. Therefore, $k \\approx \\log_\\alpha 1 / n = -\\frac{\\lg n}{\\lg\\alpha}$. The longest depth corresponds to always taking the larger subproblem. we then have an identical expression, replacing $\\alpha$ with $1 \u2212 \\alpha$. 7.2-6 $\\star$ Argue that for any constant $0 < \\alpha \\le 1 / 2$, the probability is approximately $1 - 2\\alpha$ that on a random input array, $\\text{PARTITION}$ produces a split more balanced than $1 - \\alpha$ to $\\alpha$. In order to produce a worse split than $1 - \\alpha$ to $\\alpha$, $\\text{PARTITION}$ must pick a pivot that will be either within the smallest $\\alpha n$ elements or the largest $\\alpha n$ elements. The probability of either is (approximately) $\\alpha n / n = \\alpha$ and the probability of both is $2\\alpha$. Thus, the probability of having a better partition is the complement, $1 - 2\\alpha$.","title":"7.2 Performance of quicksort"},{"location":"Chap07/7.2/#72-1","text":"Use the substitution method to prove that the recurrence $T(n) = T(n - 1) + \\Theta(n)$ has the solution $T(n) = \\Theta(n^2)$, as claimed at the beginning of section 7.2. We represent $\\Theta(n)$ as $c_2n$ and we guess that $T(n) \\le c_1n^2$, $$ \\begin{aligned} T(n) & = T(n - 1) + c_2n \\\\ & \\le c_1(n - 1)^2 + c_2n \\\\ & = c_1n^2 - 2c_1n + c_1 + c_2n & (2c_1 > c_2, n \\ge c_1 / (2c_1 - c_2)) \\\\ & \\le c_1n^2. \\end{aligned} $$","title":"7.2-1"},{"location":"Chap07/7.2/#72-2","text":"What is the running time of $\\text{QUICKSORT}$ when all elements of the array $A$ have the same value? It is $\\Theta(n^2)$, since one of the partitions is always empty (see exercise 7.1-2.)","title":"7.2-2"},{"location":"Chap07/7.2/#72-3","text":"Show that the running time of $\\text{QUICKSORT}$ is $\\Theta(n^2)$ when the array $A$ contains distinct elements and is sorted in decreasing order. If the array is already sorted in decreasing order, then, the pivot element is less than all the other elements. The partition step takes $\\Theta(n)$ time, and then leaves you with a subproblem of size $n \u2212 1$ and a subproblem of size $0$. This gives us the recurrence considered in 7.2-1. Which we showed has a solution that is $\\Theta(n^2)$.","title":"7.2-3"},{"location":"Chap07/7.2/#72-4","text":"Banks often record transactions on an account in order of the times of the transactions, but many people like to receive their bank statements with checks listed in order by check numbers. People usually write checks in order by check number, and merchants usually cash the with reasonable dispatch. The problem of converting time-of-transaction ordering to check-number ordering is therefore the problem of sorting almost-sorted input. Argue that the procedure $\\text{INSERTION-SORT}$ would tend to beat the procedure $\\text{QUICKSORT}$ on this problem. The more sorted the array is, the less work insertion sort will do. Namely, $\\text{INSERTION-SORT}$ is $\\Theta(n + d)$, where $d$ is the number of inversions in the array. In the example above the number of inversions tends to be small so insertion sort will be close to linear. On the other hand, if $\\text{PARTITION}$ does pick a pivot that does not participate in an inversion, it will produce an empty partition. Since there is a small number of inversions, $\\text{QUICKSORT}$ is very likely to produce empty partitions.","title":"7.2-4"},{"location":"Chap07/7.2/#72-5","text":"Suppose that the splits at every level of quicksort are in proportion $1 - \\alpha$ to $\\alpha$, where $0 < \\alpha \\le 1 / 2$ is a constant. Show that the minimum depth of a leaf in the recursion tree is approximately $-\\lg n / \\lg\\alpha$ and the maximum depth is approximately $-\\lg n / \\lg(1 - \\alpha)$. (Don't worry about integer round-off.) The minimum depth corresponds to repeatedly taking the smaller subproblem, that is, the branch whose size is proportional to $\\alpha$. Then, this will fall to $1$ in $k$ steps where $1 \\approx \\alpha^kn$. Therefore, $k \\approx \\log_\\alpha 1 / n = -\\frac{\\lg n}{\\lg\\alpha}$. The longest depth corresponds to always taking the larger subproblem. we then have an identical expression, replacing $\\alpha$ with $1 \u2212 \\alpha$.","title":"7.2-5"},{"location":"Chap07/7.2/#72-6-star","text":"Argue that for any constant $0 < \\alpha \\le 1 / 2$, the probability is approximately $1 - 2\\alpha$ that on a random input array, $\\text{PARTITION}$ produces a split more balanced than $1 - \\alpha$ to $\\alpha$. In order to produce a worse split than $1 - \\alpha$ to $\\alpha$, $\\text{PARTITION}$ must pick a pivot that will be either within the smallest $\\alpha n$ elements or the largest $\\alpha n$ elements. The probability of either is (approximately) $\\alpha n / n = \\alpha$ and the probability of both is $2\\alpha$. Thus, the probability of having a better partition is the complement, $1 - 2\\alpha$.","title":"7.2-6 $\\star$"},{"location":"Chap07/7.3/","text":"7.3-1 Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time? We analyze the expected run time because it represents the more typical time cost. Also, we are doing the expected run time over the possible randomness used during computation because it can't be produced adversarially, unlike when doing expected run time over all possible inputs to the algorithm. 7.3-2 When $\\text{RANDOMIZED-QUICKSORT}$ runs, how many calls are made to the random number generator $\\text{RANDOM}$ in the worst case? How about in the best case? Give your answer in terms of $\\Theta$-notation. In the worst case, the number of calls to $\\text{RANDOM}$ is $$T(n) = T(n - 1) + 1 = n = \\Theta(n).$$ As for the best case, $$T(n) = 2T(n / 2) + 1 = \\Theta(n).$$ This is not too surprising, because each third element (at least) gets picked as pivot.","title":"7.3 A randomized version of quicksort"},{"location":"Chap07/7.3/#73-1","text":"Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time? We analyze the expected run time because it represents the more typical time cost. Also, we are doing the expected run time over the possible randomness used during computation because it can't be produced adversarially, unlike when doing expected run time over all possible inputs to the algorithm.","title":"7.3-1"},{"location":"Chap07/7.3/#73-2","text":"When $\\text{RANDOMIZED-QUICKSORT}$ runs, how many calls are made to the random number generator $\\text{RANDOM}$ in the worst case? How about in the best case? Give your answer in terms of $\\Theta$-notation. In the worst case, the number of calls to $\\text{RANDOM}$ is $$T(n) = T(n - 1) + 1 = n = \\Theta(n).$$ As for the best case, $$T(n) = 2T(n / 2) + 1 = \\Theta(n).$$ This is not too surprising, because each third element (at least) gets picked as pivot.","title":"7.3-2"},{"location":"Chap07/7.4/","text":"7.4-1 Show that in the recurrence $$ \\begin{aligned} T(n) & = \\max\\limits_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n), \\\\ T(n) & = \\Omega(n^2). \\end{aligned} $$ We guess $T(n) \\ge cn^2 - 2n$, $$ \\begin{aligned} T(n) & = \\max_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n) \\\\ & \\ge \\max_{0 \\le q \\le n - 1} (cq^2 - 2q + c(n - q - 1)^2 - 2n - 2q -1) + \\Theta(n) \\\\ & \\ge c\\max_{0 \\le q \\le n - 1} (q^2 + (n - q - 1)^2 - (2n + 4q + 1) / c) + \\Theta(n) \\\\ & \\ge cn^2 - c(2n - 1) + \\Theta(n) \\\\ & \\ge cn^2 - 2cn + 2c & (c \\le 1) \\\\ & \\ge cn^2 - 2n. \\end{aligned} $$ 7.4-2 Show that quicksort's best-case running time is $\\Omega(n\\lg n)$. We'll use the substitution method to show that the best-case running time is $\\Omega(n\\lg n)$. Let $T(n)$ be the best-case time for the procedure $\\text{QUICKSORT}$ on an input of size $n$. We have $$T(n) = \\min _{1 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n).$$ Suppose that $T(n) \\ge c(n\\lg n + 2n)$ for some constant $c$. Substituting this guess into the recurrence gives $$ \\begin{aligned} T(n) & \\ge \\min _{1 \\le q \\le n - 1}(cq\\lg q + 2cq + c(n - q - 1) \\lg(n - q - 1) + 2c(n - q - 1)) + \\Theta(n) \\\\ & = (cn / 2)\\lg(n / 2) + cn + c(n / 2 - 1)\\lg(n / 2 - 1) + cn - 2c + \\Theta(n) \\\\ & \\ge (cn / 2)\\lg n - cn / 2 + c(n / 2 - 1)(\\lg n - 2) + 2cn - 2c\\Theta(n) \\\\ & = (cn / 2)\\lg n - cn / 2 + (cn / 2) \\lg n - cn - c\\lg n + 2c + 2cn - 2c\\Theta(n) \\\\ & = cn\\lg n + cn / 2 - c\\lg n + 2c - 2c\\Theta(n). \\end{aligned} $$ Taking a derivative with respect to $q$ shows that the minimum is obtained when $q = n / 2$. Taking $c$ large enough to dominate the $\u2212\\lg n + 2 \u2212 2c + \\Theta(n)$ term makes this greater than $cn\\lg n$, proving the bound. 7.4-3 Show that the expression $q^2 + (n - q - 1)^2$ achieves a maximum over $q = 0, 1, \\ldots, n - 1$ when $q = 0$ and $q = n - 1$. $$ \\begin{aligned} f(q) & = q^2 + (n - q - 1)^2 \\\\ f'(q) & = 2q - 2(n - q - 1) = 4q - 2n + 2 \\\\ f''(q) & = 4. \\\\ \\end{aligned} $$ $f'(q) = 0$ when $q = \\frac{1}{2}n - \\frac{1}{2}$. $f'(q)$ is also continious. $\\forall q: f''(q) > 0$, which means that $f'(q)$ is negative left of $f'(q) = 0$ and positive right of it, which means that this is a local minima. In this case, $f(q)$ is decreasing in the beginning of the interval and increasing in the end, which means that those two points are the only candidates for a maximum in the interval. $$ \\begin{aligned} f(0) & = (n - 1)^2 \\\\ f(n - 1) & = (n - 1)^2 + 0^2. \\end{aligned} $$ 7.4-4 Show that $\\text{RANDOMIZED-QUICKSORT}$'s expected running time is $\\Omega(n\\lg n)$. We use the same reasoning for the expected number of comparisons, we just take in a different direction. $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\ & = \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{k + 1} & (k \\ge 1) \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\Omega(\\lg n) \\\\ & = \\Omega(n\\lg n). \\end{aligned} $$ Using the master method, we get the solution $\\Theta(n\\lg n)$. 7.4-5 We can improve the running time of quicksort in practice by taking advantage of the fast running time of insertion sort when its input is \"nearly\" sorted. Upon calling quicksort on a subarray with fewer than $k$ elements, let it simply return without sorting the subarray. After the top-level call to quicksort returns, run insertion sort on the entire array to finish the sorting process. Argue that this sorting algorithm runs in $O(nk + n\\lg(n / k))$ expected time. How should we pick $k$, both in theory and practice? In the quicksort part of the proposed algorithm, the recursion stops at level $\\lg(n / k)$, which makes the expected running time $O(n\\lg(n / k))$. However, this leaves $n / k$ non-sorted, non - intersecting subarrays of (maximum) length $k$. Because of the nature of the insertion sort algorithm, it will first sort fully one such subarray before consider the next one. Thus, it has the same complexity as sorting each of those arrays, that is $\\frac{n}{k}O(k^2) = O(nk)$. In theory, if we ignore the constant factors, we need to solve $$ \\begin{aligned} & n\\lg n \\ge nk + n\\lg{n / k} \\\\ \\Rightarrow & \\lg n \\ge k + \\lg n - \\lg k \\\\ \\Rightarrow & \\lg k \\ge k. \\end{aligned} $$ Which is not possible. If we add the constant factors, we get $$ \\begin{aligned} & c_qn\\lg n \\ge c_ink + c_qn\\lg(n / k) \\\\ \\Rightarrow & c_q\\lg n \\ge c_ik + c_q\\lg n - c_q\\lg k \\\\ \\Rightarrow & \\lg k \\ge \\frac{c_i}{c_q}k. \\end{aligned} $$ Which indicates that there might be a good candidate. Furthermore, the lower-order terms should be taken into consideration too. In practice, $k$ should be chosed by experiment. 7.4-6 $\\star$ Consider modifying the $\\text{PARTITION}$ procedure by randomly picking three elements from array $A$ and partitioning about their median (the middle value of the three elements). Approximate the probability of getting at worst an $\\alpha$-to-$(1 - \\alpha)$ split, as a function of $\\alpha$ in the range $0 < \\alpha < 1$. First, for simplicity's sake, let's assume that we can pick the same element twice. Let's also assume that $0 < \\alpha \\le 1 / 2$. In order to get such a split, two out of three elements need to be in the smallest $\\alpha n$ elements. The probability of having one is $\\alpha n / n = \\alpha$. The probability of having exactly two is $\\alpha^2 - \\alpha^3$. There are three ways in which two elements can be in the smallest $\\alpha n$ and one way in which all three can be in the smallest $\\alpha n$ so the probability of getting such a median is $3\\alpha^2 - 2\\alpha^3$. We will get the same split if the median is in the largest $\\alpha n$. Since the two events are mutually exclusive, the probability is $$\\Pr\\{\\text{OK split}\\} = 6\\alpha^2 - 4\\alpha^3 = 2\\alpha^2(3 - 2\\alpha).$$","title":"7.4 Analysis of quicksort"},{"location":"Chap07/7.4/#74-1","text":"Show that in the recurrence $$ \\begin{aligned} T(n) & = \\max\\limits_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n), \\\\ T(n) & = \\Omega(n^2). \\end{aligned} $$ We guess $T(n) \\ge cn^2 - 2n$, $$ \\begin{aligned} T(n) & = \\max_{0 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n) \\\\ & \\ge \\max_{0 \\le q \\le n - 1} (cq^2 - 2q + c(n - q - 1)^2 - 2n - 2q -1) + \\Theta(n) \\\\ & \\ge c\\max_{0 \\le q \\le n - 1} (q^2 + (n - q - 1)^2 - (2n + 4q + 1) / c) + \\Theta(n) \\\\ & \\ge cn^2 - c(2n - 1) + \\Theta(n) \\\\ & \\ge cn^2 - 2cn + 2c & (c \\le 1) \\\\ & \\ge cn^2 - 2n. \\end{aligned} $$","title":"7.4-1"},{"location":"Chap07/7.4/#74-2","text":"Show that quicksort's best-case running time is $\\Omega(n\\lg n)$. We'll use the substitution method to show that the best-case running time is $\\Omega(n\\lg n)$. Let $T(n)$ be the best-case time for the procedure $\\text{QUICKSORT}$ on an input of size $n$. We have $$T(n) = \\min _{1 \\le q \\le n - 1} (T(q) + T(n - q - 1)) + \\Theta(n).$$ Suppose that $T(n) \\ge c(n\\lg n + 2n)$ for some constant $c$. Substituting this guess into the recurrence gives $$ \\begin{aligned} T(n) & \\ge \\min _{1 \\le q \\le n - 1}(cq\\lg q + 2cq + c(n - q - 1) \\lg(n - q - 1) + 2c(n - q - 1)) + \\Theta(n) \\\\ & = (cn / 2)\\lg(n / 2) + cn + c(n / 2 - 1)\\lg(n / 2 - 1) + cn - 2c + \\Theta(n) \\\\ & \\ge (cn / 2)\\lg n - cn / 2 + c(n / 2 - 1)(\\lg n - 2) + 2cn - 2c\\Theta(n) \\\\ & = (cn / 2)\\lg n - cn / 2 + (cn / 2) \\lg n - cn - c\\lg n + 2c + 2cn - 2c\\Theta(n) \\\\ & = cn\\lg n + cn / 2 - c\\lg n + 2c - 2c\\Theta(n). \\end{aligned} $$ Taking a derivative with respect to $q$ shows that the minimum is obtained when $q = n / 2$. Taking $c$ large enough to dominate the $\u2212\\lg n + 2 \u2212 2c + \\Theta(n)$ term makes this greater than $cn\\lg n$, proving the bound.","title":"7.4-2"},{"location":"Chap07/7.4/#74-3","text":"Show that the expression $q^2 + (n - q - 1)^2$ achieves a maximum over $q = 0, 1, \\ldots, n - 1$ when $q = 0$ and $q = n - 1$. $$ \\begin{aligned} f(q) & = q^2 + (n - q - 1)^2 \\\\ f'(q) & = 2q - 2(n - q - 1) = 4q - 2n + 2 \\\\ f''(q) & = 4. \\\\ \\end{aligned} $$ $f'(q) = 0$ when $q = \\frac{1}{2}n - \\frac{1}{2}$. $f'(q)$ is also continious. $\\forall q: f''(q) > 0$, which means that $f'(q)$ is negative left of $f'(q) = 0$ and positive right of it, which means that this is a local minima. In this case, $f(q)$ is decreasing in the beginning of the interval and increasing in the end, which means that those two points are the only candidates for a maximum in the interval. $$ \\begin{aligned} f(0) & = (n - 1)^2 \\\\ f(n - 1) & = (n - 1)^2 + 0^2. \\end{aligned} $$","title":"7.4-3"},{"location":"Chap07/7.4/#74-4","text":"Show that $\\text{RANDOMIZED-QUICKSORT}$'s expected running time is $\\Omega(n\\lg n)$. We use the same reasoning for the expected number of comparisons, we just take in a different direction. $$ \\begin{aligned} \\text E[X] & = \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\ & = \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{k + 1} & (k \\ge 1) \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\ & \\ge \\sum_{i = 1}^{n - 1} \\Omega(\\lg n) \\\\ & = \\Omega(n\\lg n). \\end{aligned} $$ Using the master method, we get the solution $\\Theta(n\\lg n)$.","title":"7.4-4"},{"location":"Chap07/7.4/#74-5","text":"We can improve the running time of quicksort in practice by taking advantage of the fast running time of insertion sort when its input is \"nearly\" sorted. Upon calling quicksort on a subarray with fewer than $k$ elements, let it simply return without sorting the subarray. After the top-level call to quicksort returns, run insertion sort on the entire array to finish the sorting process. Argue that this sorting algorithm runs in $O(nk + n\\lg(n / k))$ expected time. How should we pick $k$, both in theory and practice? In the quicksort part of the proposed algorithm, the recursion stops at level $\\lg(n / k)$, which makes the expected running time $O(n\\lg(n / k))$. However, this leaves $n / k$ non-sorted, non - intersecting subarrays of (maximum) length $k$. Because of the nature of the insertion sort algorithm, it will first sort fully one such subarray before consider the next one. Thus, it has the same complexity as sorting each of those arrays, that is $\\frac{n}{k}O(k^2) = O(nk)$. In theory, if we ignore the constant factors, we need to solve $$ \\begin{aligned} & n\\lg n \\ge nk + n\\lg{n / k} \\\\ \\Rightarrow & \\lg n \\ge k + \\lg n - \\lg k \\\\ \\Rightarrow & \\lg k \\ge k. \\end{aligned} $$ Which is not possible. If we add the constant factors, we get $$ \\begin{aligned} & c_qn\\lg n \\ge c_ink + c_qn\\lg(n / k) \\\\ \\Rightarrow & c_q\\lg n \\ge c_ik + c_q\\lg n - c_q\\lg k \\\\ \\Rightarrow & \\lg k \\ge \\frac{c_i}{c_q}k. \\end{aligned} $$ Which indicates that there might be a good candidate. Furthermore, the lower-order terms should be taken into consideration too. In practice, $k$ should be chosed by experiment.","title":"7.4-5"},{"location":"Chap07/7.4/#74-6-star","text":"Consider modifying the $\\text{PARTITION}$ procedure by randomly picking three elements from array $A$ and partitioning about their median (the middle value of the three elements). Approximate the probability of getting at worst an $\\alpha$-to-$(1 - \\alpha)$ split, as a function of $\\alpha$ in the range $0 < \\alpha < 1$. First, for simplicity's sake, let's assume that we can pick the same element twice. Let's also assume that $0 < \\alpha \\le 1 / 2$. In order to get such a split, two out of three elements need to be in the smallest $\\alpha n$ elements. The probability of having one is $\\alpha n / n = \\alpha$. The probability of having exactly two is $\\alpha^2 - \\alpha^3$. There are three ways in which two elements can be in the smallest $\\alpha n$ and one way in which all three can be in the smallest $\\alpha n$ so the probability of getting such a median is $3\\alpha^2 - 2\\alpha^3$. We will get the same split if the median is in the largest $\\alpha n$. Since the two events are mutually exclusive, the probability is $$\\Pr\\{\\text{OK split}\\} = 6\\alpha^2 - 4\\alpha^3 = 2\\alpha^2(3 - 2\\alpha).$$","title":"7.4-6 $\\star$"},{"location":"Chap07/Problems/7-1/","text":"The version of $\\text{PARTITION}$ given in this chapter is not the original partitioning algorithm. Here is the original partition algorithm, which is due to C.A.R. Hoare: HOARE - PARTITION ( A , p , r ) x = A [ p ] i = p - 1 j = r + 1 while true repeat j = j - 1 until A [ j ] \u2264 x repeat i = i + 1 until A [ i ] \u2265 x if i < j exchange A [ i ] with A [ j ] else return j a. Demonstrate the operation of $\\text{HOARE-PARTITION}$ on the array $A = \\langle 13, 19, 9, 5, 12, 8, 7, 4, 11, 2, 6, 21 \\rangle$, showing the values of the array and auxiliary values after each iteration of the while loop in lines 4-13. The next three questions ask you to give a careful argument that the procedure $\\text{HOARE-PARTITION}$ is correct. Assuming that the subarray $A[p..r]$ contains at least two elements, prove the following: b. The indices $i$ and $j$ are such that we never access an element of $A$ outside the subarray $A[p..r]$. c. When $\\text{HOARE-PARTITION}$ terminates, it returns a value $j$ such that $p \\le j < r$. d. Every element of $A[p..j]$ is less than or equal to every element of $A[j + 1..r]$ when $\\text{HOARE-PARTITION}$ terminates. The $\\text{PARTITION}$ procedure in section 7.1 separates the pivot value (originally in $A[r]$) from the two partitions it forms. The $\\text{HOARE-PARTITION}$ procedure, on the other hand, always places the pivot value (originally in $A[p]$) into one of the two parititions $A[p..j]$ and $A[j + 1..r]$. Since $p \\le j < r$, this split is always nontrivial. e. Rewrite the $\\text{QUICKSORT}$ procedure to use $\\text{HOARE-PARTITION}$. a. After the end of the loop, the variables have the following values: $x = 13$, $j = 9$ and $i = 10$. b. Because when $\\text{HOARE-PARTITION}$ is running, $p \\le i < j \\le r$ will always hold, $i$, $j$ won't access any element of $A$ outside the subarray $A[p..r]$. c. When $i \\ge j$, $\\text{HOARE-PARTITION}$ terminates, so $p \\le j < r$. d. When $\\text{HOARE-PARTITION}$ terminates, $A[p..j] \\le x \\le A[j + 1..r]$. e. HOARE - QUICKSORT ( A , p , r ) if p < r q = HOARE - PARTITION ( A , p , r ) HOARE - QUICKSORT ( A , p , q ) HOARE - QUICKSORT ( A , q + 1 , r )","title":"7-1 Hoare partition correctness"},{"location":"Chap07/Problems/7-2/","text":"The analysis of the expected running time of randomized quicksort in section 7.4.2 assumes that all element values are distinct. In this problem. we examine what happens when they are not. a. Suppose that all element values are equal. What would be randomized quick-sort's running time in this case? b. The $\\text{PARTITION}$ procedure returns an index $q$ such that each element of $A[p..q - 1]$ is less than or equal to $A[q]$ and each element of $A[q + 1..r]$ is greater than $A[q]$. Modify the $\\text{PARTITION}$ procedure to produce a procedure $\\text{PARTITION}'(A, p, r)$ which permutes the elements of $A[p..r]$ and returns two indices $q$ and $t$ where $p \\le q \\le t \\le r$, such that all elements of $A[q..t]$ are equal, each element of $A[p..q - 1]$ is less than $A[q]$, and each element of $A[t + 1..r]$ is greater than $A[q]$. Like $\\text{PARTITION}$, your $\\text{PARTITION}'$ procedure should take $\\Theta(r - p)$ time. c. Modify the $\\text{RANDOMIZED-QUICKSORT}$ procedure to call $\\text{PARTITION}'$, and name the new procedure $\\text{RANDOMIZED-QUICKSORT}'$. Then modify the $\\text{QUICKSORT}$ procedure to produce a procedure $\\text{QUICKSORT}'(p, r)$ that calls $\\text{RANDOMIZED-PARTITION}'$ and recurses only on partitions of elements not know to be equal to each other. d. Using $\\text{QUICKSORT}'$, how would you adjust the analysis of section 7.4.2 to avoid the assumption that all elements are distinct? a. Since all elements are equal, $\\text{RANDOMIZED-QUICKSORT}$ always returns $q = r$. We have recurrence $T(n) = T(n - 1) + \\Theta(n) = \\Theta(n^2)$. b. PARTITION ' ( A , p , r ) x = A [ p ] low = p high = p for j = p + 1 to r if A [ j ] < x y = A [ j ] A [ j ] = A [ high + 1 ] A [ high + 1 ] = A [ low ] A [ low ] = y low = low + 1 high = high + 1 else if A [ j ] == x exchange A [ high + 1 ] with A [ j ] high = high + 1 return ( low , high ) c. QUICKSORT ' ( A , p , r ) if p < r ( low , high ) = RANDOMIZED - PARTITION ' ( A , p , r ) QUICKSORT ' ( A , p , low - 1 ) QUICKSORT ' ( A , high + 1 , r ) d. Since we don't recurse on elements equal to the pivot, the subproblem sizes with $\\text{QUICKSORT}'$ are no larger than the subproblem sizes with $\\text{QUICKSORT}$ when all elements are distinct.","title":"7-2 Quicksort with equal element values"},{"location":"Chap07/Problems/7-3/","text":"An alternative analysis of the running time of randomized quicksort focuses on the expected running time of each individual recursive call to $\\text{RANDOMIZED-QUICKSORT}$, rather than on the number of comparisons performed. a. Argue that, given an array of size $n$, the probability that any particular element is chosen as the pivot is $1 / n$. Use this to define indicator random variables $$X_i = I\\{i\\text{th smallest element is chosen as the pivot}\\}.$$ What is $\\text E[X_i]$? b. Let $T(n)$ be a random variable denoting the running time of quicksort on an array of size $n$. Argue that $$\\text E[T(n)] = \\text E\\bigg[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))\\bigg]. \\tag{7.5}$$ c. Show that we can rewrite equation $\\text{(7.5)}$ as $$\\text E[T(n)] = \\frac{2}{n}\\sum_{q = 2}^{n - 1}\\text E[T(q)] + \\Theta(n). \\tag{7.6}$$ d. Show that $$\\sum_{k = 2}^{n - 1}k\\lg k \\le \\frac{1}{2}n^2\\lg n - \\frac{1}{8}n^2. \\tag{7.7}$$ ($\\textit{Hint:}$ Split the summation into two parts, one for $k = 2, 3, \\ldots, \\lceil n / 2 \\rceil - 1$ and one for $k = \\lceil n / 2 \\rceil, \\ldots, n - 1$.) e. Using the bound from equation $\\text{(7.7)}$, show that the recurrence in equation $\\text{(7.6)}$ has the solution $\\text E[T(n)] = \\Theta(n\\lg n)$. ($\\textit{Hint:}$ Show, by substitution, that $\\text E[T(n)] \\le an\\lg n$ for sufficiently large $n$ and for some positive constant $a$.) a. Since the pivot is selected as a random element in the array, which has size $n$, the probabilities of any particular element being selected are all equal, and add to one, so, are all $\\frac{1}{n}$. As such, $\\text E[X_i] = \\Pr\\{i \\text{ smallest is picked}\\} = \\frac{1}{n}$. b. We can apply linearity of expectation over all of the events $X_i$. Suppose we have a particular $X_i$ be true, then, we will have one of the sub arrays be length $i - 1$, and the other be $n - i$, and will of course still need linear time to run the partition procedure. This corresponds exactly to the summand in equation $\\text{(7.5)}$. c. $$ \\begin{aligned} & \\text E\\Bigg[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n)) \\Bigg] \\\\ & = \\sum_{q = 1}^n \\text E[X_q(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ & = \\sum_{q = 1}^n(T(q - 1) + T(n - q) + \\Theta(n))/n \\\\ & = \\Theta(n) + \\frac{1}{n} \\sum_{q = 1}^n(T(q - 1)+T(n - 1)) \\\\ & = \\Theta(n) + \\frac{1}{n} \\Big(\\sum_{q = 1}^n T(q - 1) + \\sum_{q = 1}^n T (n - q) \\Big) \\\\ & = \\Theta(n) + \\frac{1}{n} \\Big(\\sum_{q = 1}^n T(q - 1) + \\sum_{q = 1}^n T (q - 1) \\Big) \\\\ & = \\Theta(n) + \\frac{2}{n} \\sum_{q = 1}^n T(q - 1) \\\\ & = \\Theta(n) + \\frac{2}{n} \\sum_{q = 0}^{n - 1} T(q) \\\\ & = \\Theta(n) + \\frac{2}{n} \\sum_{q = 2}^{n - 1} T(q). \\end{aligned} $$ d. We will prove this inequality in a different way than suggested by the hint. If we let $f(k) = k\\lg k$ treated as a continuous function, then $f'(k) = \\lg k + 1$. Note now that the summation written out is the left hand approximation of the integral of $f(k)$ from $2$ to $n$ with step size $1$. By integration by parts, the anti-derivative of $k\\lg k$ is $$\\frac{1}{\\lg 2}(\\frac{k^2}{2}\\ln k-\\frac{k^2}{4}).$$ So, plugging in the bounds and subtracting, we get $\\frac{n^2\\lg n}{2} - \\frac{n^2}{4\\ln 2} - 1$. Since $f$ has a positive derivative over the entire interval that the integral is being evaluated over, the left hand rule provides a underapproximation of the integral, so, we have that $$ \\begin{aligned} \\sum_{k = 2}^{n - 1} k\\lg k & \\le \\frac{n^2\\lg n}{2} - \\frac{n^2}{4\\ln 2} - 1 \\\\ & \\le \\frac{n^2\\lg n}{2} - \\frac{n^2}{8}, \\end{aligned} $$ where the last inequality uses the fact that $\\ln 2 > 1 / 2$. e. Assume by induction that $T(q) \\le q \\lg(q) + \\Theta(n)$. Combining $\\text{(7.6)}$ and $\\text{(7.7)}$, we have $$ \\begin{aligned} \\text E[T(n)] & = \\frac{2}{n} \\sum_{q = 2}^{n - 1} \\text E[T(q)] + \\Theta(n) \\\\ & \\le \\frac{2}{n} \\sum_{q = 2}^{n - 1}(q\\lg q + \\Theta(n)) + \\Theta(n) \\\\ & \\le \\frac{2}{n} \\sum_{q = 2}^{n - 1}q\\lg q + \\frac{2}{n}\\Theta(n) + \\Theta(n) \\\\ & \\le \\frac{2}{n}(\\frac{1}{2}n^2\\lg n - \\frac{1}{8}n^2) + \\Theta(n) \\\\ & = n\\lg n -\\frac{1}{4}n + \\Theta(n) \\\\ & = n\\lg n+\\Theta(n). \\end{aligned} $$","title":"7-3 Alternative quicksort analysis"},{"location":"Chap07/Problems/7-4/","text":"The $\\text{QUICKSORT}$ algorithm of Section 7.1 contains two recursive calls to itself. After $\\text{QUICKSORT}$ calls $\\text{PARTITION}$, it recursively sorts the left subarray and then it recursively sorts the right subarray. The second recursive call in $\\text{QUICKSORT}$ is not really necessary; we can avoid it by using an iterative control structure. This technique, called tail recursion , is provided automatically by good compilers. Consider the following version of quicksort, which simulates tail recursion: TAIL - RECURSIVE - QUICKSORT ( A , p , r ) while p < r // Partition and sort left subarray. q = PARTITION ( A , p , r ) TAIL - RECURSIVE - QUICKSORT ( A , p , q - 1 ) p = q + 1 a. Argue that $\\text{TAIL-RECURSIVE-QUICKSORT}(A, 1, A.length)$ correctly sorts the array $A$. Compilers usually execute recursive procedures by using a stack that contains pertinent information, including the parameter values, for each recursive call. The information for the most recent call is at the top of the stack, and the information for the initial call is at the bottom. Upon calling a procedure, its information is pushed onto the stack; when it terminates, its information is popped . Since we assume that array parameters are represented by pointers, the information for each procedure call on the stack requires $O(1)$ stack space. The stack depth is the maximum amount of stack space used at any time during a computation. b. Describe a scenario in which $\\text{TAIL-RECURSIVE-QUICKSORT}$'s stack depth is $\\Theta(n)$ on an $n$-element input array. c. Modify the code for $\\text{TAIL-RECURSIVE-QUICKSORT}$ so that the worst-case stack depth is $\\Theta(\\lg n)$. Maintain the $O(n\\lg n)$ expected running time of the algorithm. a. The book proved that $\\text{QUICKSORT}$ correctly sorts the array $A$. $\\text{TAIL-RECURSIVE-QUICKSORT}$ differs from $\\text{QUICKSORT}$ in only the last line of the loop. It is clear that the conditions starting the second iteration of the while loop in $\\text{TAIL-RECURSIVE-QUICKSORT}$ are identical to the conditions starting the second recursive call in $\\text{QUICKSORT}$. Therefore, $\\text{TAIL-RECURSIVE-QUICKSORT}$ effectively performs the sort in the same manner as $\\text{QUICKSORT}$. Therefore, $\\text{TAIL-RECURSIVE-QUICKSORT}$ must correctly sort the array $A$. b. The stack depth will be $\\Theta(n)$ if the input array is already sorted. The right subarray will always have size $0$ so there will be $n \u2212 1$ recursive calls before the while -condition $p < r$ is violated. c. MODIFIED - TAIL - RECURSIVE - QUICKSORT ( A , p , r ) while p < r q = PARTITION ( A , p , r ) if q < floor (( p + r ) / 2 ) MODIFIED - TAIL - RECURSIVE - QUICKSORT ( A , p , q - 1 ) p = q + 1 else MODIFIED - TAIL - RECURSIVE - QUICKSORT ( A , q + 1 , r ) r = q - 1","title":"7-4 Stack depth for quicksort"},{"location":"Chap07/Problems/7-5/","text":"One way to improve the $\\text{RANDOMIZED-QUICKSORT}$ procedure is to partition around a pivot that is chosen more carefully than by picking a random element from the subarray. One common approach is the median-of-3 method: choose the pivot as the median (middle element) of a set of 3 elements randomly selected from the subarray. (See exercise 7.4-6.) For this problem, let us assume that the elements of the input array $A[1..n]$ are distinct and that $n \\ge 3$. We denote the sorted output array by $A'[1..n]$. Using the median-of-3 method to choose the pivot element $x$, define $p_i = \\Pr\\{x = A'[i]\\}$. a. Give an exact formula for $p_i$ as a function of $n$ and $i$ for $i = 2, 3, \\ldots, n - 1$. (Note that $p_1 = p_n = 0$.) b. By what amount have we increased the likelihood of choosing the pivot as $x = A'[\\lfloor (n + 1) / 2 \\rfloor]$, the median of $A[1..n]$, compared with the ordinary implementation? Assume that $n \\to \\infty$, and give the limiting ratio of these probabilities. c. If we define a \"good\" split to mean choosing the pivot as $x = A'[i]$, where $n / 3 \\le i \\le 2n / 3$, by what amount have we increased the likelihood of getting a good split compared with the ordinary implementation? ($\\textit{Hint:}$ Approximate the sum by an integral.) d. Argue that in the $\\Omega(n\\lg n)$ running time of quicksort, the median-of-3 method affects only the constant factor. a. $p_i$ is the probability that a randomly selected subset of size three has the $A'[i]$ as it's middle element. There are 6 possible orderings of the three elements selected. So, suppose that $S'$ is the set of three elements selected. We will compute the probability that the second element of $S'$ is $A'[i]$ among all possible $3$-sets we can pick, since there are exactly six ordered $3$-sets corresponding to each $3$-set, these probabilities will be equal. We will compute the probability that $S'[2] = A[i]$. For any such $S'$, we would need to select the first element from $[i - 1]$ and the third from ${i + 1, \\ldots , n}$. So, there are $(i - 1)(n - i)$ such $3$-sets. The total number of $3$-sets is $\\binom{n}{3} = \\frac{n(n - 1)(n - 2)}{6}$. So, $$p_i = \\frac{6(n - i)(i - 1)}{n(n - 1)(n - 2)}.$$ b. If we let $i = \\lfloor \\frac{n + 1}{2} \\rfloor$, the previous result gets us an increase of $$\\frac{6(\\lfloor\\frac{n - 1}{2}\\rfloor)(n - \\lfloor\\frac{n + 1}{2}\\rfloor)}{n(n - 1)(n - 2)} - \\frac{1}{n}$$ in the limit $n$ going to infinity, we get $$\\lim_{n \\to \\infty} \\frac{\\frac{6(\\lfloor \\frac{n - 1}{2} \\rfloor)(n - \\lfloor \\frac{n + 1}{2} \\rfloor)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} = \\frac{3}{2}.$$ c. To save the messiness, suppose $n$ is a multiple of $3$. We will approximate the sum as an integral, so, $$ \\begin{aligned} \\sum_{i = n / 3}^{2n / 3} & \\approx \\int_{n / 3}^{2n / 3} \\frac{6(-x^2 + nx + x - n)}{n(n - 1)(n - 2)}dx \\\\ & = \\frac{6(-7n^3 / 81 + 3n^3 / 18 + 3n^2 / 18 - n^2 / 3)}{n(n - 1)(n - 2)}, \\end{aligned} $$ which, in the limit $n$ goes to infinity, is $\\frac{13}{27}$ which is a constant that $>\\frac{1}{3}$ as it was in the original randomized quicksort implementation. d. Even though we always choose the middle element as the pivot (which is the best case), the height of the recursion tree will be $\\Theta(\\lg n)$. Therefore, the running time is still $\\Omega(n\\lg n)$.","title":"7-5 Median-of-3 partition"},{"location":"Chap07/Problems/7-6/","text":"Consider the problem in which we do not know the numbers exactly. Instead, for each number, we know an interval on the real line to which it belongs. That is, we are given $n$ closed intervals of the form $[a_i, b_i]$, where $a_i \\le b_i$. We wish to fuzzy-sort these intervals, i.e., to produce a permutation $\\langle i_1, i_2, \\ldots, i_n \\rangle$ of the intervals such that for $j = 1, 2, \\ldots, n$, there exists $c_j \\in [a_{i_j}, b_{i_j}]$ satisfying $c_1 \\le c_2 \\le \\cdots \\le c_n$. a. Design a randomized algorithm for fuzzy-sorting $n$ intervals. Your algorithm should have the general structure of an algorithm that quicksorts the left endpoints (the $a_i$ values), but it should take advantage of overlapping intervals to improve the running time. (As the intervals overlap more and more, the problem of fuzzy-sorting the intervals becoes progressively easier. Your algorithm should take advantage of such overlapping, to the extend that it exists.) b. Argue that your algorithm runs in expected time $\\Theta(n\\lg n)$ in general, but runs in expected time $\\Theta(n)$ when all of the intervals overlap (i.e., when there exists a value $x$ such that $x \\in [a_i, b_i]$ for all $i$). Your algorithm should not be checking for this case explicitly; rather, its performance should naturally improve as the amount of overlap increases. a. With randomly selected left endpoint for the pivot, we could trivially perform fuzzy sorting by quicksorting the left endpoints, $a_i$'s. This would achieve the worst-case expected running time of $\\Theta(n\\lg n)$. We definitely can do better by exploit the characteristic that we don't have to sort overlapping intervals. That is, for two overlapping intervals, $[a_i, b_i]$ and $[a_j, b_j]$. In such situations, we can always choose $\\{c_i, c_j\\}$ (within the intersection of these intervals) such that $c_i \\le c_j$ or $c_j \\le c_i$. Since overlapping intervals do not require sorting, we can improve the expected running time by modifying quicksort to identify overlaps: FIND - INTERSECTION ( A , p , r ) rand = RANDOM ( p , r ) exchange A [ rand ] with A [ r ] a = A [ r ]. a b = A [ r ]. b for i = p to r - 1 if A [ i ]. a \u2264 b and A [ i ]. b \u2265 a if A [ i ]. a > a a = A [ i ]. a if A [ i ]. b < b b = A [ i ]. b return ( a , b ) On lines 2 through 3 of $\\text{FIND-INTERSECTION}$, we select a random pivot interval as the initial region of overlap $[a ,b]$. There are two situations: If the intervals are all disjoint, then the estimated region of overlap will be this randomly-selected interval; otherwise, on lines 6 through 11, we loop through all intervals in arrays $A$ (except the endpoint which is the initial pivot interval). At each iteration, we determine if the current interval overlaps the current estimated region of overlap. If it does, we update the estimated region of overlap as $[a, b] = [a_i, b_i] \\cap [a, b]$. $\\text{FIND-INTERSECTION}$ has a worst-case running time $\\Theta(n)$ since we evaluate the intersection from index $1$ to $A.length$ of the array. We can extend the $\\text{QUICKSORT}$ to allow fuzzy sorting using $\\text{FIND-INTERSECTION}$. First, partition the input array into \"left\", \"middle\", and \"right\" subarrays. The \"middle\" subarray elements overlap the interval $[a, b]$ found by $\\text{FIND-INTERSECTION}$. As a result, they can appear in any order in the output. We recursively call $\\text{FUZZY-SORT}$ on the \"left\" and \"right\" subarrays to produce a fuzzy sorted array in-place. The following pseudocode implements these basic operations. One can run $\\text{FUZZY-SORT}(A, 1, A.length)$ to fuzzy-sort an array. The first and last elements in a subarray are indexed by $p$ and $r$, respectively. The index of the first and last intervals in the \"middle\" region are indexed by $q$ and $t$, respectively. FUZZY - SORT ( A , p , r ) if p < r ( a , b ) = FIND - INTERSECTION ( A , p , r ) t = PARTITION - RIGHT ( A , a , p , r ) q = PARTITION - LEFT ( A , b , p , t ) FUZZY - SORT ( A , p , q - 1 ) FUZZY - SORT ( A , t + 1 , r ) We need to determine how to partition the input arrays into \"left\", \"middle\", and \"right\" subarrays in-place. First, we $\\text{PARTITION-RIGHT}$ the entire array from $p$ to $r$ using a pivot value equal to the left endpoint $a$ found by $\\text{FIND-INTERSECTION}$, such that $a_i \\le a$. Then, we $\\text{PARTITION-LEFT}$ the subarray from $p$ to $t$ using a pivot value equal to the right endpoint $b$ found by $\\text{FIND-INTERSECTION}$, such that $b_i < b$. PARTITION - RIGHT ( A , a , p , r ) i = p - 1 for j = p to r - 1 if A [ j ]. a \u2264 a i = i + 1 exchange A [ i ] with A [ j ] exchange A [ i + 1 ] with A [ r ] return i + 1 PARTITION - LEFT ( A , b , p , t ) i = p - 1 for j = p to t - 1 if A [ j ]. b < b i = i + 1 exchange A [ i ] with A [ j ] exchange A [ i + 1 ] with A [ t ] return i + 1 The $\\text{FUZZY-SORT}$ is similar to the randomized quicksort presented in the textbook. In fact, $\\text{PARTITION-RIGHT}$ and $\\text{PARTITION-LEFT}$ are nearly identical to the $\\text{PARTITION}$ procedure on page 171. The primary difference is the value of the pivot used to sort the intervals. b. We expect $\\text{FUZZY-SORT}$ to have a worst-case running time $\\Theta(n\\lg n)$ for a set of input intervals which do not overlap each other. First, notice that lines 2 through 3 of $\\text{FIND-INTERSECTION}$ select a random interval as the initial pivot interval. Recall that if the intervals are disjoint, then $[a, b]$ will simply be this initial interval. Since for this example there are no overlaps, the \"middle\" region created by lines 4 and 5 of $\\text{FUZZY-SORT}$ will only contain the initially-selected interval. In general, line 3 is $\\Theta(n)$. Fortunately, since the pivot interval $[a, b]$ is randomly-selected, the expected sizes of the \"left\" and \"right\" subarrays are both $\\left\\lfloor \\frac{n}{2} \\right\\rfloor$. In conclusion, the reccurrence of the running time is $$ \\begin{aligned} T(n) & \\le 2T(n / 2) + \\Theta(n) \\\\ & = \\Theta(n\\lg n). \\end{aligned} $$ The $\\text{FIND-INTERSECTION}$ will always return a non-empty region of overlap $[a, b]$ containing $x$ if the intervals all overlap at $x$. For this situation, every interval will be within the \"middle\" region since the \"left\" and \"right\" subarrays will be empty, lines 6 and 7 of $\\text{FUZZY-SORT}$ are $\\Theta(1)$. As a result, there is no recursion and the running time of $\\text{FUZZY-SORT}$ is determined by the $\\Theta(n)$ running time required to find the region of overlap. Therfore, if the input intervals all overlap at a point, then the expected worst-case running time is $\\Theta(n)$.","title":"7-6 Fuzzy sorting of intervals"},{"location":"Chap08/8.1/","text":"8.1-1 What is the smallest possible depth of a leaf in a decision tree for a comparison sort? For a permutation $a_1 \\le a_2 \\le \\ldots \\le a_n$, there are $n - 1$ pairs of relative ordering, thus the smallest possible depth is $n - 1$. 8.1-2 Obtain asymptotically tight bounds on $\\lg(n!)$ without using Stirling's approximation. Instead, evaluate the summation $\\sum_{k = 1}^n\\lg k$ using techniques from Section A.2. $$ \\begin{aligned} \\sum_{k = 1}^n\\lg k & \\le \\sum_{k = 1}^n\\lg n \\\\ & = n\\lg n. \\end{aligned} $$ $$ \\begin{aligned} \\sum_{k = 1}^n\\lg k & = \\sum_{k = 2}^{n / 2} \\lg k + \\sum_{k = n / 2}^n\\lg k \\\\ & \\ge \\sum_{k = 1}^{n / 2} 1 + \\sum_{k = n / 2}^n\\lg n / 2 \\\\ & = \\frac{n}{2} + \\frac{n}{2}(\\lg n - 1) \\\\ & = \\frac{n}{2}\\lg n. \\end{aligned} $$ 8.1-3 Show that there is no comparison sort whose running time is linear for at least half of the $n!$ inputs of length $n$. What about a fraction of $1 / n$ of the inputs of length $n$? What about a fraction $1 / 2^n$? Consider a decision tree of height $h$ with $r$ reachable leaves corresponding to a comparison sort on $n$ elements. From Theorem 8.1 , We have $n! / 2 \\le n! \\le r \\le 2^h$. By taking logarithms, we have $$h \\ge \\lg (n! / 2) = \\lg (n!) - 1 = \\Theta (n\\lg n) - 1 = \\Theta (n\\lg n).$$ From the equation above, there is no comparison sort whose running time is linear for at least half of the $n!$ inputs of length $n$. Consider the $1/n$ of inputs of length $n$ condition. we have $(1/n)n! \\le n! \\le r \\le 2^h$. By taking logarithms, we have $$h \\ge \\lg (n! / n) = \\lg (n!) - \\lg n = \\Theta (n\\lg n) - \\lg n = \\Theta (n\\lg n).$$ From the equation above, there is no comparison sort whose running time is linear for $1/n$ of the $n!$ inputs of length $n$. Consider the $1 / 2^n$ of inputs of length $n$ condition. we have $(1/2^n)n! \\le n! \\le r \\le 2^h$. By taking logarithms, we have $$h \\ge \\lg (n! / 2^n) = \\lg (n!) - n = \\Theta (n\\lg n) - n = \\Theta (n\\lg n).$$ From the equation above, there is no comparison sort whose running time is linear for $1/2^n$ of the $n!$ inputs of length $n$. 8.1-4 Suppose that you are given a sequence of $n$ elements to sort. The input sequence consists of $n / k$ subsequences, each containing $k$ elements. The elements in a given subsequence are all smaller than the elements in the succeeding subsequence and larger than the elements in the preceding subsequence. Thus, all that is needed to sort the whole sequence of length $n$ is to sort the $k$ elements in each of the $n / k$ subsequences. Show an $\\Omega(n\\lg k)$ lower bound on the number of comparisons needed to solve this variant of the sorting problem. ($\\textit{Hint:}$ It is not rigorous to simply combine the lower bounds for the individual subsequences.) Assume that we need to construct a binary decision tree to represent comparisons. Since length of each subsequece is $k$, there are $(k!)^{n / k}$ possible output permutations. To compute the height $h$ of the decision tree, we must have $(k!)^{n / k} \\le 2^h$. Taking logs on both sides, we know that $$h \\ge \\frac{n}{k} \\times \\lg (k!) \\ge \\frac{n}{k} \\times \\left( \\frac{k\\ln k - k}{\\ln 2} \\right) = \\frac{n\\ln k - n}{\\ln 2} = \\Omega (n\\lg k).$$","title":"8.1 Lower bounds for sorting"},{"location":"Chap08/8.1/#81-1","text":"What is the smallest possible depth of a leaf in a decision tree for a comparison sort? For a permutation $a_1 \\le a_2 \\le \\ldots \\le a_n$, there are $n - 1$ pairs of relative ordering, thus the smallest possible depth is $n - 1$.","title":"8.1-1"},{"location":"Chap08/8.1/#81-2","text":"Obtain asymptotically tight bounds on $\\lg(n!)$ without using Stirling's approximation. Instead, evaluate the summation $\\sum_{k = 1}^n\\lg k$ using techniques from Section A.2. $$ \\begin{aligned} \\sum_{k = 1}^n\\lg k & \\le \\sum_{k = 1}^n\\lg n \\\\ & = n\\lg n. \\end{aligned} $$ $$ \\begin{aligned} \\sum_{k = 1}^n\\lg k & = \\sum_{k = 2}^{n / 2} \\lg k + \\sum_{k = n / 2}^n\\lg k \\\\ & \\ge \\sum_{k = 1}^{n / 2} 1 + \\sum_{k = n / 2}^n\\lg n / 2 \\\\ & = \\frac{n}{2} + \\frac{n}{2}(\\lg n - 1) \\\\ & = \\frac{n}{2}\\lg n. \\end{aligned} $$","title":"8.1-2"},{"location":"Chap08/8.1/#81-3","text":"Show that there is no comparison sort whose running time is linear for at least half of the $n!$ inputs of length $n$. What about a fraction of $1 / n$ of the inputs of length $n$? What about a fraction $1 / 2^n$? Consider a decision tree of height $h$ with $r$ reachable leaves corresponding to a comparison sort on $n$ elements. From Theorem 8.1 , We have $n! / 2 \\le n! \\le r \\le 2^h$. By taking logarithms, we have $$h \\ge \\lg (n! / 2) = \\lg (n!) - 1 = \\Theta (n\\lg n) - 1 = \\Theta (n\\lg n).$$ From the equation above, there is no comparison sort whose running time is linear for at least half of the $n!$ inputs of length $n$. Consider the $1/n$ of inputs of length $n$ condition. we have $(1/n)n! \\le n! \\le r \\le 2^h$. By taking logarithms, we have $$h \\ge \\lg (n! / n) = \\lg (n!) - \\lg n = \\Theta (n\\lg n) - \\lg n = \\Theta (n\\lg n).$$ From the equation above, there is no comparison sort whose running time is linear for $1/n$ of the $n!$ inputs of length $n$. Consider the $1 / 2^n$ of inputs of length $n$ condition. we have $(1/2^n)n! \\le n! \\le r \\le 2^h$. By taking logarithms, we have $$h \\ge \\lg (n! / 2^n) = \\lg (n!) - n = \\Theta (n\\lg n) - n = \\Theta (n\\lg n).$$ From the equation above, there is no comparison sort whose running time is linear for $1/2^n$ of the $n!$ inputs of length $n$.","title":"8.1-3"},{"location":"Chap08/8.1/#81-4","text":"Suppose that you are given a sequence of $n$ elements to sort. The input sequence consists of $n / k$ subsequences, each containing $k$ elements. The elements in a given subsequence are all smaller than the elements in the succeeding subsequence and larger than the elements in the preceding subsequence. Thus, all that is needed to sort the whole sequence of length $n$ is to sort the $k$ elements in each of the $n / k$ subsequences. Show an $\\Omega(n\\lg k)$ lower bound on the number of comparisons needed to solve this variant of the sorting problem. ($\\textit{Hint:}$ It is not rigorous to simply combine the lower bounds for the individual subsequences.) Assume that we need to construct a binary decision tree to represent comparisons. Since length of each subsequece is $k$, there are $(k!)^{n / k}$ possible output permutations. To compute the height $h$ of the decision tree, we must have $(k!)^{n / k} \\le 2^h$. Taking logs on both sides, we know that $$h \\ge \\frac{n}{k} \\times \\lg (k!) \\ge \\frac{n}{k} \\times \\left( \\frac{k\\ln k - k}{\\ln 2} \\right) = \\frac{n\\ln k - n}{\\ln 2} = \\Omega (n\\lg k).$$","title":"8.1-4"},{"location":"Chap08/8.2/","text":"8.2-1 Using Figure 8.2 as a model, illustrate the operation of $\\text{COUNTING-SORT}$ on the array $A = \\langle 6, 0, 2, 0, 1, 3, 4, 6, 1, 3, 2 \\rangle$. We have that $C = \\langle 2, 4, 6, 8, 9, 9, 11 \\rangle$. Then, after successive iterations of the loop on lines 10-12, we have $$ \\begin{aligned} B & = \\langle, , , , , 2, , , , , \\rangle, \\\\ B & = \\langle, , , , , 2, 3, , , \\rangle, \\\\ B & = \\langle, , , 1, , 2, 3, , , \\rangle \\end{aligned} $$ and at the end, $$B = \\langle 0, 0, 1, 1, 2, 2, 3, 3, 4, 6, 6 \\rangle.$$ 8.2-2 Prove that $\\text{COUNTING-SORT}$ is stable. Suppose positions $i$ and $j$ with $i < j$ both contain some element $k$. We consider lines 10 through 12 of $\\text{COUNTING-SORT}$, where we construct the output array. Since $j > i$, the loop will examine $A[j]$ before examining $A[i]$. When it does so, the algorithm correctly places $A[j]$ in position $m = C[k]$ of $B$. Since $C[k]$ is decremented in line 12, and is never again incremented, we are guaranteed that when the for loop examines $A[i]$ we will have $C[k] < m$. Therefore $A[i]$ will be placed in an earlier position of the output array, proving stability. 8.2-3 Suppose that we were to rewrite the for loop header in line 10 of the $\\text{COUNTING-SORT}$ as 10 for j = 1 to A . length Show that the algorithm still works properly. Is the modified algorithm stable? The algorithm still works correctly. The order that elements are taken out of $C$ and put into $B$ doesn't affect the placement of elements with the same key. It will still fill the interval $(C[k \u2212 1], C[k]]$ with elements of key $k$. The question of whether it is stable or not is not well phrased. In order for stability to make sense, we would need to be sorting items which have information other than their key, and the sort as written is just for integers, which don't. We could think of extending this algorithm by placing the elements of $A$ into a collection of elements for each cell in array $C$. Then, if we use a FIFO collection, the modification of line 10 will make it stable, if we use LILO, it will be anti-stable. 8.2-4 Describe an algorithm that, given n integers in the range $0$ to $k$, preprocesses its input and then answers any query about how many of the $n$ integers fall into a range $[a..b]$ in $O(1)$ time. Your algorithm should use $\\Theta(n + k)$ preprocessing time. The algorithm will begin by preprocessing exactly as $\\text{COUNTING-SORT}$ does in lines 1 through 9, so that $C[i]$ contains the number of elements less than or equal to $i$ in the array. When queried about how many integers fall into a range $[a..b]$, simply compute $C[b] \u2212 C[a \u2212 1]$. This takes $O(1)$ times and yields the desired output.","title":"8.2 Counting sort"},{"location":"Chap08/8.2/#82-1","text":"Using Figure 8.2 as a model, illustrate the operation of $\\text{COUNTING-SORT}$ on the array $A = \\langle 6, 0, 2, 0, 1, 3, 4, 6, 1, 3, 2 \\rangle$. We have that $C = \\langle 2, 4, 6, 8, 9, 9, 11 \\rangle$. Then, after successive iterations of the loop on lines 10-12, we have $$ \\begin{aligned} B & = \\langle, , , , , 2, , , , , \\rangle, \\\\ B & = \\langle, , , , , 2, 3, , , \\rangle, \\\\ B & = \\langle, , , 1, , 2, 3, , , \\rangle \\end{aligned} $$ and at the end, $$B = \\langle 0, 0, 1, 1, 2, 2, 3, 3, 4, 6, 6 \\rangle.$$","title":"8.2-1"},{"location":"Chap08/8.2/#82-2","text":"Prove that $\\text{COUNTING-SORT}$ is stable. Suppose positions $i$ and $j$ with $i < j$ both contain some element $k$. We consider lines 10 through 12 of $\\text{COUNTING-SORT}$, where we construct the output array. Since $j > i$, the loop will examine $A[j]$ before examining $A[i]$. When it does so, the algorithm correctly places $A[j]$ in position $m = C[k]$ of $B$. Since $C[k]$ is decremented in line 12, and is never again incremented, we are guaranteed that when the for loop examines $A[i]$ we will have $C[k] < m$. Therefore $A[i]$ will be placed in an earlier position of the output array, proving stability.","title":"8.2-2"},{"location":"Chap08/8.2/#82-3","text":"Suppose that we were to rewrite the for loop header in line 10 of the $\\text{COUNTING-SORT}$ as 10 for j = 1 to A . length Show that the algorithm still works properly. Is the modified algorithm stable? The algorithm still works correctly. The order that elements are taken out of $C$ and put into $B$ doesn't affect the placement of elements with the same key. It will still fill the interval $(C[k \u2212 1], C[k]]$ with elements of key $k$. The question of whether it is stable or not is not well phrased. In order for stability to make sense, we would need to be sorting items which have information other than their key, and the sort as written is just for integers, which don't. We could think of extending this algorithm by placing the elements of $A$ into a collection of elements for each cell in array $C$. Then, if we use a FIFO collection, the modification of line 10 will make it stable, if we use LILO, it will be anti-stable.","title":"8.2-3"},{"location":"Chap08/8.2/#82-4","text":"Describe an algorithm that, given n integers in the range $0$ to $k$, preprocesses its input and then answers any query about how many of the $n$ integers fall into a range $[a..b]$ in $O(1)$ time. Your algorithm should use $\\Theta(n + k)$ preprocessing time. The algorithm will begin by preprocessing exactly as $\\text{COUNTING-SORT}$ does in lines 1 through 9, so that $C[i]$ contains the number of elements less than or equal to $i$ in the array. When queried about how many integers fall into a range $[a..b]$, simply compute $C[b] \u2212 C[a \u2212 1]$. This takes $O(1)$ times and yields the desired output.","title":"8.2-4"},{"location":"Chap08/8.3/","text":"8.3-1 Using Figure 8.3 as a model, illustrate the operation of $\\text{RADIX-SORT}$ on the following list of English words: COW, DOG, SEA, RUG, ROW, MOB, BOX, TAB, BAR, EAR, TAR, DIG, BIG, TEA, NOW, FOX. $$ \\begin{array}{cccc} 0 & 1 & 2 & 3 \\\\ \\hline \\text{COW} & \\text{SE$\\textbf{A}$} & \\text{T$\\textbf{A}$B} & \\text{$\\textbf{B}$AR} \\\\ \\text{DOG} & \\text{TE$\\textbf{A}$} & \\text{B$\\textbf{A}$R} & \\text{$\\textbf{B}$IG} \\\\ \\text{SEA} & \\text{MO$\\textbf{B}$} & \\text{E$\\textbf{A}$R} & \\text{$\\textbf{B}$OX} \\\\ \\text{RUG} & \\text{TA$\\textbf{B}$} & \\text{T$\\textbf{A}$R} & \\text{$\\textbf{C}$OW} \\\\ \\text{ROW} & \\text{DO$\\textbf{G}$} & \\text{S$\\textbf{E}$A} & \\text{$\\textbf{D}$IG} \\\\ \\text{MOB} & \\text{RU$\\textbf{G}$} & \\text{T$\\textbf{E}$A} & \\text{$\\textbf{D}$OG} \\\\ \\text{BOX} & \\text{DI$\\textbf{G}$} & \\text{D$\\textbf{I}$G} & \\text{$\\textbf{E}$AR} \\\\ \\text{TAB} & \\text{BI$\\textbf{G}$} & \\text{B$\\textbf{I}$G} & \\text{$\\textbf{F}$OX} \\\\ \\text{BAR} & \\text{BA$\\textbf{R}$} & \\text{M$\\textbf{O}$B} & \\text{$\\textbf{M}$OB} \\\\ \\text{EAR} & \\text{EA$\\textbf{R}$} & \\text{D$\\textbf{O}$G} & \\text{$\\textbf{N}$OW} \\\\ \\text{TAR} & \\text{TA$\\textbf{R}$} & \\text{C$\\textbf{O}$W} & \\text{$\\textbf{R}$OW} \\\\ \\text{DIG} & \\text{CO$\\textbf{W}$} & \\text{R$\\textbf{O}$W} & \\text{$\\textbf{R}$UG} \\\\ \\text{BIG} & \\text{RO$\\textbf{W}$} & \\text{N$\\textbf{O}$W} & \\text{$\\textbf{S}$EA} \\\\ \\text{TEA} & \\text{NO$\\textbf{W}$} & \\text{B$\\textbf{O}$X} & \\text{$\\textbf{T}$AB} \\\\ \\text{NOW} & \\text{BO$\\textbf{X}$} & \\text{F$\\textbf{O}$X} & \\text{$\\textbf{T}$AR} \\\\ \\text{FOX} & \\text{FO$\\textbf{X}$} & \\text{R$\\textbf{U}$G} & \\text{$\\textbf{T}$EA} \\\\ \\end{array} $$ 8.3-2 Which of the following sorting algorithms are stable: insertion sort, merge sort, heapsort, and quicksort? Give a simple scheme that makes any sorting algorithm stable. How much additional time and space does your scheme entail? Insertion sort and merge sort are stable. Heapsort and quicksort are not. To make any sorting algorithm stable we can preprocess, replacing each element of an array with an ordered pair. The first entry will be the value of the element, and the second value will be the index of the element. For example, the array $[2, 1, 1, 3, 4, 4, 4]$ would become $[(2, 1), (1, 2), (1, 3), (3, 4), (4, 5), (4, 6), (4, 7)]$. We now interpret $(i, j) < (k, m)$ if $i < k$ or $i = k$ and $j < m$. Under this definition of less-than, the algorithm is guaranteed to be stable because each of our new elements is distinct and the index comparison ensures that if a repeat element appeared later in the original array, it must appear later in the sorted array. This doubles the space requirement, but the running time will be asymptotically unchanged. 8.3-3 Use induction to prove that radix sort works. Where does your proof need the assumption that the intermediate sort is stable? Loop invariant: At the beginning of the for loop, the array is sorted on the last $i \u2212 1$ digits. Initialization: The array is trivially sorted on the last $0$ digits. Maintenance: Let's assume that the array is sorted on the last $i \u2212 1$ digits. After we sort on the $i$th digit, the array will be sorted on the last $i$ digits. It is obvious that elements with different digit in the $i$th position are ordered accordingly; in the case of the same $i$th digit, we still get a correct order, because we're using a stable sort and the elements were already sorted on the last $i \u2212 1$ digits. Termination: The loop terminates when $i = d + 1$. Since the invariant holds, we have the numbers sorted on $d$ digits. 8.3-4 Show how to sort $n$ integers in the range $0$ to $n^3 - 1$ in $O(n)$ time. First run through the list of integers and convert each one to base $n$, then radix sort them. Each number will have at most $\\log_n n^3 = 3$ digits so there will only need to be $3$ passes. For each pass, there are $n$ possible values which can be taken on, so we can use counting sort to sort each digit in $O(n)$ time. 8.3-5 $\\star$ In the first card-sorting algorithm in this section, exactly how many sorting passes are needed to sort $d$-digit decimal numbers in the worst case? How many piles of cards would an operator need to keep track of in the worst case? Given $n$ $d$-digit numbers in which each digit can take on up to $k$ possible values, we'll perform $\\Theta(k^d)$ passes and keep track of $\\Theta(nk)$ piles in the worst case.","title":"8.3 Radix sort"},{"location":"Chap08/8.3/#83-1","text":"Using Figure 8.3 as a model, illustrate the operation of $\\text{RADIX-SORT}$ on the following list of English words: COW, DOG, SEA, RUG, ROW, MOB, BOX, TAB, BAR, EAR, TAR, DIG, BIG, TEA, NOW, FOX. $$ \\begin{array}{cccc} 0 & 1 & 2 & 3 \\\\ \\hline \\text{COW} & \\text{SE$\\textbf{A}$} & \\text{T$\\textbf{A}$B} & \\text{$\\textbf{B}$AR} \\\\ \\text{DOG} & \\text{TE$\\textbf{A}$} & \\text{B$\\textbf{A}$R} & \\text{$\\textbf{B}$IG} \\\\ \\text{SEA} & \\text{MO$\\textbf{B}$} & \\text{E$\\textbf{A}$R} & \\text{$\\textbf{B}$OX} \\\\ \\text{RUG} & \\text{TA$\\textbf{B}$} & \\text{T$\\textbf{A}$R} & \\text{$\\textbf{C}$OW} \\\\ \\text{ROW} & \\text{DO$\\textbf{G}$} & \\text{S$\\textbf{E}$A} & \\text{$\\textbf{D}$IG} \\\\ \\text{MOB} & \\text{RU$\\textbf{G}$} & \\text{T$\\textbf{E}$A} & \\text{$\\textbf{D}$OG} \\\\ \\text{BOX} & \\text{DI$\\textbf{G}$} & \\text{D$\\textbf{I}$G} & \\text{$\\textbf{E}$AR} \\\\ \\text{TAB} & \\text{BI$\\textbf{G}$} & \\text{B$\\textbf{I}$G} & \\text{$\\textbf{F}$OX} \\\\ \\text{BAR} & \\text{BA$\\textbf{R}$} & \\text{M$\\textbf{O}$B} & \\text{$\\textbf{M}$OB} \\\\ \\text{EAR} & \\text{EA$\\textbf{R}$} & \\text{D$\\textbf{O}$G} & \\text{$\\textbf{N}$OW} \\\\ \\text{TAR} & \\text{TA$\\textbf{R}$} & \\text{C$\\textbf{O}$W} & \\text{$\\textbf{R}$OW} \\\\ \\text{DIG} & \\text{CO$\\textbf{W}$} & \\text{R$\\textbf{O}$W} & \\text{$\\textbf{R}$UG} \\\\ \\text{BIG} & \\text{RO$\\textbf{W}$} & \\text{N$\\textbf{O}$W} & \\text{$\\textbf{S}$EA} \\\\ \\text{TEA} & \\text{NO$\\textbf{W}$} & \\text{B$\\textbf{O}$X} & \\text{$\\textbf{T}$AB} \\\\ \\text{NOW} & \\text{BO$\\textbf{X}$} & \\text{F$\\textbf{O}$X} & \\text{$\\textbf{T}$AR} \\\\ \\text{FOX} & \\text{FO$\\textbf{X}$} & \\text{R$\\textbf{U}$G} & \\text{$\\textbf{T}$EA} \\\\ \\end{array} $$","title":"8.3-1"},{"location":"Chap08/8.3/#83-2","text":"Which of the following sorting algorithms are stable: insertion sort, merge sort, heapsort, and quicksort? Give a simple scheme that makes any sorting algorithm stable. How much additional time and space does your scheme entail? Insertion sort and merge sort are stable. Heapsort and quicksort are not. To make any sorting algorithm stable we can preprocess, replacing each element of an array with an ordered pair. The first entry will be the value of the element, and the second value will be the index of the element. For example, the array $[2, 1, 1, 3, 4, 4, 4]$ would become $[(2, 1), (1, 2), (1, 3), (3, 4), (4, 5), (4, 6), (4, 7)]$. We now interpret $(i, j) < (k, m)$ if $i < k$ or $i = k$ and $j < m$. Under this definition of less-than, the algorithm is guaranteed to be stable because each of our new elements is distinct and the index comparison ensures that if a repeat element appeared later in the original array, it must appear later in the sorted array. This doubles the space requirement, but the running time will be asymptotically unchanged.","title":"8.3-2"},{"location":"Chap08/8.3/#83-3","text":"Use induction to prove that radix sort works. Where does your proof need the assumption that the intermediate sort is stable? Loop invariant: At the beginning of the for loop, the array is sorted on the last $i \u2212 1$ digits. Initialization: The array is trivially sorted on the last $0$ digits. Maintenance: Let's assume that the array is sorted on the last $i \u2212 1$ digits. After we sort on the $i$th digit, the array will be sorted on the last $i$ digits. It is obvious that elements with different digit in the $i$th position are ordered accordingly; in the case of the same $i$th digit, we still get a correct order, because we're using a stable sort and the elements were already sorted on the last $i \u2212 1$ digits. Termination: The loop terminates when $i = d + 1$. Since the invariant holds, we have the numbers sorted on $d$ digits.","title":"8.3-3"},{"location":"Chap08/8.3/#83-4","text":"Show how to sort $n$ integers in the range $0$ to $n^3 - 1$ in $O(n)$ time. First run through the list of integers and convert each one to base $n$, then radix sort them. Each number will have at most $\\log_n n^3 = 3$ digits so there will only need to be $3$ passes. For each pass, there are $n$ possible values which can be taken on, so we can use counting sort to sort each digit in $O(n)$ time.","title":"8.3-4"},{"location":"Chap08/8.3/#83-5-star","text":"In the first card-sorting algorithm in this section, exactly how many sorting passes are needed to sort $d$-digit decimal numbers in the worst case? How many piles of cards would an operator need to keep track of in the worst case? Given $n$ $d$-digit numbers in which each digit can take on up to $k$ possible values, we'll perform $\\Theta(k^d)$ passes and keep track of $\\Theta(nk)$ piles in the worst case.","title":"8.3-5 $\\star$"},{"location":"Chap08/8.4/","text":"8.4-1 Using Figure 8.4 as a model, illustrate the operation of $\\text{BUCKET-SORT}$ on the array $A = \\langle .79, .13, .16, .64, .39, .20, .89, .53, .71, .42 \\rangle$. $$ \\begin{array}{cl} R & \\\\ \\hline 0 & \\\\ 1 & .13 .16 \\\\ 2 & .20 \\\\ 3 & .39 \\\\ 4 & .42 \\\\ 5 & .53 \\\\ 6 & .64 \\\\ 7 & .79 .71 \\\\ 8 & .89 \\\\ 9 & \\\\ \\end{array} $$ $$A = \\langle.13, .16, .20, .39, .42, .53, .64, .71, .79, .89 \\rangle.$$ 8.4-2 Explain why the worst-case running time for bucket sort is $\\Theta(n^2)$. What simple change to the algorithm preserves its linear average-case running time and makes its worst-case running time $O(n\\lg n)$? If all the keys fall in the same bucket and they happen to be in reverse order, we have to sort a single bucket with $n$ items in reversed order with insertion sort. This is $\\Theta(n^2)$. We can use merge sort or heapsort to improve the worst-case running time. Insertion sort was chosen because it operates well on linked lists, which has optimal time and requires only constant extra space for short linked lists. If we use another sorting algorithm, we have to convert each list to an array, which might slow down the algorithm in practice. 8.4-3 Let $X$ be a random variable that is equal to the number of heads in two flips of a fair coin. What is $\\text E[X^2]$? What is $\\text E^2[X]$? $$ \\begin{aligned} \\text E[X] & = 2 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1 \\\\ \\text E[X^2] & = 4 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1.5 \\\\ \\text E^2[X] & = \\text E[X] \\cdot \\text E[X] = 1 \\cdot 1 = 1. \\end{aligned} $$ 8.4-4 $\\star$ We are given $n$ points in the unit circle, $p_i = (x_i, y_i)$, such that $0 < x_i^2 + y_i^2 \\le 1$ for $i = 1, 2, \\ldots, n$. Suppose that the points are uniformly distributed; that is, the probability of finding a point in any region of the circle is proportional to the area of that region. Design an algorithm with an average-case running time of $\\Theta(n)$ to sort the $n$ points by their distances $d_i = \\sqrt{x_i^2 + y_i^2}$ from the origin. ($\\textit{Hint:}$ Design the bucket sizes in $\\text{BUCKET-SORT}$ to re\ufb02ect the uniform distribution of the points in the unit circle.) Bucket sort by radius, $$ \\begin{aligned} \\pi r_i^2 & = \\frac{i}{n} \\cdot \\pi 1^2 \\\\ r_i & = \\sqrt{\\frac{i}{n}}. \\end{aligned} $$ 8.4-5 $\\star$ A probability distribution function $P(x)$ for a random variable $X$ is defined by $P(x) = \\Pr\\{X \\le x\\}$. Suppose that we draw a list of $n$ random variables $X_1, X_2, \\ldots, X_n$ from a continuous probability distribution function $P$ that is computable in $O(1)$ time. Give an algorithm that sorts these numbers in linear average-case time. Bucket sort by $p_i$, so we have $n$ buckets: $[p_0, p_1), [p_1, p_2), \\dots, [p_{n - 1}, p_n)$. Note that not all buckets are the same size, which is ok as to ensure linear run time, the inputs should on average be uniformly distributed amongst all buckets, of which the intervals defined with $p_i$ will do so. $p_i$ is defined as follows: $$P(p_i) = \\frac{i}{n}.$$","title":"8.4 Bucket sort"},{"location":"Chap08/8.4/#84-1","text":"Using Figure 8.4 as a model, illustrate the operation of $\\text{BUCKET-SORT}$ on the array $A = \\langle .79, .13, .16, .64, .39, .20, .89, .53, .71, .42 \\rangle$. $$ \\begin{array}{cl} R & \\\\ \\hline 0 & \\\\ 1 & .13 .16 \\\\ 2 & .20 \\\\ 3 & .39 \\\\ 4 & .42 \\\\ 5 & .53 \\\\ 6 & .64 \\\\ 7 & .79 .71 \\\\ 8 & .89 \\\\ 9 & \\\\ \\end{array} $$ $$A = \\langle.13, .16, .20, .39, .42, .53, .64, .71, .79, .89 \\rangle.$$","title":"8.4-1"},{"location":"Chap08/8.4/#84-2","text":"Explain why the worst-case running time for bucket sort is $\\Theta(n^2)$. What simple change to the algorithm preserves its linear average-case running time and makes its worst-case running time $O(n\\lg n)$? If all the keys fall in the same bucket and they happen to be in reverse order, we have to sort a single bucket with $n$ items in reversed order with insertion sort. This is $\\Theta(n^2)$. We can use merge sort or heapsort to improve the worst-case running time. Insertion sort was chosen because it operates well on linked lists, which has optimal time and requires only constant extra space for short linked lists. If we use another sorting algorithm, we have to convert each list to an array, which might slow down the algorithm in practice.","title":"8.4-2"},{"location":"Chap08/8.4/#84-3","text":"Let $X$ be a random variable that is equal to the number of heads in two flips of a fair coin. What is $\\text E[X^2]$? What is $\\text E^2[X]$? $$ \\begin{aligned} \\text E[X] & = 2 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1 \\\\ \\text E[X^2] & = 4 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} = 1.5 \\\\ \\text E^2[X] & = \\text E[X] \\cdot \\text E[X] = 1 \\cdot 1 = 1. \\end{aligned} $$","title":"8.4-3"},{"location":"Chap08/8.4/#84-4-star","text":"We are given $n$ points in the unit circle, $p_i = (x_i, y_i)$, such that $0 < x_i^2 + y_i^2 \\le 1$ for $i = 1, 2, \\ldots, n$. Suppose that the points are uniformly distributed; that is, the probability of finding a point in any region of the circle is proportional to the area of that region. Design an algorithm with an average-case running time of $\\Theta(n)$ to sort the $n$ points by their distances $d_i = \\sqrt{x_i^2 + y_i^2}$ from the origin. ($\\textit{Hint:}$ Design the bucket sizes in $\\text{BUCKET-SORT}$ to re\ufb02ect the uniform distribution of the points in the unit circle.) Bucket sort by radius, $$ \\begin{aligned} \\pi r_i^2 & = \\frac{i}{n} \\cdot \\pi 1^2 \\\\ r_i & = \\sqrt{\\frac{i}{n}}. \\end{aligned} $$","title":"8.4-4 $\\star$"},{"location":"Chap08/8.4/#84-5-star","text":"A probability distribution function $P(x)$ for a random variable $X$ is defined by $P(x) = \\Pr\\{X \\le x\\}$. Suppose that we draw a list of $n$ random variables $X_1, X_2, \\ldots, X_n$ from a continuous probability distribution function $P$ that is computable in $O(1)$ time. Give an algorithm that sorts these numbers in linear average-case time. Bucket sort by $p_i$, so we have $n$ buckets: $[p_0, p_1), [p_1, p_2), \\dots, [p_{n - 1}, p_n)$. Note that not all buckets are the same size, which is ok as to ensure linear run time, the inputs should on average be uniformly distributed amongst all buckets, of which the intervals defined with $p_i$ will do so. $p_i$ is defined as follows: $$P(p_i) = \\frac{i}{n}.$$","title":"8.4-5 $\\star$"},{"location":"Chap08/Problems/8-1/","text":"In this problem, we prove a probabilistic $\\Omega(n\\lg n)$ lower bound on the running time of any deterministic or randomized comparison sort on $n$ distinct input elements. We begin by examining a deterministic comparison sort $A$ with decision tree $T_A$. We assume that every permutation of $A$'s inputs is equally likely. a. Suppose that each leaf of $T_A$ is labeled with the probability that it is reached given a random input. Prove that exactly $n!$ leaves are labeled $1 / n!$ and that the rest are labeled $0$. b. Let $D(T)$ denote the external path length of a decision tree $T$; that is, $D(T)$ is the sum of the depths of all the leaves of $T$. Let $T$ be a decision tree with $k > 1$ leaves, and let $LT$ and $RT$ be the left and right subtrees of $T$. Show that $D(T) = D(LT) + D(RT)+k$. c. Let $d(k)$ be the minimum value of $D(T)$ over all decision trees $T$ with $k > 1$ leaves. Show that $d(k) = \\min _{1 \\le i \\le k - 1} \\{ d(i) + d(k - i) + k \\}$. ($\\textit{Hint:}$ Consider a decision tree $T$ with $k$ leaves that achieves the minimum. Let $i_0$ be the number of leaves in $LT$ and $k - i_0$ the number of leaves in $RT$.) d. Prove that for a given value of $k > 1$ and $i$ in the range $1 \\le i \\le k - 1$, the function $i\\lg i + (k - i) \\lg(k - i)$ is minimized at $i = k / 2$. Conclude that $d(k) = \\Omega(k\\lg k)$. e. Prove that $D(T_A) = \\Omega(n!\\lg(n!))$, and conclude that the average-case time to sort $n$ elements is $\\Omega(n\\lg n)$. Now, consider a randomized comparison sort $B$. We can extend the decision-tree model to handle randomization by incorporating two kinds of nodes: ordinary comparison nodes and \"randomization\" nodes. A randomization node models a random choice of the form $\\text{RANDOM}(1, r)$ made by algorithm $B$; the node has $r$ children, each of which is equally likely to be chosen during an execution of the algorithm. f. Show that for any randomized comparison sort $B$, there exists a deterministic comparison sort $A$ whose expected number of comparisons is no more than those made by $B$. a. There are $n!$ possible permutations of the input array because the input elements are all distinct. Since each is equally likely, the distribution is uniformly supported on this set. So, each occurs with probability $\\frac{1}{n!}$ and corresponds to a different leaf because the program needs to be able to distinguish between them. b. The depths of particular elements of $LT$ or $RT$ are all one less than their depths when considered elements of $T$. In particular, this is true for the leaves of the two subtrees. Also, $\\{LT, RT\\}$ form a partition of all the leaves of $T$. Therefore, if we let $L(T)$ denote the leaves of $T$, $$ \\begin{aligned} D(T) & = \\sum_{\\ell \\in L(T)} D_T(\\ell) \\\\ & = \\sum_{\\ell \\in L(LT)} D_T(\\ell) + \\sum_{\\ell \\in L(RT)} D_T(\\ell) \\\\ & = \\sum_{\\ell \\in L(LT)} (D_{LT}(\\ell) + 1) + \\sum_{\\ell \\in L(RT)} (D_{RT}(\\ell) + 1) \\\\ & = \\sum_{\\ell \\in L(LT)} D_{LT}(\\ell) + \\sum_{\\ell \\in L(RT)} D_{RT}(\\ell) + k \\\\ & = D(LT) + D(RT) + k. \\end{aligned} $$ c. Suppose we have a $T$ with $k$ leaves so that $D(T) = d(k)$. Let $i_0$ be the number of leaves in $LT$. Then, $d(k) = D(T) = D(LT) + D(RT) + k$. However, we can pick $LT$ and $RT$ to minimize the external path length. d. We treat $i$ as a continuous variable, and take a derivative to find critical points. The given expression has the following as a derivative with respect to $i$ $$\\frac{1}{\\ln 2} + \\lg i + \\frac{1}{\\ln 2} - \\lg(k - i) = \\frac{2}{\\ln 2} + \\lg\\left(\\frac{i}{k - i}\\right),$$ which is $0$ when we have $\\frac{i}{k - i} = 2^{-\\frac{2}{\\ln 2}} = 2^{-\\lg e^2} = e^{-2}$. Therefore, $(1 + e^{-2})i = k$, $i = \\frac{k}{1 + e^{-2}}$. Since we are picking the two subtrees to be roughly equal size, the total depth will be order $\\lg k$, with each level contributing $k$, so the total external path length is at least $k\\lg k$. e. Since before we that a tree with $k$ leaves needs to have external length $k\\lg k$, and that a sorting tree needs at least $n!$ trees, a sorting tree must have external tree length at least $n!\\lg (n!)$. Since the average case run time is the depth of a leaf weighted by the probability of that leaf being the one that occurs, we have that the run time is at least $\\frac{n!\\lg (n!)}{n!} = \\lg (n!) \\in \\Omega(n\\lg n)$. f. Since the expected runtime is the average over all possible results from the random bits, if every possible fixing of the randomness resulted in a higher runtime, the average would have to be higher as well.","title":"8-1 Probabilistic lower bounds on comparison sorting"},{"location":"Chap08/Problems/8-2/","text":"Suppose that we have an array of $n$ data records to sort and that the key of each record has the value $0$ or $1$. An algorithm for sorting such a set of records might possess some subset of the following three desirable characteristics: The algorithm runs in $O(n)$ time. The algorithm is stable. The algorithm sorts in place, using no more than a constant amount of storage space in addition to the original array. a. Give an algorithm that satisfies criteria 1 and 2 above. b. Give an algorithm that satisfies criteria 1 and 3 above. c. Give an algorithm that satisfies criteria 2 and 3 above. d. Can you use any of your sorting algorithms from parts (a)\u2013(c) as the sorting method used in line 2 of $\\text{RADIX-SORT}$, so that $\\text{RADIX-SORT}$ sorts $n$ records with $b$-bit keys in $O(bn)$ time? Explain how or why not. e. Suppose that the $n$ records have keys in the range from $1$ to $k$. Show how to modify counting sort so that it sorts the records in place in $O(n + k)$ time. You may use $O(k)$ storage outside the input array. Is your algorithm stable? ($\\textit{Hint:}$ How would you do it for $k = 3$?) a. Counting-Sort. b. Quicksort-Partition. c. Insertion-Sort. d. (a) Yes. (b) No. (c) No. e. Thanks @Gutdub for providing the solution in this issue . MODIFIED - COUNTING - SORT ( A , k ) let C [ 0. . k ] be a new array for i = 1 to k C [ i ] = 0 for j = 1 to A . length C [ A [ j ]] = C [ A [ j ]] + 1 for i = 2 to k C [ i ] = C [ i ] + C [ i - 1 ] insert sentinel element NIL at the start of A B = C [ 0. . k - 1 ] insert number 1 at the start of B // B now contains the \"endpoints\" for C for i = 2 to A . length while C [ A [ i ]] != B [ A [ i ]] key = A [ i ] exchange A [ C [ A [ i ]]] with A [ i ] while A [ C [ key ]] == key // make sure that elements with the same keys will not be swapped C [ key ] = C [ key ] - 1 remove the sentinel element return A In place (storage space is $\\Theta(k)$) but not stable.","title":"8-2 Sorting in place in linear time"},{"location":"Chap08/Problems/8-3/","text":"a. You are given an array of integers, where different integers may have different numbers of digits, but the total number of digits over all the integers in the array is $n$. Show how to sort the array in $O(n)$ time. b. You are given an array of strings, where different strings may have different numbers of characters, but the total number of characters over all the strings is $n$. Show how to sort the strings in $O(n)$ time. (Note that the desired order here is the standard alphabetical order; for example, $\\text a < \\text{ab} < \\text b$.) a. First, sort the integer according to their lengths by bucket sort, where we make a bucket for each possible number of digits. We sort each these uniform length sets of integers using radix sort. Then, we just concatenate the sorted lists obtained from each bucket. b. Make a bucket for every letter in the alphabet, each containing the words that start with that letter. Then, forget about the first letter of each of the words in the bucket, concatenate the empty word (if it's in this new set of words) with the result of recursing on these words of length one less. Since each word is processed a number of times equal to it's length, the runtime will be linear in the total number of letters.","title":"8-3 Sorting variable-length items"},{"location":"Chap08/Problems/8-4/","text":"Suppose that you are given $n$ red and $n$ blue water jugs, all of different shapes and sizes. All red jugs hold different amounts of water, as do the blue ones. Moreover, for every red jug, there is a blue jug that holds the same amount of water, and vice versa. Your task is to find a grouping of the jugs into pairs of red and blue jugs that hold the same amount of water. To do so, you may perform the following operation: pick a pair of jugs in which one is red and one is blue, fill the red jug with water, and then pour the water into the blue jug. This operation will tell you whether the red or the blue jug can hold more water, or that they have the same volume. Assume that such a comparison takes one time unit. Your goal is to find an algorithm that makes a minimum number of comparisons to determine the grouping. Remember that you may not directly compare two red jugs or two blue jugs. a. Describe a deterministic algorithm that uses $\\Theta(n^2)$ comparisons to group the jugs into pairs. b. Prove a lower bound of $\\Omega(n\\lg n)$ for the number of comparisons that an algorithm solving this problem must make. c. Give a randomized algorithm whose expected number of comparisons is $O(n\\lg n)$, and prove that this bound is correct. What is the worst-case number of comparisons for your algorithm? a. Select a red jug. Compare it to blue jugs until you find one which matches. Set that pair aside, and repeat for the next red jug. This will use at most $\\sum_{i = 1}^{n - 1} i = n(n - 1) / 2 = \\Theta(n^2)$ comparisons. b. We can imagine first lining up the red jugs in some order. Then a solution to this problem becomes a permutation of the blue jugs such that the $i$th blue jug is the same size as the $i$th red jug. As in section 8.1, we can make a decision tree which represents comparisons made between blue jugs and red jugs. An internal node represents a comparison between a specific pair of red and blue jugs, and a leaf node represents a permutation of the blue jugs based on the results of the comparison. We are interested in when one jug is greater than, less than, or equal in size to another jug, so the tree should have $3$ children per node. Since there must be at least $n!$ leaf nodes, the decision tree must have height at least $\\log_3 (n!)$. Since a solution corresponds to a simple path from root to leaf, an algorithm must make at least $\\Theta(n\\lg n)$ comparisons to reach any leaf. c. We use an algorithm analogous to randomized quicksort. Select a blue jug at random. Partition the red jugs into those which are smaller than the blue jug, and those which are larger. At some point in the comparisons, you will find the red jug which is of equal size. Once the red jugs have been divided by size, use the red jug of equal size to partition the blue jugs into those which are smaller and those which are larger. If $k$ red jugs are smaller than the originally chosen jug, we need to solve the original problem on input of size $k \u2212 1$ and size $n \u2212 k$, which we will do in the same manner. A subproblem of size $1$ is trivially solved because if there is only one red jug and one blue jug, they must be the same size. The analysis of expected number of comparisons is exactly the same as that of $\\text{RANDOMIZED-QUICKSORT}$ given on pages 181-184. We are running the procedure twice so the expected number of comparisons is doubled, but this is absorbed by the big-$O$ notation. In the worst case, we pick the largest jug each time, which results in $\\sum_{i = 2}^n i + i - 1 = n^2$ comparisons.","title":"8-4 Water jugs"},{"location":"Chap08/Problems/8-5/","text":"Suppose that, instead of sorting an array, we just require that the elements increase on average. More precisely, we call an $n$-element array $A$ k-sorted if, for all $i = 1, 2, \\ldots, n - k$, the following holds: $$\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\le \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}.$$ a. What does it mean for an array to be $1$-sorted? b. Give a permutation of the numbers $1, 2, \\ldots, 10$ that is $2$-sorted, but not sorted. c. Prove that an $n$-element array is $k$-sorted if and only if $A[i] \\le A[i + k]$ for all $i = 1, 2, \\ldots, n - k$. d. Give an algorithm that $k$-sorts an $n$-element array in $O(n\\lg (n / k))$ time. We can also show a lower bound on the time to produce a $k$-sorted array, when $k$ is a constant. e. Show that we can sort a $k$-sorted array of length $n$ in $O(n\\lg k)$ time. ($\\textit{Hint:}$ Use the solution to Exercise 6.5-9.) f. Show that when $k$ is a constant, $k$-sorting an $n$-element array requires $\\Omega(n\\lg n)$ time. ($\\textit{Hint:}$ Use the solution to the previous part along with the lower bound on comparison sorts.) a. Ordinary sorting b. $2, 1, 4, 3, 6, 5, 8, 7, 10, 9$. c. $$ \\begin{aligned} \\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} & \\le \\frac{\\sum_{j = i + 1}^{i + k}A[j]}{k} \\\\ \\sum_{j = i}^{i + k- 1 } A[j] & \\le \\sum_{j = i + 1}^{i + k} A[j] \\\\ A[i] & \\le A[i + k]. \\end{aligned} $$ d. Shell-Sort, i.e., We split the $n$-element array into $k$ part. For each part, we use Insertion-Sort (or Quicksort) to sort in $O(n / k \\lg(n / k))$ time. Therefore, the total running time is $k \\cdot O(n / k \\lg(n / k)) = O(n\\lg(n / k))$. e. Using a heap, we can sort a $k$-sorted array of length $n$ in $O(n\\lg k)$ time. (The height of the heap is $\\lg k$, the solution to Exercise 6.5-9.) f. The lower bound of sorting each part is $\\Omega(n / k\\lg(n / k))$, so the total lower bound is $\\Theta(n\\lg (n/k))$. Since $k$ is a constant, therefore $\\Theta(n\\lg(n / k)) = \\Omega(n\\lg n)$.","title":"8-5 Average sorting"},{"location":"Chap08/Problems/8-6/","text":"The problem of merging two sorted lists arises frequently. We have seen a procedure for it as the subroutine $\\text{MERGE}$ in Section 2.3.1. In this problem, we will prove a lower bound of $2n - 1$ on the worst-case number of comparisons required to merge two sorted lists, each containing $n$ items. First we will show a lower bound of $2n - o(n)$ comparisons by using a decision tree. a. Given $2n$ numbers, compute the number of possible ways to divide them into two sorted lists, each with $n$ numbers. b. Using a decision tree and your answer to part (a), show that any algorithm that correctly merges two sorted lists must perform at least $2n - o(n)$ comparisons. Now we will show a slightly tighter $2n - 1$ bound. c. Show that if two elements are consecutive in the sorted order and from different lists, then they must be compared. d. Use your answer to the previous part to show a lower bound of $2n - 1$ comparisons for merging two sorted lists. a. There are $\\binom{2n}{n}$ ways to divide $2n$ numbers into two sorted lists, each with $n$ numbers. b. Based on Exercise C.1.13, $$ \\begin{aligned} \\binom{2n}{n} & \\le 2^h \\\\ h & \\ge \\lg\\frac{(2n)!}{(n!)^2} \\\\ & = \\lg (2n!) - 2\\lg (n!) \\\\ & = \\Theta(2n\\lg 2n) - 2\\Theta(n\\lg n) \\\\ & = \\Theta(2n). \\end{aligned} $$ c. We have to know the order of the two consecutive elements. d. Let list $A = 1, 3, 5, \\ldots, 2n - 1$ and $B = 2, 4, 6, \\ldots, 2n$. By part (c), we must compare $1$ with $2$, $2$ with $3$, $3$ with $4$, and so on up until we compare $2n - 1$ with $2n$. This amounts to a total of $2n - 1$ comparisons.","title":"8-6 Lower bound on merging sorted lists"},{"location":"Chap08/Problems/8-7/","text":"A compare-exchange operation on two array elements $A[i]$ and $A[j]$, where $i < j$, has the form COMPARE - EXCHANGE ( A , i , j ) if A [ i ] > A [ j ] exchange A [ i ] with A [ j ] After the compare-exchange operation, we know that $A[i] \\le A[j]$. An oblivious compare-exchange algorithm operates solely by a sequence of prespecified compare-exchange operations. The indices of the positions compared in the sequence must be determined in advance, and although they can depend on the number of elements being sorted, they cannot depend on the values being sorted, nor can they depend on the result of any prior compare-exchange operation. For example, here is insertion sort expressed as an oblivious compare-exchange algorithm: INSERTION - SORT ( A ) for j = 2 to A . length for i = j - 1 downto 1 COMPARE - EXCHANGE ( A , i , i + 1 ) The 0-1 sorting lemma provides a powerful way to prove that an oblivious compare-exchange algorithm produces a sorted result. It states that if an oblivious compare-exchange algorithm correctly sorts all input sequences consisting of only $0$s and $1$s, then it correctly sorts all inputs containing arbitrary values. You will prove the $0$-$1$ sorting lemma by proving its contrapositive: if an oblivious compare-exchange algorithm fails to sort an input containing arbitrary values, then it fails to sort some $0$-$1$ input. Assume that an oblivious compare-exchange algorithm $\\text X$ fails to correctly sort the array $A[1..n]$. Let $A[p]$ be the smallest value in $A$ that algorithm $\\text X$ puts into the wrong location, and let $A[q]$ be the value that algorithm $\\text X$ moves to the location into which $A[p]$ should have gone. Define an array $B[1..n]$ of $0$s and $1$s as follows: $$ B[i] = \\begin{cases} 0 & \\text{if $A[i] \\le A[p]$}, \\\\ 1 & \\text{if $A[i] > A[p]$}. \\end{cases} $$ a. Argue that $A[q] > A[p]$, so that $B[p] = 0$ and $B[q] = 1$. b. To complete the proof of the $0$-$1$ sorting lemma, prove that algorithm $\\text X$ fails to sort array $B$ correctly. Now you will use the $0$-$1$ sorting lemma to prove that a particular sorting algorithm works correctly. The algorithm, columnsort , works on a rectangular array of $n$ elements. The array has $r$ rows and $s$ columns (so that $n = rs$), subject to three restrictions: $r$ must be even, $s$ must be a divisor of $r$, and $r \\ge 2 s^2$. When columnsort completes, the array is sorted in column-major order : reading down the columns, from left to right, the elements monotonically increase. Columnsort operates in eight steps, regardless of the value of $n$. The odd steps are all the same: sort each column individually. Each even step is a fixed permutation. Here are the steps: Sort each column. Transpose the array, but reshape it back to $r$ rows and $s$ columns. In other words, turn the leftmost column into the top $r / s$ rows, in order; turn the next column into the next $r / s$ rows, in order; and so on. Sort each column. Perform the inverse of the permutation performed in step 2. Sort each column. Shift the top half of each column into the bottom half of the same column, and shift the bottom half of each column into the top half of the next column to the right. Leave the top half of the leftmost column empty. Shift the bottom half of the last column into the top half of a new rightmost column, and leave the bottom half of this new column empty. Sort each column. Perform the inverse of the permutation performed in step 6. Figure 8.5 shows an example of the steps of columnsort with $r = 6$ and $s = 3$. (Even though this example violates the requirement that $r \\ge 2s^2$, it happens to work.) c. Argue that we can treat columnsort as an oblivious compare-exchange algorithm, even if we do not know what sorting method the odd steps use. Although it might seem hard to believe that columnsort actually sorts, you will use the $0$-$1$ sorting lemma to prove that it does. The $0$-$1$ sorting lemma applies because we can treat columnsort as an oblivious compare-exchange algorithm. A couple of definitions will help you apply the $0$-$1$ sorting lemma. We say that an area of an array is clean if we know that it contains either all $0$s or all $1$s. Otherwise, the area might contain mixed $0$s and $1$s, and it is dirty . From here on, assume that the input array contains only $0$s and $1$s, and that we can treat it as an array with $r$ rows and $s$ columns. d. Prove that after steps 1\u20133, the array consists of some clean rows of $0$s at the top, some clean rows of $1$s at the bottom, and at most $s$ dirty rows between them. e. Prove that after step 4, the array, read in column - major order, starts with a clean area of $0$s, ends with a clean area of $1$s, and has a dirty area of at most $s^2$ elements in the middle. f. Prove that steps 5\u20138 produce a fully sorted $0$-$1$ output. Conclude that columnsort correctly sorts all inputs containing arbitrary values. g. Now suppose that $s$ does not divide $r$. Prove that after steps 1\u20133, the array consists of some clean rows of $0$s at the top, some clean rows of $1$s at the bottom, and at most $2s - 1$ dirty rows between them. How large must $r$ be, compared with $s$, for columnsort to correctly sort when $s$ does not divide $r$? h. Suggest a simple change to step 1 that allows us to maintain the requirement that $r \\ge 2s^2$ even when $s$ does not divide $r$, and prove that with your change, columnsort correctly sorts. (Removed)","title":"8-7 The $0$-$1$ sorting lemma and columnsort"},{"location":"Chap09/9.1/","text":"9.1-1 Show that the second smallest of $n$ elements can be found with $n + \\lceil \\lg n \\rceil - 2$ comparisons in the worst case. ($\\textit{Hint:}$ Also find the smallest element.) We can compare the elements in a tournament fashion - we split them into pairs, compare each pair and then proceed to compare the winners in the same fashion. We need to keep track of each \"match\" the potential winners have participated in. We select a winner in $n \u2212 1$ matches. At this point, we know that the second smallest element is one of the lgn elements that lost to the smallest - each of them is smaller than the ones it has been compared to, prior to losing. In another $\\lceil \\lg n \\rceil \u2212 1$ comparisons we can find the smallest element out of those. 9.1-2 $\\star$ Prove the lower bound of $\\lceil 3n / 2 \\rceil - 2$ comparisons in the worst case to find both the maximum and minimum of $n$ numbers. ($\\textit{Hint:}$ Consider how many numbers are potentially either the maximum or minimum, and investigate how a comparison affects these counts.) If $n$ is odd, there are $$ \\begin{aligned} 1 + \\frac{3(n-3)}{2} + 2 & = \\frac{3n}{2} - \\frac{3}{2} \\\\ & = (\\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - \\frac{1}{2}) - \\frac{3}{2} \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons. If $n$ is even, there are $$ \\begin{aligned} 1 + \\frac{3(n - 2)}{2} & = \\frac{3n}{2} - 2 \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons.","title":"9.1 Minimum and maximum"},{"location":"Chap09/9.1/#91-1","text":"Show that the second smallest of $n$ elements can be found with $n + \\lceil \\lg n \\rceil - 2$ comparisons in the worst case. ($\\textit{Hint:}$ Also find the smallest element.) We can compare the elements in a tournament fashion - we split them into pairs, compare each pair and then proceed to compare the winners in the same fashion. We need to keep track of each \"match\" the potential winners have participated in. We select a winner in $n \u2212 1$ matches. At this point, we know that the second smallest element is one of the lgn elements that lost to the smallest - each of them is smaller than the ones it has been compared to, prior to losing. In another $\\lceil \\lg n \\rceil \u2212 1$ comparisons we can find the smallest element out of those.","title":"9.1-1"},{"location":"Chap09/9.1/#91-2-star","text":"Prove the lower bound of $\\lceil 3n / 2 \\rceil - 2$ comparisons in the worst case to find both the maximum and minimum of $n$ numbers. ($\\textit{Hint:}$ Consider how many numbers are potentially either the maximum or minimum, and investigate how a comparison affects these counts.) If $n$ is odd, there are $$ \\begin{aligned} 1 + \\frac{3(n-3)}{2} + 2 & = \\frac{3n}{2} - \\frac{3}{2} \\\\ & = (\\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - \\frac{1}{2}) - \\frac{3}{2} \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons. If $n$ is even, there are $$ \\begin{aligned} 1 + \\frac{3(n - 2)}{2} & = \\frac{3n}{2} - 2 \\\\ & = \\bigg\\lceil \\frac{3n}{2} \\bigg\\rceil - 2 \\end{aligned} $$ comparisons.","title":"9.1-2 $\\star$"},{"location":"Chap09/9.2/","text":"9.2-1 Show that $\\text{RANDOMIZED-SELECT}$ never makes a recursive call to a $0$-length array. Calling a $0$-length array would mean that the second and third arguments are equal. So, if the call is made on line 8, we would need that $p = q - 1$, which means that $q - p + 1 = 0$. However, $i$ is assumed to be a nonnegative number, and to be executing line 8, we would need that $i < k = q - p + 1 = 0$, a contradiction. The other possibility is that the bad recursive call occurs on line 9. This would mean that $q + 1 = r$. To be executing line 9, we need that $i > k = q - p + 1 = r - p$. This would be a nonsensical original call to the array though because we are asking for the ith element from an array of strictly less size. 9.2-2 Argue that the indicator random variable $X_k$ and the value $T(\\max(k - 1, n - k))$ are independent. The probability that $X_k$ is equal to $1$ is unchanged when we know the max of $k - 1$ and $n - k$. In other words, $\\Pr\\{X_k = a \\mid \\max(k - 1, n - k) = m\\} = \\Pr\\{X_k = a\\}$ for $a = 0, 1$ and $m = k - 1, n - k$ so $X_k$ and $\\max(k - 1, n - k)$ are independent. By C.3-5, so are $X_k$ and $T(\\max(k - 1, n - k))$. 9.2-3 Write an iterative version of $\\text{RANDOMIZED-SELECT}$. PARTITION ( A , p , r ) x = A [ r ] i = p for k = p - 1 to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap A [ i ] with A [ r ] return i RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p - 1 , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return A [ q ] if i < k r = q else p = q i = i - k 9.2-4 Suppose we use $\\text{RANDOMIZED-SELECT}$ to select the minimum element of the array $A = \\langle 3, 2, 9, 0, 7, 5, 4, 8, 6, 1 \\rangle$. Describe a sequence of partitions that results in a worst-case performance of $\\text{RANDOMIZED-SELECT}$. When the partition selected is always the maximum element of the array we get worst-case performance. In the example, the sequence would be $\\langle 9, 8, 7, 6, 5, 4, 3, 2, 1, 0 \\rangle$.","title":"9.2 Selection in expected linear time"},{"location":"Chap09/9.2/#92-1","text":"Show that $\\text{RANDOMIZED-SELECT}$ never makes a recursive call to a $0$-length array. Calling a $0$-length array would mean that the second and third arguments are equal. So, if the call is made on line 8, we would need that $p = q - 1$, which means that $q - p + 1 = 0$. However, $i$ is assumed to be a nonnegative number, and to be executing line 8, we would need that $i < k = q - p + 1 = 0$, a contradiction. The other possibility is that the bad recursive call occurs on line 9. This would mean that $q + 1 = r$. To be executing line 9, we need that $i > k = q - p + 1 = r - p$. This would be a nonsensical original call to the array though because we are asking for the ith element from an array of strictly less size.","title":"9.2-1"},{"location":"Chap09/9.2/#92-2","text":"Argue that the indicator random variable $X_k$ and the value $T(\\max(k - 1, n - k))$ are independent. The probability that $X_k$ is equal to $1$ is unchanged when we know the max of $k - 1$ and $n - k$. In other words, $\\Pr\\{X_k = a \\mid \\max(k - 1, n - k) = m\\} = \\Pr\\{X_k = a\\}$ for $a = 0, 1$ and $m = k - 1, n - k$ so $X_k$ and $\\max(k - 1, n - k)$ are independent. By C.3-5, so are $X_k$ and $T(\\max(k - 1, n - k))$.","title":"9.2-2"},{"location":"Chap09/9.2/#92-3","text":"Write an iterative version of $\\text{RANDOMIZED-SELECT}$. PARTITION ( A , p , r ) x = A [ r ] i = p for k = p - 1 to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap A [ i ] with A [ r ] return i RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p - 1 , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return A [ q ] if i < k r = q else p = q i = i - k","title":"9.2-3"},{"location":"Chap09/9.2/#92-4","text":"Suppose we use $\\text{RANDOMIZED-SELECT}$ to select the minimum element of the array $A = \\langle 3, 2, 9, 0, 7, 5, 4, 8, 6, 1 \\rangle$. Describe a sequence of partitions that results in a worst-case performance of $\\text{RANDOMIZED-SELECT}$. When the partition selected is always the maximum element of the array we get worst-case performance. In the example, the sequence would be $\\langle 9, 8, 7, 6, 5, 4, 3, 2, 1, 0 \\rangle$.","title":"9.2-4"},{"location":"Chap09/9.3/","text":"9.3-1 In the algorithm $\\text{SELECT}$, the input elements are divided into groups of $5$. Will the algorithm work in linear time if they are divided into groups of $7$? Argue that $\\text{SELECT}$ does not run in linear time if groups of $3$ are used. It will still work if they are divided into groups of $7$, because we will still know that the median of medians is less than at least $4$ elements from half of the $\\lceil n / 7 \\rceil$ groups, so, it is greater than roughly $4n / 14$ of the elements. Similarly, it is less than roughly $4n / 14$ of the elements. So, we are never calling it recursively on more than $10n / 14$ elements. $T(n) \\le T(n / 7) + T(10n / 14) + O(n)$. So, we can show by substitution this is linear. We guess $T(n) < cn$ for $n < k$. Then, for $m \\ge k$, $$ \\begin{aligned} T(m) & \\le T(m / 7) + T(10m / 14) + O(m) \\\\ & \\le cm(1 / 7 + 10 / 14) + O(m), \\end{aligned} $$ therefore, as long as we have that the constant hidden in the big-Oh notation is less than $c / 7$, we have the desired result. Suppose now that we use groups of size $3$ instead. So, For similar reasons, we have that the recurrence we are able to get is $T(n) = T(\\lceil n / 3 \\rceil) + T(4n / 6) + O(n) \\ge T(n / 3) + T(2n / 3) + O(n)$ So, we will show it is $\\ge cn \\lg n$. $$ \\begin{aligned} T(m) & \\ge c(m / 3)\\lg (m / 3) + c(2m / 3) \\lg (2m / 3) + O(m) \\\\ & \\ge cm\\lg m + O(m), \\end{aligned} $$ therefore, we have that it grows more quickly than linear. 9.3-2 Analyze $\\text{SELECT}$ to show that if $n \\ge 140$, then at least $\\lceil n / 4 \\rceil$ elements are greater than the median-of-medians $x$ and at least $\\lceil n / 4 \\rceil$ elements are less than $x$. $$ \\begin{aligned} \\frac{3n}{10} - 6 & \\ge \\lceil \\frac{n}{4} \\rceil \\\\ \\frac{3n}{10} - 6 & \\ge \\frac{n}{4} + 1 \\\\ 12n - 240 & \\ge 10n + 40 \\\\ n & \\ge 140. \\end{aligned} $$ 9.3-3 Show how quicksort can be made to run in $O(n\\lg n)$ time in the worst case, assuming that all elements are distinct. We can modify quicksort to run in worst case $n\\lg n$ time by choosing our pivot element to be the exact median by using quick select. Then, we are guaranteed that our pivot will be good, and the time taken to find the median is on the same order of the rest of the partitioning. 9.3-4 $\\star$ Suppose that an algorithm uses only comparisons to find the $i$th smallest element in a set of $n$ elements. Show that it can also find the $i - 1$ smaller elements and $n - i$ larger elements without performing additional comparisons. Create a graph with $n$ vertices and draw a directed edge from vertex $i$ to vertex $j$ if the $i$th and $j$th elements of the array are compared in the algorithm and we discover that $A[i] \\ge A[j]$. Observe that $A[i]$ is one of the $i - 1$ smaller elements if there exists a path from $x$ to $i$ in the graph, and $A[i]$ is one of the $n - i$ larger elements if there exists a path from $i$ to $x$ in the graph. Every vertex $i$ must either lie on a path to or from $x$ because otherwise the algorithm can't distinguish between $i \\le x$ and $i \\ge x$. Moreover, if a vertex $i$ lies on both a path to $x$ and a path from $x$, then it must be such that $x \\le A[i] \\le x$,so $x = A[i]$. In this case, we can break ties arbitrarily. 9.3-5 Suppose that you have a \"black-box\" worst-case linear-time median subroutine. Give a simple, linear-time algorithm that solves the selection problem for an arbitrary order statistic. To use it, just find the median, partition the array based on that median. If $i$ is less than half the length of the original array, recurse on the first half. If $i$ is half the length of the array, return the element coming from the median finding black box. If $i$ is more than half the length of the array, subtract half the length of the array, and then recurse on the second half of the array. 9.3-6 The $k$th quantiles of an $n$-element set are the $k - 1$ order statistics that divide the sorted set into $k$ equal-sized sets (to within $1$). Give an $O(n\\lg k)$-time algorithm to list the $k$th quantiles of a set. Pre-calculate the positions of the quantiles in $O(k)$, we use the $O(n)$ select algorithm to find the $\\lfloor k / 2 \\rfloor$th position, after that the elements are divided into two sets by the pivot the $\\lfloor k / 2 \\rfloor$th position, we do it recursively in the two sets to find other positions. Since the maximum depth is $\\lceil \\lg k \\rceil$, the total running time is $O(n\\lg k)$. PARTITION ( A , p , r ) x = A [ r ] i = p for k = p to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap a [ i ] with a [ r ] return i RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return p , A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return q , A [ q ] if i < k r = q else p = q + 1 i = i - k k - QUANTITLES - SUB ( A , p , r , pos , f , e , quantiles ) if f + 1 > e return mid = ( f + e ) / 2 q , val = RANDOMIZED - SELECT ( A , p , r , pos [ mid ]) quantiles [ mid ] = val k = q - p + 1 for i = mid + 1 to e pos [ i ] = pos [ i ] - k k - QUANTILES - SUB ( A , q + 1 , r , pos , mid + 1 , e , quantiles ) k - QUANTITLES ( A , k ) num = A . size () / k mod = A . size () % k pos = num [ 1. . k ] for i = 1 to mod pos [ i ] = pos [ i ] + 1 for i = 1 to k pos [ i ] = pos [ i ] + pos [ i - 1 ] quantiles = [ 1. . k ] k - QUANTITLES - SUB ( A , 0 , A . length , pos , 0 , pos . size (), quantiles ) return quantiles 9.3-7 Describe an $O(n)$-time algorithm that, given a set $S$ of $n$ distinct numbers and a positive integer $k \\le n$, determines the $k$ numbers in $S$ that are closest to the median of $S$. Find the median in $O(n)$; create a new array, each element is the absolute value of the original value subtract the median; find the $k$th smallest number in $O(n)$, then the desired values are the elements whose absolute difference with the median is less than or equal to the $k$th smallest number in the new array. 9.3-8 Let $X[1..n]$ and $Y[1..n]$ be two arrays, each containing $n$ numbers already in sorted order. Give an $O(\\lg n)$-time algorithm to find the median of all $2n$ elements in arrays $X$ and $Y$. Without loss of generality, assume $n$ is a power of $2$. MEDIAN ( X , Y , n ) if n == 1 return min ( X [ 1 ], Y [ 1 ]) if X [ n / 2 ] < Y [ n / 2 ] return MEDIAN ( X [ n / 2 + 1. . n ], Y [ 1. . n / 2 ], n / 2 ) return MEDIAN ( X [ 1. . n / 2 ], Y [ n / 2 + 1. . n ], n / 2 ) 9.3-9 Professor Olay is consulting for an oil company, which is planning a large pipeline running east to west through an oil field of $n$ wells. The company wants to connect a spur pipeline from each well directly to the main pipeline along a shortest route (either north or south), as shown in Figure 9.2. Given the $x$- and $y$-coordinates of the wells, how should the professor pick the optimal location of the main pipeline, which would be the one that minimizes the total length of the spurs? Show how to determine the optimal location in linear time. If $n$ is odd, we pick the $y$ coordinate of the main pipeline to be equal to the median of all the $y$ coordinates of the wells. If $n$ is even, we pick the $y$ coordinate of the pipeline to be anything between the $y$ coordinates of the wells with $y$-coordinates which have order statistics $\\lfloor (n + 1) / 2 \\rfloor$ and the $\\lceil (n + 1) / 2 \\rceil$. These can all be found in linear time using the algorithm from this section.","title":"9.3 Selection in worst-case linear time"},{"location":"Chap09/9.3/#93-1","text":"In the algorithm $\\text{SELECT}$, the input elements are divided into groups of $5$. Will the algorithm work in linear time if they are divided into groups of $7$? Argue that $\\text{SELECT}$ does not run in linear time if groups of $3$ are used. It will still work if they are divided into groups of $7$, because we will still know that the median of medians is less than at least $4$ elements from half of the $\\lceil n / 7 \\rceil$ groups, so, it is greater than roughly $4n / 14$ of the elements. Similarly, it is less than roughly $4n / 14$ of the elements. So, we are never calling it recursively on more than $10n / 14$ elements. $T(n) \\le T(n / 7) + T(10n / 14) + O(n)$. So, we can show by substitution this is linear. We guess $T(n) < cn$ for $n < k$. Then, for $m \\ge k$, $$ \\begin{aligned} T(m) & \\le T(m / 7) + T(10m / 14) + O(m) \\\\ & \\le cm(1 / 7 + 10 / 14) + O(m), \\end{aligned} $$ therefore, as long as we have that the constant hidden in the big-Oh notation is less than $c / 7$, we have the desired result. Suppose now that we use groups of size $3$ instead. So, For similar reasons, we have that the recurrence we are able to get is $T(n) = T(\\lceil n / 3 \\rceil) + T(4n / 6) + O(n) \\ge T(n / 3) + T(2n / 3) + O(n)$ So, we will show it is $\\ge cn \\lg n$. $$ \\begin{aligned} T(m) & \\ge c(m / 3)\\lg (m / 3) + c(2m / 3) \\lg (2m / 3) + O(m) \\\\ & \\ge cm\\lg m + O(m), \\end{aligned} $$ therefore, we have that it grows more quickly than linear.","title":"9.3-1"},{"location":"Chap09/9.3/#93-2","text":"Analyze $\\text{SELECT}$ to show that if $n \\ge 140$, then at least $\\lceil n / 4 \\rceil$ elements are greater than the median-of-medians $x$ and at least $\\lceil n / 4 \\rceil$ elements are less than $x$. $$ \\begin{aligned} \\frac{3n}{10} - 6 & \\ge \\lceil \\frac{n}{4} \\rceil \\\\ \\frac{3n}{10} - 6 & \\ge \\frac{n}{4} + 1 \\\\ 12n - 240 & \\ge 10n + 40 \\\\ n & \\ge 140. \\end{aligned} $$","title":"9.3-2"},{"location":"Chap09/9.3/#93-3","text":"Show how quicksort can be made to run in $O(n\\lg n)$ time in the worst case, assuming that all elements are distinct. We can modify quicksort to run in worst case $n\\lg n$ time by choosing our pivot element to be the exact median by using quick select. Then, we are guaranteed that our pivot will be good, and the time taken to find the median is on the same order of the rest of the partitioning.","title":"9.3-3"},{"location":"Chap09/9.3/#93-4-star","text":"Suppose that an algorithm uses only comparisons to find the $i$th smallest element in a set of $n$ elements. Show that it can also find the $i - 1$ smaller elements and $n - i$ larger elements without performing additional comparisons. Create a graph with $n$ vertices and draw a directed edge from vertex $i$ to vertex $j$ if the $i$th and $j$th elements of the array are compared in the algorithm and we discover that $A[i] \\ge A[j]$. Observe that $A[i]$ is one of the $i - 1$ smaller elements if there exists a path from $x$ to $i$ in the graph, and $A[i]$ is one of the $n - i$ larger elements if there exists a path from $i$ to $x$ in the graph. Every vertex $i$ must either lie on a path to or from $x$ because otherwise the algorithm can't distinguish between $i \\le x$ and $i \\ge x$. Moreover, if a vertex $i$ lies on both a path to $x$ and a path from $x$, then it must be such that $x \\le A[i] \\le x$,so $x = A[i]$. In this case, we can break ties arbitrarily.","title":"9.3-4 $\\star$"},{"location":"Chap09/9.3/#93-5","text":"Suppose that you have a \"black-box\" worst-case linear-time median subroutine. Give a simple, linear-time algorithm that solves the selection problem for an arbitrary order statistic. To use it, just find the median, partition the array based on that median. If $i$ is less than half the length of the original array, recurse on the first half. If $i$ is half the length of the array, return the element coming from the median finding black box. If $i$ is more than half the length of the array, subtract half the length of the array, and then recurse on the second half of the array.","title":"9.3-5"},{"location":"Chap09/9.3/#93-6","text":"The $k$th quantiles of an $n$-element set are the $k - 1$ order statistics that divide the sorted set into $k$ equal-sized sets (to within $1$). Give an $O(n\\lg k)$-time algorithm to list the $k$th quantiles of a set. Pre-calculate the positions of the quantiles in $O(k)$, we use the $O(n)$ select algorithm to find the $\\lfloor k / 2 \\rfloor$th position, after that the elements are divided into two sets by the pivot the $\\lfloor k / 2 \\rfloor$th position, we do it recursively in the two sets to find other positions. Since the maximum depth is $\\lceil \\lg k \\rceil$, the total running time is $O(n\\lg k)$. PARTITION ( A , p , r ) x = A [ r ] i = p for k = p to r if A [ k ] < x i = i + 1 swap A [ i ] with A [ k ] i = i + 1 swap a [ i ] with a [ r ] return i RANDOMIZED - PARTITION ( A , p , r ) x = RANDOM ( p , r ) swap A [ x ] with A [ r ] return PARTITION ( A , p , r ) RANDOMIZED - SELECT ( A , p , r , i ) while true if p == r return p , A [ p ] q = RANDOMIZED - PARTITION ( A , p , r ) k = q - p + 1 if i == k return q , A [ q ] if i < k r = q else p = q + 1 i = i - k k - QUANTITLES - SUB ( A , p , r , pos , f , e , quantiles ) if f + 1 > e return mid = ( f + e ) / 2 q , val = RANDOMIZED - SELECT ( A , p , r , pos [ mid ]) quantiles [ mid ] = val k = q - p + 1 for i = mid + 1 to e pos [ i ] = pos [ i ] - k k - QUANTILES - SUB ( A , q + 1 , r , pos , mid + 1 , e , quantiles ) k - QUANTITLES ( A , k ) num = A . size () / k mod = A . size () % k pos = num [ 1. . k ] for i = 1 to mod pos [ i ] = pos [ i ] + 1 for i = 1 to k pos [ i ] = pos [ i ] + pos [ i - 1 ] quantiles = [ 1. . k ] k - QUANTITLES - SUB ( A , 0 , A . length , pos , 0 , pos . size (), quantiles ) return quantiles","title":"9.3-6"},{"location":"Chap09/9.3/#93-7","text":"Describe an $O(n)$-time algorithm that, given a set $S$ of $n$ distinct numbers and a positive integer $k \\le n$, determines the $k$ numbers in $S$ that are closest to the median of $S$. Find the median in $O(n)$; create a new array, each element is the absolute value of the original value subtract the median; find the $k$th smallest number in $O(n)$, then the desired values are the elements whose absolute difference with the median is less than or equal to the $k$th smallest number in the new array.","title":"9.3-7"},{"location":"Chap09/9.3/#93-8","text":"Let $X[1..n]$ and $Y[1..n]$ be two arrays, each containing $n$ numbers already in sorted order. Give an $O(\\lg n)$-time algorithm to find the median of all $2n$ elements in arrays $X$ and $Y$. Without loss of generality, assume $n$ is a power of $2$. MEDIAN ( X , Y , n ) if n == 1 return min ( X [ 1 ], Y [ 1 ]) if X [ n / 2 ] < Y [ n / 2 ] return MEDIAN ( X [ n / 2 + 1. . n ], Y [ 1. . n / 2 ], n / 2 ) return MEDIAN ( X [ 1. . n / 2 ], Y [ n / 2 + 1. . n ], n / 2 )","title":"9.3-8"},{"location":"Chap09/9.3/#93-9","text":"Professor Olay is consulting for an oil company, which is planning a large pipeline running east to west through an oil field of $n$ wells. The company wants to connect a spur pipeline from each well directly to the main pipeline along a shortest route (either north or south), as shown in Figure 9.2. Given the $x$- and $y$-coordinates of the wells, how should the professor pick the optimal location of the main pipeline, which would be the one that minimizes the total length of the spurs? Show how to determine the optimal location in linear time. If $n$ is odd, we pick the $y$ coordinate of the main pipeline to be equal to the median of all the $y$ coordinates of the wells. If $n$ is even, we pick the $y$ coordinate of the pipeline to be anything between the $y$ coordinates of the wells with $y$-coordinates which have order statistics $\\lfloor (n + 1) / 2 \\rfloor$ and the $\\lceil (n + 1) / 2 \\rceil$. These can all be found in linear time using the algorithm from this section.","title":"9.3-9"},{"location":"Chap09/Problems/9-1/","text":"Given a set of $n$ numbers, we wish to find the $i$ largest in sorted order using a comparison-based algorithm. Find the algorithm that implements each of the following methods with the best asymptotic worst-case running time, and analyze the running times of the algorithms in terms of $n$ and $i$ . a. Sort the numbers, and list the $i$ largest. b. Build a max-priority queue from the numbers, and call $\\text{EXTRACT-MAX}$ $i$ times. c. Use an order-statistic algorithm to find the $i$th largest number, partition around that number, and sort the $i$ largest numbers. a. The running time of sorting the numbers is $O(n\\lg n)$, and the running time of listing the $i$ largest is $O(i)$. Therefore, the total running time is $O(n\\lg n + i)$. b. The running time of building a max-priority queue (using a heap) from the numbers is $O(n)$, and the running time of each call $\\text{EXTRACT-MAX}$ is $O(\\lg n)$. Therefore, the total running time is $O(n + i\\lg n)$. c. The running time of finding and partitioning around the $i$th largest number is $O(n)$, and the running time of sorting the $i$ largest numbers is $O(i\\lg i)$. Therefore, the total running time is $O(n + i\\lg i)$.","title":"9-1 Largest $i$ numbers in sorted order"},{"location":"Chap09/Problems/9-2/","text":"For $n$ distinct elements $x_1, x_2, \\ldots, x_n$ with positive weights $w_1, w_2, \\ldots, w_n$ such that $\\sum_{i = 1}^n w_i = 1$, the weighted (lower) median is the element $x_k$ satisfying $$\\sum_{x_i < x_k} w_i < \\frac{1}{2}$$ and $$\\sum_{x_i > x_k} w_i \\le \\frac{1}{2}.$$ For example, if the elements are $0.1, 0.35, 0.05, 0.1, 0.15, 0.05, 0.2$ and each element equals its weight (that is, $w_i = x_i$ for $i = 1, 2, \\ldots, 7$), then the median is $0.1$, but the weighted median is $0.2$. a. Argue that the median of $x_1, x_2, \\ldots, x_n$ is the weighted median of the $x_i$ with weights $w_i = 1 / n$ for $i = 1, 2, \\ldots, n$. b. Show how to compute the weighted median of $n$ elements in $O(n\\lg n)$ worstcase time using sorting. c. Show how to compute the weighted median in $\\Theta(n)$ worst-case time using a linear-time median algorithm such as $\\text{SELECT}$ from Section 9.3. The post-office location problem is defined as follows. We are given $n$ points $p_1, p_2, \\ldots, p_n$ with associated weights $w_1, w_2, \\ldots, w_n$. We wish to find a point $p$ (not necessarily one of the input points) that minimizes the sum $\\sum_{i = 1}^n w_i d(p, p_i)$, where $d(a, b)$ is the distance between points $a$ and $b$. d. Argue that the weighted median is a best solution for the $1$-dimensional postoffice location problem, in which points are simply real numbers and the distance between points $a$ and $b$ is $d(a, b) = |a - b|$. e. Find the best solution for the $2$-dimensional post-office location problem, in which the points are $(x,y)$ coordinate pairs and the distance between points $a = (x_1, y_1)$ and $b = (x_2, y_2)$ is the Manhattan distance given by $d(a, b) = |x_1 - x_2| + |y_1 - y_2|$. a. Let $m_k$ be the number of $x_i$ smaller than $x_k$. When weights of $1 / n$ are assigned to each $x_i$, we have $\\sum_{x_i < x_k} w_i = m_k / n$ and $\\sum_{x_i > x_k} w_i = (n - m_k - 1) / 2$. The only value of $m_k$ which makes these sums $< 1 / 2$ and $\\le 1 / 2$ respectively is when $\\lceil n / 2 \\rceil - 1$, and this value $x$ must be the median since it has equal numbers of $x_i's$ which are larger and smaller than it. b. First use mergesort to sort the $x_i$'s by value in $O(n\\lg n)$ time. Let $S_i$ be the sum of the weights of the first $i$ elements of this sorted array and note that it is $O(1)$ to update $S_i$. Compute $S_1, S_2, \\dots$ until you reach $k$ such that $S_{k \u2212 1} < 1 / 2$ and $S_k \\ge 1 / 2$. The weighted median is $x_k$. c. We modify $\\text{SELECT}$ to do this in linear time. Let $x$ be the median of medians. Compute $\\sum_{x_i < x} w_i$ and $\\sum_{x_i > x} w_i$ and check if either of these is larger than $1 / 2$. If not, stop. If so, recurse on the collection of smaller or larger elements known to contain the weighted median. This doesn't change the runtime, so it is $\\Theta(n)$. d. Let $p$ be the minimizer, and suppose that $p$ is not the weighted median. Let\u000f $\\epsilon$ be small enough such that $\\epsilon < \\min_i(|p \u2212 p_i|)$, where we don't include $k$ if $p = p_k$. If $p_m$ is the weighted median and $p < p_m$, choose $\\epsilon > 0$. Otherwise choose \u000f$\\epsilon < 0$. Thus, we have $$\\sum_{i = 1}^n w_id(p + \\epsilon, p_i) = \\sum_{i = 1}^n w_id(p, p_i) + \\epsilon\\left(\\sum_{p_i < p} w_i - \\sum_{p_i > p} w_i \\right) < \\sum_{i = 1}^n w_id(p, p_i),$$ the difference in sums will take the opposite sign of epsilon. e. Observe that $$\\sum_{i = 1}^n w_id(p, p_i) = \\sum_{i = 1}^n w_i |p_x - (p_i)_x| + \\sum_{i = 1}^n w_i|p_y - (p_i)_y|.$$ It will suffice to minimize each sum separately, which we can do since we choose $p_x$ and $p_y$ individually. By part (e), we simply take $p = (p_x, p_y)$ to be such that $p_x$ is the weighted median of the $x$-coordinates of the $p_i$'s and py is the weighted medain of the $y$-coordiantes of the $p_i$'s.","title":"9-2 Weighted median"},{"location":"Chap09/Problems/9-3/","text":"We showed that the worst-case number $T(n)$ of comparisons used by $\\text{SELECT}$ to select the $i$th order statistic from $n$ numbers satisfies $T(n) = \\Theta(n)$, but the constant hidden by the $\\Theta$-notation is rather large. When $i$ is small relative to $n$, we can implement a different procedure that uses $\\text{SELECT}$ as a subroutine but makes fewer comparisons in the worst case. a. Describe an algorithm that uses $U_i(n)$ comparisons to find the $i$th smallest of $n$ elements, where $$ U_i(n) = \\begin{cases} T(n) & \\text{if $i \\ge n / 2$}, \\\\ \\lfloor n / 2 \\rfloor + U_i(\\lceil n / 2 \\rceil) + T(2i) & \\text{otherwise}. \\end{cases} $$ ($\\textit{Hint:}$ Begin with $\\lfloor n / 2 \\rfloor$ disjoint pairwise comparisons, and recurse on the set containing the smaller element from each pair.) b. Show that, if $i < n / 2$, then $U_i(n) = n + O(T(2i)\\lg(n / i))$. c. Show that if $i$ is a constant less than $n / 2$, then $U_i(n) = n + O(\\lg n)$. d. Show that if $i = n / k$ for $k \\ge 2$, then $U_i(n) = n + O(T(2n / k)\\lg k)$. (Removed)","title":"9-3 Small order statistics"},{"location":"Chap09/Problems/9-4/","text":"In this problem, we use indicator random variables to analyze the $\\text{RANDOMIZED-SELECT}$ procedure in a manner akin to our analysis of $\\text{RANDOMIZED-QUICKSORT}$ in Section 7.4.2. As in the quicksort analysis, we assume that all elements are distinct, and we rename the elements of the input array $A$ as $z_1, z_2, \\ldots, z_n$, where $z_i$ is the $i$th smallest element. Thus, the call $\\text{RANDOMIZED-SELECT}(A, 1, n, k)$ returns $z_k$. For $1 \\le i < j \\le n$, let $$X_{ijk} = \\text{I \\{$z_i$ is compared with $z_j$ sometime during the execution of the algorithm to find $z_k$\\}}.$$ a. Give an exact expression for $\\text E[X_{ijk}]$. ($\\textit{Hint:}$ Your expression may have different values, depending on the values of $i$, $j$, and $k$.) b. Let $X_k$ denote the total number of comparisons between elements of array $A$ when finding $z_k$. Show that $$\\text E[X_k] \\le 2 \\Bigg (\\sum_{i = 1}^{k}\\sum_{j = k}^n \\frac{1}{j - i + 1} + \\sum_{j = k + 1}^{n} \\frac{j - k - 1}{j - k + 1} + \\sum_{i = 1}^{k-2} \\frac{k - i - 1}{k - i + 1} \\Bigg).$$ c. Show that $\\text E[X_k] \\le 4n$. d. Conclude that, assuming all elements of array $A$ are distinct, $\\text{RANDOMIZED-SELECT}$ runs in expected time $O(n)$. (Removed)","title":"9-4 Alternative analysis of randomized selection"},{"location":"Chap10/10.1/","text":"10.1-1 Using Figure 10.1 as a model, illustrate the result of each operation in the sequence $\\text{PUSH}(S, 4)$, $\\text{PUSH}(S, 1)$, $\\text{PUSH}(S, 3)$, $\\text{POP}(S)$, $\\text{PUSH}(S, 8)$, and $\\text{POP}(S)$ on an initially empty stack $S$ stored in array $S[1..6]$. $$ \\begin{array}{l|ccc} \\text{PUSH($S, 4$)} & 4 & & \\\\ \\text{PUSH($S, 1$)} & 4 & 1 & \\\\ \\text{PUSH($S, 3$)} & 4 & 1 & 3 \\\\ \\text{POP($S$)} & 4 & 1 & \\\\ \\text{PUSH($S, 8$)} & 4 & 1 & 8 \\\\ \\text{POP($S$)} & 4 & 1 & \\end{array} $$ 10.1-2 Explain how to implement two stacks in one array $A[1..n]$ in such a way that neither stack overflows unless the total number of elements in both stacks together is $n$. The $\\text{PUSH}$ and $\\text{POP}$ operations should run in $O(1)$ time. The first stack starts at $1$ and grows up towards n, while the second starts form $n$ and grows down towards $1$. Stack overflow happens when an element is pushed when the two stack pointers are adjacent. 10.1-3 Using Figure 10.2 as a model, illustrate the result of each operation in the sequence $\\text{ENQUEUE}(Q, 4)$, $\\text{ENQUEUE}(Q ,1)$, $\\text{ENQUEUE}(Q, 3)$, $\\text{DEQUEUE}(Q)$, $\\text{ENQUEUE}(Q, 8)$, and $\\text{DEQUEUE}(Q)$ on an initially empty queue $Q$ stored in array $Q[1..6]$. $$ \\begin{array}{l|cccc} \\text{ENQUEUE($Q, 4$)} & 4 & & & \\\\ \\text{ENQUEUE($Q, 1$)} & 4 & 1 & & \\\\ \\text{ENQUEUE($Q, 3$)} & 4 & 1 & 3 & \\\\ \\text{DEQUEUE($Q$)} & & 1 & 3 & \\\\ \\text{ENQUEUE($Q, 8$)} & & 1 & 3 & 8 \\\\ \\text{DEQUEUE($Q$)} & & & 3 & 8 \\end{array} $$ 10.1-4 Rewrite $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ to detect underflow and overflow of a queue. To detect underflow and overflow of a queue, we can implement $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ first. QUEUE - EMPTY ( Q ) if Q . head == Q . tail return true else return false QUEUE - FULL ( Q ) if Q . head == Q . tail + 1 or ( Q . head == 1 and Q . tail == Q . length ) return true else return false ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x 10.1-5 Whereas a stack allows insertion and deletion of elements at only one end, and a queue allows insertion at one end and deletion at the other end, a deque (double-ended queue) allows insertion and deletion at both ends. Write four $O(1)$-time procedures to insert elements into and delete elements from both ends of a deque implemented by an array. The procedures $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ are implemented in Exercise 10.1-4. HEAD - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else if Q . head == 1 Q . head = Q . length else Q . head = Q . head - 1 Q [ Q . head ] = x TAIL - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 HEAD - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x TAIL - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else if Q . tail == 1 Q . tail = Q . length else Q . tail = Q . tail - 1 x = Q [ Q . tail ] return x 10.1-6 Show how to implement a queue using two stacks. Analyze the running time of the queue operations. $\\text{ENQUEUE}$: $\\Theta(1)$. $\\text{DEQUEUE}$: worst $O(n)$, amortized $\\Theta(1)$. Let the two stacks be $A$ and $B$. $\\text{ENQUEUE}$ pushes elements on $B$. $\\text{DEQUEUE}$ pops elements from $A$. If $A$ is empty, the contents of $B$ are transfered to $A$ by popping them out of $B$ and pushing them to $A$. That way they appear in reverse order and are popped in the original. A $\\text{DEQUEUE}$ operation can perform in $\\Theta(n)$ time, but that will happen only when $A$ is empty. If many $\\text{ENQUEUE}$s and $\\text{DEQUEUE}$s are performed, the total time will be linear to the number of elements, not to the largest length of the queue. 10.1-7 Show how to implement a stack using two queues. Analyze the running time of the stack operations. $\\text{PUSH}$: $\\Theta(1)$. $\\text{POP}$: $\\Theta(n)$. We have two queues and mark one of them as active. $\\text{PUSH}$ queues an element on the active queue. $\\text{POP}$ should dequeue all but one element of the active queue and queue them on the inactive. The roles of the queues are then reversed, and the final element left in the (now) inactive queue is returned. The $\\text{PUSH}$ operation is $\\Theta(1)$, but the $\\text{POP}$ operation is $\\Theta(n)$ where $n$ is the number of elements in the stack.","title":"10.1 Stacks and queues"},{"location":"Chap10/10.1/#101-1","text":"Using Figure 10.1 as a model, illustrate the result of each operation in the sequence $\\text{PUSH}(S, 4)$, $\\text{PUSH}(S, 1)$, $\\text{PUSH}(S, 3)$, $\\text{POP}(S)$, $\\text{PUSH}(S, 8)$, and $\\text{POP}(S)$ on an initially empty stack $S$ stored in array $S[1..6]$. $$ \\begin{array}{l|ccc} \\text{PUSH($S, 4$)} & 4 & & \\\\ \\text{PUSH($S, 1$)} & 4 & 1 & \\\\ \\text{PUSH($S, 3$)} & 4 & 1 & 3 \\\\ \\text{POP($S$)} & 4 & 1 & \\\\ \\text{PUSH($S, 8$)} & 4 & 1 & 8 \\\\ \\text{POP($S$)} & 4 & 1 & \\end{array} $$","title":"10.1-1"},{"location":"Chap10/10.1/#101-2","text":"Explain how to implement two stacks in one array $A[1..n]$ in such a way that neither stack overflows unless the total number of elements in both stacks together is $n$. The $\\text{PUSH}$ and $\\text{POP}$ operations should run in $O(1)$ time. The first stack starts at $1$ and grows up towards n, while the second starts form $n$ and grows down towards $1$. Stack overflow happens when an element is pushed when the two stack pointers are adjacent.","title":"10.1-2"},{"location":"Chap10/10.1/#101-3","text":"Using Figure 10.2 as a model, illustrate the result of each operation in the sequence $\\text{ENQUEUE}(Q, 4)$, $\\text{ENQUEUE}(Q ,1)$, $\\text{ENQUEUE}(Q, 3)$, $\\text{DEQUEUE}(Q)$, $\\text{ENQUEUE}(Q, 8)$, and $\\text{DEQUEUE}(Q)$ on an initially empty queue $Q$ stored in array $Q[1..6]$. $$ \\begin{array}{l|cccc} \\text{ENQUEUE($Q, 4$)} & 4 & & & \\\\ \\text{ENQUEUE($Q, 1$)} & 4 & 1 & & \\\\ \\text{ENQUEUE($Q, 3$)} & 4 & 1 & 3 & \\\\ \\text{DEQUEUE($Q$)} & & 1 & 3 & \\\\ \\text{ENQUEUE($Q, 8$)} & & 1 & 3 & 8 \\\\ \\text{DEQUEUE($Q$)} & & & 3 & 8 \\end{array} $$","title":"10.1-3"},{"location":"Chap10/10.1/#101-4","text":"Rewrite $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ to detect underflow and overflow of a queue. To detect underflow and overflow of a queue, we can implement $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ first. QUEUE - EMPTY ( Q ) if Q . head == Q . tail return true else return false QUEUE - FULL ( Q ) if Q . head == Q . tail + 1 or ( Q . head == 1 and Q . tail == Q . length ) return true else return false ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x","title":"10.1-4"},{"location":"Chap10/10.1/#101-5","text":"Whereas a stack allows insertion and deletion of elements at only one end, and a queue allows insertion at one end and deletion at the other end, a deque (double-ended queue) allows insertion and deletion at both ends. Write four $O(1)$-time procedures to insert elements into and delete elements from both ends of a deque implemented by an array. The procedures $\\text{QUEUE-EMPTY}$ and $\\text{QUEUE-FULL}$ are implemented in Exercise 10.1-4. HEAD - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else if Q . head == 1 Q . head = Q . length else Q . head = Q . head - 1 Q [ Q . head ] = x TAIL - ENQUEUE ( Q , x ) if QUEUE - FULL ( Q ) error \"overflow\" else Q [ Q . tail ] = x if Q . tail == Q . length Q . tail = 1 else Q . tail = Q . tail + 1 HEAD - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else x = Q [ Q . head ] if Q . head == Q . length Q . head = 1 else Q . head = Q . head + 1 return x TAIL - DEQUEUE ( Q ) if QUEUE - EMPTY ( Q ) error \"underflow\" else if Q . tail == 1 Q . tail = Q . length else Q . tail = Q . tail - 1 x = Q [ Q . tail ] return x","title":"10.1-5"},{"location":"Chap10/10.1/#101-6","text":"Show how to implement a queue using two stacks. Analyze the running time of the queue operations. $\\text{ENQUEUE}$: $\\Theta(1)$. $\\text{DEQUEUE}$: worst $O(n)$, amortized $\\Theta(1)$. Let the two stacks be $A$ and $B$. $\\text{ENQUEUE}$ pushes elements on $B$. $\\text{DEQUEUE}$ pops elements from $A$. If $A$ is empty, the contents of $B$ are transfered to $A$ by popping them out of $B$ and pushing them to $A$. That way they appear in reverse order and are popped in the original. A $\\text{DEQUEUE}$ operation can perform in $\\Theta(n)$ time, but that will happen only when $A$ is empty. If many $\\text{ENQUEUE}$s and $\\text{DEQUEUE}$s are performed, the total time will be linear to the number of elements, not to the largest length of the queue.","title":"10.1-6"},{"location":"Chap10/10.1/#101-7","text":"Show how to implement a stack using two queues. Analyze the running time of the stack operations. $\\text{PUSH}$: $\\Theta(1)$. $\\text{POP}$: $\\Theta(n)$. We have two queues and mark one of them as active. $\\text{PUSH}$ queues an element on the active queue. $\\text{POP}$ should dequeue all but one element of the active queue and queue them on the inactive. The roles of the queues are then reversed, and the final element left in the (now) inactive queue is returned. The $\\text{PUSH}$ operation is $\\Theta(1)$, but the $\\text{POP}$ operation is $\\Theta(n)$ where $n$ is the number of elements in the stack.","title":"10.1-7"},{"location":"Chap10/10.2/","text":"10.2-1 Can you implement the dynamic-set operation $\\text{INSERT}$ on a singly linked list in $O(1)$ time? How about $\\text{DELETE}$? $\\text{INSERT}$: can be implemented in constant time by prepending it to the list. LIST - INSERT ( L , x ) x . next = L . head L . head = x $\\text{DELETE}$: you can copy the value from the successor to element you want to delete, and then you can delete the successor in $O(1)$ time. This solution is not good in situations when you have a large object, in that case copying the whole object will be a bad idea. 10.2-2 Implement a stack using a singly linked list $L$. The operations $\\text{PUSH}$ and $\\text{POP}$ should still take $O(1)$ time. STACK - EMPTY ( L ) if L . head == NIL return true else return false $\\text{PUSH}$: adds an element in the beginning of the list. PUSH ( L , x ) x . next = L . head L . head = x $\\text{POP}$: removes the first element from the list. POP ( L ) if STACK - EMPTY ( L ) error \"underflow\" else x = L . head L . head = L . head . next return x 10.2-3 Implement a queue by a singly linked list $L$. The operations $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ should still take $O(1)$ time. QUEUE - EMPTY ( L ) if L . head == NIL return true else return false $\\text{ENQUEUE}$: inserts an element at the end of the list. In this case we need to keep track of the last element of the list. We can do that with a sentinel. ENQUEUE ( L , x ) if QUEUE - EMPTY ( L ) L . head = x else L . tail . next = x L . tail = x x . next = NIL $\\text{DEQUEUE}$: removes an element from the beginning of the list. DEQUEUE ( L ) if QUEUE - EMPTY ( L ) error \"underflow\" else x = L . head if L . head == L . tail L . tail = NIL L . head = L . head . next return x 10.2-4 As written, each loop iteration in the $\\text{LIST-SEARCH}'$ procedure requires two tests: one for $x \\ne L.nil$ and one for $x.key \\ne k$. Show how to eliminate the test for $x \\ne L.nil$ in each iteration. LIST - SEARCH ' ( L , k ) x = L . nil . next L . nil . key = k while x . key != k x = x . next return x 10.2-5 Implement the dictionary operations $\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$ using singly linked, circular lists. What are the running times of your procedures? $\\text{INSERT}$: $O(1)$. LIST - INSERT '' ( L , x ) x . next = L . nil . next L . nil . next = x $\\text{DELETE}$: $O(n)$. LIST - DELETE '' ( L , x ) prev = L . nil while prev . next != x if prev . next == L . nil error \"element not exist\" prev = prev . next prev . next = x . next $\\text{SEARCH}$: $O(n)$. LIST - SEARCH '' ( L , k ) x = L . nil . next while x != L . nil and x . key != k x = x . next return x 10.2-6 The dynamic-set operation $\\text{UNION}$ takes two disjoint sets $S_1$ and $S_2$ as input, and it returns a set $S = S_1 \\cup S_2$ consisting of all the elements of $S_1$ and $S_2$. The sets $S_1$ and $S_2$ are usually destroyed by the operation. Show how to support $\\text{UNION}$ in $O(1)$ time using a suitable list data structure. If both sets are a doubly linked lists, we just point link the last element of the first list to the first element in the second. If the implementation uses sentinels, we need to destroy one of them. LIST - UNION ( L [ 1 ], L [ 2 ]) L [ 2 ]. nil . next . prev = L [ 1 ]. nil . prev L [ 1 ]. nil . prev . next = L [ 2 ]. nil . next L [ 2 ]. nil . prev . next = L [ 1 ]. nil L [ 1 ]. nil . prev = L [ 2 ]. nil . prev 10.2-7 Give a $\\Theta(n)$-time nonrecursive procedure that reverses a singly linked list of $n$ elements. The procedure should use no more than constant storage beyond that needed for the list itself. LIST - REVERSE ( L ) p [ 1 ] = NIL p [ 2 ] = L . head while p [ 2 ] != NIL p [ 3 ] = p [ 2 ]. next p [ 2 ]. next = p [ 1 ] p [ 1 ] = p [ 2 ] p [ 2 ] = p [ 3 ] L . head = p [ 1 ] 10.2-8 $\\star$ Explain how to implement doubly linked lists using only one pointer value $x.np$ per item instead of the usual two ($next$ and $prev$). Assume all pointer values can be interpreted as $k$-bit integers, and define $x.np$ to be $x.np = x.next \\text{ XOR } x.prev$, the $k$-bit \"exclusive-or\" of $x.next$ and $x.prev$. (The value $\\text{NIL}$ is represented by $0$.) Be sure to describe what information you need to access the head of the list. Show how to implement the $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ operations on such a list. Also show how to reverse such a list in $O(1)$ time. LIST - SEARCH ( L , k ) prev = NIL x = L . head while x != NIL and x . key != k next = prev XOR x . np prev = x x = next return x LIST - INSERT ( L , x ) x . np = NIL XOR L . tail if L . tail != NIL L . tail . np = ( L . tail . np XOR NIL ) XOR x // tail.prev XOR x if L . head == NIL L . head = x L . tail = x LIST - DELETE ( L , x ) y = L . head prev = NIL while y != NIL next = prev XOR y . np if y != x prev = y y = next else if prev != NIL prev . np = ( prev . np XOR y ) XOR next // prev.prev XOR next else L . head = next if next != NIL next . np = prev XOR ( y XOR next . np ) // prev XOR next.next else L . tail = prev LIST - REVERSE ( L ) tmp = L . head L . head = L . tail L . tail = tmp","title":"10.2 Linked lists"},{"location":"Chap10/10.2/#102-1","text":"Can you implement the dynamic-set operation $\\text{INSERT}$ on a singly linked list in $O(1)$ time? How about $\\text{DELETE}$? $\\text{INSERT}$: can be implemented in constant time by prepending it to the list. LIST - INSERT ( L , x ) x . next = L . head L . head = x $\\text{DELETE}$: you can copy the value from the successor to element you want to delete, and then you can delete the successor in $O(1)$ time. This solution is not good in situations when you have a large object, in that case copying the whole object will be a bad idea.","title":"10.2-1"},{"location":"Chap10/10.2/#102-2","text":"Implement a stack using a singly linked list $L$. The operations $\\text{PUSH}$ and $\\text{POP}$ should still take $O(1)$ time. STACK - EMPTY ( L ) if L . head == NIL return true else return false $\\text{PUSH}$: adds an element in the beginning of the list. PUSH ( L , x ) x . next = L . head L . head = x $\\text{POP}$: removes the first element from the list. POP ( L ) if STACK - EMPTY ( L ) error \"underflow\" else x = L . head L . head = L . head . next return x","title":"10.2-2"},{"location":"Chap10/10.2/#102-3","text":"Implement a queue by a singly linked list $L$. The operations $\\text{ENQUEUE}$ and $\\text{DEQUEUE}$ should still take $O(1)$ time. QUEUE - EMPTY ( L ) if L . head == NIL return true else return false $\\text{ENQUEUE}$: inserts an element at the end of the list. In this case we need to keep track of the last element of the list. We can do that with a sentinel. ENQUEUE ( L , x ) if QUEUE - EMPTY ( L ) L . head = x else L . tail . next = x L . tail = x x . next = NIL $\\text{DEQUEUE}$: removes an element from the beginning of the list. DEQUEUE ( L ) if QUEUE - EMPTY ( L ) error \"underflow\" else x = L . head if L . head == L . tail L . tail = NIL L . head = L . head . next return x","title":"10.2-3"},{"location":"Chap10/10.2/#102-4","text":"As written, each loop iteration in the $\\text{LIST-SEARCH}'$ procedure requires two tests: one for $x \\ne L.nil$ and one for $x.key \\ne k$. Show how to eliminate the test for $x \\ne L.nil$ in each iteration. LIST - SEARCH ' ( L , k ) x = L . nil . next L . nil . key = k while x . key != k x = x . next return x","title":"10.2-4"},{"location":"Chap10/10.2/#102-5","text":"Implement the dictionary operations $\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$ using singly linked, circular lists. What are the running times of your procedures? $\\text{INSERT}$: $O(1)$. LIST - INSERT '' ( L , x ) x . next = L . nil . next L . nil . next = x $\\text{DELETE}$: $O(n)$. LIST - DELETE '' ( L , x ) prev = L . nil while prev . next != x if prev . next == L . nil error \"element not exist\" prev = prev . next prev . next = x . next $\\text{SEARCH}$: $O(n)$. LIST - SEARCH '' ( L , k ) x = L . nil . next while x != L . nil and x . key != k x = x . next return x","title":"10.2-5"},{"location":"Chap10/10.2/#102-6","text":"The dynamic-set operation $\\text{UNION}$ takes two disjoint sets $S_1$ and $S_2$ as input, and it returns a set $S = S_1 \\cup S_2$ consisting of all the elements of $S_1$ and $S_2$. The sets $S_1$ and $S_2$ are usually destroyed by the operation. Show how to support $\\text{UNION}$ in $O(1)$ time using a suitable list data structure. If both sets are a doubly linked lists, we just point link the last element of the first list to the first element in the second. If the implementation uses sentinels, we need to destroy one of them. LIST - UNION ( L [ 1 ], L [ 2 ]) L [ 2 ]. nil . next . prev = L [ 1 ]. nil . prev L [ 1 ]. nil . prev . next = L [ 2 ]. nil . next L [ 2 ]. nil . prev . next = L [ 1 ]. nil L [ 1 ]. nil . prev = L [ 2 ]. nil . prev","title":"10.2-6"},{"location":"Chap10/10.2/#102-7","text":"Give a $\\Theta(n)$-time nonrecursive procedure that reverses a singly linked list of $n$ elements. The procedure should use no more than constant storage beyond that needed for the list itself. LIST - REVERSE ( L ) p [ 1 ] = NIL p [ 2 ] = L . head while p [ 2 ] != NIL p [ 3 ] = p [ 2 ]. next p [ 2 ]. next = p [ 1 ] p [ 1 ] = p [ 2 ] p [ 2 ] = p [ 3 ] L . head = p [ 1 ]","title":"10.2-7"},{"location":"Chap10/10.2/#102-8-star","text":"Explain how to implement doubly linked lists using only one pointer value $x.np$ per item instead of the usual two ($next$ and $prev$). Assume all pointer values can be interpreted as $k$-bit integers, and define $x.np$ to be $x.np = x.next \\text{ XOR } x.prev$, the $k$-bit \"exclusive-or\" of $x.next$ and $x.prev$. (The value $\\text{NIL}$ is represented by $0$.) Be sure to describe what information you need to access the head of the list. Show how to implement the $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ operations on such a list. Also show how to reverse such a list in $O(1)$ time. LIST - SEARCH ( L , k ) prev = NIL x = L . head while x != NIL and x . key != k next = prev XOR x . np prev = x x = next return x LIST - INSERT ( L , x ) x . np = NIL XOR L . tail if L . tail != NIL L . tail . np = ( L . tail . np XOR NIL ) XOR x // tail.prev XOR x if L . head == NIL L . head = x L . tail = x LIST - DELETE ( L , x ) y = L . head prev = NIL while y != NIL next = prev XOR y . np if y != x prev = y y = next else if prev != NIL prev . np = ( prev . np XOR y ) XOR next // prev.prev XOR next else L . head = next if next != NIL next . np = prev XOR ( y XOR next . np ) // prev XOR next.next else L . tail = prev LIST - REVERSE ( L ) tmp = L . head L . head = L . tail L . tail = tmp","title":"10.2-8 $\\star$"},{"location":"Chap10/10.3/","text":"10.3-1 Draw a picture of the sequence $\\langle 13, 4, 8, 19, 5, 11 \\rangle$ stored as a doubly linked list using the multiple-array representation. Do the same for the single-array representation. A multiple-array representation with $L = 2$, $$ \\begin{array}{|r|c|c|c|c|c|c|c|} \\hline index & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline next & & 3 & 4 & 5 & 6 & 7 & \\diagup \\\\ \\hline key & & 13 & 4 & 8 & 19 & 5 & 11 \\\\ \\hline prev & & \\diagup & 2 & 3 & 4 & 5 & 6 \\\\ \\hline \\end{array} $$ A single-array version with $L = 1$, $$ \\begin{array}{|r|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \\hline index & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 \\\\ \\hline key & 13 & 4 & \\diagup & 4 & 7 & 1 & 8 & 10 & 4 & 19 & 13 & 7 & 5 & 16 & 10 & 11 & \\diagup & 13 \\\\ \\hline \\end{array} $$ 10.3-2 Write the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ for a homogeneous collection of objects implemented by the single-array representation. ALLOCATE - OBJECT () if free == NIL error \"out of space\" else x = free free = A [ x + 1 ] return x FREE - OBJECT ( x ) A [ x + 1 ] = free free = x 10.3-3 Why don't we need to set or reset the $prev$ attributes of objects in the implementation of the $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures? We implement $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ in the hope of managing the storage of currently non-used object in the free list so that one can be allocated for reusing. As the free list acts like a stack, to maintain this stack-like collection, we merely remember its first pointer and set the $next$ attribute of objects. There is no need to worry the $prev$ attribute, for it hardly has any impact on the resulting free list. 10.3-4 It is often desirable to keep all elements of a doubly linked list compact in storage, using, for example, the first $m$ index locations in the multiple-array representation. (This is the case in a paged, virtual-memory computing environment.) Explain how to implement the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ so that the representation is compact. Assume that there are no pointers to elements of the linked list outside the list itself. ($\\textit{Hint:}$ Use the array implementation of a stack.) ALLOCATE - OBJECT () if STACK - EMPTY ( F ) error \"out of space\" else x = POP ( F ) return x FREE - OBJECT ( x ) p = F . top - 1 p . prev . next = x p . next . prev = x x . key = p . key x . prev = p . prev x . next = p . next PUSH ( F , p ) 10.3-5 Let $L$ be a doubly linked list of length $n$ stored in arrays $key$, $prev$, and $next$ of length $m$. Suppose that these arrays are managed by $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures that keep a doubly linked free list $F$. Suppose further that of the $m$ items, exactly $n$ are on list $L$ and $m - n$ are on the free list. Write a procedure $\\text{COMPACTIFY-LIST}(L, F)$ that, given the list $L$ and the free list $F$, moves the items in $L$ so that they occupy array positions $1, 2, \\ldots, n$ and adjusts the free list $F$ so that it remains correct, occupying array positions $n + 1, n + 2, \\ldots, m$. The running time of your procedure should be $\\Theta(n)$, and it should use only a constant amount of extra space. Argue that your procedure is correct. We represent the combination of arrays $key$, $prev$, and $next$ by a multible-array $A$. Each object of $A$'s is either in list $L$ or in the free list $F$, but not in both. The procedure $\\text{COMPACTIFY-LIST}$ transposes the first object in $L$ with the first object in $A$, the second objects until the list $L$ is exhausted. COMPACTIFY - LIST ( L , F ) TRANSPOSE ( A [ L . head ], A [ 1 ]) if F . head == 1 F . head = L . head L . head = 1 l = A [ L . head ]. next i = 2 while l != NIL TRANSPOSE ( A [ l ], A [ i ]) if F == i F = l l = A [ l ]. next i = i + 1 TRANSPOSE ( a , b ) SWAP ( a . prev . next , b . prev . next ) SWAP ( a . prev , b . prev ) SWAP ( a . next . prev , b . next . prev ) SWAP ( a . next , b . next )","title":"10.3 Implementing pointers and objects"},{"location":"Chap10/10.3/#103-1","text":"Draw a picture of the sequence $\\langle 13, 4, 8, 19, 5, 11 \\rangle$ stored as a doubly linked list using the multiple-array representation. Do the same for the single-array representation. A multiple-array representation with $L = 2$, $$ \\begin{array}{|r|c|c|c|c|c|c|c|} \\hline index & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline next & & 3 & 4 & 5 & 6 & 7 & \\diagup \\\\ \\hline key & & 13 & 4 & 8 & 19 & 5 & 11 \\\\ \\hline prev & & \\diagup & 2 & 3 & 4 & 5 & 6 \\\\ \\hline \\end{array} $$ A single-array version with $L = 1$, $$ \\begin{array}{|r|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \\hline index & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 \\\\ \\hline key & 13 & 4 & \\diagup & 4 & 7 & 1 & 8 & 10 & 4 & 19 & 13 & 7 & 5 & 16 & 10 & 11 & \\diagup & 13 \\\\ \\hline \\end{array} $$","title":"10.3-1"},{"location":"Chap10/10.3/#103-2","text":"Write the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ for a homogeneous collection of objects implemented by the single-array representation. ALLOCATE - OBJECT () if free == NIL error \"out of space\" else x = free free = A [ x + 1 ] return x FREE - OBJECT ( x ) A [ x + 1 ] = free free = x","title":"10.3-2"},{"location":"Chap10/10.3/#103-3","text":"Why don't we need to set or reset the $prev$ attributes of objects in the implementation of the $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures? We implement $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ in the hope of managing the storage of currently non-used object in the free list so that one can be allocated for reusing. As the free list acts like a stack, to maintain this stack-like collection, we merely remember its first pointer and set the $next$ attribute of objects. There is no need to worry the $prev$ attribute, for it hardly has any impact on the resulting free list.","title":"10.3-3"},{"location":"Chap10/10.3/#103-4","text":"It is often desirable to keep all elements of a doubly linked list compact in storage, using, for example, the first $m$ index locations in the multiple-array representation. (This is the case in a paged, virtual-memory computing environment.) Explain how to implement the procedures $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ so that the representation is compact. Assume that there are no pointers to elements of the linked list outside the list itself. ($\\textit{Hint:}$ Use the array implementation of a stack.) ALLOCATE - OBJECT () if STACK - EMPTY ( F ) error \"out of space\" else x = POP ( F ) return x FREE - OBJECT ( x ) p = F . top - 1 p . prev . next = x p . next . prev = x x . key = p . key x . prev = p . prev x . next = p . next PUSH ( F , p )","title":"10.3-4"},{"location":"Chap10/10.3/#103-5","text":"Let $L$ be a doubly linked list of length $n$ stored in arrays $key$, $prev$, and $next$ of length $m$. Suppose that these arrays are managed by $\\text{ALLOCATE-OBJECT}$ and $\\text{FREE-OBJECT}$ procedures that keep a doubly linked free list $F$. Suppose further that of the $m$ items, exactly $n$ are on list $L$ and $m - n$ are on the free list. Write a procedure $\\text{COMPACTIFY-LIST}(L, F)$ that, given the list $L$ and the free list $F$, moves the items in $L$ so that they occupy array positions $1, 2, \\ldots, n$ and adjusts the free list $F$ so that it remains correct, occupying array positions $n + 1, n + 2, \\ldots, m$. The running time of your procedure should be $\\Theta(n)$, and it should use only a constant amount of extra space. Argue that your procedure is correct. We represent the combination of arrays $key$, $prev$, and $next$ by a multible-array $A$. Each object of $A$'s is either in list $L$ or in the free list $F$, but not in both. The procedure $\\text{COMPACTIFY-LIST}$ transposes the first object in $L$ with the first object in $A$, the second objects until the list $L$ is exhausted. COMPACTIFY - LIST ( L , F ) TRANSPOSE ( A [ L . head ], A [ 1 ]) if F . head == 1 F . head = L . head L . head = 1 l = A [ L . head ]. next i = 2 while l != NIL TRANSPOSE ( A [ l ], A [ i ]) if F == i F = l l = A [ l ]. next i = i + 1 TRANSPOSE ( a , b ) SWAP ( a . prev . next , b . prev . next ) SWAP ( a . prev , b . prev ) SWAP ( a . next . prev , b . next . prev ) SWAP ( a . next , b . next )","title":"10.3-5"},{"location":"Chap10/10.4/","text":"10.4-1 Draw the binary tree rooted at index $6$ that is represented by the following attributes: $$ \\begin{array}{cccc} \\text{index} & key & left & right \\\\ \\hline 1 & 12 & 7 & 3 \\\\ 2 & 15 & 8 & \\text{NIL} \\\\ 3 & 4 & 10 & \\text{NIL} \\\\ 4 & 10 & 5 & 9 \\\\ 5 & 2 & \\text{NIL} & \\text{NIL} \\\\ 6 & 18 & 1 & 4 \\\\ 7 & 7 & \\text{NIL} & \\text{NIL} \\\\ 8 & 14 & 6 & 2 \\\\ 9 & 21 & \\text{NIL} & \\text{NIL} \\\\ 10 & 5 & \\text{NIL} & \\text{NIL} \\end{array} $$ 10.4-2 Write an $O(n)$-time recursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. PRINT - BINARY - TREE ( T ) x = T . root if x != NIL PRINT - BINARY - TREE ( x . left ) print x . key PRINT - BINARY - TREE ( x . right ) 10.4-3 Write an O$(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. Use a stack as an auxiliary data structure. PRINT - BINARY - TREE ( T , S ) PUSH ( S , T . root ) while ! STACK - EMPTY ( S ) x = S [ S . top ] while x != NIL // store all nodes on the path towards the leftmost leaf PUSH ( S , x . left ) x = S [ S . top ] POP ( S ) // S has NIL on its top, so pop it if ! STACK - EMPTY ( S ) // print this nodes, leap to its in-order successor x = POP ( S ) print x . key PUSH ( S , x . right ) 10.4-4 Write an $O(n)$-time procedure that prints all the keys of an arbitrary rooted tree with $n$ nodes, where the tree is stored using the left-child, right-sibling representation. PRINT - LCRS - TREE ( T ) x = T . root if x != NIL print x . key lc = x . left - child if lc != NIL PRINT - LCRS - TREE ( lc ) rs = lc . right - sibling while rs != NIL PRINT - LCRS - TREE ( rs ) rs = rs . right - sibling 10.4-5 $\\star$ Write an $O(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node. Use no more than constant extra space outside of the tree itself and do not modify the tree, even temporarily, during the procedure. PRINT - KEY ( T ) prev = NIL x = T . root while x != NIL if prev = x . parent print x . key prev = x if x . left x = x . left else if x . right x = x . right else x = x . parent else if prev == x . left and x . right != NIL prev = x x = x . right else prev = x x = x . parent 10.4-6 $\\star$ The left-child, right-sibling representation of an arbitrary rooted tree uses three pointers in each node: left-child , right-sibling , and parent . From any node, its parent can be reached and identified in constant time and all its children can be reached and identified in time linear in the number of children. Show how to use only two pointers and one boolean value in each node so that the parent of a node or all of its children can be reached and identified in time linear in the number of children. Use boolean to identify the last sibling, and the last sibling's right-sibling points to the parent.","title":"10.4 Representing rooted trees"},{"location":"Chap10/10.4/#104-1","text":"Draw the binary tree rooted at index $6$ that is represented by the following attributes: $$ \\begin{array}{cccc} \\text{index} & key & left & right \\\\ \\hline 1 & 12 & 7 & 3 \\\\ 2 & 15 & 8 & \\text{NIL} \\\\ 3 & 4 & 10 & \\text{NIL} \\\\ 4 & 10 & 5 & 9 \\\\ 5 & 2 & \\text{NIL} & \\text{NIL} \\\\ 6 & 18 & 1 & 4 \\\\ 7 & 7 & \\text{NIL} & \\text{NIL} \\\\ 8 & 14 & 6 & 2 \\\\ 9 & 21 & \\text{NIL} & \\text{NIL} \\\\ 10 & 5 & \\text{NIL} & \\text{NIL} \\end{array} $$","title":"10.4-1"},{"location":"Chap10/10.4/#104-2","text":"Write an $O(n)$-time recursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. PRINT - BINARY - TREE ( T ) x = T . root if x != NIL PRINT - BINARY - TREE ( x . left ) print x . key PRINT - BINARY - TREE ( x . right )","title":"10.4-2"},{"location":"Chap10/10.4/#104-3","text":"Write an O$(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node in the tree. Use a stack as an auxiliary data structure. PRINT - BINARY - TREE ( T , S ) PUSH ( S , T . root ) while ! STACK - EMPTY ( S ) x = S [ S . top ] while x != NIL // store all nodes on the path towards the leftmost leaf PUSH ( S , x . left ) x = S [ S . top ] POP ( S ) // S has NIL on its top, so pop it if ! STACK - EMPTY ( S ) // print this nodes, leap to its in-order successor x = POP ( S ) print x . key PUSH ( S , x . right )","title":"10.4-3"},{"location":"Chap10/10.4/#104-4","text":"Write an $O(n)$-time procedure that prints all the keys of an arbitrary rooted tree with $n$ nodes, where the tree is stored using the left-child, right-sibling representation. PRINT - LCRS - TREE ( T ) x = T . root if x != NIL print x . key lc = x . left - child if lc != NIL PRINT - LCRS - TREE ( lc ) rs = lc . right - sibling while rs != NIL PRINT - LCRS - TREE ( rs ) rs = rs . right - sibling","title":"10.4-4"},{"location":"Chap10/10.4/#104-5-star","text":"Write an $O(n)$-time nonrecursive procedure that, given an $n$-node binary tree, prints out the key of each node. Use no more than constant extra space outside of the tree itself and do not modify the tree, even temporarily, during the procedure. PRINT - KEY ( T ) prev = NIL x = T . root while x != NIL if prev = x . parent print x . key prev = x if x . left x = x . left else if x . right x = x . right else x = x . parent else if prev == x . left and x . right != NIL prev = x x = x . right else prev = x x = x . parent","title":"10.4-5 $\\star$"},{"location":"Chap10/10.4/#104-6-star","text":"The left-child, right-sibling representation of an arbitrary rooted tree uses three pointers in each node: left-child , right-sibling , and parent . From any node, its parent can be reached and identified in constant time and all its children can be reached and identified in time linear in the number of children. Show how to use only two pointers and one boolean value in each node so that the parent of a node or all of its children can be reached and identified in time linear in the number of children. Use boolean to identify the last sibling, and the last sibling's right-sibling points to the parent.","title":"10.4-6 $\\star$"},{"location":"Chap10/Problems/10-1/","text":"For each of the four types of lists in the following table, what is the asymptotic worst-case running time for each dynamic-set operation listed? $$ \\begin{array}{l|c|c|c|c|} & \\text{unsorted, singly linked} & \\text{sorted, singly linked} & \\text{unsorted, doubly linked} & \\text{sorted, doubly linked} \\\\ \\hline \\text{SEARCH($L, k$)} & & & & \\\\ \\hline \\text{INSERT($L, x$)} & & & & \\\\ \\hline \\text{DELETE($L, x$)} & & & & \\\\ \\hline \\text{SUCCESSOR($L, x$)} & & & & \\\\ \\hline \\text{PREDECESSOR($L, x$)} & & & & \\\\ \\hline \\text{MINIMUM($L$)} & & & & \\\\ \\hline \\text{MAXIMUM($L$)} & & & & \\\\ \\hline \\end{array} $$ $$ \\begin{array}{l|c|c|c|c|} & \\text{unsorted, singly linked} & \\text{sorted, singly linked} & \\text{unsorted, doubly linked} & \\text{sorted, doubly linked} \\\\ \\hline \\text{SEARCH($L, k$)} & \\Theta(n) & \\Theta(n) & \\Theta(n) & \\Theta(n) \\\\ \\hline \\text{INSERT($L, x$)} & \\Theta(1) & \\Theta(n) & \\Theta(1) & \\Theta(n) \\\\ \\hline \\text{DELETE($L, x$)} & \\Theta(n) & \\Theta(n) & \\Theta(1) & \\Theta(1) \\\\ \\hline \\text{SUCCESSOR($L, x$)} & \\Theta(n) & \\Theta(1) & \\Theta(n) & \\Theta(1) \\\\ \\hline \\text{PREDECESSOR($L, x$)} & \\Theta(n) & \\Theta(n) & \\Theta(n) & \\Theta(1) \\\\ \\hline \\text{MINIMUM($L$)} & \\Theta(n) & \\Theta(1) & \\Theta(n) & \\Theta(1) \\\\ \\hline \\text{MAXIMUM($L$)} & \\Theta(n) & \\Theta(n) & \\Theta(n) & \\Theta(1) \\\\ \\hline \\end{array} $$","title":"10-1 Comparisons among lists"},{"location":"Chap10/Problems/10-2/","text":"A mergeable heap supports the following operations: $\\text{MAKE-HEAP}$ (which creates an empty mergeable heap), $\\text{INSERT}$, $\\text{MINIMUM}$, $\\text{EXTRACT-MIN}$, and $\\text{UNION}$. Show how to implement mergeable heaps using linked lists in each of the following cases. Try to make each operation as efficient as possible. Analyze the running time of each operation in terms of the size of the dynamic set(s) being operated on. a. Lists are sorted. b. Lists are unsorted. c. Lists are unsorted, and dynamic sets to be merged are disjoint. In all three cases, $\\text{MAKE-HEAP}$ simply creates a new list $L$, sets $L.head = \\text{NIL}$, and returns $L$ in constant time. Assume lists are doubly linked. To realize a linked list as a heap, we imagine the usual array implementation of a binary heap, where the children of the $i$th element are $2i$ and $2i + 1$. a. To insert, we perform a linear scan to see where to insert an element such that the list remains sorted. This takes linear time. The first element in the list is the minimum element, and we can find it in constant time. $\\text{EXTRACT-MIN}$ returns the first element of the list, then deletes it. Union performs a merge operation between the two sorted lists, interleaving their entries such that the resulting list is sorted. This takes time linear in the sum of the lengths of the two lists. b. To insert an element $x$ into the heap, begin linearly scanning the list until the first instance of an element $y$ which is strictly larger than $x$. If no such larger element exists, simply insert $x$ at the end of the list. If $y$ does exist, replace $y \\text t$ by $x$. This maintains the min-heap property because $x \\le y$ and $y$ was smaller than each of its children, so $x$ must be as well. Moreover, $x$ is larger than its parent because $y$ was the first element in the list to exceed $x$. Now insert $y$, starting the scan at the node following $x$. Since we check each node at most once, the time is linear in the size of the list. To get the minimum element, return the key of the head of the list in constant time. To extract the minimum element, we first call $\\text{MINIMUM}$. Next, we'll replace the key of the head of the list by the key of the second smallest element $y$ in the list. We'll take the key stored at the end of the list and use it to replace the key of $y$. Finally, we'll delete the last element of the list, and call $\\text{MIN-HEAPIFY}$ on the list. To implement this with linked lists, we need to step through the list to get from element $i$ to element $2i$. We omit this detail from the code, but we'll consider it for runtime analysis. Since the value of $i$ on which $\\text{MIN-HEAPIFY}$ is called is always increasing and we never need to step through elements multiple times, the runtime is linear in the length of the list. EXTRACT - MIN ( L ) min = MINIMIM ( L ) linearly scan for the second smallest element , located in position i L . head . key = L [ i ] L [ i ]. key = L [ L . length ]. key DELETE ( L , L [ L . length ]) MIN - HEAPIFY ( L [ i ], i ) return min MIN - HEAPIFY ( L [ i ], i ) l = L [ 2 i ]. key r = L [ 2 i + 1 ]. key p = L [ i ]. key smallest = i if L [ 2 i ] != NIL and l < p smallest = 2 i if L [ 2 i + 1 ] != NIL and r < L [ smallest ] smallest = 2 i + 1 if smallest != i exchange L [ i ] with L [ smallest ] MIN - HEAPIFY ( L [ smallest ], smallest ]) Union is implemented below, where we assume $A$ and $B$ are the two list representations of heaps to be merged. The runtime is again linear in the lengths of the lists to be merged. UNION ( A , B ) if A . head == NIL return B x = A . head while B . head != NIL if B . head . key \u2264 x . key INSERT ( B , x . key ) x . key = B . head . key DELETE ( B , B . head ) x = x . next return A c. Since the algorithms in part (b) didn't depend on the elements being distinct, we can use the same ones.","title":"10-2 Mergeable heaps using linked lists"},{"location":"Chap10/Problems/10-3/","text":"Exercise 10.3-4 asked how we might maintain an $n$-element list compactly in the first $n$ positions of an array. We shall assume that all keys are distinct and that the compact list is also sorted, that is, $key[i] < key[next[i]]$ for all $i = 1, 2, \\ldots, n$ such that $next[i] \\ne \\text{NIL}$. We will also assume that we have a variable $L$ that contains the index of the first element on the list. Under these assumptions, you will show that we can use the following randomized algorithm to search the list in $O(\\sqrt n)$ expected time. COMPACT - LIST - SEARCH ( L , n , k ) i = L while i != NIL and key [ i ] < k j = RANDOM ( 1 , n ) if key [ i ] < key [ j ] and key [ j ] \u2264 k i = j if key [ i ] == k return i i = next [ i ] if i == NIL or key [ i ] > k return NIL else return i If we ignore lines 3\u20137 of the procedure, we have an ordinary algorithm for searching a sorted linked list, in which index $i$ points to each position of the list in turn. The search terminates once the index $i$ \"falls off\" the end of the list or once $key[i] \\ge k$. In the latter case, if $key[i] = k$, clearly we have found a key with the value $k$. If, however, $key[i] > k$, then we will never find a key with the value $k$, and so terminating the search was the right thing to do. Lines 3\u20137 attempt to skip ahead to a randomly chosen position $j$. Such a skip benefits us if $key[j]$ is larger than $key[i]$ and no larger than $k$; in such a case, $j$ marks a position in the list that $i$ would have to reach during an ordinary list search. Because the list is compact, we know that any choice of $j$ between $1$ and $n$ indexes some object in the list rather than a slot on the free list. Instead of analyzing the performance of $\\text{COMPACT-LIST-SEARCH}$ directly, we shall analyze a related algorithm, $\\text{COMPACT-LIST-SEARCH}'$, which executes two separate loops. This algorithm takes an additional parameter $t$ which determines an upper bound on the number of iterations of the first loop. COMPACT - LIST - SEARCH ' ( L , n , k , t ) i = L for q = 1 to t j = RANDOM ( 1 , n ) if key [ i ] < key [ j ] and key [ j ] \u2264 k i = j if key [ i ] == k return i while i != NIL and key [ i ] < k i = next [ i ] if i == NIL or key [ i ] > k return NIL else return i To compare the execution of the algorithms $\\text{COMPACT-LIST-SEARCH}(L, n, k)$ and $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$, assume that the sequence of integers returned by the calls of $\\text{RANDOM}(1, n)$ is the same for both algorithms. a. Suppose that $\\text{COMPACT-LIST-SEARCH}(L, n, k)$ takes $t$ iterations of the while loop of lines 2\u20138. Argue that $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$ returns the same answer and that the total number of iterations of both the for and while loops within $\\text{COMPACT-LIST-SEARCH}'$ is at least $t$. In the call $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$, let $X_t$ be the random variable that describes the distance in the linked list (that is, through the chain of $next$ pointers) from position $i$ to the desired key $k$ after $t$ iterations of the for loop of lines 2\u20137 have occurred. b. Argue that the expected running time of $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$ is $O(t + \\text E[X_t])$. c. Show that $\\text E[X_t] \\le \\sum_{r = 1}^n (1 - r / n)^t$. ($\\textit{Hint:}$ Use equation $\\text{(C.25)}$.) d. Show that $\\sum_{r = 0}^{n - 1} r^t \\le n^{t + 1} / (t + 1)$. e. Prove that $\\text E[X_t] \\le n / (t + 1)$. f. Show that $\\text{COMPACT-LIST-SEARCH}'(L, n, k, t)$ runs in $O(t + n / t)$ expected time. g. Conclude that $\\text{COMPACT-LIST-SEARCH}$ runs in $O(\\sqrt n)$ expected time. h. Why do we assume that all keys are distinct in $\\text{COMPACT-LIST-SEARCH}$? Argue that random skips do not necessarily help asymptotically when the list contains repeated key values. a. If the original version of the algorithm takes only $t$ iterations, then, we have that it was only at most t random skips though the list to get to the desired value, since each iteration of the original while loop is a possible random jump followed by a normal step through the linked list. b. The for loop on lines 2\u20137 will get run exactly $t$ times, each of which is constant runtime. After that, the while loop on lines 8\u20139 will be run exactly $X_t$ times. So, the total runtime is $O(t + \\text E[X_t])$. c. Using equation $\\text{C.25}$, we have that $\\text E[X_t] = \\sum_{i = 1}^\\infty \\Pr\\{X_t \\ge i\\}$. So, we need to show that $\\Pr\\{X_t \\ge i\\} \\le (1 - i / n)^t$. This can be seen because having $X_t$ being greater than $i$ means that each random choice will result in an element that is either at least $i$ steps before the desired element, or is after the desired element. There are $n - i$ such elements, out of the total $n$ elements that we were pricking from. So, for a single one of the choices to be from such a range, we have a probability of $(n - i) / n = (1 - i / n)$. Since each of the selections was independent, the total probability that all of them were is $(1 - i / n)^t$, as desired. Lastly, we can note that since the linked list has length $n$, the probability that $X_t$ is greater than $n$ is equal to zero. d. Since we have that $t > 0$, we know that the function $f(x) = x^t$ is increasing, so, that means that $\\lfloor x \\rfloor^t \\le f(x)$. So, $$\\sum_{r = 0}^{n - 1} r^t = \\int_0^n \\lfloor r \\rfloor^t dr \\le \\int_0^n f(r)dr = \\frac{n^{t + 1}}{t + 1}.$$ e. $$ \\begin{aligned} \\text E[X_t] & \\le \\sum_{r = 1}^n (1 - r / n)^t & \\text{from part (c)} \\\\ & = \\sum_{r = 1}^n \\frac{(n - r)^t}{n^t} \\\\ & = \\frac{1}{n^t} \\sum_{r = 1}^n (n - r)^t, \\end{aligned} $$ and $$ \\begin{aligned} \\sum_{r = 1}^n (n - r)^t & = (n - 1)^t + (n - 2)^t + \\cdots + 1^t + 0^t \\\\ & = \\sum_{r = 0}^{n - 1} r^t. \\end{aligned} $$ So, $$ \\begin{aligned} \\text E[X_t] & = \\frac{1}{n^t} \\sum_{r = 0}^{n - 1} r^t \\\\ & \\le \\frac{1}{n^t} \\cdot \\frac{n^{t + 1}}{t + 1} & \\text{from part (d)} \\\\ & = \\frac{n}{t + 1}. \\end{aligned} $$ f. We just put together parts (b) and (e) to get that it runs in time $O(t + n / (t + 1))$. But, this is the same as $O(t + n / t)$. g. Since we have that for any number of iterations $t$ that the first algorithm takes to find its answer, the second algorithm will return it in time $O(t + n / t)$. In particular, if we just have that $t = \\sqrt n$. The second algorithm takes time only $O(\\sqrt n)$. This means that tihe first list search algorithm is $O(\\sqrt n)$ as well. h. If we don't have distinct key values, then, we may randomly select an element that is further along than we had been before, but not jump to it because it has the same key as what we were currently at. The analysis will break when we try to bound the probability that $X_t \\ge i$.","title":"10-3 Searching a sorted compact list"},{"location":"Chap11/11.1/","text":"11.1-1 Suppose that a dynamic set $S$ is represented by a direct-address table $T$ of length $m$. Describe a procedure that finds the maximum element of $S$. What is the worst-case performance of your procedure? As the dynamic set $S$ is represented by the direct-address table $T$, for each key $k$ in $S$, there is a slot $k$ in $T$ points to it. If no element with key $k$ in $S$, then $T[k] = \\text{NIL}$. Using this property, we can find the maximum element of $S$ by traversing down from the highest slot to seek the first non-$\\text{NIL}$ one. MAXIMUM ( S ) return TABLE - MAXIMUM ( T , m - 1 ) TABLE - MAXIMUM ( T , l ) if l < 0 return NIL else if DIRECT - ADDRESS - SEARCH ( T , l ) != NIL return l else return TABLE - MAXIMUM ( T , l - 1 ) The $\\text{TABLE-MAXIMUM}$ procedure gest down and checks $1$ sloc at a time, linearly approaches the solution. In the worst case where $S$ is empty, $\\text{TABLE-MAXIMUM}$ examines $m$ slots. Therefore, the worst-case performance of $\\text{MAXIMUM}$ is $O(m)$, where $m$ is the length of the direct-address table $T$. 11.1-2 A bit vector is simply an array of bits ($0$s and $1$s). A bit vector of length $m$ takes much less space than an array of $m$ pointers. Describe how to use a bit vector to represent a dynamic set of distinct elements with no satellite data. Dictionary operations should run in $O(1)$ time. Using the bit vector data structure, we can represent keys less than $m$ by a string of $m$ bits, denoted by $V[0..m - 1]$, in which each position that occupied by the bit $1$, corresponds to a key in the set $S$. If the set contains no element with key $k$, then $V[k] = 0$. For instance, we can store the set $\\{2, 4, 6, 10, 16\\}$ in a bit vector of length $20$: $$001010100010000010000$$ BITMAP - SEARCH ( V , k ) if V [ k ] != 0 return k else return NIL BITMAP - INSERT ( V , x ) V [ x ] = 1 BITMAP - DELETE ( V , x ) V [ x ] = 0 Each of these operations takes only $O(1)$ time. 11.1-3 Suggest how to implement a direct-address table in which the keys of stored elements do not need to be distinct and the elements can have satellite data. All three dictionary operations ($\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$) should run in $O(1)$ time. (Don't forget that $\\text{DELETE}$ takes as an argument a pointer to an object to be deleted, not a key.) Assuming that fetching an element should return the satellite data of all the stored elements, we can have each key map to a doubly linked list. $\\text{INSERT}$: appends the element to the list in constant time $\\text{DELETE}$: removes the element from the linked list in constant time (the element contains pointers to the previous and next element) $\\text{SEARCH}$: returns the first element, which is a node in a linked list, in constant time 11.1-4 $\\star$ We wish to implement a dictionary by using direct addressing on a huge array. At the start, the array entries may contain garbage, and initializing the entire array is impractical because of its size. Describe a scheme for implementing a direct-address dictionary on a huge array. Each stored object should use $O(1)$ space; the operations $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ should take $O(1)$ time each; and initializing the data structure should take $O(1)$ time. ($\\textit{Hint:}$ Use an additional array, treated somewhat like a stack whose size is the number of keys actually stored in the dictionary, to help determine whether a given entry in the huge array is valid or not.) The additional data structure will be a stack $S$. Initially, set $S$ to be empty, and do nothing to initialize the huge array. Each object stored in the huge array will have two parts: the key value, and a pointer to an element of $S$, which contains a pointer back to the object in the huge array. To insert $x$, push an element $y$ to the stack which contains a pointer to position $x$ in the huge array. Update position $A[x]$ in the huge array $A$ to contain a pointer to $y$ in $S$. To search for $x$, go to position $x$ of $A$ and go to the location stored there. If that location is an element of $S$ which contains a pointer to $A[x]$, then we know $x$ is in $A$. Otherwise, $x \\notin A$. To delete $x$, invalidate the element of $S$ which is pointed to by $A[x]$. Because there may be \"holes\" in $S$ now, we need to pop an item from $S$, move it to the position of the \"hole\", and update the pointer in $A$ accordingly. Each of these takes $O(1)$ time and there are at most as many elements in $S$ as there are valid elements in $A$.","title":"11.1 Direct-address tables"},{"location":"Chap11/11.1/#111-1","text":"Suppose that a dynamic set $S$ is represented by a direct-address table $T$ of length $m$. Describe a procedure that finds the maximum element of $S$. What is the worst-case performance of your procedure? As the dynamic set $S$ is represented by the direct-address table $T$, for each key $k$ in $S$, there is a slot $k$ in $T$ points to it. If no element with key $k$ in $S$, then $T[k] = \\text{NIL}$. Using this property, we can find the maximum element of $S$ by traversing down from the highest slot to seek the first non-$\\text{NIL}$ one. MAXIMUM ( S ) return TABLE - MAXIMUM ( T , m - 1 ) TABLE - MAXIMUM ( T , l ) if l < 0 return NIL else if DIRECT - ADDRESS - SEARCH ( T , l ) != NIL return l else return TABLE - MAXIMUM ( T , l - 1 ) The $\\text{TABLE-MAXIMUM}$ procedure gest down and checks $1$ sloc at a time, linearly approaches the solution. In the worst case where $S$ is empty, $\\text{TABLE-MAXIMUM}$ examines $m$ slots. Therefore, the worst-case performance of $\\text{MAXIMUM}$ is $O(m)$, where $m$ is the length of the direct-address table $T$.","title":"11.1-1"},{"location":"Chap11/11.1/#111-2","text":"A bit vector is simply an array of bits ($0$s and $1$s). A bit vector of length $m$ takes much less space than an array of $m$ pointers. Describe how to use a bit vector to represent a dynamic set of distinct elements with no satellite data. Dictionary operations should run in $O(1)$ time. Using the bit vector data structure, we can represent keys less than $m$ by a string of $m$ bits, denoted by $V[0..m - 1]$, in which each position that occupied by the bit $1$, corresponds to a key in the set $S$. If the set contains no element with key $k$, then $V[k] = 0$. For instance, we can store the set $\\{2, 4, 6, 10, 16\\}$ in a bit vector of length $20$: $$001010100010000010000$$ BITMAP - SEARCH ( V , k ) if V [ k ] != 0 return k else return NIL BITMAP - INSERT ( V , x ) V [ x ] = 1 BITMAP - DELETE ( V , x ) V [ x ] = 0 Each of these operations takes only $O(1)$ time.","title":"11.1-2"},{"location":"Chap11/11.1/#111-3","text":"Suggest how to implement a direct-address table in which the keys of stored elements do not need to be distinct and the elements can have satellite data. All three dictionary operations ($\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$) should run in $O(1)$ time. (Don't forget that $\\text{DELETE}$ takes as an argument a pointer to an object to be deleted, not a key.) Assuming that fetching an element should return the satellite data of all the stored elements, we can have each key map to a doubly linked list. $\\text{INSERT}$: appends the element to the list in constant time $\\text{DELETE}$: removes the element from the linked list in constant time (the element contains pointers to the previous and next element) $\\text{SEARCH}$: returns the first element, which is a node in a linked list, in constant time","title":"11.1-3"},{"location":"Chap11/11.1/#111-4-star","text":"We wish to implement a dictionary by using direct addressing on a huge array. At the start, the array entries may contain garbage, and initializing the entire array is impractical because of its size. Describe a scheme for implementing a direct-address dictionary on a huge array. Each stored object should use $O(1)$ space; the operations $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ should take $O(1)$ time each; and initializing the data structure should take $O(1)$ time. ($\\textit{Hint:}$ Use an additional array, treated somewhat like a stack whose size is the number of keys actually stored in the dictionary, to help determine whether a given entry in the huge array is valid or not.) The additional data structure will be a stack $S$. Initially, set $S$ to be empty, and do nothing to initialize the huge array. Each object stored in the huge array will have two parts: the key value, and a pointer to an element of $S$, which contains a pointer back to the object in the huge array. To insert $x$, push an element $y$ to the stack which contains a pointer to position $x$ in the huge array. Update position $A[x]$ in the huge array $A$ to contain a pointer to $y$ in $S$. To search for $x$, go to position $x$ of $A$ and go to the location stored there. If that location is an element of $S$ which contains a pointer to $A[x]$, then we know $x$ is in $A$. Otherwise, $x \\notin A$. To delete $x$, invalidate the element of $S$ which is pointed to by $A[x]$. Because there may be \"holes\" in $S$ now, we need to pop an item from $S$, move it to the position of the \"hole\", and update the pointer in $A$ accordingly. Each of these takes $O(1)$ time and there are at most as many elements in $S$ as there are valid elements in $A$.","title":"11.1-4 $\\star$"},{"location":"Chap11/11.2/","text":"11.2-1 Suppose we use a hash function $h$ to hash $n$ distinct keys into an array $T$ of length $m$. Assuming simple uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of $\\{\\{k, l\\}: k \\ne l \\text{ and } h(k) = h(l)\\}$? Under the assumption of simple uniform hashing, we will use linearity of expectation to compute this. Suppose that all the keys are totally ordered $\\{k_1, \\dots, k_n\\}$. Let $X_i$ be the number of $\\ell$'s such that $\\ell > k_i$ and $h(\\ell) = h(k_i)$. So $X_i$ is the (expected) number of times that key $k_i$ is collided by those keys hashed afterward. Note, that this is the same thing as $\\sum_{j > i} \\Pr(h(k_j) = h(k_i)) = \\sum_{j > i} 1 / m = (n - i) / m$. Then, by linearity of expectation, the number of collisions is the sum of the number of collisions for each possible smallest element in the collision. The expected number of collisions is $$\\sum_{i = 1}^n \\frac{n - i}{m} = \\frac{n^2 - n(n + 1) / 2}{m} = \\frac{n^2 - n}{2m}.$$ 11.2-2 Demonstrate what happens when we insert the keys $5, 28, 19, 15, 20, 33, 12, 17, 10$ into a hash table with collisions resolved by chaining. Let the table have $9$ slots, and let the hash function be $h(k) = k \\mod 9$. Let us number our slots $0, 1, \\dots, 8$. Then our resulting hash table will look like following: $$ \\begin{array}{c|l} h(k) & \\text{keys} \\\\ \\hline 0 \\mod 9 & \\\\ 1 \\mod 9 & 10 \\to 19 \\to 28 \\\\ 2 \\mod 9 & 20 \\\\ 3 \\mod 9 & 12 \\\\ 4 \\mod 9 & \\\\ 5 \\mod 9 & 5 \\\\ 6 \\mod 9 & 33 \\to 15 \\\\ 7 \\mod 9 & \\\\ 8 \\mod 9 & 17 \\end{array} $$ 11.2-3 Professor Marley hypothesizes that he can obtain substantial performance gains by modifying the chaining scheme to keep each list in sorted order. How does the professor's modification affect the running time for successful searches, unsuccessful searches, insertions, and deletions? Successful searches: no difference, $\\Theta(1 + \\alpha)$. Unsuccessful searches: faster but still $\\Theta(1 + \\alpha)$. Insertions: same as successful searches, $\\Theta(1 + \\alpha)$. Deletions: same as before if we use doubly linked lists, $\\Theta(1)$. 11.2-4 Suggest how to allocate and deallocate storage for elements within the hash table itself by linking all unused slots into a free list. Assume that one slot can store a flag and either one element plus a pointer or two pointers. All dictionary and free-list operations should run in $O(1)$ expected time. Does the free list need to be doubly linked, or does a singly linked free list suffice? The flag in each slot of the hash table will be $1$ if the element contains a value, and $0$ if it is free. The free list must be doubly linked. Search is unmodified, so it has expected time $O(1)$. To insert an element $x$, first check if $T[h(x.key)]$ is free. If it is, delete $T[h(x.key)]$ and change the flag of $T[h(x.key)]$ to $1$. If it wasn't free to begin with, simply insert $x.key$ at the start of the list stored there. To delete, first check if $x.prev$ and $x.next$ are $\\text{NIL}$. If they are, then the list will be empty upon deletion of $x$, so insert $T[h(x.key)]$ into the free list, update the flag of $T[h(x.key)]$ to $0$, and delete $x$ from the list it's stored in. Since deletion of an element from a singly linked list isn't $O(1)$, we must use a doubly linked list. All other operations are $O(1)$. 11.2-5 Suppose that we are storing a set of $n$ keys into a hash table of size $m$. Show that if the keys are drawn from a universe $U$ with $|U| > nm$, then $U$ has a subset of size $n$ consisting of keys that all hash to the same slot, so that the worst-case searching time for hashing with chaining is $\\Theta(n)$. Suppose the $m - 1$ slots contains at most $n - 1$ elements, then the remaining slot should have $$|U| - (m - 1)(n - 1) > nm - (m - 1)(n - 1) = n + m - 1 \\ge n$$ elements, thus $U$ has a subset of size $n$. 11.2-6 Suppose we have stored $n$ keys in a hash table of size $m$, with collisions resolved by chaining, and that we know the length of each chain, including the length $L$ of the longest chain. Describe a procedure that selects a key uniformly at random from among the keys in the hash table and returns it in expected time $O(L \\cdot (1 + 1 / \\alpha))$. Choose one of the $m$ spots in the hash table at random. Let $n_k$ denote the number of elements stored at $T[k]$. Next pick a number $x$ from $1$ to $L$ uniformly at random. If $x < n_j$, then return the $x$th element on the list. Otherwise, repeat this process. Any element in the hash table will be selected with probability $1 / mL$, so we return any key with equal probability. Let $X$ be the random variable which counts the number of times we must repeat this process before we stop and $p$ be the probability that we return on a given attempt. Then $E[X] = p(1 + \\alpha) + (1 \u2212 p)(1 + E[X])$ since we'd expect to take $1 + \\alpha$ steps to reach an element on the list, and since we know how many elements are on each list, if the element doesn't exist we'll know right away. Then we have $E[X] = \\alpha + 1 / p$. The probability of picking a particular element is $n / mL = \\alpha / L$. Therefore, we have $$ \\begin{aligned} E[X] & = \\alpha + L / \\alpha \\\\ & = L(\\alpha/L + 1/\\alpha) \\\\ & = O(L(1 + 1/\\alpha)) \\end{aligned} $$ since $\\alpha \\le L$.","title":"11.2 Hash tables"},{"location":"Chap11/11.2/#112-1","text":"Suppose we use a hash function $h$ to hash $n$ distinct keys into an array $T$ of length $m$. Assuming simple uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of $\\{\\{k, l\\}: k \\ne l \\text{ and } h(k) = h(l)\\}$? Under the assumption of simple uniform hashing, we will use linearity of expectation to compute this. Suppose that all the keys are totally ordered $\\{k_1, \\dots, k_n\\}$. Let $X_i$ be the number of $\\ell$'s such that $\\ell > k_i$ and $h(\\ell) = h(k_i)$. So $X_i$ is the (expected) number of times that key $k_i$ is collided by those keys hashed afterward. Note, that this is the same thing as $\\sum_{j > i} \\Pr(h(k_j) = h(k_i)) = \\sum_{j > i} 1 / m = (n - i) / m$. Then, by linearity of expectation, the number of collisions is the sum of the number of collisions for each possible smallest element in the collision. The expected number of collisions is $$\\sum_{i = 1}^n \\frac{n - i}{m} = \\frac{n^2 - n(n + 1) / 2}{m} = \\frac{n^2 - n}{2m}.$$","title":"11.2-1"},{"location":"Chap11/11.2/#112-2","text":"Demonstrate what happens when we insert the keys $5, 28, 19, 15, 20, 33, 12, 17, 10$ into a hash table with collisions resolved by chaining. Let the table have $9$ slots, and let the hash function be $h(k) = k \\mod 9$. Let us number our slots $0, 1, \\dots, 8$. Then our resulting hash table will look like following: $$ \\begin{array}{c|l} h(k) & \\text{keys} \\\\ \\hline 0 \\mod 9 & \\\\ 1 \\mod 9 & 10 \\to 19 \\to 28 \\\\ 2 \\mod 9 & 20 \\\\ 3 \\mod 9 & 12 \\\\ 4 \\mod 9 & \\\\ 5 \\mod 9 & 5 \\\\ 6 \\mod 9 & 33 \\to 15 \\\\ 7 \\mod 9 & \\\\ 8 \\mod 9 & 17 \\end{array} $$","title":"11.2-2"},{"location":"Chap11/11.2/#112-3","text":"Professor Marley hypothesizes that he can obtain substantial performance gains by modifying the chaining scheme to keep each list in sorted order. How does the professor's modification affect the running time for successful searches, unsuccessful searches, insertions, and deletions? Successful searches: no difference, $\\Theta(1 + \\alpha)$. Unsuccessful searches: faster but still $\\Theta(1 + \\alpha)$. Insertions: same as successful searches, $\\Theta(1 + \\alpha)$. Deletions: same as before if we use doubly linked lists, $\\Theta(1)$.","title":"11.2-3"},{"location":"Chap11/11.2/#112-4","text":"Suggest how to allocate and deallocate storage for elements within the hash table itself by linking all unused slots into a free list. Assume that one slot can store a flag and either one element plus a pointer or two pointers. All dictionary and free-list operations should run in $O(1)$ expected time. Does the free list need to be doubly linked, or does a singly linked free list suffice? The flag in each slot of the hash table will be $1$ if the element contains a value, and $0$ if it is free. The free list must be doubly linked. Search is unmodified, so it has expected time $O(1)$. To insert an element $x$, first check if $T[h(x.key)]$ is free. If it is, delete $T[h(x.key)]$ and change the flag of $T[h(x.key)]$ to $1$. If it wasn't free to begin with, simply insert $x.key$ at the start of the list stored there. To delete, first check if $x.prev$ and $x.next$ are $\\text{NIL}$. If they are, then the list will be empty upon deletion of $x$, so insert $T[h(x.key)]$ into the free list, update the flag of $T[h(x.key)]$ to $0$, and delete $x$ from the list it's stored in. Since deletion of an element from a singly linked list isn't $O(1)$, we must use a doubly linked list. All other operations are $O(1)$.","title":"11.2-4"},{"location":"Chap11/11.2/#112-5","text":"Suppose that we are storing a set of $n$ keys into a hash table of size $m$. Show that if the keys are drawn from a universe $U$ with $|U| > nm$, then $U$ has a subset of size $n$ consisting of keys that all hash to the same slot, so that the worst-case searching time for hashing with chaining is $\\Theta(n)$. Suppose the $m - 1$ slots contains at most $n - 1$ elements, then the remaining slot should have $$|U| - (m - 1)(n - 1) > nm - (m - 1)(n - 1) = n + m - 1 \\ge n$$ elements, thus $U$ has a subset of size $n$.","title":"11.2-5"},{"location":"Chap11/11.2/#112-6","text":"Suppose we have stored $n$ keys in a hash table of size $m$, with collisions resolved by chaining, and that we know the length of each chain, including the length $L$ of the longest chain. Describe a procedure that selects a key uniformly at random from among the keys in the hash table and returns it in expected time $O(L \\cdot (1 + 1 / \\alpha))$. Choose one of the $m$ spots in the hash table at random. Let $n_k$ denote the number of elements stored at $T[k]$. Next pick a number $x$ from $1$ to $L$ uniformly at random. If $x < n_j$, then return the $x$th element on the list. Otherwise, repeat this process. Any element in the hash table will be selected with probability $1 / mL$, so we return any key with equal probability. Let $X$ be the random variable which counts the number of times we must repeat this process before we stop and $p$ be the probability that we return on a given attempt. Then $E[X] = p(1 + \\alpha) + (1 \u2212 p)(1 + E[X])$ since we'd expect to take $1 + \\alpha$ steps to reach an element on the list, and since we know how many elements are on each list, if the element doesn't exist we'll know right away. Then we have $E[X] = \\alpha + 1 / p$. The probability of picking a particular element is $n / mL = \\alpha / L$. Therefore, we have $$ \\begin{aligned} E[X] & = \\alpha + L / \\alpha \\\\ & = L(\\alpha/L + 1/\\alpha) \\\\ & = O(L(1 + 1/\\alpha)) \\end{aligned} $$ since $\\alpha \\le L$.","title":"11.2-6"},{"location":"Chap11/11.3/","text":"11.3-1 Suppose we wish to search a linked list of length $n$, where each element contains a key $k$ along with a hash value $h(k)$. Each key is a long character string. How might we take advantage of the hash values when searching the list for an element with a given key? If every element also contained a hash of the long character string, when we are searching for the desired element, we'll first check if the hashvalue of the node in the linked list, and move on if it disagrees. This can increase the runtime by a factor proportional to the length of the long character strings. 11.3-2 Suppose that we hash a string of $r$ characters into $m$ slots by treating it as a radix-128 number and then using the division method. We can easily represent the number $m$ as a 32-bit computer word, but the string of $r$ characters, treated as a radix-128 number, takes many words. How can we apply the division method to compute the hash value of the character string without using more than a constant number of words of storage outside the string itself? sum = 0 for i = 1 to r sum = ( sum * 128 + s [ i ]) % m Use sum as the key. 11.3-3 Consider a version of the division method in which $h(k) = k \\mod m$, where $m = 2^p - 1$ and $k$ is a character string interpreted in radix $2^p$. Show that if we can derive string $x$ from string $y$ by permuting its characters, then $x$ and $y$ hash to the same value. Give an example of an application in which this property would be undesirable in a hash function. We will show that each string hashes to the sum of it's digits $\\mod 2^p \u2212 1$. We will do this by induction on the length of the string. Base case Suppose the string is a single character, then the value of that character is the value of $k$ which is then taken $\\mod m$. Inductive step. Let $w = w_1w_2$ where $|w_1| \\ge 1$ and $|w_2| = 1$. Suppose $h(w_1) = k_1$. Then, $h(w) = h(w_1)2^p + h(w_2) \\mod 2^p \u2212 1 = h(w_1) + h(w_2) \\mod 2^p \u2212 1$. So, since $h(w_1)$ was the sum of all but the last digit $\\mod m$, and we are adding the last digit $\\mod m$, we have the desired conclusion. 11.3-4 Consider a hash table of size $m = 1000$ and a corresponding hash function $h(k) = \\lfloor m (kA \\mod 1) \\rfloor$ for $A = (\\sqrt 5 - 1) / 2$. Compute the locations to which the keys $61$, $62$, $63$, $64$, and $65$ are mapped. $h(61) = \\lfloor 1000(61 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 700$. $h(62) = \\lfloor 1000(62 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 318$. $h(63) = \\lfloor 1000(63 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 936$. $h(64) = \\lfloor 1000(64 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 554$. $h(65) = \\lfloor 1000(65 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 172$. 11.3-5 $\\star$ Define a family $\\mathcal H$ of hash functions from a finite set $U$ to a finite set $B$ to be $\\epsilon$-universal if for all pairs of distinct elements $k$ and $l$ in $U$, $$\\Pr\\{h(k) = h(l)\\} \\le \\epsilon,$$ where the probability is over the choice of the hash function $h$ drawn at random from the family $\\mathcal H$. Show that an $\\epsilon$-universal family of hash functions must have $$\\epsilon \\ge \\frac{1}{|B|} - \\frac{1}{|U|}.$$ As a simplifying assumption, assume that $|B|$ divides $|U|$. It's just a bit messier if it doesn't divide evenly. Suppose to a contradiction that $\\epsilon > \\frac{1}{|B|} - \\frac{1}{|U|}$. This means that $\\forall$ pairs $k, \\ell \\in U$, we have that the number $n_{k, \\ell}$ of hash functions in $\\mathcal H$ that have a collision on those two elements satisfies $n_{k, \\ell} \\le \\frac{|\\mathcal H}{|B|} - \\frac{|\\mathcal H}{|U|}$. So, summing over all pairs of elements in $U$, we have that the total number is $\\le \\frac{|\\mathcal H||U|^2}{2|B|} - \\frac{|\\mathcal H||U|}{2}$. Any particular hash function must have that there are at least $|B|\\binom{|U| / |B|}{2} = |B|\\frac{|U|^2 - |U||B|}{2|B|^2} = \\frac{|U|^2}{2|B|} - \\frac{|U|}{2}$ colliding pairs for that hash function, summing over all hash functions, we get that there are at least $|\\mathcal H| \\left(\\frac{|U|^2}{2|B|} - \\frac{|U|}{2}\\right)$ colliding pairs total. Since we have that there are at most some number less than this many, we have a contradiction, and so must have the desired restriction on $\\epsilon$. 11.3-6 $\\star$ Let $U$ be the set of $n$-tuples of values drawn from $\\mathbb Z_p$, and let $B = \\mathbb Z_p$, where $p$ is prime. Define the hash function $h_b: U \\rightarrow B$ for $b \\in \\mathbb Z_p$ on an input $n$-tuple $\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle$ from $U$ as $$h_b(\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle) = \\Bigg(\\sum_{j = 0}^{n - 1} a_jb^j \\Bigg) \\mod p,$$ and let $\\mathcal{H} = \\{h_b : b \\in \\mathbb Z_p\\}$. Argue that $\\mathcal H$ is $((n - 1) / p)$-universal according to the definition of $\\epsilon$-universal in Exercise 11.3-5. ($\\textit{Hint:}$ See Exercise 31.4-4.) Fix $b \\in \\mathbb Z_p$. By exercise 31.4-4, $h_b(x)$ collides with $h_b(y)$ for at most $n - 1$ other $y \\in U$. Since there are a total of $p$ possible values that $h_b$ takes on, the probability that $h_b(x) = h_b(y)$ is bounded from above by $\\frac{n - 1}{p}$, since this holds for any value of $b$, $\\mathcal H$ is $((n - 1 ) /p)$-universal.","title":"11.3 Hash functions"},{"location":"Chap11/11.3/#113-1","text":"Suppose we wish to search a linked list of length $n$, where each element contains a key $k$ along with a hash value $h(k)$. Each key is a long character string. How might we take advantage of the hash values when searching the list for an element with a given key? If every element also contained a hash of the long character string, when we are searching for the desired element, we'll first check if the hashvalue of the node in the linked list, and move on if it disagrees. This can increase the runtime by a factor proportional to the length of the long character strings.","title":"11.3-1"},{"location":"Chap11/11.3/#113-2","text":"Suppose that we hash a string of $r$ characters into $m$ slots by treating it as a radix-128 number and then using the division method. We can easily represent the number $m$ as a 32-bit computer word, but the string of $r$ characters, treated as a radix-128 number, takes many words. How can we apply the division method to compute the hash value of the character string without using more than a constant number of words of storage outside the string itself? sum = 0 for i = 1 to r sum = ( sum * 128 + s [ i ]) % m Use sum as the key.","title":"11.3-2"},{"location":"Chap11/11.3/#113-3","text":"Consider a version of the division method in which $h(k) = k \\mod m$, where $m = 2^p - 1$ and $k$ is a character string interpreted in radix $2^p$. Show that if we can derive string $x$ from string $y$ by permuting its characters, then $x$ and $y$ hash to the same value. Give an example of an application in which this property would be undesirable in a hash function. We will show that each string hashes to the sum of it's digits $\\mod 2^p \u2212 1$. We will do this by induction on the length of the string. Base case Suppose the string is a single character, then the value of that character is the value of $k$ which is then taken $\\mod m$. Inductive step. Let $w = w_1w_2$ where $|w_1| \\ge 1$ and $|w_2| = 1$. Suppose $h(w_1) = k_1$. Then, $h(w) = h(w_1)2^p + h(w_2) \\mod 2^p \u2212 1 = h(w_1) + h(w_2) \\mod 2^p \u2212 1$. So, since $h(w_1)$ was the sum of all but the last digit $\\mod m$, and we are adding the last digit $\\mod m$, we have the desired conclusion.","title":"11.3-3"},{"location":"Chap11/11.3/#113-4","text":"Consider a hash table of size $m = 1000$ and a corresponding hash function $h(k) = \\lfloor m (kA \\mod 1) \\rfloor$ for $A = (\\sqrt 5 - 1) / 2$. Compute the locations to which the keys $61$, $62$, $63$, $64$, and $65$ are mapped. $h(61) = \\lfloor 1000(61 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 700$. $h(62) = \\lfloor 1000(62 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 318$. $h(63) = \\lfloor 1000(63 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 936$. $h(64) = \\lfloor 1000(64 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 554$. $h(65) = \\lfloor 1000(65 \\cdot \\frac{\\sqrt 5 - 1}{2} \\mod 1) \\rfloor = 172$.","title":"11.3-4"},{"location":"Chap11/11.3/#113-5-star","text":"Define a family $\\mathcal H$ of hash functions from a finite set $U$ to a finite set $B$ to be $\\epsilon$-universal if for all pairs of distinct elements $k$ and $l$ in $U$, $$\\Pr\\{h(k) = h(l)\\} \\le \\epsilon,$$ where the probability is over the choice of the hash function $h$ drawn at random from the family $\\mathcal H$. Show that an $\\epsilon$-universal family of hash functions must have $$\\epsilon \\ge \\frac{1}{|B|} - \\frac{1}{|U|}.$$ As a simplifying assumption, assume that $|B|$ divides $|U|$. It's just a bit messier if it doesn't divide evenly. Suppose to a contradiction that $\\epsilon > \\frac{1}{|B|} - \\frac{1}{|U|}$. This means that $\\forall$ pairs $k, \\ell \\in U$, we have that the number $n_{k, \\ell}$ of hash functions in $\\mathcal H$ that have a collision on those two elements satisfies $n_{k, \\ell} \\le \\frac{|\\mathcal H}{|B|} - \\frac{|\\mathcal H}{|U|}$. So, summing over all pairs of elements in $U$, we have that the total number is $\\le \\frac{|\\mathcal H||U|^2}{2|B|} - \\frac{|\\mathcal H||U|}{2}$. Any particular hash function must have that there are at least $|B|\\binom{|U| / |B|}{2} = |B|\\frac{|U|^2 - |U||B|}{2|B|^2} = \\frac{|U|^2}{2|B|} - \\frac{|U|}{2}$ colliding pairs for that hash function, summing over all hash functions, we get that there are at least $|\\mathcal H| \\left(\\frac{|U|^2}{2|B|} - \\frac{|U|}{2}\\right)$ colliding pairs total. Since we have that there are at most some number less than this many, we have a contradiction, and so must have the desired restriction on $\\epsilon$.","title":"11.3-5 $\\star$"},{"location":"Chap11/11.3/#113-6-star","text":"Let $U$ be the set of $n$-tuples of values drawn from $\\mathbb Z_p$, and let $B = \\mathbb Z_p$, where $p$ is prime. Define the hash function $h_b: U \\rightarrow B$ for $b \\in \\mathbb Z_p$ on an input $n$-tuple $\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle$ from $U$ as $$h_b(\\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle) = \\Bigg(\\sum_{j = 0}^{n - 1} a_jb^j \\Bigg) \\mod p,$$ and let $\\mathcal{H} = \\{h_b : b \\in \\mathbb Z_p\\}$. Argue that $\\mathcal H$ is $((n - 1) / p)$-universal according to the definition of $\\epsilon$-universal in Exercise 11.3-5. ($\\textit{Hint:}$ See Exercise 31.4-4.) Fix $b \\in \\mathbb Z_p$. By exercise 31.4-4, $h_b(x)$ collides with $h_b(y)$ for at most $n - 1$ other $y \\in U$. Since there are a total of $p$ possible values that $h_b$ takes on, the probability that $h_b(x) = h_b(y)$ is bounded from above by $\\frac{n - 1}{p}$, since this holds for any value of $b$, $\\mathcal H$ is $((n - 1 ) /p)$-universal.","title":"11.3-6 $\\star$"},{"location":"Chap11/11.4/","text":"11.4-1 Consider inserting the keys $10, 22, 31, 4, 15, 28, 17, 88, 59$ into a hash table of length $m = 11$ using open addressing with the auxiliary hash function $h'(k) = k$. Illustrate the result of inserting these keys using linear probing, using quadratic probing with $c_1 = 1$ and $c_2 = 3$, and using double hashing with $h_1(k) = k$ and $h_2(k) = 1 + (k \\mod (m - 1))$. We use $T_t$ to represent each time stamp $t$ starting with $i = 0$, and if encountering a collision, then we iterate $i$ from $i = 1$ to $i = m - 1 = 10$ until there is no collision. Linear probing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & 88 & 88 \\\\ 2 \\mod 11 & & & & & & & & & \\\\ 3 \\mod 11 & & & & & & & & & \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 8 \\mod 11 & & & & & & & & & 59 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Quadradic probing , it will look identical until there is a collision on inserting the fifth element: $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i + 3i^2) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & 88 & 88 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & & & & & \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & & 59 \\\\ 8 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Note that there is no way to insert the element $59$ now, because the offsets coming from $c_1 = 1$ and $c_2 = 3$ can only be even, and an odd offset would be required to insert $59$ because $59 \\mod 11 = 4$ and all the empty positions are at odd indices. Double hashing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i(1 + k \\mod 10)) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & & 59 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & 88 & 88 \\\\ 8 \\mod 11 & & & & & & & & & \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ 11.4-2 Write pseudocode for $\\text{HASH-DELETE}$ as outlined in the text, and modify $\\text{HASH-INSERT}$ to handle the special value $\\text{DELETED}$. HASH - DELETE ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == k T [ j ] = DELETE return j else i = i + 1 until T [ j ] == NIL or i == m error \"element not exist\" By implementing $\\text{HASH-DELETE}$ in this way, the $\\text{HASH-INSERT}$ need to be modified to treat $\\text{NIL}$ slots as empty ones. HASH - INSERT ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == NIL or T [ j ] == DELETE T [ j ] = k return j else i = i + 1 until i == m error \"hash table overflow\" 11.4-3 Consider an open-address hash table with uniform hashing. Give upper bounds on the expected number of probes in an unsuccessful search and on the expected number of probes in a successful search when the load factor is $3 / 4$ and when it is $7 / 8$. $\\alpha = 3 / 4$, unsuccessful: $\\frac{1}{1 - \\frac{3}{4}} = 4$ probes, successful: $\\frac{1}{\\frac{3}{4}} \\ln\\frac{1}{1-\\frac{3}{4}} \\approx 1.848$ probes. $\\alpha = 7 / 8$, unsuccessful: $\\frac{1}{1 - \\frac{7}{8}} = 8$ probes, successful: $\\frac{1}{\\frac{7}{8}} \\ln\\frac{1}{1 - \\frac{7}{8}} \\approx 2.377$ probes. 11.4-4 $\\star$ Suppose that we use double hashing to resolve collisions\u2014that is, we use the hash function $h(k, i) = (h_1(k) + ih_2(k)) \\mod m$. Show that if $m$ and $h_2(k)$ have greatest common divisor $d \\ge 1$ for some key $k$, then an unsuccessful search for key $k$ examines $(1/d)$th of the hash table before returning to slot $h_1(k)$. Thus, when $d = 1$, so that $m$ and $h_2(k)$ are relatively prime, the search may examine the entire hash table. ($\\textit{Hint:}$ See Chapter 31.) Suppose $d = \\gcd(m, h_2(k))$, the $\\text{LCM}$ $l = m \\cdot h_2(k) / d$. Since $d | h_2(k)$, then $m \\cdot h_2(k) / d \\mod m = 0 \\cdot (h_2(k) / d \\mod m) = 0$, therefore $(l + ih_2(k)) \\mod m = ih_2(k) \\mod m$, which means $ih_2(k) \\mod m$ has a period of $m / d$. 11.4-5 $\\star$ Consider an open-address hash table with a load factor $\\alpha$. Find the nonzero value $\\alpha$ for which the expected number of probes in an unsuccessful search equals twice the expected number of probes in a successful search. Use the upper bounds given by Theorems 11.6 and 11.8 for these expected numbers of probes. $$ \\begin{aligned} \\frac{1}{1 - \\alpha} & = 2 \\cdot \\frac{1}{\\alpha} \\ln\\frac{1}{1 - \\alpha} \\\\ \\alpha & = 0.71533. \\end{aligned} $$","title":"11.4 Open addressing"},{"location":"Chap11/11.4/#114-1","text":"Consider inserting the keys $10, 22, 31, 4, 15, 28, 17, 88, 59$ into a hash table of length $m = 11$ using open addressing with the auxiliary hash function $h'(k) = k$. Illustrate the result of inserting these keys using linear probing, using quadratic probing with $c_1 = 1$ and $c_2 = 3$, and using double hashing with $h_1(k) = k$ and $h_2(k) = 1 + (k \\mod (m - 1))$. We use $T_t$ to represent each time stamp $t$ starting with $i = 0$, and if encountering a collision, then we iterate $i$ from $i = 1$ to $i = m - 1 = 10$ until there is no collision. Linear probing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & 88 & 88 \\\\ 2 \\mod 11 & & & & & & & & & \\\\ 3 \\mod 11 & & & & & & & & & \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 8 \\mod 11 & & & & & & & & & 59 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Quadradic probing , it will look identical until there is a collision on inserting the fifth element: $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i + 3i^2) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & 88 & 88 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & & & & & \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & & 59 \\\\ 8 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$ Note that there is no way to insert the element $59$ now, because the offsets coming from $c_1 = 1$ and $c_2 = 3$ can only be even, and an odd offset would be required to insert $59$ because $59 \\mod 11 = 4$ and all the empty positions are at odd indices. Double hashing : $$ \\begin{array}{r|ccccccccc} h(k, i) = (k + i(1 + k \\mod 10)) \\mod 11 & T_0 & T_1 & T_2 & T_3 & T_4 & T_5 & T_6 & T_7 & T_8 \\\\ \\hline 0 \\mod 11 & & 22 & 22 & 22 & 22 & 22 & 22 & 22 & 22 \\\\ 1 \\mod 11 & & & & & & & & & \\\\ 2 \\mod 11 & & & & & & & & & 59 \\\\ 3 \\mod 11 & & & & & & & 17 & 17 & 17 \\\\ 4 \\mod 11 & & & & 4 & 4 & 4 & 4 & 4 & 4 \\\\ 5 \\mod 11 & & & & & 15 & 15 & 15 & 15 & 15 \\\\ 6 \\mod 11 & & & & & & 28 & 28 & 28 & 28 \\\\ 7 \\mod 11 & & & & & & & & 88 & 88 \\\\ 8 \\mod 11 & & & & & & & & & \\\\ 9 \\mod 11 & & & 31 & 31 & 31 & 31 & 31 & 31 & 31 \\\\ 10 \\mod 11 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 & 10 \\end{array} $$","title":"11.4-1"},{"location":"Chap11/11.4/#114-2","text":"Write pseudocode for $\\text{HASH-DELETE}$ as outlined in the text, and modify $\\text{HASH-INSERT}$ to handle the special value $\\text{DELETED}$. HASH - DELETE ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == k T [ j ] = DELETE return j else i = i + 1 until T [ j ] == NIL or i == m error \"element not exist\" By implementing $\\text{HASH-DELETE}$ in this way, the $\\text{HASH-INSERT}$ need to be modified to treat $\\text{NIL}$ slots as empty ones. HASH - INSERT ( T , k ) i = 0 repeat j = h ( k , i ) if T [ j ] == NIL or T [ j ] == DELETE T [ j ] = k return j else i = i + 1 until i == m error \"hash table overflow\"","title":"11.4-2"},{"location":"Chap11/11.4/#114-3","text":"Consider an open-address hash table with uniform hashing. Give upper bounds on the expected number of probes in an unsuccessful search and on the expected number of probes in a successful search when the load factor is $3 / 4$ and when it is $7 / 8$. $\\alpha = 3 / 4$, unsuccessful: $\\frac{1}{1 - \\frac{3}{4}} = 4$ probes, successful: $\\frac{1}{\\frac{3}{4}} \\ln\\frac{1}{1-\\frac{3}{4}} \\approx 1.848$ probes. $\\alpha = 7 / 8$, unsuccessful: $\\frac{1}{1 - \\frac{7}{8}} = 8$ probes, successful: $\\frac{1}{\\frac{7}{8}} \\ln\\frac{1}{1 - \\frac{7}{8}} \\approx 2.377$ probes.","title":"11.4-3"},{"location":"Chap11/11.4/#114-4-star","text":"Suppose that we use double hashing to resolve collisions\u2014that is, we use the hash function $h(k, i) = (h_1(k) + ih_2(k)) \\mod m$. Show that if $m$ and $h_2(k)$ have greatest common divisor $d \\ge 1$ for some key $k$, then an unsuccessful search for key $k$ examines $(1/d)$th of the hash table before returning to slot $h_1(k)$. Thus, when $d = 1$, so that $m$ and $h_2(k)$ are relatively prime, the search may examine the entire hash table. ($\\textit{Hint:}$ See Chapter 31.) Suppose $d = \\gcd(m, h_2(k))$, the $\\text{LCM}$ $l = m \\cdot h_2(k) / d$. Since $d | h_2(k)$, then $m \\cdot h_2(k) / d \\mod m = 0 \\cdot (h_2(k) / d \\mod m) = 0$, therefore $(l + ih_2(k)) \\mod m = ih_2(k) \\mod m$, which means $ih_2(k) \\mod m$ has a period of $m / d$.","title":"11.4-4 $\\star$"},{"location":"Chap11/11.4/#114-5-star","text":"Consider an open-address hash table with a load factor $\\alpha$. Find the nonzero value $\\alpha$ for which the expected number of probes in an unsuccessful search equals twice the expected number of probes in a successful search. Use the upper bounds given by Theorems 11.6 and 11.8 for these expected numbers of probes. $$ \\begin{aligned} \\frac{1}{1 - \\alpha} & = 2 \\cdot \\frac{1}{\\alpha} \\ln\\frac{1}{1 - \\alpha} \\\\ \\alpha & = 0.71533. \\end{aligned} $$","title":"11.4-5 $\\star$"},{"location":"Chap11/11.5/","text":"11.5-1 $\\star$ Suppose that we insert $n$ keys into a hash table of size $m$ using open addressing and uniform hashing. Let $p(n, m)$ be the probability that no collisions occur. Show that $p(n, m) \\le e^{-n(n - 1) / 2m}$. ($\\textit{Hint:}$ See equation $\\text{(3.12)}$.) Argue that when $n$ exceeds $\\sqrt m$, the probability of avoiding collisions goes rapidly to zero. $$ \\begin{aligned} p(n, m) & = \\frac{m}{m} \\cdot \\frac{m - 1}{m} \\cdots \\frac{m - n + 1}{m} \\\\ & = \\frac{m \\cdot (m - 1) \\cdots (m - n + 1)}{m^n}. \\end{aligned} $$ $$ \\begin{aligned} (m - i) \\cdot (m - n + i) & = (m - \\frac{n}{2} + \\frac{n}{2} - i) \\cdot (m - \\frac{n}{2} - \\frac{n}{2} + i) \\\\ & = (m - \\frac{n}{2})^2 - (i - \\frac{n}{2})^2 \\\\ & \\le (m - \\frac{n}{2})^2. \\end{aligned} $$ $$ \\begin{aligned} p(n, m) & \\le \\frac{m \\cdot (m - \\frac{n}{2})^{n - 1}}{m^n} \\\\ & = (1 - \\frac{n}{2m}) ^ {n - 1}. \\end{aligned} $$ Based on equation $\\text{(3.12)}$, $e^x \\ge 1 + x$, $$ \\begin{aligned} p(n, m) & \\le (e^{-n / 2m})^{n - 1} \\\\ & = e^{-n(n - 1) / 2m}. \\end{aligned} $$","title":"11.5 Perfect hashing"},{"location":"Chap11/11.5/#115-1-star","text":"Suppose that we insert $n$ keys into a hash table of size $m$ using open addressing and uniform hashing. Let $p(n, m)$ be the probability that no collisions occur. Show that $p(n, m) \\le e^{-n(n - 1) / 2m}$. ($\\textit{Hint:}$ See equation $\\text{(3.12)}$.) Argue that when $n$ exceeds $\\sqrt m$, the probability of avoiding collisions goes rapidly to zero. $$ \\begin{aligned} p(n, m) & = \\frac{m}{m} \\cdot \\frac{m - 1}{m} \\cdots \\frac{m - n + 1}{m} \\\\ & = \\frac{m \\cdot (m - 1) \\cdots (m - n + 1)}{m^n}. \\end{aligned} $$ $$ \\begin{aligned} (m - i) \\cdot (m - n + i) & = (m - \\frac{n}{2} + \\frac{n}{2} - i) \\cdot (m - \\frac{n}{2} - \\frac{n}{2} + i) \\\\ & = (m - \\frac{n}{2})^2 - (i - \\frac{n}{2})^2 \\\\ & \\le (m - \\frac{n}{2})^2. \\end{aligned} $$ $$ \\begin{aligned} p(n, m) & \\le \\frac{m \\cdot (m - \\frac{n}{2})^{n - 1}}{m^n} \\\\ & = (1 - \\frac{n}{2m}) ^ {n - 1}. \\end{aligned} $$ Based on equation $\\text{(3.12)}$, $e^x \\ge 1 + x$, $$ \\begin{aligned} p(n, m) & \\le (e^{-n / 2m})^{n - 1} \\\\ & = e^{-n(n - 1) / 2m}. \\end{aligned} $$","title":"11.5-1 $\\star$"},{"location":"Chap11/Problems/11-1/","text":"Suppose that we use an open-addressed hash table of size $m$ to store $n \\le m / 2$ items. a. Assuming uniform hashing, show that for $i = 1, 2, \\ldots, n$, the probability is at most $2^{-k}$ that the $i$th insertion requires strictly more than $k$ probes. b. Show that for $i = 1, 2, \\ldots, n$, the probability is $O(1 / n^2)$ that the $i$th insertion requires more than $2\\lg n$ probes. Let the random variable $X_i$ denote the number of probes required by the $i$th insertion. You have shown in part (b) that $\\Pr\\{X_i > 2\\lg n\\} = O(1 / n^2)$. Let the random variable $X = \\max_{1 \\le i \\le n} X_i$ denote the maximum number of probes required by any of the $n$ insertions. c. Show that $\\Pr\\{X > 2\\lg n\\} = O(1 / n)$. d. Show that the expected length $\\text E[X]$ of the longest probe sequence is $O(\\lg n)$. (Removed)","title":"11-1 Longest-probe bound for hashing"},{"location":"Chap11/Problems/11-2/","text":"Suppose that we have a hash table with $n$ slots, with collisions resolved by chaining, and suppose that $n$ keys are inserted into the table. Each key is equally likely to be hashed to each slot. Let $M$ be the maximum number of keys in any slot after all the keys have been inserted. Your mission is to prove an $O(\\lg n / \\lg\\lg n)$ upper bound on $\\text E[M]$, the expected value of $M$. a. Argue that the probability $Q_k$ that exactly $k$ keys hash to a particular slot is given by $$Q_k = \\bigg(\\frac{1}{n} \\bigg)^k \\bigg(1 - \\frac{1}{n} \\bigg)^{n - k} \\binom{n}{k}.$$ b. Let $P_k$ be the probability that $M = k$, that is, the probability that the slot containing the most keys contains $k$ keys. Show that $P_k \\le n Q_k$. c. Use Stirling's approximation, equation $\\text{(3.18)}$, to show that $Q_k < e^k / k^k$. d. Show that there exists a constant $c > 1$ such that $Q_{k_0} < 1 / n^3$ for $k_0 = c\\lg n / \\lg\\lg n$. Conclude that $P_k < 1 / n^2$ for $k \\ge k_0 = c\\lg n / \\lg\\lg n$. e. Argue that $$\\text E[M] \\le \\Pr\\bigg\\{M > \\frac{c\\lg n}{\\lg\\lg n}\\bigg\\} \\cdot n + \\Pr\\bigg\\{M \\le \\frac{c\\lg n}{\\lg\\lg n}\\bigg\\} \\cdot \\frac{c\\lg n}{\\lg\\lg n}.$$ Conclude that $\\text E[M] = O(\\lg n / \\lg\\lg n)$. (Removed)","title":"11-2 Slot-size bound for chaining"},{"location":"Chap11/Problems/11-3/","text":"Suppose that we are given a key $k$ to search for in a hash table with positions $0, 1, \\ldots, m - 1$, and suppose that we have a hash function $h$ mapping the key space into the set $\\{0, 1, \\ldots, m - 1\\}$. The search scheme is as follows: Compute the value $j = h(k)$, and set $i = 0$. Probe in position $j$ for the desired key $k$. If you find it, or if this position is empty, terminate the search. Set $i = i + 1$. If $i$ now equals $m$, the table is full, so terminate the search. Otherwise, set $j = (i + j) \\mod m$, and return to step 2. Assume that $m$ is a power of $2$. a. Show that this scheme is an instance of the general \"quadratic probing\" scheme by exhibiting the appropriate constants $c_1$ and $c_2$ for equation $\\text{(11.5)}$. b. Prove that this algorithm examines every table position in the worst case. (Removed)","title":"11-3 Quadratic probing"},{"location":"Chap11/Problems/11-4/","text":"Let $\\mathcal H$ be a class of hash functions in which each hash function $h \\in \\mathcal H$ maps the universe $U$ of keys to $\\{0, 1, \\ldots, m - 1 \\}$. We say that $\\mathcal H$ is k-universal if, for every fixed sequence of $k$ distinct keys $\\langle x^{(1)}, x^{(2)}, \\ldots, x^{(k)} \\rangle$ and for any $h$ chosen at random from $\\mathcal H$, the sequence $\\langle h(x^{(1)}), h(x^{(2)}), \\ldots, h(x^{(k)}) \\rangle$ is equally likely to be any of the $m^k$ sequences of length $k$ with elements drawn from $\\{0, 1, \\ldots, m - 1 \\}$. a. Show that if the family $\\mathcal H$ of hash functions is $2$-universal, then it is universal. b. Suppose that the universe $U$ is the set of $n$-tuples of values drawn from $\\mathbb Z_p = \\{0, 1, \\ldots, p - 1\\}$, where $p$ is prime. Consider an element $x = \\langle x_0, x_1, \\ldots, x_{n - 1} \\rangle \\in U$. For any $n$-tuple $a = \\langle a_0, a_1, \\ldots, a_{n - 1} \\rangle \\in U$, define the hash function $h_a$ by $$h_a(x) = \\Bigg(\\sum_{j = 0}^{n - 1} a_j x_j \\Bigg) \\mod p.$$ Let $\\mathcal H = \\{h_a\\}$. Show that $\\mathcal H$ is universal, but not $2$-universal. ($\\textit{Hint:}$ Find a key for which all hash functions in $\\mathcal H$ produce the same value.) c. Suppose that we modify $\\mathcal H$ slightly from part (b): for any $a \\in U$ and for any $b \\in \\mathbb Z_p$, define $$h'_{ab}(x) = \\Bigg(\\sum_{j = 0}^{n - 1} a_j x_j + b \\Bigg) \\mod p$$ and $\\mathcal h' = \\{h'_{ab}\\}$. Argue that $\\mathcal H'$ is $2$-universal. ($\\textit{Hint:}$ Consider fixed $n$-tuples $x \\in U$ and $y \\in U$, with $x_i \\ne y_i$ for some $i$. What happens to $h'_{ab}(x)$ and $h'_{ab}(y)$ as $a_i$ and $b$ range over $\\mathbb Z_p$?) d. Suppose that Alice and Bob secretly agree on a hash function $h$ form $2$-universal family $\\mathcal H$ of hash functions. Each $h \\in \\mathcal H$ maps from a universe of keys $u$ to $\\mathbb Z_p$, where $p$ is aprime. Later, Alice sends a message $m$ to Bob over the Internet, where $m \\in U$. She authenticates this message to Bob by also sending an authentication tag $t = h(m)$, and Bob checks that the pair $(m, t)$ he receives indeed satisfies $t = h(m)$. Suppose that an adversary intercepts $(m, t)$ en route and tries to fool Bob by replacing the pair $(m, t)$ with a different pair $(m', t')$. Argue that the probability that the adversary succeeds in fooling Bob into accepting $(m', t')$ is at most $1 / p$, no matter how much computing power the adversary has, and even if the adversary knows the family $\\mathcal H$ of hash functions used. a. The number of hash functions for which $h(k) = h(l)$ is $\\frac{m}{m^2}|\\mathcal H| = \\frac{1}{m}|\\mathcal H|$, therefore the family is universal. b. For $x = \\langle 0, 0, \\ldots, 0 \\rangle$, $\\mathcal H$ could not be $2$-universal. c. Let $x, y \\in U$ be fixed, distinct $n$-tuples. As $a_i$ and $b$ range over $\\mathbb Z_p, h'_{ab}(x)$ is equally likely to achieve every value from $1$ to $p$ since for any sequence $a$, we can let $b$ vary from $1$ to $p - 1$. Thus, $\\langle h'_{ab}(x), h'_{ab}(y) \\rangle$ is equally likely to be any of the $p^2$ sequences, so $\\mathcal H$ is $2$-universal. d. Since $\\mathcal H$ is $2$-universal, every pair of $\\langle t, t' \\rangle$ is equally likely to appear, thus $t'$ could be any value from $\\mathbb Z_p$. Even the adversary knows $\\mathcal H$, since $\\mathcal H$ is $2$-universal, then $\\mathcal H$ is universal, the probability of choosing a hash function that $h(k) = h(l)$ is at most $1 / p$, therefore the probability is at most $1 / p$.","title":"11-4 Hashing and authentication"},{"location":"Chap12/12.1/","text":"12.1-1 For the set of $\\{ 1, 4, 5, 10, 16, 17, 21 \\}$ of keys, draw binary search trees of heights $2$, $3$, $4$, $5$, and $6$. $height = 2$: $height = 3$: $height = 4$: $height = 5$: $height = 6$: 12.1-2 What is the difference between the binary-search-tree property and the min-heap property (see page 153)? Can the min-heap property be used to print out the keys of an $n$-node tree in sorted order in $O(n)$ time? Show how, or explain why not. The binary-search-tree property guarantees that all nodes in the left subtree are smaller, and all nodes in the right subtree are larger. The min-heap property only guarantees the general child-larger-than-parent relation, but doesn't distinguish between left and right children. For this reason, the min-heap property can't be used to print out the keys in sorted order in linear time because we have no way of knowing which subtree contains the next smallest element. 12.1-3 Give a nonrecursive algorithm that performs an inorder tree walk. ($\\textit{Hint:}$ An easy solution uses a stack as an auxiliary data structure. A more complicated, but elegant, solution uses no stack but assumes that we can test two pointers for equality.) INORDER - TREE - WALK ( T ) let S be an empty stack current = T . root done = 0 while ! done if current != NIL PUSH ( S , current ) current = current . left else if ! S . EMPTY () current = POP ( S ) print current current = current . right else done = 1 12.1-4 Give recursive algorithms that perform preorder and postorder tree walks in $\\Theta(n)$ time on a tree of $n$ nodes. PREORDER - TREE - WALK ( x ) if x != NIL print x . key PREORDER - TREE - WALK ( x . left ) PREORDER - TREE - WALK ( x . right ) POSTORDER - TREE - WALK ( x ) if x != NIL POSTORDER - TREE - WALK ( x . left ) POSTORDER - TREE - WALK ( x . right ) print x . key 12.1-5 Argue that since sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case in the comparison model, any comparison-based algorithm for constructing a binary search tree from an arbitrary list of $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case. Assume, for the sake of contradiction, that we can construct the binary search tree by comparison-based algorithm using less than $\\Omega(n\\lg n)$ time, since the inorder tree walk is $\\Theta(n)$, then we can get the sorted elements in less than $\\Omega(n\\lg n)$ time, which contradicts the fact that sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case.","title":"12.1 What is a binary search tree?"},{"location":"Chap12/12.1/#121-1","text":"For the set of $\\{ 1, 4, 5, 10, 16, 17, 21 \\}$ of keys, draw binary search trees of heights $2$, $3$, $4$, $5$, and $6$. $height = 2$: $height = 3$: $height = 4$: $height = 5$: $height = 6$:","title":"12.1-1"},{"location":"Chap12/12.1/#121-2","text":"What is the difference between the binary-search-tree property and the min-heap property (see page 153)? Can the min-heap property be used to print out the keys of an $n$-node tree in sorted order in $O(n)$ time? Show how, or explain why not. The binary-search-tree property guarantees that all nodes in the left subtree are smaller, and all nodes in the right subtree are larger. The min-heap property only guarantees the general child-larger-than-parent relation, but doesn't distinguish between left and right children. For this reason, the min-heap property can't be used to print out the keys in sorted order in linear time because we have no way of knowing which subtree contains the next smallest element.","title":"12.1-2"},{"location":"Chap12/12.1/#121-3","text":"Give a nonrecursive algorithm that performs an inorder tree walk. ($\\textit{Hint:}$ An easy solution uses a stack as an auxiliary data structure. A more complicated, but elegant, solution uses no stack but assumes that we can test two pointers for equality.) INORDER - TREE - WALK ( T ) let S be an empty stack current = T . root done = 0 while ! done if current != NIL PUSH ( S , current ) current = current . left else if ! S . EMPTY () current = POP ( S ) print current current = current . right else done = 1","title":"12.1-3"},{"location":"Chap12/12.1/#121-4","text":"Give recursive algorithms that perform preorder and postorder tree walks in $\\Theta(n)$ time on a tree of $n$ nodes. PREORDER - TREE - WALK ( x ) if x != NIL print x . key PREORDER - TREE - WALK ( x . left ) PREORDER - TREE - WALK ( x . right ) POSTORDER - TREE - WALK ( x ) if x != NIL POSTORDER - TREE - WALK ( x . left ) POSTORDER - TREE - WALK ( x . right ) print x . key","title":"12.1-4"},{"location":"Chap12/12.1/#121-5","text":"Argue that since sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case in the comparison model, any comparison-based algorithm for constructing a binary search tree from an arbitrary list of $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case. Assume, for the sake of contradiction, that we can construct the binary search tree by comparison-based algorithm using less than $\\Omega(n\\lg n)$ time, since the inorder tree walk is $\\Theta(n)$, then we can get the sorted elements in less than $\\Omega(n\\lg n)$ time, which contradicts the fact that sorting $n$ elements takes $\\Omega(n\\lg n)$ time in the worst case.","title":"12.1-5"},{"location":"Chap12/12.2/","text":"12.2-1 Suppose that we have numbers between $1$ and $1000$ in a binary search tree, and we want to search for the number $363$. Which of the following sequences could not be the sequence of nodes examined? a. $2, 252, 401, 398, 330, 344, 397, 363$. b. $924, 220, 911, 244, 898, 258, 362, 363$. c. $925, 202, 911, 240, 912, 245, 363$. d. $2, 399, 387, 219, 266, 382, 381, 278, 363$. e. $935, 278, 347, 621, 299, 392, 358, 363$. c. could not be the sequence of nodes explored because we take the left child from the $911$ node, and yet somehow manage to get to the $912$ node which cannot belong the left subtree of $911$ because it is greater. e. is also impossible because we take the right subtree on the $347$ node and yet later come across the $299$ node. 12.2-2 Write recursive versions of $\\text{TREE-MINIMUM}$ and $\\text{TREE-MAXIMUM}$. TREE - MINIMUM ( x ) if x . left != NIL return TREE - MINIMUM ( x . left ) else return x TREE - MAXIMUM ( x ) if x . right != NIL return TREE - MAXIMUM ( x . right ) else return x 12.2-3 Write the $\\text{TREE-PREDECESSOR}$ procedure. TREE - PREDECESSOR ( x ) if x . left != NIL return TREE - MAXIMUM ( x . left ) y = x . p while y != NIL and x == y . left x = y y = y . p return y 12.2-4 Professor Bunyan thinks he has discovered a remarkable property of binary search trees. Suppose that the search for key $k$ in a binary search tree ends up in a leaf. Consider three sets: $A$, the keys to the left of the search path; $B$, the keys on the search path; and $C$, the keys to the right of the search path. Professor Bunyan claims that any three keys $a \\in A$, $b \\in B$, and $c \\in C$ must satisfy $a \\le b \\le c$. Give a smallest possible counterexample to the professor's claim. Search for $9$ in this tree. Then $A = \\{7\\}$, $B = \\{5, 8, 9\\}$ and $C = \\{\\}$. So, since $7 > 5$ it breaks professor's claim. 12.2-5 Show that if a node in a binary search tree has two children, then its successor has no left child and its predecessor has no right child. Suppose the node $x$ has two children. Then it's successor is the minimum element of the BST rooted at $x.right$. If it had a left child then it wouldn't be the minimum element. So, it must not have a left child. Similarly, the predecessor must be the maximum element of the left subtree, so cannot have a right child. 12.2-6 Consider a binary search tree $T$ whose keys are distinct. Show that if the right subtree of a node $x$ in $T$ is empty and $x$ has a successor $y$, then $y$ is the lowest ancestor of $x$ whose left child is also an ancestor of $x$. (Recall that every node is its own ancestor.) First we establish that $y$ must be an ancestor of $x$. If $y$ weren't an ancestor of $x$, then let $z$ denote the first common ancestor of $x$ and $y$. By the binary-search-tree property, $x < z < y$, so $y$ cannot be the successor of $x$. Next observe that $y.left$ must be an ancestor of $x$ because if it weren't, then $y.right$ would be an ancestor of $x$, implying that $x > y$. Finally, suppose that $y$ is not the lowest ancestor of $x$ whose left child is also an ancestor of $x$. Let $z$ denote this lowest ancestor. Then $z$ must be in the left subtree of $y$, which implies $z < y$, contradicting the fact that $y$ is the successor if $x$. 12.2-7 An alternative method of performing an inorder tree walk of an $n$-node binary search tree finds the minimum element in the tree by calling $\\text{TREE-MINIMUM}$ and then making $n - 1$ calls to $\\text{TREE-SUCCESSOR}$. Prove that this algorithm runs in $\\Theta(n)$ time. To show this bound on the runtime, we will show that using this procedure, we traverse each edge twice. This will suffice because the number of edges in a tree is one less than the number of vertices. Consider a vertex of a BST, say $x$. Then, we have that the edge between $x.p$ and $x$ gets used when successor is called on $x.p$ and gets used again when it is called on the largest element in the subtree rooted at $x$. Since these are the only two times that that edge can be used, apart from the initial finding of tree minimum. We have that the runtime is $O(n)$. We trivially get the runtime is $\\Omega(n)$ because that is the size of the output. 12.2-8 Prove that no matter what node we start at in a height-$h$ binary search tree, $k$ successive calls to $\\text{TREE-SUCCESSOR}$ take $O(k + h)$ time. Suppose $x$ is the starting node and $y$ is the ending node. The distance between $x$ and $y$ is at most $2h$, and all the edges connecting the $k$ nodes are visited twice, therefore it takes $O(k + h)$ time. 12.2-9 Let $T$ be a binary search tree whose keys are distinct, let $x$ be a leaf node, and let $y$ be its parent. Show that $y.key$ is either the smallest key in $T$ larger than $x.key$ or the largest key in $T$ smaller than $x.key$. If $x = y.left$, then calling successor on $x$ will result in no iterations of the while loop, and so will return $y$. If $x = y.right$, the while loop for calling predecessor (see exercise 3) will be run no times, and so $y$ will be returned.","title":"12.2 Querying a binary search tree"},{"location":"Chap12/12.2/#122-1","text":"Suppose that we have numbers between $1$ and $1000$ in a binary search tree, and we want to search for the number $363$. Which of the following sequences could not be the sequence of nodes examined? a. $2, 252, 401, 398, 330, 344, 397, 363$. b. $924, 220, 911, 244, 898, 258, 362, 363$. c. $925, 202, 911, 240, 912, 245, 363$. d. $2, 399, 387, 219, 266, 382, 381, 278, 363$. e. $935, 278, 347, 621, 299, 392, 358, 363$. c. could not be the sequence of nodes explored because we take the left child from the $911$ node, and yet somehow manage to get to the $912$ node which cannot belong the left subtree of $911$ because it is greater. e. is also impossible because we take the right subtree on the $347$ node and yet later come across the $299$ node.","title":"12.2-1"},{"location":"Chap12/12.2/#122-2","text":"Write recursive versions of $\\text{TREE-MINIMUM}$ and $\\text{TREE-MAXIMUM}$. TREE - MINIMUM ( x ) if x . left != NIL return TREE - MINIMUM ( x . left ) else return x TREE - MAXIMUM ( x ) if x . right != NIL return TREE - MAXIMUM ( x . right ) else return x","title":"12.2-2"},{"location":"Chap12/12.2/#122-3","text":"Write the $\\text{TREE-PREDECESSOR}$ procedure. TREE - PREDECESSOR ( x ) if x . left != NIL return TREE - MAXIMUM ( x . left ) y = x . p while y != NIL and x == y . left x = y y = y . p return y","title":"12.2-3"},{"location":"Chap12/12.2/#122-4","text":"Professor Bunyan thinks he has discovered a remarkable property of binary search trees. Suppose that the search for key $k$ in a binary search tree ends up in a leaf. Consider three sets: $A$, the keys to the left of the search path; $B$, the keys on the search path; and $C$, the keys to the right of the search path. Professor Bunyan claims that any three keys $a \\in A$, $b \\in B$, and $c \\in C$ must satisfy $a \\le b \\le c$. Give a smallest possible counterexample to the professor's claim. Search for $9$ in this tree. Then $A = \\{7\\}$, $B = \\{5, 8, 9\\}$ and $C = \\{\\}$. So, since $7 > 5$ it breaks professor's claim.","title":"12.2-4"},{"location":"Chap12/12.2/#122-5","text":"Show that if a node in a binary search tree has two children, then its successor has no left child and its predecessor has no right child. Suppose the node $x$ has two children. Then it's successor is the minimum element of the BST rooted at $x.right$. If it had a left child then it wouldn't be the minimum element. So, it must not have a left child. Similarly, the predecessor must be the maximum element of the left subtree, so cannot have a right child.","title":"12.2-5"},{"location":"Chap12/12.2/#122-6","text":"Consider a binary search tree $T$ whose keys are distinct. Show that if the right subtree of a node $x$ in $T$ is empty and $x$ has a successor $y$, then $y$ is the lowest ancestor of $x$ whose left child is also an ancestor of $x$. (Recall that every node is its own ancestor.) First we establish that $y$ must be an ancestor of $x$. If $y$ weren't an ancestor of $x$, then let $z$ denote the first common ancestor of $x$ and $y$. By the binary-search-tree property, $x < z < y$, so $y$ cannot be the successor of $x$. Next observe that $y.left$ must be an ancestor of $x$ because if it weren't, then $y.right$ would be an ancestor of $x$, implying that $x > y$. Finally, suppose that $y$ is not the lowest ancestor of $x$ whose left child is also an ancestor of $x$. Let $z$ denote this lowest ancestor. Then $z$ must be in the left subtree of $y$, which implies $z < y$, contradicting the fact that $y$ is the successor if $x$.","title":"12.2-6"},{"location":"Chap12/12.2/#122-7","text":"An alternative method of performing an inorder tree walk of an $n$-node binary search tree finds the minimum element in the tree by calling $\\text{TREE-MINIMUM}$ and then making $n - 1$ calls to $\\text{TREE-SUCCESSOR}$. Prove that this algorithm runs in $\\Theta(n)$ time. To show this bound on the runtime, we will show that using this procedure, we traverse each edge twice. This will suffice because the number of edges in a tree is one less than the number of vertices. Consider a vertex of a BST, say $x$. Then, we have that the edge between $x.p$ and $x$ gets used when successor is called on $x.p$ and gets used again when it is called on the largest element in the subtree rooted at $x$. Since these are the only two times that that edge can be used, apart from the initial finding of tree minimum. We have that the runtime is $O(n)$. We trivially get the runtime is $\\Omega(n)$ because that is the size of the output.","title":"12.2-7"},{"location":"Chap12/12.2/#122-8","text":"Prove that no matter what node we start at in a height-$h$ binary search tree, $k$ successive calls to $\\text{TREE-SUCCESSOR}$ take $O(k + h)$ time. Suppose $x$ is the starting node and $y$ is the ending node. The distance between $x$ and $y$ is at most $2h$, and all the edges connecting the $k$ nodes are visited twice, therefore it takes $O(k + h)$ time.","title":"12.2-8"},{"location":"Chap12/12.2/#122-9","text":"Let $T$ be a binary search tree whose keys are distinct, let $x$ be a leaf node, and let $y$ be its parent. Show that $y.key$ is either the smallest key in $T$ larger than $x.key$ or the largest key in $T$ smaller than $x.key$. If $x = y.left$, then calling successor on $x$ will result in no iterations of the while loop, and so will return $y$. If $x = y.right$, the while loop for calling predecessor (see exercise 3) will be run no times, and so $y$ will be returned.","title":"12.2-9"},{"location":"Chap12/12.3/","text":"12.3-1 Give a recursive version of the $\\text{TREE-INSERT}$ procedure. RECURSIVE - TREE - INSERT ( T , z ) if T . root == NIL T . root = z else INSERT ( NIL , T . root , z ) INSERT ( p , x , z ) if x == NIL z . p = p if z . key < p . key p . left = z else p . right = z else if z . key < x . key INSERT ( x , x . left , z ) else INSERT ( x , x . right , z ) 12.3-2 Suppose that we construct a binary search tree by repeatedly inserting distinct values into the tree. Argue that the number of nodes examined in searching for a value in the tree is one plus the number of nodes examined when the value was first inserted into the tree. Number of nodes examined while searching also includes the node which is searched for, which isn't the case when we inserted it. 12.3-3 We can sort a given set of $n$ numbers by first building a binary search tree containing these numbers (using $\\text{TREE-INSERT}$ repeatedly to insert the numbers one by one) and then printing the numbers by an inorder tree walk. What are the worst-case and best-case running times for this sorting algorithm? The worst-case is that the tree formed has height $n$ because we were inserting them in already sorted order. This will result in a runtime of $\\Theta(n^2)$. The best-case is that the tree formed is approximately balanced. This will mean that the height doesn't exceed $O(\\lg n)$. Note that it can't have a smaller height, because a complete binary tree of height $h$ only has $\\Theta(2^h)$ elements. This will result in a rutime of $O(n\\lg n)$. We showed $\\Omega(n\\lg n)$ in exercise 12.1-5 . 12.3-4 Is the operation of deletion \"commutative\" in the sense that deleting $x$ and then $y$ from a binary search tree leaves the same tree as deleting $y$ and then $x$? Argue why it is or give a counterexample. No, giving the following courterexample. Delete $A$ first, then delete $B$: A C C / \\ / \\ \\ B D B D D / C Delete $B$ first, then delete $A$: A A D / \\ \\ / B D D C / / C C 12.3-5 Suppose that instead of each node $x$ keeping the attribute $x.p$, pointing to $x$'s parent, it keeps $x.succ$, pointing to $x$'s successor. Give pseudocode for $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ on a binary search tree $T$ using this representation. These procedures should operate in time $O(h)$, where $h$ is the height of the tree $T$. ($\\textit{Hint:}$ You may wish to implement a subroutine that returns the parent of a node.) We don't need to change $\\text{SEARCH}$. We have to implement $\\text{PARENT}$, which facilitates us a lot. PARENT ( T , x ) if x == T . root return NIL y = TREE - MAXIMUM ( x ). succ if y == NIL y = T . root else if y . left == x return y y = y . left while y . right != x y = y . right return y INSERT ( T , z ) y = NIL x = T . root pred = NIL while x != NIL y = x if z . key < x . key x = x . left else pred = x x = x . right if y == NIL T . root = z z . succ = NIL else if z . key < y . key y . left = z z . succ = y if pred != NIL pred . succ = z else y . right = z z . succ = y . succ y . succ = z We modify $\\text{TRANSPLANT}$ a bit since we no longer have to keep the pointer of $p$. TRANSPLANT ( T , u , v ) p = PARENT ( T , u ) if p == NIL T . root = v else if u == p . left p . left = v else p . right = v Also, we have to implement $\\text{TREE-PREDECESSOR}$, which helps us easily find the predecessor in line 2 of $\\text{DELETE}$. TREE - PREDECESSOR ( T , x ) if x . left != NIL return TREE - MAXIMUM ( x . left ) y = T . root pred = NIL while y != NIL if y . key == x . key break if y . key < x . key pred = y y = y . right else y = y . left return pred DELETE ( T , z ) pred = TREE - PREDECESSOR ( T , z ) pred . succ = z . succ if z . left == NIL TRANSPLANT ( T , z , z . right ) else if z . right == NIL TRANSPLANT ( T , z , z . left ) else y = TREE - MIMIMUM ( z . right ) if PARENT ( T , y ) != z TRANSPLANT ( T , y , y . right ) y . right = z . right TRANSPLANT ( T , z , y ) y . left = z . left Therefore, all these five algorithms are still $O(h)$ despite the increase in the hidden constant factor. 12.3-6 When node $z$ in $\\text{TREE-DELETE}$ has two children, we could choose node $y$ as its predecessor rather than its successor. What other changes to $\\text{TREE-DELETE}$ would be necessary if we did so? Some have argued that a fair strategy, giving equal priority to predecessor and successor, yields better empirical performance. How might $\\text{TREE-DELETE}$ be changed to implement such a fair strategy? Update line 5 so that $y$ is set equal to $\\text{TREE-MAXIMUM}(z.left)$ and lines 6-12 so that every $y.left$ and $z.left$ is replaced with $y.right$ and $z.right$ and vice versa. To implement the fair strategy, we could randomly decide each time $\\text{TREE-DELETE}$ is called whether or not to use the predecessor or successor.","title":"12.3 Insertion and deletion"},{"location":"Chap12/12.3/#123-1","text":"Give a recursive version of the $\\text{TREE-INSERT}$ procedure. RECURSIVE - TREE - INSERT ( T , z ) if T . root == NIL T . root = z else INSERT ( NIL , T . root , z ) INSERT ( p , x , z ) if x == NIL z . p = p if z . key < p . key p . left = z else p . right = z else if z . key < x . key INSERT ( x , x . left , z ) else INSERT ( x , x . right , z )","title":"12.3-1"},{"location":"Chap12/12.3/#123-2","text":"Suppose that we construct a binary search tree by repeatedly inserting distinct values into the tree. Argue that the number of nodes examined in searching for a value in the tree is one plus the number of nodes examined when the value was first inserted into the tree. Number of nodes examined while searching also includes the node which is searched for, which isn't the case when we inserted it.","title":"12.3-2"},{"location":"Chap12/12.3/#123-3","text":"We can sort a given set of $n$ numbers by first building a binary search tree containing these numbers (using $\\text{TREE-INSERT}$ repeatedly to insert the numbers one by one) and then printing the numbers by an inorder tree walk. What are the worst-case and best-case running times for this sorting algorithm? The worst-case is that the tree formed has height $n$ because we were inserting them in already sorted order. This will result in a runtime of $\\Theta(n^2)$. The best-case is that the tree formed is approximately balanced. This will mean that the height doesn't exceed $O(\\lg n)$. Note that it can't have a smaller height, because a complete binary tree of height $h$ only has $\\Theta(2^h)$ elements. This will result in a rutime of $O(n\\lg n)$. We showed $\\Omega(n\\lg n)$ in exercise 12.1-5 .","title":"12.3-3"},{"location":"Chap12/12.3/#123-4","text":"Is the operation of deletion \"commutative\" in the sense that deleting $x$ and then $y$ from a binary search tree leaves the same tree as deleting $y$ and then $x$? Argue why it is or give a counterexample. No, giving the following courterexample. Delete $A$ first, then delete $B$: A C C / \\ / \\ \\ B D B D D / C Delete $B$ first, then delete $A$: A A D / \\ \\ / B D D C / / C C","title":"12.3-4"},{"location":"Chap12/12.3/#123-5","text":"Suppose that instead of each node $x$ keeping the attribute $x.p$, pointing to $x$'s parent, it keeps $x.succ$, pointing to $x$'s successor. Give pseudocode for $\\text{SEARCH}$, $\\text{INSERT}$, and $\\text{DELETE}$ on a binary search tree $T$ using this representation. These procedures should operate in time $O(h)$, where $h$ is the height of the tree $T$. ($\\textit{Hint:}$ You may wish to implement a subroutine that returns the parent of a node.) We don't need to change $\\text{SEARCH}$. We have to implement $\\text{PARENT}$, which facilitates us a lot. PARENT ( T , x ) if x == T . root return NIL y = TREE - MAXIMUM ( x ). succ if y == NIL y = T . root else if y . left == x return y y = y . left while y . right != x y = y . right return y INSERT ( T , z ) y = NIL x = T . root pred = NIL while x != NIL y = x if z . key < x . key x = x . left else pred = x x = x . right if y == NIL T . root = z z . succ = NIL else if z . key < y . key y . left = z z . succ = y if pred != NIL pred . succ = z else y . right = z z . succ = y . succ y . succ = z We modify $\\text{TRANSPLANT}$ a bit since we no longer have to keep the pointer of $p$. TRANSPLANT ( T , u , v ) p = PARENT ( T , u ) if p == NIL T . root = v else if u == p . left p . left = v else p . right = v Also, we have to implement $\\text{TREE-PREDECESSOR}$, which helps us easily find the predecessor in line 2 of $\\text{DELETE}$. TREE - PREDECESSOR ( T , x ) if x . left != NIL return TREE - MAXIMUM ( x . left ) y = T . root pred = NIL while y != NIL if y . key == x . key break if y . key < x . key pred = y y = y . right else y = y . left return pred DELETE ( T , z ) pred = TREE - PREDECESSOR ( T , z ) pred . succ = z . succ if z . left == NIL TRANSPLANT ( T , z , z . right ) else if z . right == NIL TRANSPLANT ( T , z , z . left ) else y = TREE - MIMIMUM ( z . right ) if PARENT ( T , y ) != z TRANSPLANT ( T , y , y . right ) y . right = z . right TRANSPLANT ( T , z , y ) y . left = z . left Therefore, all these five algorithms are still $O(h)$ despite the increase in the hidden constant factor.","title":"12.3-5"},{"location":"Chap12/12.3/#123-6","text":"When node $z$ in $\\text{TREE-DELETE}$ has two children, we could choose node $y$ as its predecessor rather than its successor. What other changes to $\\text{TREE-DELETE}$ would be necessary if we did so? Some have argued that a fair strategy, giving equal priority to predecessor and successor, yields better empirical performance. How might $\\text{TREE-DELETE}$ be changed to implement such a fair strategy? Update line 5 so that $y$ is set equal to $\\text{TREE-MAXIMUM}(z.left)$ and lines 6-12 so that every $y.left$ and $z.left$ is replaced with $y.right$ and $z.right$ and vice versa. To implement the fair strategy, we could randomly decide each time $\\text{TREE-DELETE}$ is called whether or not to use the predecessor or successor.","title":"12.3-6"},{"location":"Chap12/12.4/","text":"12.4-1 Prove equation $\\text{(12.3)}$. Consider all the possible positions of the largest element of the subset of $n + 3$ of size $4$. Suppose it were in position $i + 4$ for some $i \\le n \u2212 1$. Then, we have that there are $i + 3$ positions from which we can select the remaining three elements of the subset. Since every subset with different largest element is different, we get the total by just adding them all up (inclusion exclusion principle). 12.4-2 Describe a binary search tree on n nodes such that the average depth of a node in the tree is $\\Theta(\\lg n)$ but the height of the tree is $\\omega(\\lg n)$. Give an asymptotic upper bound on the height of an $n$-node binary search tree in which the average depth of a node is $\\Theta(\\lg n)$. To keep the average depth low but maximize height, the desired tree will be a complete binary search tree, but with a chain of length $c(n)$ hanging down from one of the leaf nodes. Let $k = \\lg(n \u2212 c(n))$ be the height of the complete binary search tree. Then the average height is approximately given by $$\\frac{1}{n} \\left[\\sum_{i = 1}^{n - c(n)} \\lg i + (k + 1) + (k + 2) + \\cdots + (k + c(n))\\right] \\approx \\lg(n - c(n)) + \\frac{c(n)^2}{2n}.$$ The upper bound is given by the largest $c(n)$ such that $\\lg(n \u2212 c(n))+ \\frac{c(n)^2}{2n} = \\Theta(\\lg n)$ and $c(n) = \\omega(\\lg n)$. One function which works is $\\sqrt n$. 12.4-3 Show that the notion of a randomly chosen binary search tree on $n$ keys, where each binary search tree of $n$ keys is equally likely to be chosen, is different from the notion of a randomly built binary search tree given in this section. ($\\textit{Hint:}$ List the possibilities when $n = 3$.) Suppose we have the elements $\\{1, 2, 3\\}$. Then, if we construct a tree by a random ordering, then, we get trees which appear with probabilities some multiple of $\\frac{1}{6}$. However, if we consider all the valid binary search trees on the key set of $\\{1, 2, 3\\}$. Then, we will have only five different possibilities. So, each will occur with probability $\\frac{1}{5}$, which is a different probability distribution. 12.4-4 Show that the function $f(x) = 2^x$ is convex. The second derivative is $2^x\\ln^2 2$ which is always positive, so the function is convex 12.4-5 $\\star$ Consider $\\text{RANDOMIZED-QUICKSORT}$ operating on a sequence of $n$ distinct input numbers. Prove that for any constant $k > 0$, all but $O(1 / n^k)$ of the $n!$ input permutations yield an $O(n\\lg n)$ running time. Let $A(n)$ denote the probability that when quicksorting a list of length $n$, some pivot is selected to not be in the middle $n^{1 - k / 2}$ of the numberes. This doesn't happen with probability $\\frac{1}{n^{k / 2}}$. Then, we have that the two subproblems are of size $n_1, n_2$ with $n_1 + n_2 = n - 1$, then $$A(n) \\le \\frac{1}{n^{k / 2}} + T(n_1)+T(n_2).$$ Since we bounded the depth by $O(1 / \\lg n)$ let $\\{a_{i, j}\\}_i$ be all the subproblem sizes left at depth $j$, $$A(n) \\le \\frac{1}{n^{k / 2}} \\sum_j\\sum_i \\frac{1}{a}.$$","title":"12.4 Randomly built binary search trees"},{"location":"Chap12/12.4/#124-1","text":"Prove equation $\\text{(12.3)}$. Consider all the possible positions of the largest element of the subset of $n + 3$ of size $4$. Suppose it were in position $i + 4$ for some $i \\le n \u2212 1$. Then, we have that there are $i + 3$ positions from which we can select the remaining three elements of the subset. Since every subset with different largest element is different, we get the total by just adding them all up (inclusion exclusion principle).","title":"12.4-1"},{"location":"Chap12/12.4/#124-2","text":"Describe a binary search tree on n nodes such that the average depth of a node in the tree is $\\Theta(\\lg n)$ but the height of the tree is $\\omega(\\lg n)$. Give an asymptotic upper bound on the height of an $n$-node binary search tree in which the average depth of a node is $\\Theta(\\lg n)$. To keep the average depth low but maximize height, the desired tree will be a complete binary search tree, but with a chain of length $c(n)$ hanging down from one of the leaf nodes. Let $k = \\lg(n \u2212 c(n))$ be the height of the complete binary search tree. Then the average height is approximately given by $$\\frac{1}{n} \\left[\\sum_{i = 1}^{n - c(n)} \\lg i + (k + 1) + (k + 2) + \\cdots + (k + c(n))\\right] \\approx \\lg(n - c(n)) + \\frac{c(n)^2}{2n}.$$ The upper bound is given by the largest $c(n)$ such that $\\lg(n \u2212 c(n))+ \\frac{c(n)^2}{2n} = \\Theta(\\lg n)$ and $c(n) = \\omega(\\lg n)$. One function which works is $\\sqrt n$.","title":"12.4-2"},{"location":"Chap12/12.4/#124-3","text":"Show that the notion of a randomly chosen binary search tree on $n$ keys, where each binary search tree of $n$ keys is equally likely to be chosen, is different from the notion of a randomly built binary search tree given in this section. ($\\textit{Hint:}$ List the possibilities when $n = 3$.) Suppose we have the elements $\\{1, 2, 3\\}$. Then, if we construct a tree by a random ordering, then, we get trees which appear with probabilities some multiple of $\\frac{1}{6}$. However, if we consider all the valid binary search trees on the key set of $\\{1, 2, 3\\}$. Then, we will have only five different possibilities. So, each will occur with probability $\\frac{1}{5}$, which is a different probability distribution.","title":"12.4-3"},{"location":"Chap12/12.4/#124-4","text":"Show that the function $f(x) = 2^x$ is convex. The second derivative is $2^x\\ln^2 2$ which is always positive, so the function is convex","title":"12.4-4"},{"location":"Chap12/12.4/#124-5-star","text":"Consider $\\text{RANDOMIZED-QUICKSORT}$ operating on a sequence of $n$ distinct input numbers. Prove that for any constant $k > 0$, all but $O(1 / n^k)$ of the $n!$ input permutations yield an $O(n\\lg n)$ running time. Let $A(n)$ denote the probability that when quicksorting a list of length $n$, some pivot is selected to not be in the middle $n^{1 - k / 2}$ of the numberes. This doesn't happen with probability $\\frac{1}{n^{k / 2}}$. Then, we have that the two subproblems are of size $n_1, n_2$ with $n_1 + n_2 = n - 1$, then $$A(n) \\le \\frac{1}{n^{k / 2}} + T(n_1)+T(n_2).$$ Since we bounded the depth by $O(1 / \\lg n)$ let $\\{a_{i, j}\\}_i$ be all the subproblem sizes left at depth $j$, $$A(n) \\le \\frac{1}{n^{k / 2}} \\sum_j\\sum_i \\frac{1}{a}.$$","title":"12.4-5 $\\star$"},{"location":"Chap12/Problems/12-1/","text":"Equal keys pose a problem for the implementation of binary search trees. a. What is the asymptotic performance of $\\text{TREE-INSERT}$ when used to insert $n$ items with identical keys into an initially empty binary search tree? We propose to improve $\\text{TREE-INSERT}$ by testing before line 5 to determine whether $z.key = x.key$ and by testing before line 11 to determine whether $z.key = y.key$. If equality holds, we implement one of the following strategies. For each strategy, find the asymptotic performance of inserting $n$ items with identical keys into an initially empty binary search tree. (The strategies are described for line 5, in which we compare the keys of $z$ and $x$. Substitute $y$ for $x$ to arrive at the strategies for line 11.) b. Keep a boolean flag $x.b$ at node $x$, and set $x$ to either $x.left$ or $x.right$ based on the value of $x.b$, which alternates between $\\text{FALSE}$ and $\\text{TRUE}$ each time we visit $x$ while inserting a node with the same key as $x$. c. Keep a list of nodes with equal keys at $x$, and insert $z$ into the list. d. Randomly set $x$ to either $x.left$ or $x.right$. (Give the worst-case performance and informally derive the expected running time.) a. Each insertion will add the element to the right of the rightmost leaf because the inequality on line 11 will always evaluate to false. This will result in the runtime being $\\sum_{i = 1}^n i \\in \\Theta(n^2)$. b. This strategy will result in each of the two children subtrees having a difference in size at most one. This means that the height will be $\\Theta(\\lg n)$. So, the total runtime will be $\\sum_{i = 1}^n \\lg n \\in \\Theta(n\\lg n)$. c. This will only take linear time since the tree itself will be height $0$, and a single insertion into a list can be done in constant time. d. Worst-case: every random choice is to the right (or all to the left) this will result in the same behavior as in the first part of this problem, $\\Theta(n^2)$. Expected running time: notice that when randomly choosing, we will pick left roughly half the time, so, the tree will be roughly balanced, so, we have that the depth is roughly $\\lg(n)$, $\\Theta(n\\lg n)$.","title":"12-1 Binary search trees with equal keys"},{"location":"Chap12/Problems/12-2/","text":"Given two strings $a = a_0a_1 \\ldots a_p$ and $b = b_0b_1 \\ldots b_q$, where each $a_i$ and each $b_j$ is in some ordered set of characters, we say that string $a$ is lexicographically less than string $b$ if either there exists an integer $j$, where $0 \\le j \\le \\min(p, q)$, such that $a_i = b_i$ for all $i = 0, 1, \\ldots j - 1$ and $a_j < b_j$, or $p < q$ and $a_i = b_i$ for all $i = 0, 1, \\ldots, p$. For example, if $a$ and $b$ are bit strings, then $10100 < 10110$ by rule 1 (letting $j = 3$) and $10100 < 101000$ by rule 2. This ordering is similar to that used in English-language dictionaries. The radix tree data structure shown in Figure 12.5 stores the bit strings $1011, 10, 011, 100$, and $0$. When searching for a key $a = a_0a_1 \\ldots a_p$, we go left at a node of depth $i$ if $a_i = 0$ and right if $a_i = 1$. Let $S$ be a set of distinct bit strings whose lengths sum to $n$. Show how to use a radix tree to sort $S$ lexicographically in $\\Theta(n)$ time. For the example in Figure 12.5, the output of the sort should be the sequence $0, 011, 10, 100, 1011$. (Removed)","title":"12-2 Radix trees"},{"location":"Chap12/Problems/12-3/","text":"In this problem, we prove that the average depth of a node in a randomly built binary search tree with $n$ nodes is $O(\\lg n)$. Although this result is weaker than that of Theorem 12.4, the technique we shall use reveals a surprising similarity between the building of a binary search tree and the execution of $\\text{RANDOMIZED-QUICKSORT}$ from Section 7.3. We define the total path length $P(T)$ of a binary tree $T$ as the sum, over all nodes $x$ in $T$, of the depth of node $x$, which we denote by $d(x, T)$. a. Argue that the average depth of a node in $T$ is $$\\frac{1}{n} \\sum_{x \\in T} d(x, T) = \\frac{1}{n} P(T).$$ Thus, we wish to show that the expected value of $P(T)$ is $O(n\\lg n)$. b. Let $T_L$ and $T_R$ denote the left and right subtrees of tree $T$, respectively. Argue that if $T$ has $n$ nodes, then $$P(T) = P(T_L) + P(T_R) + n - 1.$$ c. Let $P(n)$ denote the average total path length of a randomly built binary search tree with n nodes. Show that $$P(n) = \\frac{1}{n} \\sum_{i = 0}^{n - 1} (P(i) + P(n - i - 1) + n - 1).$$ d. Show how to rewrite $P(n)$ as $$P(n) = \\frac{2}{n} \\sum_{k = 1}^{n - 1} P(k) + \\Theta(n).$$ e. Recalling the alternative analysis of the randomized version of quicksort given in Problem 7-3, conclude that $P(n) = O(n\\lg n)$. At each recursive invocation of quicksort, we choose a random pivot element to partition the set of elements being sorted. Each node of a binary search tree partitions the set of elements that fall into the subtree rooted at that node. f. Describe an implementation of quicksort in which the comparisons to sort a set of elements are exactly the same as the comparisons to insert the elements into a binary search tree. (The order in which comparisons are made may differ, but the same comparisons must occur.) (Removed)","title":"12-3 Average node depth in a randomly built binary search tree"},{"location":"Chap12/Problems/12-4/","text":"Let $b_n$ denote the number of different binary trees with $n$ nodes. In this problem, you will find a formula for $b_n$, as well as an asymptotic estimate. a. Show that $b_0 = 1$ and that, for $n \\ge 1$, $$b_n = \\sum_{k = 0}^{n - 1} b_k b_{n - 1 - k}.$$ b. Referring to Problem 4-4 for the definition of a generating function, let $B(x)$ be the generating function $$B(x) = \\sum_{n = 0}^\\infty b_n x^n.$$ Show that $B(x) = xB(x)^2 + 1$, and hence one way to express $B(x)$ in closed form is $$B(x) = \\frac{1}{2x} (1 - \\sqrt{1 - 4x}).$$ The Taylor expansion of $f(x)$ around the point $x = a$ is given by $$f(x) = \\sum_{k = 0}^\\infty \\frac{f^{(k)}(a)}{k!} (x - a)^k,$$ where $f^{(k)}(x)$ is the $k$th derivative of $f$ evaluated at $x$. c. Show that $$b_n = \\frac{1}{n + 1} \\binom{2n}{n}$$ (the $n$th Catalan number ) by using the Taylor expansion of $\\sqrt{1 - 4x}$ around $x = 0$. (If you wish, instead of using the Taylor expansion, you may use the generalization of the binomial expansion (C.4) to nonintegral exponents $n$, where for any real number $n$ and for any integer $k$, we interpret $\\binom{n}{k}$ to be $n(n - 1) \\cdots (n - k + 1) / k!$ if $k \\ge 0$, and $0$ otherwise.) d. Show that $$b_n = \\frac{4^n}{\\sqrt{\\pi}n^{3 / 2}} (1 + O(1 / n)).$$ a. A root with two subtree. b. $$ \\begin{aligned} B(x)^2 & = (b_0 x^0 + b_1 x^1 + b_2 x^2 + \\cdots)^2 \\\\ & = b_0^2 x^0 + (b_0 b_1 + b_1 b_0) x^1 + (b_0 b_2 + b_1 b_1 + b_2 b_0) x^2 + \\cdots \\\\ & = \\sum_{k = 0}^0 b_k b_{0 - k} x^0 + \\sum_{k = 0}^1 b_k b_{1 - k} x^1 + \\sum_{k = 0}^2 b_k b_{2 - k} x^2 + \\cdots \\end{aligned} $$ $$ \\begin{aligned} xB(x)^2 + 1 & = 1 + \\sum_{k = 0}^0 b_k b_{1 - 1 - k} x^1 + \\sum_{k = 0}^2 b_k b_{2-1 - k} x^3 + \\sum_{k = 0}^2 b_k b_{3-1 - k} x^2 + \\cdots \\\\ & = 1 + b_1 x^1 + b_2 x^2 + b_3 x^3 + \\cdots \\\\ & = b_0 x^0 + b_1 x^1 + b_2 x^2 + b_3 x^3 + \\cdots \\\\ & = \\sum_{n = 0}^\\infty b_n x^n \\\\ & = B(x). \\end{aligned} $$ $$ \\begin{aligned} x B(x)^2 + 1 & = x \\cdot \\frac{1}{4x^2} (1 + 1 - 4x - 2\\sqrt{1 - 4x}) + 1 \\\\ & = \\frac{1}{4x} (2 - 2\\sqrt{1 - 4x}) - 1 + 1 \\\\ & = \\frac{1}{2x} (1 - \\sqrt{1 - 4x}) \\\\ & = B(x). \\end{aligned} $$ c. Let $f(x) = \\sqrt{1 - 4x}$, the numerator of the derivative is $$ \\begin{aligned} 2 \\cdot (1 \\cdot 2) \\cdot (3 \\cdot 2) \\cdot (5 \\cdot 2) \\cdots & = 2^k \\cdot \\prod_{i = 0}^{k - 2} (2k + 1) \\\\ & = 2^k \\cdot \\frac{(2(k - 1))!}{2^{k - 1}(k - 1)!} \\\\ & = \\frac{2(2(k - 1))!}{(k - 1)!}. \\end{aligned} $$ $$f(x) = 1 - 2x - 2x^2 - 4 x^3 - 10x^4 - 28x^5 - \\cdots.$$ The coefficient is $\\frac{2(2(k - 1))!}{k!(k - 1)!}$. $$ \\begin{aligned} B(x) & = \\frac{1}{2x}(1 - f(x)) \\\\ & = 1 + x + 2x^2 + 5x^3 + 14x^4 + \\cdots \\\\ & = \\sum_{n = 0}^\\infty \\frac{(2n)!}{(n + 1)!n!} x \\\\ & = \\sum_{n = 0}^\\infty \\frac{1}{n + 1} \\frac{(2n)!}{n!n!} x \\\\ & = \\sum_{n = 0}^\\infty \\frac{1}{n + 1} \\binom{2n}{n} x. \\end{aligned} $$ $$b_n = \\frac{1}{n + 1} \\binom{2n}{n}.$$ d. $$ \\begin{aligned} b_n & = \\frac{1}{n + 1} \\frac{(2n)!}{n!n!} \\\\ & \\approx \\frac{1}{n + 1} \\frac{\\sqrt{4 \\pi n}(2n / e)^{2n}}{2 \\pi n (n / e)^{2n}} \\\\ & = \\frac{1}{n + 1} \\frac{4^n}{\\sqrt{\\pi n} } \\\\ & = (\\frac{1}{n} + (\\frac{1}{n + 1} - \\frac{1}{n})) \\frac{4^n}{\\sqrt{\\pi n}} \\\\ & = (\\frac{1}{n} - \\frac{1}{n^2 + n}) \\frac{4^n}{\\sqrt{\\pi n}} \\\\ & = \\frac{1}{n} (1 - \\frac{1}{n + 1}) \\frac{4^n}{\\sqrt{\\pi n}} \\\\ & = \\frac{4^n}{\\sqrt{\\pi}n^{3 / 2}} (1 + O(1 / n)). \\end{aligned} $$","title":"12-4 Number of different binary trees"},{"location":"Chap13/13.1/","text":"13.1-1 In the style of Figure 13.1(a), draw the complete binary search tree of height $3$ on the keys $\\{1, 2, \\ldots, 15\\}$. Add the $\\text{NIL}$ leaves and color the nodes in three different ways such that the black-heights of the resulting red-black trees are $2$, $3$, and $4$. Complete binary tree of $height = 3$: Red-black tree of $black\\text-heights = 2$: Red-black tree of $black\\text-heights = 3$: Red-black tree of $black\\text-heights = 4$: 13.1-2 Draw the red-black tree that results after $\\text{TREE-INSERT}$ is called on the tree in Figure 13.1 with key $36$. If the inserted node is colored red, is the resulting tree a red-black tree? What if it is colored black? If the inserted node is colored red, the tree doesn't satisfy property 4 because $35$ will be the parent of $36$, which is also colored red. If the inserted node is colored black, the tree doesn't satisfy property 5 because there will be two paths from node $38$ to $T.nil$ which contain different numbers of black nodes. We don't draw the wrong red-black tree; however, we draw the adjusted correct tree: 13.1-3 Let us define a relaxed red-black tree as a binary search tree that satisfies red-black properties 1, 3, 4, and 5. In other words, the root may be either red or black. Consider a relaxed red-black tree $T$ whose root is red. If we color the root of $T$ black but make no other changes to $T$, is the resulting tree a red-black tree? Yes, it is. Property 1 is trivially satisfied since only one node is changed and it is not changed to some mysterious third color. Property 3 is trivially satisfied since no new leaves are introduced. Property 4 is satisfied since there was no red node introduced, and root is in every path from the root to the leaves, but no others. Property 5 is satisfied since the only paths we will be changing the number of black nodes in are those coming from the root. All of these will increase by $1$, and so will all be equal. 13.1-4 Suppose that we \"absorb\" every red node in a red-black tree into its black parent, so that the children of the red node become children of the black parent. (Ignore what happens to the keys.) What are the possible degrees of a black node after all its red children are absorbed? What can you say about the depths of the leaves of the resulting tree? The possible degrees are $0$ through $5$, based on whether or not the black node was a root and whether it had one or two red children, each with either one or two black children. The depths could shrink by at most a factor of $1 / 2$. 13.1-5 Show that the longest simple path from a node $x$ in a red-black tree to a descendant leaf has length at most twice that of the shortest simple path from node $x$ to a descendant leaf. Suppose we have the longest simple path $(a_1, a_2, \\dots, a_s)$ and the shortest simple path $(b_1, b_2, \\dots, b_t)$. Then, by property 5 we know they have equal numbers of black nodes. By property 4, we know that neither contains a repeated red node. This tells us that at most $\\left\\lfloor \\frac{s - 1}{2} \\right\\rfloor$ of the nodes in the longest path are red. This means that at least $\\left\\lceil \\frac{s + 1}{2} \\right\\rceil$ are black, so, $t \\ge \\left\\lceil \\frac{s + 1}{2} \\right\\rceil$. Therefore, if, by way of contradiction, we had that $s > t \\cdot 2$, then $t \\ge \\left\\lceil \\frac{s + 1}{2} \\right\\rceil \\ge \\left\\lceil \\frac{2t + 2}{2} \\right\\rceil = t + 1$ results a contradiction. 13.1-6 What is the largest possible number of internal nodes in a red-black tree with black-height $k$? What is the smallest possible number? The largest is a path with half black nodes and half red nodes, which has $2^{2k} - 1$ internal nodes. The smallest is a path with all black nodes, which has $2^k - 1$ internal nodes. 13.1-7 Describe a red-black tree on $n$ keys that realizes the largest possible ratio of red internal nodes to black internal nodes. What is this ratio? What tree has the smallest possible ratio, and what is the ratio? The largest ratio is $2$, each black node has two red children. The smallest ratio is $0$.","title":"13.1 Properties of red-black trees"},{"location":"Chap13/13.1/#131-1","text":"In the style of Figure 13.1(a), draw the complete binary search tree of height $3$ on the keys $\\{1, 2, \\ldots, 15\\}$. Add the $\\text{NIL}$ leaves and color the nodes in three different ways such that the black-heights of the resulting red-black trees are $2$, $3$, and $4$. Complete binary tree of $height = 3$: Red-black tree of $black\\text-heights = 2$: Red-black tree of $black\\text-heights = 3$: Red-black tree of $black\\text-heights = 4$:","title":"13.1-1"},{"location":"Chap13/13.1/#131-2","text":"Draw the red-black tree that results after $\\text{TREE-INSERT}$ is called on the tree in Figure 13.1 with key $36$. If the inserted node is colored red, is the resulting tree a red-black tree? What if it is colored black? If the inserted node is colored red, the tree doesn't satisfy property 4 because $35$ will be the parent of $36$, which is also colored red. If the inserted node is colored black, the tree doesn't satisfy property 5 because there will be two paths from node $38$ to $T.nil$ which contain different numbers of black nodes. We don't draw the wrong red-black tree; however, we draw the adjusted correct tree:","title":"13.1-2"},{"location":"Chap13/13.1/#131-3","text":"Let us define a relaxed red-black tree as a binary search tree that satisfies red-black properties 1, 3, 4, and 5. In other words, the root may be either red or black. Consider a relaxed red-black tree $T$ whose root is red. If we color the root of $T$ black but make no other changes to $T$, is the resulting tree a red-black tree? Yes, it is. Property 1 is trivially satisfied since only one node is changed and it is not changed to some mysterious third color. Property 3 is trivially satisfied since no new leaves are introduced. Property 4 is satisfied since there was no red node introduced, and root is in every path from the root to the leaves, but no others. Property 5 is satisfied since the only paths we will be changing the number of black nodes in are those coming from the root. All of these will increase by $1$, and so will all be equal.","title":"13.1-3"},{"location":"Chap13/13.1/#131-4","text":"Suppose that we \"absorb\" every red node in a red-black tree into its black parent, so that the children of the red node become children of the black parent. (Ignore what happens to the keys.) What are the possible degrees of a black node after all its red children are absorbed? What can you say about the depths of the leaves of the resulting tree? The possible degrees are $0$ through $5$, based on whether or not the black node was a root and whether it had one or two red children, each with either one or two black children. The depths could shrink by at most a factor of $1 / 2$.","title":"13.1-4"},{"location":"Chap13/13.1/#131-5","text":"Show that the longest simple path from a node $x$ in a red-black tree to a descendant leaf has length at most twice that of the shortest simple path from node $x$ to a descendant leaf. Suppose we have the longest simple path $(a_1, a_2, \\dots, a_s)$ and the shortest simple path $(b_1, b_2, \\dots, b_t)$. Then, by property 5 we know they have equal numbers of black nodes. By property 4, we know that neither contains a repeated red node. This tells us that at most $\\left\\lfloor \\frac{s - 1}{2} \\right\\rfloor$ of the nodes in the longest path are red. This means that at least $\\left\\lceil \\frac{s + 1}{2} \\right\\rceil$ are black, so, $t \\ge \\left\\lceil \\frac{s + 1}{2} \\right\\rceil$. Therefore, if, by way of contradiction, we had that $s > t \\cdot 2$, then $t \\ge \\left\\lceil \\frac{s + 1}{2} \\right\\rceil \\ge \\left\\lceil \\frac{2t + 2}{2} \\right\\rceil = t + 1$ results a contradiction.","title":"13.1-5"},{"location":"Chap13/13.1/#131-6","text":"What is the largest possible number of internal nodes in a red-black tree with black-height $k$? What is the smallest possible number? The largest is a path with half black nodes and half red nodes, which has $2^{2k} - 1$ internal nodes. The smallest is a path with all black nodes, which has $2^k - 1$ internal nodes.","title":"13.1-6"},{"location":"Chap13/13.1/#131-7","text":"Describe a red-black tree on $n$ keys that realizes the largest possible ratio of red internal nodes to black internal nodes. What is this ratio? What tree has the smallest possible ratio, and what is the ratio? The largest ratio is $2$, each black node has two red children. The smallest ratio is $0$.","title":"13.1-7"},{"location":"Chap13/13.2/","text":"13.2-1 Write pseudocode for $\\text{RIGHT-ROTATE}$. RIGHT - ROTATE ( T , y ) x = y . left y . left = x . right if x . right != T . nil x . right . p = y x . p = y . p if y . p == T . nil T . root = x else if y == y . p . right y . p . right = x else y . p . left = x x . right = y y . p = x 13.2-2 Argue that in every $n$-node binary search tree, there are exactly $n - 1$ possible rotations. Every node can rotate with its parent, only the root does not have a parent, therefore there are $n - 1$ possible rotations. 13.2-3 Let $a$, $b$, and $c$ be arbitrary nodes in subtrees $\\alpha$, $\\beta$, and $\\gamma$, respectively, in the left tree of Figure 13.2. How do the depths of $a$, $b$, and $c$ change when a left rotation is performed on node $x$ in the figure? $a$: increase by $1$. $b$: unchanged. $c$: decrease by $1$. 13.2-4 Show that any arbitrary $n$-node binary search tree can be transformed into any other arbitrary $n$-node binary search tree using $O(n)$ rotations. ($\\textit{Hint:}$ First show that at most $n - 1$ right rotations suffice to transform the tree into a right-going chain.) Consider transforming an arbitrary $n$-node binary tree into a right-going chain as follows: Let the root and all successive right children of the root be the elements of the chain initial chain. For any node $x$ which is a left child of a node on the chain, a single right rotation on the parent of $x$ will add that node to the chain and not remove any elements from the chain. Thus, we can convert any binary search tree to a right chain with at most $n \u2212 1$ right rotations. Let $r_1, r_2, \\dots, r_k$ be the sequence of rotations required to convert some binary search tree $T_1$ into a right-going chain, and let $s_1, s_2, \\dots, s_m$ be the sequence of rotations required to convert some other binary search tree $T_2$ to a right-going chain. Then $k < n$ and $m < n$, and we can convert $T_1$ to $T_2$ by performing the sequence $r_1, r_2, \\dots, r_k, s_m', s_{m - 1}', \\dots, s_1'$ where $s_i'$ is the opposite rotation of $s_i$. Since $k + m < 2n$, the number of rotations required is $O(n)$. 13.2-5 $\\star$ We say that a binary search tree $T_1$ can be right-converted to binary search tree $T_2$ if it is possible to obtain $T_2$ from $T_1$ via a series of calls to $\\text{RIGHT-ROTATE}$. Give an example of two trees $T_1$ and $T_2$ such that $T_1$ cannot be right-converted to $T_2$. Then, show that if a tree $T_1$ can be right-converted to $T_2$, it can be right-converted using $O(n^2)$ calls to $\\text{RIGHT-ROTATE}$. We can use $O(n)$ calls to rotate the node which is the root in $T_2$ to $T_1$'s root, then use the same operation in the two subtrees. There are $n$ nodes, therefore the upper bound is $O(n^2)$.","title":"13.2 Rotations"},{"location":"Chap13/13.2/#132-1","text":"Write pseudocode for $\\text{RIGHT-ROTATE}$. RIGHT - ROTATE ( T , y ) x = y . left y . left = x . right if x . right != T . nil x . right . p = y x . p = y . p if y . p == T . nil T . root = x else if y == y . p . right y . p . right = x else y . p . left = x x . right = y y . p = x","title":"13.2-1"},{"location":"Chap13/13.2/#132-2","text":"Argue that in every $n$-node binary search tree, there are exactly $n - 1$ possible rotations. Every node can rotate with its parent, only the root does not have a parent, therefore there are $n - 1$ possible rotations.","title":"13.2-2"},{"location":"Chap13/13.2/#132-3","text":"Let $a$, $b$, and $c$ be arbitrary nodes in subtrees $\\alpha$, $\\beta$, and $\\gamma$, respectively, in the left tree of Figure 13.2. How do the depths of $a$, $b$, and $c$ change when a left rotation is performed on node $x$ in the figure? $a$: increase by $1$. $b$: unchanged. $c$: decrease by $1$.","title":"13.2-3"},{"location":"Chap13/13.2/#132-4","text":"Show that any arbitrary $n$-node binary search tree can be transformed into any other arbitrary $n$-node binary search tree using $O(n)$ rotations. ($\\textit{Hint:}$ First show that at most $n - 1$ right rotations suffice to transform the tree into a right-going chain.) Consider transforming an arbitrary $n$-node binary tree into a right-going chain as follows: Let the root and all successive right children of the root be the elements of the chain initial chain. For any node $x$ which is a left child of a node on the chain, a single right rotation on the parent of $x$ will add that node to the chain and not remove any elements from the chain. Thus, we can convert any binary search tree to a right chain with at most $n \u2212 1$ right rotations. Let $r_1, r_2, \\dots, r_k$ be the sequence of rotations required to convert some binary search tree $T_1$ into a right-going chain, and let $s_1, s_2, \\dots, s_m$ be the sequence of rotations required to convert some other binary search tree $T_2$ to a right-going chain. Then $k < n$ and $m < n$, and we can convert $T_1$ to $T_2$ by performing the sequence $r_1, r_2, \\dots, r_k, s_m', s_{m - 1}', \\dots, s_1'$ where $s_i'$ is the opposite rotation of $s_i$. Since $k + m < 2n$, the number of rotations required is $O(n)$.","title":"13.2-4"},{"location":"Chap13/13.2/#132-5-star","text":"We say that a binary search tree $T_1$ can be right-converted to binary search tree $T_2$ if it is possible to obtain $T_2$ from $T_1$ via a series of calls to $\\text{RIGHT-ROTATE}$. Give an example of two trees $T_1$ and $T_2$ such that $T_1$ cannot be right-converted to $T_2$. Then, show that if a tree $T_1$ can be right-converted to $T_2$, it can be right-converted using $O(n^2)$ calls to $\\text{RIGHT-ROTATE}$. We can use $O(n)$ calls to rotate the node which is the root in $T_2$ to $T_1$'s root, then use the same operation in the two subtrees. There are $n$ nodes, therefore the upper bound is $O(n^2)$.","title":"13.2-5 $\\star$"},{"location":"Chap13/13.3/","text":"13.3-1 In line 16 of $\\text{RB-INSERT}$, we set the color of the newly inserted node $z$ to red. Observe that if we had chosen to set $z$'s color to black, then property 4 of a red-black tree would not be violated. Why didn't we choose to set $z$'s color to black? If we chose to set the color of $z$ to black then we would be violating property 5 of being a red-black tree. Because any path from the root to a leaf under $z$ would have one more black node than the paths to the other leaves. 13.3-2 Show the red-black trees that result after successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty red-black tree. insert $41$: insert $38$: insert $31$: insert $12$: insert $19$: insert $8$: 13.3-3 Suppose that the black-height of each of the subtrees $\\alpha, \\beta, \\gamma, \\delta, \\epsilon$ in Figures 13.5 and 13.6 is $k$. Label each node in each figure with its black-height to verify that the indicated transformation preserves property 5. (Removed) 13.3-4 Professor Teach is concerned that $\\text{RB-INSERT-FIXUP}$ might set $T.nil.color$ to $\\text{RED}$, in which case the test in line 1 would not cause the loop to terminate when $z$ is the root. Show that the professor's concern is unfounded by arguing that $\\text{RB-INSERT-FIXUP}$ never sets $T.nil.color$ to $\\text{RED}$. First observe that $\\text{RB-INSERT-FIXUP}$ only modifies the child of a node if it is already $\\text{RED}$, so we will never modify a child which is set to $T.nil$. We just need to check that the parent of the root is never set to $\\text{RED}$. Since the root and the parent of the root are automatically black, if $z$ is at depth less than $2$, the while loop will be broken. We only modify colors of nodes at most two levels above $z$, so the only case we need to worry about is when $z$ is at depth $2$. In this case we risk modifying the root to be $\\text{RED}$, but this is handled in line 16. When $z$ is updated, it will be either the root or the child of the root. Either way, the root and the parent of the root are still $\\text{BLACK}$, so the while condition is violated, making it impossibly to modify $T.nil$ to be $\\text{RED}$. 13.3-5 Consider a red-black tree formed by inserting $n$ nodes with $\\text{RB-INSERT}$. Argue that if $n > 1$, the tree has at least one red node. Case 1: $z$ and $z.p.p$ are $\\text{RED}$, if the loop terminates, then $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Case 2: $z$ and $z.p$ are $\\text{RED}$, and after the rotation $z.p$ could not be the root, thus $z.p$ is $\\text{RED}$ after the fix up. Case 3: $z$ is $\\text{RED}$ and $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Therefore, there is always at least one red node. 13.3-6 Suggest how to implement $\\text{RB-INSERT}$ efficiently if the representation for red-black trees includes no storage for parent pointers. Use stack to record the path to the inserted node, then parent is the top element in the stack. Case 1: we pop $z.p$ and $z.p.p$. Case 2: we pop $z.p$ and $z.p.p$, then push $z.p.p$ and $z$. Case 3: we pop $z.p$, $z.p.p$ and $z.p.p.p$, then push $z.p$.","title":"13.3 Insertion"},{"location":"Chap13/13.3/#133-1","text":"In line 16 of $\\text{RB-INSERT}$, we set the color of the newly inserted node $z$ to red. Observe that if we had chosen to set $z$'s color to black, then property 4 of a red-black tree would not be violated. Why didn't we choose to set $z$'s color to black? If we chose to set the color of $z$ to black then we would be violating property 5 of being a red-black tree. Because any path from the root to a leaf under $z$ would have one more black node than the paths to the other leaves.","title":"13.3-1"},{"location":"Chap13/13.3/#133-2","text":"Show the red-black trees that result after successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty red-black tree. insert $41$: insert $38$: insert $31$: insert $12$: insert $19$: insert $8$:","title":"13.3-2"},{"location":"Chap13/13.3/#133-3","text":"Suppose that the black-height of each of the subtrees $\\alpha, \\beta, \\gamma, \\delta, \\epsilon$ in Figures 13.5 and 13.6 is $k$. Label each node in each figure with its black-height to verify that the indicated transformation preserves property 5. (Removed)","title":"13.3-3"},{"location":"Chap13/13.3/#133-4","text":"Professor Teach is concerned that $\\text{RB-INSERT-FIXUP}$ might set $T.nil.color$ to $\\text{RED}$, in which case the test in line 1 would not cause the loop to terminate when $z$ is the root. Show that the professor's concern is unfounded by arguing that $\\text{RB-INSERT-FIXUP}$ never sets $T.nil.color$ to $\\text{RED}$. First observe that $\\text{RB-INSERT-FIXUP}$ only modifies the child of a node if it is already $\\text{RED}$, so we will never modify a child which is set to $T.nil$. We just need to check that the parent of the root is never set to $\\text{RED}$. Since the root and the parent of the root are automatically black, if $z$ is at depth less than $2$, the while loop will be broken. We only modify colors of nodes at most two levels above $z$, so the only case we need to worry about is when $z$ is at depth $2$. In this case we risk modifying the root to be $\\text{RED}$, but this is handled in line 16. When $z$ is updated, it will be either the root or the child of the root. Either way, the root and the parent of the root are still $\\text{BLACK}$, so the while condition is violated, making it impossibly to modify $T.nil$ to be $\\text{RED}$.","title":"13.3-4"},{"location":"Chap13/13.3/#133-5","text":"Consider a red-black tree formed by inserting $n$ nodes with $\\text{RB-INSERT}$. Argue that if $n > 1$, the tree has at least one red node. Case 1: $z$ and $z.p.p$ are $\\text{RED}$, if the loop terminates, then $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Case 2: $z$ and $z.p$ are $\\text{RED}$, and after the rotation $z.p$ could not be the root, thus $z.p$ is $\\text{RED}$ after the fix up. Case 3: $z$ is $\\text{RED}$ and $z$ could not be the root, thus $z$ is $\\text{RED}$ after the fix up. Therefore, there is always at least one red node.","title":"13.3-5"},{"location":"Chap13/13.3/#133-6","text":"Suggest how to implement $\\text{RB-INSERT}$ efficiently if the representation for red-black trees includes no storage for parent pointers. Use stack to record the path to the inserted node, then parent is the top element in the stack. Case 1: we pop $z.p$ and $z.p.p$. Case 2: we pop $z.p$ and $z.p.p$, then push $z.p.p$ and $z$. Case 3: we pop $z.p$, $z.p.p$ and $z.p.p.p$, then push $z.p$.","title":"13.3-6"},{"location":"Chap13/13.4/","text":"13.4-1 Argue that after executing $\\text{RB-DELETE-FIXUP}$, the root of the tree must be black. Case 1: transform to 2, 3, 4. Case 2: if terminates, the root of the subtree (the new $x$) is set to black. Case 3: transform to 4. Case 4: the root (the new $x$) is set to black. 13.4-2 Argue that if in $\\text{RB-DELETE}$ both $x$ and $x.p$ are red, then property 4 is restored by the call to $\\text{RB-DELETE-FIXUP}(T, x)$. Suppose that both $x$ and $x.p$ are red in $\\text{RB-DELETE}$. This can only happen in the else-case of line 9. Since we are deleting from a red-black tree, the other child of y.p which becomes $x$'s sibling in the call to $\\text{RB-TRANSPLANT}$ on line 14 must be black, so $x$ is the only child of $x.p$ which is red. The while-loop condition of $\\text{RB-DELETE-FIXUP}(T, x)$ is immediately violated so we simply set $x.color = black$, restoring property 4. 13.4-3 In Exercise 13.3-2, you found the red-black tree that results from successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty tree. Now show the red-black trees that result from the successive deletion of the keys in the order $8, 12, 19, 31, 38, 41$. initial: delete $8$: delete $12$: delete $19$: delete $31$: delete $38$: delete $41$: 13.4-4 In which lines of the code for $\\text{RB-DELETE-FIXUP}$ might we examine or modify the sentinel $T.nil$? When the node $y$ in $\\text{RB-DELETE}$ has no children, the node $x = T.nil$, so we'll examine the line 2 of $\\text{RB-DELETE-FIXUP}$. When the root node is deleted, $x = T.nil$ and the root at this time is $x$, so the line 23 of $\\text{RB-DELETE-FIXUP}$ will draw $x$ to black. 13.4-5 In each of the cases of Figure 13.7, give the count of black nodes from the root of the subtree shown to each of the subtrees $\\alpha, \\beta, \\ldots, \\zeta$, and verify that each count remains the same after the transformation. When a node has a $color$ attribute $c$ or $c'$, use the notation $\\text{count}(c)$ or $\\text{count}(c')$ symbolically in your count. Our count will include the root (if it is black). Case 1: For each subtree, it is $2$ both before and after. Case 2: For $\\alpha$ and $\\beta$, it is $1 + \\text{count}(c)$ in both cases. For the rest of the subtrees, it is from $2 + \\text{count}(c)$ to $1 + \\text{count}(c)$. This decrease in the count for the other subtreese is handled by then having $x$ represent an additional black. Case 3: For $\\epsilon$ and $\\zeta$, it is $2+\\text{count}(c)$ both before and after. For all the other subtrees, it is $1+\\text{count}(c)$ both before and after. Case 4: For $\\alpha$ and $\\beta$, it is from $1 + \\text{count}(c)$ to $2 + \\text{count}(c)$. For $\\gamma$ and $\\delta$, it is $1 + \\text{count}(c) + \\text{count}(c')$ both before and after. For $\\epsilon$ and $\\zeta$, it is $1 + \\text{count}(c)$ both before and after. This increase in the count for $\\alpha$ and $\\beta$ is because $x$ before indicated an extra black. 13.4-6 Professors Skelton and Baron are concerned that at the start of case 1 of $\\text{RB-DELETE-FIXUP}$, the node $x.p$ might not be black. If the professors are correct, then lines 5\u20136 are wrong. Show that $x.p$ must be black at the start of case 1, so that the professors have nothing to worry about. At the start of case 1 we have set $w$ to be the sibling of $x$. We check on line 4 that $w.color == red$, which means that the parent of $x$ and $w$ cannot be red. Otherwise property 4 is violated. Thus, their concerns are unfounded. 13.4-7 Suppose that a node $x$ is inserted into a red-black tree with $\\text{RB-INSERT}$ and then is immediately deleted with $\\text{RB-DELETE}$. Is the resulting red-black tree the same as the initial red-black tree? Justify your answer. No, the red-black tree will not necessarily be the same. Example 1: initial: insert $1$: delete $1$: Example 2: initial: insert $1$: delete $1$:","title":"13.4 Deletion"},{"location":"Chap13/13.4/#134-1","text":"Argue that after executing $\\text{RB-DELETE-FIXUP}$, the root of the tree must be black. Case 1: transform to 2, 3, 4. Case 2: if terminates, the root of the subtree (the new $x$) is set to black. Case 3: transform to 4. Case 4: the root (the new $x$) is set to black.","title":"13.4-1"},{"location":"Chap13/13.4/#134-2","text":"Argue that if in $\\text{RB-DELETE}$ both $x$ and $x.p$ are red, then property 4 is restored by the call to $\\text{RB-DELETE-FIXUP}(T, x)$. Suppose that both $x$ and $x.p$ are red in $\\text{RB-DELETE}$. This can only happen in the else-case of line 9. Since we are deleting from a red-black tree, the other child of y.p which becomes $x$'s sibling in the call to $\\text{RB-TRANSPLANT}$ on line 14 must be black, so $x$ is the only child of $x.p$ which is red. The while-loop condition of $\\text{RB-DELETE-FIXUP}(T, x)$ is immediately violated so we simply set $x.color = black$, restoring property 4.","title":"13.4-2"},{"location":"Chap13/13.4/#134-3","text":"In Exercise 13.3-2, you found the red-black tree that results from successively inserting the keys $41, 38, 31, 12, 19, 8$ into an initially empty tree. Now show the red-black trees that result from the successive deletion of the keys in the order $8, 12, 19, 31, 38, 41$. initial: delete $8$: delete $12$: delete $19$: delete $31$: delete $38$: delete $41$:","title":"13.4-3"},{"location":"Chap13/13.4/#134-4","text":"In which lines of the code for $\\text{RB-DELETE-FIXUP}$ might we examine or modify the sentinel $T.nil$? When the node $y$ in $\\text{RB-DELETE}$ has no children, the node $x = T.nil$, so we'll examine the line 2 of $\\text{RB-DELETE-FIXUP}$. When the root node is deleted, $x = T.nil$ and the root at this time is $x$, so the line 23 of $\\text{RB-DELETE-FIXUP}$ will draw $x$ to black.","title":"13.4-4"},{"location":"Chap13/13.4/#134-5","text":"In each of the cases of Figure 13.7, give the count of black nodes from the root of the subtree shown to each of the subtrees $\\alpha, \\beta, \\ldots, \\zeta$, and verify that each count remains the same after the transformation. When a node has a $color$ attribute $c$ or $c'$, use the notation $\\text{count}(c)$ or $\\text{count}(c')$ symbolically in your count. Our count will include the root (if it is black). Case 1: For each subtree, it is $2$ both before and after. Case 2: For $\\alpha$ and $\\beta$, it is $1 + \\text{count}(c)$ in both cases. For the rest of the subtrees, it is from $2 + \\text{count}(c)$ to $1 + \\text{count}(c)$. This decrease in the count for the other subtreese is handled by then having $x$ represent an additional black. Case 3: For $\\epsilon$ and $\\zeta$, it is $2+\\text{count}(c)$ both before and after. For all the other subtrees, it is $1+\\text{count}(c)$ both before and after. Case 4: For $\\alpha$ and $\\beta$, it is from $1 + \\text{count}(c)$ to $2 + \\text{count}(c)$. For $\\gamma$ and $\\delta$, it is $1 + \\text{count}(c) + \\text{count}(c')$ both before and after. For $\\epsilon$ and $\\zeta$, it is $1 + \\text{count}(c)$ both before and after. This increase in the count for $\\alpha$ and $\\beta$ is because $x$ before indicated an extra black.","title":"13.4-5"},{"location":"Chap13/13.4/#134-6","text":"Professors Skelton and Baron are concerned that at the start of case 1 of $\\text{RB-DELETE-FIXUP}$, the node $x.p$ might not be black. If the professors are correct, then lines 5\u20136 are wrong. Show that $x.p$ must be black at the start of case 1, so that the professors have nothing to worry about. At the start of case 1 we have set $w$ to be the sibling of $x$. We check on line 4 that $w.color == red$, which means that the parent of $x$ and $w$ cannot be red. Otherwise property 4 is violated. Thus, their concerns are unfounded.","title":"13.4-6"},{"location":"Chap13/13.4/#134-7","text":"Suppose that a node $x$ is inserted into a red-black tree with $\\text{RB-INSERT}$ and then is immediately deleted with $\\text{RB-DELETE}$. Is the resulting red-black tree the same as the initial red-black tree? Justify your answer. No, the red-black tree will not necessarily be the same. Example 1: initial: insert $1$: delete $1$: Example 2: initial: insert $1$: delete $1$:","title":"13.4-7"},{"location":"Chap13/Problems/13-1/","text":"During the course of an algorithm, we sometimes find that we need to maintain past versions of a dynamic set as it is updated. We call such a set persistent . One way to implement a persistent set is to copy the entire set whenever it is modified, but this approach can slow down a program and also consume much space. Sometimes, we can do much better. Consider a persistent set $S$ with the operations $\\text{INSERT}$, $\\text{DELETE}$, and $\\text{SEARCH}$, which we implement using binary search trees as shown in Figure 13.8(a). We maintain a separate root for every version of the set. In order to insert the key $5$ into the set, we create a new node with key $5$. This node becomes the left child of a new node with key $7$, since we cannot modify the existing node with key $7$. Similarly, the new node with key $7$ becomes the left child of a new node with key $8$ whose right child is the existing node with key $10$. The new node with key $8$ becomes, in turn, the right child of a new root $r'$ with key $4$ whose left child is the existing node with key $3$. We thus copy only part of the tree and share some of the nodes with the original tree, as shown in Figure 13.8(b). Assume that each tree node has the attributes $key$, $left$, and $right$ but no parent. (See also Exercise 13.3-6.) a. For a general persistent binary search tree, identify the nodes that we need to change to insert a key $k$ or delete a node $y$. b. Write a procedure $\\text{PERSISTENT-TREE-INSERT}$ that, given a persistent tree $T$ and a key $k$ to insert, returns a new persistent tree $T'$ that is the result of inserting $k$ into $T$. c. If the height of the persistent binary search tree $T$ is $h$, what are the time and space requirements of your implementation of $\\text{PERSISTENT-TREE-INSERT}$? (The space requirement is proportional to the number of new nodes allocated.) d. Suppose that we had included the parent attribute in each node. In this case, $\\text{PERSISTENT-TREE-INSERT}$ would need to perform additional copying. Prove that $\\text{PERSISTENT-TREE-INSERT}$ would then require $\\Omega(n)$ time and space, where $n$ is the number of nodes in the tree. e. Show how to use red-black trees to guarantee that the worst-case running time and space are $O(\\lg n)$ per insertion or deletion. (Removed)","title":"13-1 Persistent dynamic sets"},{"location":"Chap13/Problems/13-2/","text":"The join operation takes two dynamic sets $S_1$ and $S_2$ and an element $x$ such that for any $x_1 \\in S_1$ and $x_2 \\in S_2$, we have $x_1.key \\le x.key \\le x_2.key$. It returns a set $S = S_1 \\cup \\{x\\} \\cup S_2$. In this problem, we investigate how to implement the join operation on red-black trees. a. Given a red-black tree $T$, let us store its black-height as the new attribute $T.bh$. Argue that $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ can maintain the $bh$ attribute without requiring extra storage in the nodes of the tree and without increasing the asymptotic running times. Show that while descending through $T$, we can determine the black-height of each node we visit in $O(1)$ time per node visited. We wish to implement the operation $\\text{RB-JOIN}(T_1, x, T_2)$, which destroys $T_1$ and $T_2$ and returns a red-black tree $T = T_1 \\cup \\{x\\} \\cup T_2$. Let $n$ be the total number of nodes in $T_1$ and $T_2$. b. Assume that $T_1.bh \\ge T_2.bh$. Describe an $O(\\lg n)$-time algorithm that finds a black node $y$ in $T_1$ with the largest key from among those nodes whose black-height is $T_2.bh$. c. Let $T_y$ be the subtree rooted at $y$. Describe how $T_y \\cup \\{x\\} \\cup T_2$ can replace $T_y$ in $O(1)$ time without destroying the binary-search-tree property. d. What color should we make $x$ so that red-black properties 1, 3, and 5 are maintained? Describe how to enforce properties 2 and 4 in $O(\\lg n)$ time. e. Argue that no generality is lost by making the assumption in part (b). Describe the symmetric situation that arises when $T_1.bh \\le T_2.bh$. f. Argue that the running time of $\\text{RB-JOIN}$ is $O(\\lg n)$. a. Initialize: $bh = 0$. $\\text{RB-INSERT}$: if in the last step the root is red, we increase $bh$ by $1$. $\\text{RB-DELETE}$: if $x$ is root, we decrease $bh$ by $1$. Each node: in the simple path, decrease $bh$ by $1$ each time we find a black node. b. Move to the right child if the node has a right child, otherwise move to the left child. If the node is black, we decease $bh$ by $1$. Repeat the step until $bh = T_2.bh$. c. The time complexity is $O(1)$. RB - JOIN ' ( T [ y ], x , T [ 2 ]) TRANSPLANT ( T [ y ], x ) x . left = T [ y ] x . right = T [ 2 ] T [ y ]. parent = x T [ 2 ]. parent = x d. Red. Call $\\text{INSERT-FIXUP(T[1], x)}$. The time complexity is $O(\\lg n)$. e. Same, if $T_1.bh\\le T_2.bh$, then we can use the above algorithm symmetrically. f. $O(1) + O(\\lg n) = O(\\lg n)$.","title":"13-2 Join operation on red-black trees"},{"location":"Chap13/Problems/13-3/","text":"An AVL tree is a binary search tree that is height balanced : for each node $x$, the heights of the left and right subtrees of $x$ differ by at most $1$. To implement an AVL tree, we maintain an extra attribute in each node: $x.h$ is the height of node $x$. As for any other binary search tree $T$, we assume that $T.root$ points to the root node. a. Prove that an AVL tree with $n$ nodes has height $O(\\lg n)$. ($\\textit{Hint:}$ Prove that an AVL tree of height $h$ has at least $F_h$ nodes, where $F_h$ is the $h$th Fibonacci number.) b. To insert into an AVL tree, we first place a node into the appropriate place in binary search tree order. Afterward, the tree might no longer be height balanced. Specifically, the heights of the left and right children of some node might differ by $2$. Describe a procedure $\\text{BALANCE}(x)$, which takes a subtree rooted at $x$ whose left and right children are height balanced and have heights that differ by at most $2$, i.e., $|x.right.h - x.left.h| \\le 2$, and alters the subtree rooted at $x$ to be height balanced. ($\\textit{Hint:}$ Use rotations.) c. Using part (b), describe a recursive procedure $\\text{AVL-INSERT}(x, z)$ that takes a node $x$ within an AVL tree and a newly created node $z$ (whose key has already been filled in), and adds $z$ to the subtree rooted at $x$, maintaining the property that $x$ is the root of an AVL tree. As in $\\text{TREE-INSERT}$ from Section 12.3, assume that $z.key$ has already been filled in and that $z.left = \\text{NIL}$ and $z.right = \\text{NIL}$; also assume that $z.h = 0$. Thus, to insert the node $z$ into the AVL tree $T$, we call $\\text{AVL-INSERT}(T.root, z)$. d. Show that $\\text{AVL-INSERT}$, run on an $n$-node AVL tree, takes $O(\\lg n)$ time and performs $O(1)$ rotations. a. Let $T(h)$ denote the minimum size of an AVL tree of height $h$. Since it is height $h$, it must have the max of it's children's heights is equal to $h - 1$. Since we are trying to get as few notes total as possible, suppose that the other child has as small of a height as is allowed. Because of the restriction of AVL trees, we have that the smaller child must be at least one less than the larger one, so, we have that $$T(h) \\ge T(h - 1) + T(h - 2) + 1,$$ where the $+1$ is coming from counting the root node. We can get inequality in the opposite direction by simply taking a tree that achieves the minimum number of number of nodes on height $h - 1$ and on $h - 2$ and join them together under another node. So, we have that $$T(h) = T(h - 1) + T(h - 2) + 1, \\text{ where } T(0) = 0, T(1) = 1.$$ This is both the same recurrence and initial conditions as the Fibonacci numbers. So, recalling equation $\\text{(3.25)}$, we have that $$T(h) = \\Big\\lfloor \\frac{\\phi^h}{\\sqrt 5} + \\frac{1}{2} \\Big\\rfloor \\le n.$$ Rearranging for $h$, we have $$ \\begin{aligned} \\frac{\\phi^h}{\\sqrt 5} - \\frac{1}{2} & \\le n \\\\ \\phi^h & \\le \\sqrt 5(n + \\frac{1}{2}) \\\\ h & \\le \\frac{\\lg \\sqrt 5 + \\lg(n + \\frac{1}{2})}{\\lg\\phi} \\in O(\\lg n). \\end{aligned} $$ b. Let $\\text{UNBAL}(x)$ denote $x.left.h - x.right.h$. Then, the algorithm $\\text{BALANCE}$ does what is desired. Note that because we are only rotating a single element at a time, the value of $\\text{UNBAL}(x)$ can only change by at most $2$ in each step. Also, it must eventually start to change as the tree that was shorter becomes saturated with elements. We also fix any breaking of the AVL property that rotating may of caused by our recursive calls to the children. BALANCE ( x ) while | UNBAL ( x ) | > 1 if UNBAL ( x ) > 0 RIGHT - ROTATE ( T , x ) else LEFT - ROTATE ( T , x ) BALANCE ( x . left ) BALANCE ( x . right ) c. For the given algorithm $\\text{AVL-INSERT}(x, z)$, it correctly maintains the fact that it is a BST by the way we search for the correct spot to insert $z$. Also, we can see that it maintains the property of being AVL, because after inserting the element, it checks all of the parents for the AVL property, since those are the only places it could of broken. It then fixes it and also updates the height attribute for any of the nodes for which it may of changed. d. Both for loops only run for $O(h) = O(\\lg(n))$ iterations. Also, only a single rotation will occur in the second while loop because when we do it, we will be decreasing the height of the subtree rooted there, which means that it's back down to what it was before, so all of it's ancestors will have unchanged heights, so, no further balancing will be required. AVL - INSERT ( x , z ) w = x while w != NIL y = w if z . key > y . key w = w . right else w = w . left if z . key > y . key y . right = z if y . left = NIL y . h = 1 else y . left = z if y . right = NIL y . h = 1 while y != x y . h = 1 + max ( y . left . h , y . right . h ) if y . left . h > y . right . h + 1 RIGHT - ROTATE ( T , y ) if y . right . h > y . left . h + 1 LEFT - ROTATE ( T , y ) y = y . p","title":"13-3 AVL trees"},{"location":"Chap13/Problems/13-4/","text":"If we insert a set of $n$ items into a binary search tree, the resulting tree may be horribly unbalanced, leading to long search times. As we saw in Section 12.4, however, randomly built binary search trees tend to be balanced. Therefore, one strategy that, on average, builds a balanced tree for a fixed set of items would be to randomly permute the items and then insert them in that order into the tree. What if we do not have all the items at once? If we receive the items one at a time, can we still randomly build a binary search tree out of them? We will examine a data structure that answers this question in the affirmative. A treap is a binary search tree with a modified way of ordering the nodes. Figure 13.9 shows an example. As usual, each node $x$ in the tree has a key value $x.key$. In addition, we assign $x.priority$, which is a random number chosen independently for each node. We assume that all priorities are distinct and also that all keys are distinct. The nodes of the treap are ordered so that the keys obey the binary-search-tree property and the priorities obey the min-heap order property: If $v$ is a left child of $u$, then $v.key < u.key$. If $v$ is a right child of $u$, then $v.key > u.key$. If $v$ is a child of $u$, then $v.priority > u.priority$. (This combination of properties is why the tree is called a \"treap\": it has features of both a binary search tree and a heap.) It helps to think of treaps in the following way. Suppose that we insert nodes $x_1, x_2, \\ldots,x_n$, with associated keys, into a treap. Then the resulting treap is the tree that would have been formed if the nodes had been inserted into a normal binary search tree in the order given by their (randomly chosen) priorities, i.e., $x_i.priority < x_j.priority$ means that we had inserted $x_i$ before $x_j$. a. Show that given a set of nodes $x_1, x_2, \\ldots, x_n$, with associated keys and priorities, all distinct, the treap associated with these nodes is unique. b. Show that the expected height of a treap is $\\Theta(\\lg n)$, and hence the expected time to search for a value in the treap is $\\Theta(\\lg n)$. Let us see how to insert a new node into an existing treap. The first thing we do is assign to the new node a random priority. Then we call the insertion algorithm, which we call $\\text{TREAP-INSERT}$, whose operation is illustrated in Figure 13.10. c. Explain how $\\text{TREAP-INSERT}$ works. Explain the idea in English and give pseudocode. ($\\textit{Hint:}$ Execute the usual binary-search-tree insertion procedure and then perform rotations to restore the min-heap order property.) d. Show that the expected running time of $\\text{TREAP-INSERT}$ is $\\Theta(\\lg n)$. $\\text{TREAP-INSERT}$ performs a search and then a sequence of rotations. Although these two operations have the same expected running time, they have different costs in practice. A search reads information from the treap without modifying it. In contrast, a rotation changes parent and child pointers within the treap. On most computers, read operations are much faster than write operations. Thus we would like $\\text{TREAP-INSERT}$ to perform few rotations. We will show that the expected number of rotations performed is bounded by a constant. In order to do so, we will need some definitions, which Figure 13.11 depicts. The left spine of a binary search tree $T$ is the simple path from the root to the node with the smallest key. In other words, the left spine is the simple path from the root that consists of only left edges. Symmetrically, the right spine of $T$ is the simple path from the root consisting of only right edges. The length of a spine is the number of nodes it contains. e. Consider the treap $T$ immediately after $\\text{TREAP-INSERT}$ has inserted node $x$. Let $C$ be the length of the right spine of the left subtree of $x$. Let $D$ be the length of the left spine of the right subtree of $x$. Prove that the total number of rotations that were performed during the insertion of $x$ is equal to $C + D$. We will now calculate the expected values of $C$ and $D$. Without loss of generality, we assume that the keys are $1, 2, \\ldots, n$ since we are comparing them only to one another. For nodes $x$ and $y$ in treap $T$, where $y \\ne x$, let $k = x.key$ and $i = y.key$. We define indicator random variables $$X_{ik} = \\text{I\\{\\$y\\$ is in the right spine of the left subtree of \\$x\\$\\}}.$$ f. Show that $X_{ik} = 1$ if and only if $y.priority > x.priority$, $y.key < x.key$, and, for every $z$ such that $y.key < z.key < x.key$, we have $y.priority < z.priority$. g. Show that $$ \\begin{aligned} \\Pr\\{X_{ik} = 1\\} & = \\frac{(k - i - 1)!}{(k - i + 1)!} \\\\ & = \\frac{1}{(k - i + 1)(k - i)}. \\\\ \\end{aligned} $$ h. Show that $$ \\begin{aligned} \\text E[C] & = \\sum_{j = 1}^{k - 1} \\frac{1}{j(j + 1)} \\\\ & = 1 - \\frac{1}{k}. \\end{aligned} $$ i. Use a symmetry argument to show that $$\\text E[D] = 1 - \\frac{1}{n - k + 1}.$$ j. Conclude that the expected number of rotations performed when inserting a node into a treap is less than $2$. a. The root is the node with smallest priority, the root divides the sets into two subsets based on the key. In each subset, the node with smallest priority is selected as the root, thus we can uniquely determine a treap with a specific input. b. For the priority of all nodes, each permutation corresponds to exactly one treap, that is, all nodes forms a BST in priority, since the priority of all nodes is spontaneous, treap is, essentially, randomly built binary search tress. Therefore, the expected height of a treap is $\\Theta(\\lg n)$. c. First insert a node as usual using the binary-search-tree insertion procedure. Then perform left and right rotations until the parent of the inserted node no longer has larger priority. d. Rotation is $\\Theta(1)$, at most $h$ rotations, therefore the expected running time is $\\Theta(\\lg n)$. e. Left rotation increase $C$ by $1$, right rotation increase $D$ by $1$. f. The first two are obvious. The min-heap property will not hold if $y.priority > z.priority$. g. $$\\Pr\\{X_{ik} = 1\\} = \\frac{(k - i - 1)!}{(k - i + 1)!} = \\frac{1}{(k - i + 1)(k - i)}.$$ h. $$ \\begin{aligned} \\text E[C] & = \\sum_{j = 1}^{k - 1} \\frac{1}{(k - i + 1)(k - i)} \\\\ & = \\sum_{j = 1}^{k - 1} (\\frac{1}{k - i} - \\frac{1}{k - i + 1}) \\\\ & = 1 - \\frac{1}{k}. \\end{aligned} $$ i. $$ \\begin{aligned} \\text E[D] & = \\sum_{j = 1}^{n - k} \\frac{1}{(k - i + 1)(k - i)} \\\\ & = 1 - \\frac{1}{n - k + 1}. \\end{aligned} $$ j. By part (e), the number of rotations is $C + D$. By linearity of expectation, $\\text E[C + D] = 2 - \\frac{1}{k} - \\frac{1}{n - k + 1} \\le 2$ for any choice of $k$.","title":"13-4 Treaps"},{"location":"Chap14/14.1/","text":"14.1-1 Show how $\\text{OS-SELECT}(T.root, 10)$ operates on the red-black tree $T$ of Figure 14.1. $26: r = 13, i = 10$, go left. $17: r = 8, i = 10$, go right. $21: r = 3, i = 2$, go left. $19: r = 1, i = 2$, go right. $20: r = 1, i = 1$, choose $20$. 14.1-2 Show how $\\text{OS-RANK}(T, x)$ operates on the red-black tree $T$ of Figure 14.1 and the node $x$ with $x.key = 35$. $35: r = 1$. $38: r = 1$. $30: r = r + 2 = 3$. $41: r = 3$. $26: r = r + 13 = 16$. 14.1-3 Write a nonrecursive version of $\\text{OS-SELECT}$. OS - SELECT ( x , i ) r = x . left . size + 1 while r != i if i < r x = x . left else x = x . right i = i - r r = x . left . size + 1 return x 14.1-4 Write a recursive procedure $\\text{OS-KEY-RANK}(T, k)$ that takes as input an order-statistic tree $T$ and a key $k$ and returns the rank of $k$ in the dynamic set represented by $T$. Assume that the keys of $T$ are distinct. OS - KEY - RANK ( T , k ) if k == T . root . key return T . root . left . size + 1 else if T . root . key > k return OS - KEY - RANK ( T . left , k ) else return T . root . left . size + 1 + OS - KEY - RANK ( T . right , k ) 14.1-5 Given an element $x$ in an $n$-node order-statistic tree and a natural number $i$, how can we determine the $i$th successor of $x$ in the linear order of the tree in $O(\\lg n)$ time? The desired result is $\\text{OS-SELECT}(T, \\text{OS-RANK}(T, x) + i)$. This has runtime $O(h)$, which by the properties of red black trees, is $O(\\lg n)$. 14.1-6 Observe that whenever we reference the size attribute of a node in either $\\text{OS-SELECT}$ or $\\text{OS-RANK}$, we use it only to compute a rank. Accordingly, suppose we store in each node its rank in the subtree of which it is the root. Show how to maintain this information during insertion and deletion. (Remember that these two operations can cause rotations.) First perform the usual BST insertion procedure on $z$, the node to be inserted. Then add $1$ to the rank of every node on the path from the root to $z$ such that $z$ is in the left subtree of that node. Since the added node is a leaf, it will have no subtrees so its rank will always be $1$. When a left rotation is performed on $x$, its rank within its subtree will remain the same. The rank of $x.right$ will be increased by the rank of $x$, plus one. If we perform a right rotation on a node $y$, its rank will decrement by $y.left.rank + 1$. The rank of $y.left$ will remain unchanged. For deletion of $z$, decrement the rank of every node on the path from $z$ to the root such that $z$ is in the left subtree of that node. For any rotations, use the same rules as before. 14.1-7 Show how to use an order-statistic tree to count the number of inversions (see Problem 2-4) in an array of size $n$ in time $O(n\\lg n)$. The runtime to build a red-black tree is $O(n\\lg n)$, so we need to calculate inversions while building trees. Every time $\\text{INSERT}$, we can use $\\text{OS-RANK}$ to calculate the rank of the node, thus calculating inversions. 14.1-8 $\\star$ Consider $n$ chords on a circle, each defined by its endpoints. Describe an $O(n\\lg n)$-time algorithm to determine the number of pairs of chords that intersect inside the circle. (For example, if the $n$ chords are all diameters that meet at the center, then the correct answer is $\\binom{n}{2}$.) Assume that no two chords share an endpoint. Sort the vertices in clock-wise order, and assign a unique value to each vertex. For each chord its two vertices are $u_i$, $v_i$ and $u_i < v_i$. Add the vertices one by one in clock-wise order, if we meet a $u_i$, we add it to the order-statistic tree, if we meet a $v_i$, we calculate how many nodes are larger than $u_i$ (which is the number of intersects with chord $i$), and remove $u_i$.","title":"14.1 Dynamic order statistics"},{"location":"Chap14/14.1/#141-1","text":"Show how $\\text{OS-SELECT}(T.root, 10)$ operates on the red-black tree $T$ of Figure 14.1. $26: r = 13, i = 10$, go left. $17: r = 8, i = 10$, go right. $21: r = 3, i = 2$, go left. $19: r = 1, i = 2$, go right. $20: r = 1, i = 1$, choose $20$.","title":"14.1-1"},{"location":"Chap14/14.1/#141-2","text":"Show how $\\text{OS-RANK}(T, x)$ operates on the red-black tree $T$ of Figure 14.1 and the node $x$ with $x.key = 35$. $35: r = 1$. $38: r = 1$. $30: r = r + 2 = 3$. $41: r = 3$. $26: r = r + 13 = 16$.","title":"14.1-2"},{"location":"Chap14/14.1/#141-3","text":"Write a nonrecursive version of $\\text{OS-SELECT}$. OS - SELECT ( x , i ) r = x . left . size + 1 while r != i if i < r x = x . left else x = x . right i = i - r r = x . left . size + 1 return x","title":"14.1-3"},{"location":"Chap14/14.1/#141-4","text":"Write a recursive procedure $\\text{OS-KEY-RANK}(T, k)$ that takes as input an order-statistic tree $T$ and a key $k$ and returns the rank of $k$ in the dynamic set represented by $T$. Assume that the keys of $T$ are distinct. OS - KEY - RANK ( T , k ) if k == T . root . key return T . root . left . size + 1 else if T . root . key > k return OS - KEY - RANK ( T . left , k ) else return T . root . left . size + 1 + OS - KEY - RANK ( T . right , k )","title":"14.1-4"},{"location":"Chap14/14.1/#141-5","text":"Given an element $x$ in an $n$-node order-statistic tree and a natural number $i$, how can we determine the $i$th successor of $x$ in the linear order of the tree in $O(\\lg n)$ time? The desired result is $\\text{OS-SELECT}(T, \\text{OS-RANK}(T, x) + i)$. This has runtime $O(h)$, which by the properties of red black trees, is $O(\\lg n)$.","title":"14.1-5"},{"location":"Chap14/14.1/#141-6","text":"Observe that whenever we reference the size attribute of a node in either $\\text{OS-SELECT}$ or $\\text{OS-RANK}$, we use it only to compute a rank. Accordingly, suppose we store in each node its rank in the subtree of which it is the root. Show how to maintain this information during insertion and deletion. (Remember that these two operations can cause rotations.) First perform the usual BST insertion procedure on $z$, the node to be inserted. Then add $1$ to the rank of every node on the path from the root to $z$ such that $z$ is in the left subtree of that node. Since the added node is a leaf, it will have no subtrees so its rank will always be $1$. When a left rotation is performed on $x$, its rank within its subtree will remain the same. The rank of $x.right$ will be increased by the rank of $x$, plus one. If we perform a right rotation on a node $y$, its rank will decrement by $y.left.rank + 1$. The rank of $y.left$ will remain unchanged. For deletion of $z$, decrement the rank of every node on the path from $z$ to the root such that $z$ is in the left subtree of that node. For any rotations, use the same rules as before.","title":"14.1-6"},{"location":"Chap14/14.1/#141-7","text":"Show how to use an order-statistic tree to count the number of inversions (see Problem 2-4) in an array of size $n$ in time $O(n\\lg n)$. The runtime to build a red-black tree is $O(n\\lg n)$, so we need to calculate inversions while building trees. Every time $\\text{INSERT}$, we can use $\\text{OS-RANK}$ to calculate the rank of the node, thus calculating inversions.","title":"14.1-7"},{"location":"Chap14/14.1/#141-8-star","text":"Consider $n$ chords on a circle, each defined by its endpoints. Describe an $O(n\\lg n)$-time algorithm to determine the number of pairs of chords that intersect inside the circle. (For example, if the $n$ chords are all diameters that meet at the center, then the correct answer is $\\binom{n}{2}$.) Assume that no two chords share an endpoint. Sort the vertices in clock-wise order, and assign a unique value to each vertex. For each chord its two vertices are $u_i$, $v_i$ and $u_i < v_i$. Add the vertices one by one in clock-wise order, if we meet a $u_i$, we add it to the order-statistic tree, if we meet a $v_i$, we calculate how many nodes are larger than $u_i$ (which is the number of intersects with chord $i$), and remove $u_i$.","title":"14.1-8 $\\star$"},{"location":"Chap14/14.2/","text":"14.2-1 Show, by adding pointers to the nodes, how to support each of the dynamic-set queries $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, and $\\text{PREDECESSOR}$ in $O(1)$worstcase time on an augmented order-statistic tree. The asymptotic performance of other operations on order-statistic trees should not be affected. MINIMUM: A pointer points to the minimum node, if the node is being deleted, move the pointer to its successor. MAXIMUM: Similar to $\\text{MINIMUM}$. SUCCESSOR: Every node records its successor, the insertion and deletion is similar to that in linked list. PREDECESSOR: Similar to $\\text{MAXIMUM}$. 14.2-2 Can we maintain the black-heights of nodes in a red-black tree as attributes in the nodes of the tree without affecting the asymptotic performance of any of the redblack tree operations? Show how, or argue why not. How about maintaining the depths of nodes? Since the black height of a node depends only on the black height and color of its children, Theorem 14.1 implies that we can maintain the attribute without affecting the asymptotic performance of the other red-black tree operations. The same is not true for maintaining the depths of nodes. If we delete the root of a tree we could potentially have to update the depths of $O(n)$ nodes, making the $\\text{DELETE}$ operation asymptotically slower than before. 14.2-3 $\\star$ Let $\\otimes$ be an associative binary operator, and let $a$ be an attribute maintained in each node of a red-black tree. Suppose that we want to include in each node $x$ an additional attribute $f$ such that $x.f = x_1.a \\otimes x_2.a \\otimes \\cdots \\otimes x_m.a$, where $x_1, x_2, \\ldots ,x_m$ is the inorder listing of nodes in the subtree rooted at $x$. Show how to update the $f$ attributes in $O(1)$ time after a rotation. Modify your argument slightly to apply it to the $size$ attributes in order-statistic trees. $x.f = x.left.f \\otimes x.a \\otimes x.right.f$. 14.2-4 $\\star$ We wish to augment red-black trees with an operation $\\text{RB-ENUMERATE}(x, a, b)$ that outputs all the keys $k$ such that $a \\le k \\le b$ in a red-black tree rooted at $x$. Describe how to implement $\\text{RB-ENUMERATE}$ in $\\Theta(m+\\lg n)$ time, where $m$ is the number of keys that are output and $n$ is the number of internal nodes in the tree. ($\\textit{Hint:}$ You do not need to add new attributes to the red-black tree.) $\\Theta(\\lg n)$: Find the smallest key that larger than or equal to $a$. $\\Theta(m)$: Based on Exercise 14.2-1, find the $m$ successor.","title":"14.2 How to augment a data structure"},{"location":"Chap14/14.2/#142-1","text":"Show, by adding pointers to the nodes, how to support each of the dynamic-set queries $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, and $\\text{PREDECESSOR}$ in $O(1)$worstcase time on an augmented order-statistic tree. The asymptotic performance of other operations on order-statistic trees should not be affected. MINIMUM: A pointer points to the minimum node, if the node is being deleted, move the pointer to its successor. MAXIMUM: Similar to $\\text{MINIMUM}$. SUCCESSOR: Every node records its successor, the insertion and deletion is similar to that in linked list. PREDECESSOR: Similar to $\\text{MAXIMUM}$.","title":"14.2-1"},{"location":"Chap14/14.2/#142-2","text":"Can we maintain the black-heights of nodes in a red-black tree as attributes in the nodes of the tree without affecting the asymptotic performance of any of the redblack tree operations? Show how, or argue why not. How about maintaining the depths of nodes? Since the black height of a node depends only on the black height and color of its children, Theorem 14.1 implies that we can maintain the attribute without affecting the asymptotic performance of the other red-black tree operations. The same is not true for maintaining the depths of nodes. If we delete the root of a tree we could potentially have to update the depths of $O(n)$ nodes, making the $\\text{DELETE}$ operation asymptotically slower than before.","title":"14.2-2"},{"location":"Chap14/14.2/#142-3-star","text":"Let $\\otimes$ be an associative binary operator, and let $a$ be an attribute maintained in each node of a red-black tree. Suppose that we want to include in each node $x$ an additional attribute $f$ such that $x.f = x_1.a \\otimes x_2.a \\otimes \\cdots \\otimes x_m.a$, where $x_1, x_2, \\ldots ,x_m$ is the inorder listing of nodes in the subtree rooted at $x$. Show how to update the $f$ attributes in $O(1)$ time after a rotation. Modify your argument slightly to apply it to the $size$ attributes in order-statistic trees. $x.f = x.left.f \\otimes x.a \\otimes x.right.f$.","title":"14.2-3 $\\star$"},{"location":"Chap14/14.2/#142-4-star","text":"We wish to augment red-black trees with an operation $\\text{RB-ENUMERATE}(x, a, b)$ that outputs all the keys $k$ such that $a \\le k \\le b$ in a red-black tree rooted at $x$. Describe how to implement $\\text{RB-ENUMERATE}$ in $\\Theta(m+\\lg n)$ time, where $m$ is the number of keys that are output and $n$ is the number of internal nodes in the tree. ($\\textit{Hint:}$ You do not need to add new attributes to the red-black tree.) $\\Theta(\\lg n)$: Find the smallest key that larger than or equal to $a$. $\\Theta(m)$: Based on Exercise 14.2-1, find the $m$ successor.","title":"14.2-4 $\\star$"},{"location":"Chap14/14.3/","text":"14.3-1 Write pseudocode for $\\text{LEFT-ROTATE}$ that operates on nodes in an interval tree and updates the $max$ attributes in $O(1)$ time. Add 2 lines in $\\text{LEFT-ROTATE}$ in 13.2 y . max = x . max x . max = max ( x . high , x . left . max , x . right . max ) 14.3-2 Rewrite the code for $\\text{INTERVAL-SEARCH}$ so that it works properly when all intervals are open. INTERVAL - SEARCH ( T , i ) x = T . root while x != T . nil and i does not overlap x . int if x . left != T . nil and x . left . max > i . low x = x . left else x = x . right return x 14.3-3 Describe an efficient algorithm that, given an interval $i$ , returns an interval overlapping $i$ that has the minimum low endpoint, or $T.nil$ if no such interval exists. Consider the usual interval search given, but, instead of breaking out of the loop as soon as we have an overlap, we just keep track of the overlap that has the minimum low endpoint, and continue the loop. After the loop terminates, we return the overlap stored. 14.3-4 Given an interval tree $T$ and an interval $i$, describe how to list all intervals in $T$ that overlap $i$ in $O(\\min(n, k \\lg n))$ time, where $k$ is the number of intervals in the output list. ($\\textit{Hint:}$ One simple method makes several queries, modifying the tree between queries. A slightly more complicated method does not modify the tree.) INTERVALS - SEARCH ( T , x , i ) let list be an empty array if i overlaps x . int list . APPEND ( x ) if x . left != T . nil and x . left . max > i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . left , i )) if x . right != T . nil and x . int . low \u2264 i . high and x . right . max \u2265 i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . right , i )) return list 14.3-5 Suggest modifications to the interval-tree procedures to support the new operation $\\text{INTERVAL-SEARCH-EXACTLY}(T, i)$, where $T$ is an interval tree and $i$ is an interval. The operation should return a pointer to a node $x$ in $T$ such that $x.int.low = i.low$ and $x.int.high = i.high$, or $T.nil$ if $T$ contains no such node. All operations, including $\\text{INTERVAL-SEARCH-EXACTLY}$, should run in $O(\\lg n)$ time on an $n$-node interval tree. Search for nodes which has exactly the same low value. INTERVAL - SEARCH - EXACTLY ( T , i ) x = T . root while x != T . nil and i not exactly overlap x if i . high > x . max x = T . nil else if i . low < x . low x = x . left else if i . low > x . low x = x . right else x = T . nil return x 14.3-6 Show how to maintain a dynamic set $Q$ of numbers that supports the operation $\\text{MIN-GAP}$, which gives the magnitude of the difference of the two closest numbers in $Q$. For example, if $Q = \\{1, 5, 9, 15, 18, 22 \\}$, then $\\text{MIN-GAP}(Q)$ returns $18 - 15 = 3$, since $15$ and $18$ are the two closest numbers in $Q$. Make the operations $\\text{INSERT}$, $\\text{DELETE}$, $\\text{SEARCH}$, and $\\text{MIN-GAP}$ as efficient as possible, and analyze their running times. Store the elements in a red-black tree, where the key value is the value of each number itself. The auxiliary attribute stored at a node $x$ will be the min gap between elements in the subtree rooted at $x$, the maximum value contained in subtree rooted at $x$, and the minimum value contained in the subtree rooted at $x$. The min gap at a leaf will be $\\infty$. Since we can determine the attributes of a node $x$ using only the information about the key at $x$, and the attributes in $x.left$ and $x.right$, Theorem 14.1 implies that we can maintain the values in all nodes of the tree during insertion and deletion without asymptotically affecting their $O(\\lg n)$ performance. For $\\text{MIN-GAP}$, just check the min gap at the root, in constant time. 14.3-7 $\\star$ VLSI databases commonly represent an integrated circuit as a list of rectangles. Assume that each rectangle is rectilinearly oriented (sides parallel to the $x$- and $y$-axes), so that we represent a rectangle by its minimum and maximum $x$ and $y$-coordinates. Give an $O(n\\lg n)$-time algorithm to decide whether or not a set of $n$ rectangles so represented contains two rectangles that overlap. Your algorithm need not report all intersecting pairs, but it must report that an overlap exists if one rectangle entirely covers another, even if the boundary lines do not intersect. ($\\textit{Hint:}$ Move a \"sweep\" line across the set of rectangles.) Let $L$ be the set of left coordinates of rectangles. Let $R$ be the set of right coordinates of rectangles. Sort both of these sets in $O(n\\lg n)$ time. Then, we will have a pointer to $L$ and a pointer to $R$. If the pointer to $L$ is smaller, call interval search on $T$ for the up-down interval corresponding to this left hand side. If it contains something that intersects the up-down bounds of this rectangle, there is an intersection, so stop. Otherwise add this interval to $T$ and increment the pointer to $L$. If $R$ is the smaller one, remove the up-down interval that that right hand side corresponds to and increment the pointer to $R$. Since all the interval tree operations used run in time $O(\\lg n)$ and we only call them at most $3n$ times, we have that the runtime is $O(n\\lg n)$.","title":"14.3 Interval trees"},{"location":"Chap14/14.3/#143-1","text":"Write pseudocode for $\\text{LEFT-ROTATE}$ that operates on nodes in an interval tree and updates the $max$ attributes in $O(1)$ time. Add 2 lines in $\\text{LEFT-ROTATE}$ in 13.2 y . max = x . max x . max = max ( x . high , x . left . max , x . right . max )","title":"14.3-1"},{"location":"Chap14/14.3/#143-2","text":"Rewrite the code for $\\text{INTERVAL-SEARCH}$ so that it works properly when all intervals are open. INTERVAL - SEARCH ( T , i ) x = T . root while x != T . nil and i does not overlap x . int if x . left != T . nil and x . left . max > i . low x = x . left else x = x . right return x","title":"14.3-2"},{"location":"Chap14/14.3/#143-3","text":"Describe an efficient algorithm that, given an interval $i$ , returns an interval overlapping $i$ that has the minimum low endpoint, or $T.nil$ if no such interval exists. Consider the usual interval search given, but, instead of breaking out of the loop as soon as we have an overlap, we just keep track of the overlap that has the minimum low endpoint, and continue the loop. After the loop terminates, we return the overlap stored.","title":"14.3-3"},{"location":"Chap14/14.3/#143-4","text":"Given an interval tree $T$ and an interval $i$, describe how to list all intervals in $T$ that overlap $i$ in $O(\\min(n, k \\lg n))$ time, where $k$ is the number of intervals in the output list. ($\\textit{Hint:}$ One simple method makes several queries, modifying the tree between queries. A slightly more complicated method does not modify the tree.) INTERVALS - SEARCH ( T , x , i ) let list be an empty array if i overlaps x . int list . APPEND ( x ) if x . left != T . nil and x . left . max > i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . left , i )) if x . right != T . nil and x . int . low \u2264 i . high and x . right . max \u2265 i . low list = list . APPEND ( INTERVALS - SEARCH ( T , x . right , i )) return list","title":"14.3-4"},{"location":"Chap14/14.3/#143-5","text":"Suggest modifications to the interval-tree procedures to support the new operation $\\text{INTERVAL-SEARCH-EXACTLY}(T, i)$, where $T$ is an interval tree and $i$ is an interval. The operation should return a pointer to a node $x$ in $T$ such that $x.int.low = i.low$ and $x.int.high = i.high$, or $T.nil$ if $T$ contains no such node. All operations, including $\\text{INTERVAL-SEARCH-EXACTLY}$, should run in $O(\\lg n)$ time on an $n$-node interval tree. Search for nodes which has exactly the same low value. INTERVAL - SEARCH - EXACTLY ( T , i ) x = T . root while x != T . nil and i not exactly overlap x if i . high > x . max x = T . nil else if i . low < x . low x = x . left else if i . low > x . low x = x . right else x = T . nil return x","title":"14.3-5"},{"location":"Chap14/14.3/#143-6","text":"Show how to maintain a dynamic set $Q$ of numbers that supports the operation $\\text{MIN-GAP}$, which gives the magnitude of the difference of the two closest numbers in $Q$. For example, if $Q = \\{1, 5, 9, 15, 18, 22 \\}$, then $\\text{MIN-GAP}(Q)$ returns $18 - 15 = 3$, since $15$ and $18$ are the two closest numbers in $Q$. Make the operations $\\text{INSERT}$, $\\text{DELETE}$, $\\text{SEARCH}$, and $\\text{MIN-GAP}$ as efficient as possible, and analyze their running times. Store the elements in a red-black tree, where the key value is the value of each number itself. The auxiliary attribute stored at a node $x$ will be the min gap between elements in the subtree rooted at $x$, the maximum value contained in subtree rooted at $x$, and the minimum value contained in the subtree rooted at $x$. The min gap at a leaf will be $\\infty$. Since we can determine the attributes of a node $x$ using only the information about the key at $x$, and the attributes in $x.left$ and $x.right$, Theorem 14.1 implies that we can maintain the values in all nodes of the tree during insertion and deletion without asymptotically affecting their $O(\\lg n)$ performance. For $\\text{MIN-GAP}$, just check the min gap at the root, in constant time.","title":"14.3-6"},{"location":"Chap14/14.3/#143-7-star","text":"VLSI databases commonly represent an integrated circuit as a list of rectangles. Assume that each rectangle is rectilinearly oriented (sides parallel to the $x$- and $y$-axes), so that we represent a rectangle by its minimum and maximum $x$ and $y$-coordinates. Give an $O(n\\lg n)$-time algorithm to decide whether or not a set of $n$ rectangles so represented contains two rectangles that overlap. Your algorithm need not report all intersecting pairs, but it must report that an overlap exists if one rectangle entirely covers another, even if the boundary lines do not intersect. ($\\textit{Hint:}$ Move a \"sweep\" line across the set of rectangles.) Let $L$ be the set of left coordinates of rectangles. Let $R$ be the set of right coordinates of rectangles. Sort both of these sets in $O(n\\lg n)$ time. Then, we will have a pointer to $L$ and a pointer to $R$. If the pointer to $L$ is smaller, call interval search on $T$ for the up-down interval corresponding to this left hand side. If it contains something that intersects the up-down bounds of this rectangle, there is an intersection, so stop. Otherwise add this interval to $T$ and increment the pointer to $L$. If $R$ is the smaller one, remove the up-down interval that that right hand side corresponds to and increment the pointer to $R$. Since all the interval tree operations used run in time $O(\\lg n)$ and we only call them at most $3n$ times, we have that the runtime is $O(n\\lg n)$.","title":"14.3-7 $\\star$"},{"location":"Chap14/Problems/14-1/","text":"Suppose that we wish to keep track of a point of maximum overlap in a set of intervals\u2014a point with the largest number of intervals in the set that overlap it. a. Show that there will always be a point of maximum overlap that is an endpoint of one of the segments. b. Design a data structure that efficiently supports the operations $\\text{INTERVAL-INSERT}$, $\\text{INTERVAL-DELETE}$, and $\\text{FIND-POM}$, which returns a point of maximum overlap. ($\\textit{Hint:}$ Keep a red-black tree of all the endpoints. Associate a value of $+1$ with each left endpoint, and associate a value of $-1$ with each right endpoint. Augment each node of the tree with some extra information to maintain the point of maximum overlap.) (Removed)","title":"14-1 Point of maximum overlap"},{"location":"Chap14/Problems/14-2/","text":"We define the Josephus problem as follows. Suppose that $n$ people form a circle and that we are given a positive integer $m \\le n$. Beginning with a designated first person, we proceed around the circle, removing every $m$th person. After each person is removed, counting continues around the circle that remains. This process continues until we have removed all $n$ people. The order in which the people are removed from the circle defines the $(n, m)$-Josephus permutation of the integers $1, 2, \\ldots, n$. For example, the $(7, 3)$-Josephus permutation is $\\langle 3, 6, 2, 7, 5, 1, 4 \\rangle$. a. Suppose that $m$ is a constant. Describe an $O(n)$-time algorithm that, given an integer $n$, outputs the $(n, m)$-Josephus permutation. b. Suppose that $m$ is not a constant. Describe an $O(n\\lg n)$-time algorithm that, given integers $n$ and $m$, outputs the $(n, m)$-Josephus permutation. (Removed)","title":"14-2 Josephus permutation"},{"location":"Chap15/15.1/","text":"15.1-1 Show that equation $\\text{(15.4)}$ follows from equation $\\text{(15.3)}$ and the initial condition $T(0) = 1$. For $n = 0$, this holds since $2^0 = 1$. For $n > 0$, substituting into the recurrence, we have $$ \\begin{aligned} T(n) & = 1 + \\sum_{j = 0}^{n - 1} 2^j \\\\ & = 1 + (2^n - 1) \\\\ & = 2^n. \\end{aligned} $$ 15.1-2 Show, by means of a counterexample, that the following \"greedy\" strategy does not always determine an optimal way to cut rods. Define the density of a rod of length $i$ to be $p_i / i$, that is, its value per inch. The greedy strategy for a rod of length $n$ cuts off a first piece of length $i$, where $1 \\le i \\le n$, having maximum density. It then continues by applying the greedy strategy to the remaining piece of length $n - i$. The counterexample: $$ \\begin{array}{c|cccc} \\text{length $i$} & 1 & 2 & 3 & 4 \\\\ \\hline \\text{price $p_i$} & 1 & 20 & 33 & 36 \\\\ p_i / i & 1 & 10 & 11 & 9 \\end{array} $$ 15.1-3 Consider a modification of the rod-cutting problem in which, in addition to a price $p_i$ for each rod, each cut incurs a fixed cost of $c$. The revenue associated with a solution is now the sum of the prices of the pieces minus the costs of making the cuts. Give a dynamic-programming algorithm to solve this modified problem. We can modify $\\text{BOTTOM-UP-CUT-ROD}$ algorithm from section 15.1 as follows: MODIFIED - CUT - ROD ( p , n , c ) let r [ 0. . n ] be a new array r [ 0 ] = 0 for j = 1 to n q = p [ j ] for i = 1 to j - 1 q = max ( q , p [ i ] + r [ j - i ] - c ) r [ j ] = q return r [ n ] We need to account for cost $c$ on every iteration of the loop in lines 5-6 but the last one, when $i = j$ (no cuts). We make the loop run to $j - 1$ instead of $j$, make sure $c$ is subtracted from the candidate revenue in line 6, then pick the greater of current best revenue $q$ and $p[j]$ (no cuts) in line 7. 15.1-4 Modify $\\text{MEMOIZED-CUT-ROD}$ to return not only the value but the actual solution, too. MEMOIZED - CUT - ROD ( p , n ) let r [ 0. . n ] and s [ 0. . n ] be new arrays for i = 0 to n r [ i ] = - \u221e ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) print \"The optimal value is\" val \"and the cuts are at\" s j = n while j > 0 print s [ j ] j = j - s [ j ] MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) if r [ n ] \u2265 0 return r [ n ] if n == 0 q = 0 else q = - \u221e for i = 1 to n ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n - i , r , s ) if q < p [ i ] + val q = p [ i ] + val s [ n ] = i r [ n ] = q return ( q , s ) 15.1-5 The Fibonacci numbers are defined by recurrence $\\text{(3.22)}$. Give an $O(n)$-time dynamic-programming algorithm to compute the nth Fibonacci number. Draw the subproblem graph. How many vertices and edges are in the graph? FIBONACCI ( n ) let fib [ 0. . n ] be a new array fib [ 0 ] = 1 fib [ 1 ] = 1 for i = 2 to n fib [ i ] = fib [ i - 1 ] + fib [ i - 2 ] return fib [ n ] There are $n + 1$ vertices in the subproblem graph, i.e., $v_0, v_1, \\dots, v_n$. For $v_0, v_1$, each has $0$ leaving edge. For $v_2, v_3, \\dots, v_n$, each has $2$ leaving edges. Thus, there are $2n - 2$ edges in the subproblem graph.","title":"15.1 Rod cutting"},{"location":"Chap15/15.1/#151-1","text":"Show that equation $\\text{(15.4)}$ follows from equation $\\text{(15.3)}$ and the initial condition $T(0) = 1$. For $n = 0$, this holds since $2^0 = 1$. For $n > 0$, substituting into the recurrence, we have $$ \\begin{aligned} T(n) & = 1 + \\sum_{j = 0}^{n - 1} 2^j \\\\ & = 1 + (2^n - 1) \\\\ & = 2^n. \\end{aligned} $$","title":"15.1-1"},{"location":"Chap15/15.1/#151-2","text":"Show, by means of a counterexample, that the following \"greedy\" strategy does not always determine an optimal way to cut rods. Define the density of a rod of length $i$ to be $p_i / i$, that is, its value per inch. The greedy strategy for a rod of length $n$ cuts off a first piece of length $i$, where $1 \\le i \\le n$, having maximum density. It then continues by applying the greedy strategy to the remaining piece of length $n - i$. The counterexample: $$ \\begin{array}{c|cccc} \\text{length $i$} & 1 & 2 & 3 & 4 \\\\ \\hline \\text{price $p_i$} & 1 & 20 & 33 & 36 \\\\ p_i / i & 1 & 10 & 11 & 9 \\end{array} $$","title":"15.1-2"},{"location":"Chap15/15.1/#151-3","text":"Consider a modification of the rod-cutting problem in which, in addition to a price $p_i$ for each rod, each cut incurs a fixed cost of $c$. The revenue associated with a solution is now the sum of the prices of the pieces minus the costs of making the cuts. Give a dynamic-programming algorithm to solve this modified problem. We can modify $\\text{BOTTOM-UP-CUT-ROD}$ algorithm from section 15.1 as follows: MODIFIED - CUT - ROD ( p , n , c ) let r [ 0. . n ] be a new array r [ 0 ] = 0 for j = 1 to n q = p [ j ] for i = 1 to j - 1 q = max ( q , p [ i ] + r [ j - i ] - c ) r [ j ] = q return r [ n ] We need to account for cost $c$ on every iteration of the loop in lines 5-6 but the last one, when $i = j$ (no cuts). We make the loop run to $j - 1$ instead of $j$, make sure $c$ is subtracted from the candidate revenue in line 6, then pick the greater of current best revenue $q$ and $p[j]$ (no cuts) in line 7.","title":"15.1-3"},{"location":"Chap15/15.1/#151-4","text":"Modify $\\text{MEMOIZED-CUT-ROD}$ to return not only the value but the actual solution, too. MEMOIZED - CUT - ROD ( p , n ) let r [ 0. . n ] and s [ 0. . n ] be new arrays for i = 0 to n r [ i ] = - \u221e ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) print \"The optimal value is\" val \"and the cuts are at\" s j = n while j > 0 print s [ j ] j = j - s [ j ] MEMOIZED - CUT - ROD - AUX ( p , n , r , s ) if r [ n ] \u2265 0 return r [ n ] if n == 0 q = 0 else q = - \u221e for i = 1 to n ( val , s ) = MEMOIZED - CUT - ROD - AUX ( p , n - i , r , s ) if q < p [ i ] + val q = p [ i ] + val s [ n ] = i r [ n ] = q return ( q , s )","title":"15.1-4"},{"location":"Chap15/15.1/#151-5","text":"The Fibonacci numbers are defined by recurrence $\\text{(3.22)}$. Give an $O(n)$-time dynamic-programming algorithm to compute the nth Fibonacci number. Draw the subproblem graph. How many vertices and edges are in the graph? FIBONACCI ( n ) let fib [ 0. . n ] be a new array fib [ 0 ] = 1 fib [ 1 ] = 1 for i = 2 to n fib [ i ] = fib [ i - 1 ] + fib [ i - 2 ] return fib [ n ] There are $n + 1$ vertices in the subproblem graph, i.e., $v_0, v_1, \\dots, v_n$. For $v_0, v_1$, each has $0$ leaving edge. For $v_2, v_3, \\dots, v_n$, each has $2$ leaving edges. Thus, there are $2n - 2$ edges in the subproblem graph.","title":"15.1-5"},{"location":"Chap15/15.2/","text":"15.2-1 Find an optimal parenthesization of a matrix-chain product whose sequence of dimensions is $\\langle 5, 10, 3, 12, 5, 50, 6 \\rangle$. $$((5 \\times 10)(10 \\times 3))(((3 \\times 12)(12 \\times 5))((5 \\times 50)(50 \\times 6))).$$ 15.2-2 Give a recursive algorithm $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j)$ that actually performs the optimal matrix-chain multiplication, given the sequence of matrices $\\langle A_1, A_2, \\ldots ,A_n \\rangle$, the $s$ table computed by $\\text{MATRIX-CHAIN-ORDER}$, and the indices $i$ and $j$. (The initial call would be $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)$.) MATRIX - CHAIN - MULTIPLY ( A , s , i , j ) if i == j return A [ i ] if i + 1 == j return A [ i ] * A [ j ] b = MATRIX - CHAIN - MULTIPLY ( A , s , i , s [ i , j ]) c = MATRIX - CHAIN - MULTIPLY ( A , s , s [ i , j ] + 1 , j ) return b * c 15.2-3 Use the substitution method to show that the solution to the recurrence $\\text{(15.6)}$ is $\\Omega(2^n)$. Suppose $P(n) \\ge c2^n$, $$ \\begin{aligned} P(n) & \\ge \\sum_{k = 1}^{n - 1} c2^k \\cdot c2^{n - k} \\\\ & = \\sum_{k = 1}^{n - 1} c^2 2^n \\\\ & = c^2 (n - 1) 2^n \\\\ & \\ge c^2 2^n & (n > 1) \\\\ & \\ge c 2^n. & (c \\ge 1) \\end{aligned} $$ 15.2-4 Describe the subproblem graph for matrix-chain multiplication with an input chain of length $n$. How many vertices does it have? How many edges does it have, and which edges are they? The vertices of the subproblem graph are the ordered pair $v_{ij}$, where $i \\le j$. If $i = j$, the vertex $v_{ij}$ has no output edge. If $i < j$, for each $k$, s.t. $i \\le k < j$, the subproblem graph contains edges $(v_{ij}, v_{ik})$ and $(v_{ij}, v_{k+1, j})$, and these edges indicate that to solve the subproblem of optimally parenthesizing the product $A_i \\cdots A_j$, we need to solve subproblems of optimally parenthesizing the products $A_i \\cdots A_k$ and $A_{k + 1} \\cdots A_j$. The number of vertices is $$\\sum_{i = 1}^n \\sum_{j = i}^n = \\frac{n(n + 1)}{2}.$$ The number of edges is $$\\sum_{i = 1}^n \\sum_{j = i}^n (j - i) = \\frac{(n - 1)n(n + 1)}{6}.$$ 15.2-5 Let $R(i, j)$ be the number of times that table entry $m[i, j]$ is referenced while computing other table entries in a call of $\\text{MATRIX-CHAIN-ORDER}$. Show that the total number of references for the entire table is $$\\sum_{i = 1}^n \\sum_{j = i}^n R(i, j) = \\frac{n^3 - n}{3}.$$ ($\\textit{Hint:}$ You may find equation $\\text{(A.3)}$ useful.) We count the number of times that we reference a different entry in $m$ than the one we are computing, that is, $2$ times the number of times that line 10 runs. $$ \\begin{aligned} \\sum_{l = 2}^n \\sum_{i = 1}^{n - l + 1} \\sum_{k = i}^{i + l - 2} 2 & = \\sum_{l = 2}^n \\sum_{i = 1}^{n - l + 1} 2(l - 1) \\\\ & = \\sum_{l = 2}^n 2(l - 1)(n - l + 1) \\\\ & = \\sum_{l = 1}^{n - 1} 2l(n - l) \\\\ & = 2n \\sum_{l = 1}^{n - 1} l - 2 \\sum_{l = 1}^{n - 1} l^2 \\\\ & = n^2(n - 1) - 2 \\cdot \\frac{(n - 1)n(2n - 1)}{6} \\\\ & = n^3 - n^2 - \\frac{2n^3 - 3n^2 + n}{3} \\\\ & = \\frac{n^3 - n}{3}. \\end{aligned} $$ 15.2-6 Show that a full parenthesization of an $n$-element expression has exactly $n - 1$ pairs of parentheses. We proceed by induction on the number of matrices. A single matrix has no pairs of parentheses. Assume that a full parenthesization of an $n$-element expression has exactly $n \u2212 1$ pairs of parentheses. Given a full parenthesization of an $(n + 1)$-element expression, there must exist some $k$ such that we first multiply $B = A_1 \\cdots A_k$ in some way, then multiply $C = A_{k + 1} \\cdots A_{n + 1}$ in some way, then multiply $B$ and $C$. By our induction hypothesis, we have $k \u2212 1$ pairs of parentheses for the full parenthesization of $B$ and $n + 1 \u2212 k \u2212 1$ pairs of parentheses for the full parenthesization of $C$. Adding these together, plus the pair of outer parentheses for the entire expression, yields $k - 1 + n + 1 - k - 1 + 1 = (n + 1) - 1$ parentheses, as desired.","title":"15.2 Matrix-chain multiplication"},{"location":"Chap15/15.2/#152-1","text":"Find an optimal parenthesization of a matrix-chain product whose sequence of dimensions is $\\langle 5, 10, 3, 12, 5, 50, 6 \\rangle$. $$((5 \\times 10)(10 \\times 3))(((3 \\times 12)(12 \\times 5))((5 \\times 50)(50 \\times 6))).$$","title":"15.2-1"},{"location":"Chap15/15.2/#152-2","text":"Give a recursive algorithm $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, i, j)$ that actually performs the optimal matrix-chain multiplication, given the sequence of matrices $\\langle A_1, A_2, \\ldots ,A_n \\rangle$, the $s$ table computed by $\\text{MATRIX-CHAIN-ORDER}$, and the indices $i$ and $j$. (The initial call would be $\\text{MATRIX-CHAIN-MULTIPLY}(A, s, 1, n)$.) MATRIX - CHAIN - MULTIPLY ( A , s , i , j ) if i == j return A [ i ] if i + 1 == j return A [ i ] * A [ j ] b = MATRIX - CHAIN - MULTIPLY ( A , s , i , s [ i , j ]) c = MATRIX - CHAIN - MULTIPLY ( A , s , s [ i , j ] + 1 , j ) return b * c","title":"15.2-2"},{"location":"Chap15/15.2/#152-3","text":"Use the substitution method to show that the solution to the recurrence $\\text{(15.6)}$ is $\\Omega(2^n)$. Suppose $P(n) \\ge c2^n$, $$ \\begin{aligned} P(n) & \\ge \\sum_{k = 1}^{n - 1} c2^k \\cdot c2^{n - k} \\\\ & = \\sum_{k = 1}^{n - 1} c^2 2^n \\\\ & = c^2 (n - 1) 2^n \\\\ & \\ge c^2 2^n & (n > 1) \\\\ & \\ge c 2^n. & (c \\ge 1) \\end{aligned} $$","title":"15.2-3"},{"location":"Chap15/15.2/#152-4","text":"Describe the subproblem graph for matrix-chain multiplication with an input chain of length $n$. How many vertices does it have? How many edges does it have, and which edges are they? The vertices of the subproblem graph are the ordered pair $v_{ij}$, where $i \\le j$. If $i = j$, the vertex $v_{ij}$ has no output edge. If $i < j$, for each $k$, s.t. $i \\le k < j$, the subproblem graph contains edges $(v_{ij}, v_{ik})$ and $(v_{ij}, v_{k+1, j})$, and these edges indicate that to solve the subproblem of optimally parenthesizing the product $A_i \\cdots A_j$, we need to solve subproblems of optimally parenthesizing the products $A_i \\cdots A_k$ and $A_{k + 1} \\cdots A_j$. The number of vertices is $$\\sum_{i = 1}^n \\sum_{j = i}^n = \\frac{n(n + 1)}{2}.$$ The number of edges is $$\\sum_{i = 1}^n \\sum_{j = i}^n (j - i) = \\frac{(n - 1)n(n + 1)}{6}.$$","title":"15.2-4"},{"location":"Chap15/15.2/#152-5","text":"Let $R(i, j)$ be the number of times that table entry $m[i, j]$ is referenced while computing other table entries in a call of $\\text{MATRIX-CHAIN-ORDER}$. Show that the total number of references for the entire table is $$\\sum_{i = 1}^n \\sum_{j = i}^n R(i, j) = \\frac{n^3 - n}{3}.$$ ($\\textit{Hint:}$ You may find equation $\\text{(A.3)}$ useful.) We count the number of times that we reference a different entry in $m$ than the one we are computing, that is, $2$ times the number of times that line 10 runs. $$ \\begin{aligned} \\sum_{l = 2}^n \\sum_{i = 1}^{n - l + 1} \\sum_{k = i}^{i + l - 2} 2 & = \\sum_{l = 2}^n \\sum_{i = 1}^{n - l + 1} 2(l - 1) \\\\ & = \\sum_{l = 2}^n 2(l - 1)(n - l + 1) \\\\ & = \\sum_{l = 1}^{n - 1} 2l(n - l) \\\\ & = 2n \\sum_{l = 1}^{n - 1} l - 2 \\sum_{l = 1}^{n - 1} l^2 \\\\ & = n^2(n - 1) - 2 \\cdot \\frac{(n - 1)n(2n - 1)}{6} \\\\ & = n^3 - n^2 - \\frac{2n^3 - 3n^2 + n}{3} \\\\ & = \\frac{n^3 - n}{3}. \\end{aligned} $$","title":"15.2-5"},{"location":"Chap15/15.2/#152-6","text":"Show that a full parenthesization of an $n$-element expression has exactly $n - 1$ pairs of parentheses. We proceed by induction on the number of matrices. A single matrix has no pairs of parentheses. Assume that a full parenthesization of an $n$-element expression has exactly $n \u2212 1$ pairs of parentheses. Given a full parenthesization of an $(n + 1)$-element expression, there must exist some $k$ such that we first multiply $B = A_1 \\cdots A_k$ in some way, then multiply $C = A_{k + 1} \\cdots A_{n + 1}$ in some way, then multiply $B$ and $C$. By our induction hypothesis, we have $k \u2212 1$ pairs of parentheses for the full parenthesization of $B$ and $n + 1 \u2212 k \u2212 1$ pairs of parentheses for the full parenthesization of $C$. Adding these together, plus the pair of outer parentheses for the entire expression, yields $k - 1 + n + 1 - k - 1 + 1 = (n + 1) - 1$ parentheses, as desired.","title":"15.2-6"},{"location":"Chap15/15.3/","text":"15.3-1 Which is a more efficient way to determine the optimal number of multiplications in a matrix-chain multiplication problem: enumerating all the ways of parenthesizing the product and computing the number of multiplications for each, or running $\\text{RECURSIVE-MATRIX-CHAIN}$? Justify your answer. Running $\\text{RECURSIVE-MATRIX-CHAIN}$ is asymptotically more efficient than enumerating all the ways of parenthesizing the product and computing the number of multiplications for each. Consider the treatment of subproblems by each approach: For each possible place to split the matrix chain, the enumeration approach finds all ways to parenthesize the left half, finds all ways to parenthesize the right half, and looks at all possible combinations of the left half with the right half. The amount of work to look at each combination of left and right half subproblem results is thus the product of the number of ways to parenthesize the left half and the number of ways to parenthesize the right half. For each possible place to split the matrix chain, $\\text{RECURSIVE-MATRIX-CHAIN}$ finds the best way to parenthesize the left half, finds the best way to parenthesize the right half, and combines just those two results. Thus the amount of work to combine the left and right half subproblem results is $O(1)$. 15.3-2 Draw the recursion tree for the $\\text{MERGE-SORT}$ procedure from Section 2.3.1 on an array of $16$ elements. Explain why memoization fails to speed up a good divide-and-conquer algorithm such as $\\text{MERGE-SORT}$. Draw a recursion tree. The $\\text{MERGE-SORT}$ procedure performs at most a single call to any pair of indices of the array that is being sorted. In other words, the subproblems do not overlap and therefore memoization will not improve the running time. 15.3-3 Consider a variant of the matrix-chain multiplication problem in which the goal is to parenthesize the sequence of matrices so as to maximize, rather than minimize, the number of scalar multiplications. Does this problem exhibit optimal substructure? Yes, this problem also exhibits optimal substructure. If we know that we need the subproduct $(A_l \\cdot A_r)$, then we should still find the most expensive way to compute it \u2014 otherwise, we could do better by substituting in the most expensive way. 15.3-4 As stated, in dynamic programming we first solve the subproblems and then choose which of them to use in an optimal solution to the problem. Professor Capulet claims that we do not always need to solve all the subproblems in order to find an optimal solution. She suggests that we can find an optimal solution to the matrix-chain multiplication problem by always choosing the matrix $A_k$ at which to split the subproduct $A_i A_{i + 1} \\cdots A_j$ (by selecting $k$ to minimize the quantity $p_{i - 1} p_k p_j$) before solving the subproblems. Find an instance of the matrix-chain multiplication problem for which this greedy approach yields a suboptimal solution. Suppose that we are given matrices $A_1$, $A_2$, $A_3$, and $A_4$ with dimensions such that $$p_0, p_1, p_2, p_3, p_4 = 1000, 100, 20, 10, 1000.$$ Then $p_0 p_k p_4$ is minimized when $k = 3$, so we need to solve the subproblem of multiplying $A_1 A_2 A_3$, and also $A_4$ which is solved automatically. By her algorithm, this is solved by splitting at $k = 2$. Thus, the full parenthesization is $(((A_1A_2)A_3)A_4)$. This requires $$1000 \\cdot 100 \\cdot 20 + 1000 \\cdot 20 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 12200000$$ scalar multiplications. On the other hand, suppose we had fully parenthesized the matrices to multiply as $((A_1(A_2A_3))A_4)$. Then we would only require $$100 \\cdot 20 \\cdot 10 + 1000 \\cdot 100 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 11020000$$ scalar multiplications, which is fewer than Professor Capulet's method. Therefore her greedy approach yields a suboptimal solution. 15.3-5 Suppose that in the rod-cutting problem of Section 15.1, we also had limit $l_i$ on the number of pieces of length $i$ that we are allowed to produce, for $i = 1, 2, \\ldots, n$. Show that the optimal-substructure property described in Section 15.1 no longer holds. The optimal substructure property doesn't hold because the number of pieces of length $i$ used on one side of the cut affects the number allowed on the other. That is, there is information about the particular solution on one side of the cut that changes what is allowed on the other. To make this more concrete, suppose the rod was length $4$, the values were $l_1 = 2, l_2 = l_3 = l_4 = 1$, and each piece has the same worth regardless of length. Then, if we make our first cut in the middle, we have that the optimal solution for the two rods left over is to cut it in the middle, which isn't allowed because it increases the total number of rods of length $1$ to be too large. 15.3-6 Imagine that you wish to exchange one currency for another. You realize that instead of directly exchanging one currency for another, you might be better off making a series of trades through other currencies, winding up with the currency you want. Suppose that you can trade $n$ different currencies, numbered $1, 2, \\ldots, n$, where you start with currency $1$ and wish to wind up with currency $n$. You are given, for each pair of currencies $i$ and $j$ , an exchange rate $r_{ij}$, meaning that if you start with $d$ units of currency $i$ , you can trade for $dr_{ij}$ units of currency $j$. A sequence of trades may entail a commission, which depends on the number of trades you make. Let $c_k$ be the commission that you are charged when you make $k$ trades. Show that, if $c_k = 0$ for all $k = 1, 2, \\ldots, n$, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ exhibits optimal substructure. Then show that if commissions $c_k$ are arbitrary values, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ does not necessarily exhibit optimal substructure. First we assume that the commission is always zero. Let $k$ denote a currency which appears in an optimal sequence $s$ of trades to go from currency $1$ to currency $n$. $p_k$ denote the first part of this sequence which changes currencies from $1$ to $k$ and $q_k$ denote the rest of the sequence. Then $p_k$ and $q_k$ are both optimal sequences for changing from $1$ to $k$ and $k$ to $n$ respectively. To see this, suppose that $p_k$ wasn't optimal but that $p_k'$ was. Then by changing currencies according to the sequence $p_k'q_k$ we would have a sequence of changes which is better than $s$, a contradiction since $s$ was optimal. The same argument applies to $q_k$. Now suppose that the commissions can take on arbitrary values. Suppose we have currencies $1$ through $6$, and $r_{12} = r_{23} = r_{34} = r_{45} = 2$, $r_{13} = r_{35} = 6$, and all other exchanges are such that $r_{ij} = 100$. Let $c_1 = 0$, $c_2 = 1$, and $c_k = 10$ for $k \\ge 3$. The optimal solution in this setup is to change $1$ to $3$, then $3$ to $5$, for a total cost of $13$. An optimal solution for changing $1$ to $3$ involves changing $1$ to $2$ then $2$ to $3$, for a cost of $5$, and an optimal solution for changing $3$ to $5$ is to change $3$ to $4$ then $4$ to $5$, for a total cost of $5$. However, combining these optimal solutions to subproblems means making more exchanges overall, and the total cost of combining them is $18$, which is not optimal.","title":"15.3 Elements of dynamic programming"},{"location":"Chap15/15.3/#153-1","text":"Which is a more efficient way to determine the optimal number of multiplications in a matrix-chain multiplication problem: enumerating all the ways of parenthesizing the product and computing the number of multiplications for each, or running $\\text{RECURSIVE-MATRIX-CHAIN}$? Justify your answer. Running $\\text{RECURSIVE-MATRIX-CHAIN}$ is asymptotically more efficient than enumerating all the ways of parenthesizing the product and computing the number of multiplications for each. Consider the treatment of subproblems by each approach: For each possible place to split the matrix chain, the enumeration approach finds all ways to parenthesize the left half, finds all ways to parenthesize the right half, and looks at all possible combinations of the left half with the right half. The amount of work to look at each combination of left and right half subproblem results is thus the product of the number of ways to parenthesize the left half and the number of ways to parenthesize the right half. For each possible place to split the matrix chain, $\\text{RECURSIVE-MATRIX-CHAIN}$ finds the best way to parenthesize the left half, finds the best way to parenthesize the right half, and combines just those two results. Thus the amount of work to combine the left and right half subproblem results is $O(1)$.","title":"15.3-1"},{"location":"Chap15/15.3/#153-2","text":"Draw the recursion tree for the $\\text{MERGE-SORT}$ procedure from Section 2.3.1 on an array of $16$ elements. Explain why memoization fails to speed up a good divide-and-conquer algorithm such as $\\text{MERGE-SORT}$. Draw a recursion tree. The $\\text{MERGE-SORT}$ procedure performs at most a single call to any pair of indices of the array that is being sorted. In other words, the subproblems do not overlap and therefore memoization will not improve the running time.","title":"15.3-2"},{"location":"Chap15/15.3/#153-3","text":"Consider a variant of the matrix-chain multiplication problem in which the goal is to parenthesize the sequence of matrices so as to maximize, rather than minimize, the number of scalar multiplications. Does this problem exhibit optimal substructure? Yes, this problem also exhibits optimal substructure. If we know that we need the subproduct $(A_l \\cdot A_r)$, then we should still find the most expensive way to compute it \u2014 otherwise, we could do better by substituting in the most expensive way.","title":"15.3-3"},{"location":"Chap15/15.3/#153-4","text":"As stated, in dynamic programming we first solve the subproblems and then choose which of them to use in an optimal solution to the problem. Professor Capulet claims that we do not always need to solve all the subproblems in order to find an optimal solution. She suggests that we can find an optimal solution to the matrix-chain multiplication problem by always choosing the matrix $A_k$ at which to split the subproduct $A_i A_{i + 1} \\cdots A_j$ (by selecting $k$ to minimize the quantity $p_{i - 1} p_k p_j$) before solving the subproblems. Find an instance of the matrix-chain multiplication problem for which this greedy approach yields a suboptimal solution. Suppose that we are given matrices $A_1$, $A_2$, $A_3$, and $A_4$ with dimensions such that $$p_0, p_1, p_2, p_3, p_4 = 1000, 100, 20, 10, 1000.$$ Then $p_0 p_k p_4$ is minimized when $k = 3$, so we need to solve the subproblem of multiplying $A_1 A_2 A_3$, and also $A_4$ which is solved automatically. By her algorithm, this is solved by splitting at $k = 2$. Thus, the full parenthesization is $(((A_1A_2)A_3)A_4)$. This requires $$1000 \\cdot 100 \\cdot 20 + 1000 \\cdot 20 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 12200000$$ scalar multiplications. On the other hand, suppose we had fully parenthesized the matrices to multiply as $((A_1(A_2A_3))A_4)$. Then we would only require $$100 \\cdot 20 \\cdot 10 + 1000 \\cdot 100 \\cdot 10 + 1000 \\cdot 10 \\cdot 1000 = 11020000$$ scalar multiplications, which is fewer than Professor Capulet's method. Therefore her greedy approach yields a suboptimal solution.","title":"15.3-4"},{"location":"Chap15/15.3/#153-5","text":"Suppose that in the rod-cutting problem of Section 15.1, we also had limit $l_i$ on the number of pieces of length $i$ that we are allowed to produce, for $i = 1, 2, \\ldots, n$. Show that the optimal-substructure property described in Section 15.1 no longer holds. The optimal substructure property doesn't hold because the number of pieces of length $i$ used on one side of the cut affects the number allowed on the other. That is, there is information about the particular solution on one side of the cut that changes what is allowed on the other. To make this more concrete, suppose the rod was length $4$, the values were $l_1 = 2, l_2 = l_3 = l_4 = 1$, and each piece has the same worth regardless of length. Then, if we make our first cut in the middle, we have that the optimal solution for the two rods left over is to cut it in the middle, which isn't allowed because it increases the total number of rods of length $1$ to be too large.","title":"15.3-5"},{"location":"Chap15/15.3/#153-6","text":"Imagine that you wish to exchange one currency for another. You realize that instead of directly exchanging one currency for another, you might be better off making a series of trades through other currencies, winding up with the currency you want. Suppose that you can trade $n$ different currencies, numbered $1, 2, \\ldots, n$, where you start with currency $1$ and wish to wind up with currency $n$. You are given, for each pair of currencies $i$ and $j$ , an exchange rate $r_{ij}$, meaning that if you start with $d$ units of currency $i$ , you can trade for $dr_{ij}$ units of currency $j$. A sequence of trades may entail a commission, which depends on the number of trades you make. Let $c_k$ be the commission that you are charged when you make $k$ trades. Show that, if $c_k = 0$ for all $k = 1, 2, \\ldots, n$, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ exhibits optimal substructure. Then show that if commissions $c_k$ are arbitrary values, then the problem of finding the best sequence of exchanges from currency $1$ to currency $n$ does not necessarily exhibit optimal substructure. First we assume that the commission is always zero. Let $k$ denote a currency which appears in an optimal sequence $s$ of trades to go from currency $1$ to currency $n$. $p_k$ denote the first part of this sequence which changes currencies from $1$ to $k$ and $q_k$ denote the rest of the sequence. Then $p_k$ and $q_k$ are both optimal sequences for changing from $1$ to $k$ and $k$ to $n$ respectively. To see this, suppose that $p_k$ wasn't optimal but that $p_k'$ was. Then by changing currencies according to the sequence $p_k'q_k$ we would have a sequence of changes which is better than $s$, a contradiction since $s$ was optimal. The same argument applies to $q_k$. Now suppose that the commissions can take on arbitrary values. Suppose we have currencies $1$ through $6$, and $r_{12} = r_{23} = r_{34} = r_{45} = 2$, $r_{13} = r_{35} = 6$, and all other exchanges are such that $r_{ij} = 100$. Let $c_1 = 0$, $c_2 = 1$, and $c_k = 10$ for $k \\ge 3$. The optimal solution in this setup is to change $1$ to $3$, then $3$ to $5$, for a total cost of $13$. An optimal solution for changing $1$ to $3$ involves changing $1$ to $2$ then $2$ to $3$, for a cost of $5$, and an optimal solution for changing $3$ to $5$ is to change $3$ to $4$ then $4$ to $5$, for a total cost of $5$. However, combining these optimal solutions to subproblems means making more exchanges overall, and the total cost of combining them is $18$, which is not optimal.","title":"15.3-6"},{"location":"Chap15/15.4/","text":"15.4-1 Determine an $\\text{LCS}$ of $\\langle 1, 0, 0, 1, 0, 1, 0, 1 \\rangle$ and $\\langle 0, 1, 0, 1, 1, 0, 1, 1, 0 \\rangle$. $\\langle 1, 0, 0, 1, 1, 0 \\rangle$ or $\\langle 1, 0, 1, 0, 1, 0 \\rangle$. 15.4-2 Give pseudocode to reconstruct an $\\text{LCS}$ from the completed $c$ table and the original sequences $X = \\langle x_1, x_2, \\ldots, x_m \\rangle$ and $Y = \\langle y_1, y_2, \\ldots, y_n \\rangle$ in $O(m + n)$ time, without using the $b$ table. PRINT - LCS ( c , X , Y , i , j ) if c [ i , j ] == 0 return if X [ i ] == Y [ j ] PRINT - LCS ( c , X , Y , i - 1 , j - 1 ) print X [ i ] else if c [ i - 1 , j ] > c [ i , j - 1 ] PRINT - LCS ( c , X , Y , i - 1 , j ) else PRINT - LCS ( c , X , Y , i , j - 1 ) 15.4-3 Give a memoized version of $\\text{LCS-LENGTH}$ that runs in $O(mn)$ time. MEMOIZED - LCS - LENGTH ( X , Y , i , j ) if c [ i , j ] > -1 return c [ i , j ] if i == 0 or j == 0 return c [ i , j ] = 0 if x [ i ] == y [ j ] return c [ i , j ] = LCS - LENGTH ( X , Y , i - 1 , j - 1 ) + 1 return c [ i , j ] = max ( LCS - LENGTH ( X , Y , i - 1 , j ), LCS - LENGTH ( X , Y , i , j - 1 )) 15.4-4 Show how to compute the length of an $\\text{LCS}$ using only $2 \\cdot \\min(m, n)$ entries in the $c$ table plus $O(1)$ additional space. Then show how to do the same thing, but using $\\min(m, n)$ entries plus $O(1)$ additional space. Since we only use the previous row of the $c$ table to compute the current row, we compute as normal, but when we go to compute row $k$, we free row $k - 2$ since we will never need it again to compute the length. To use even less space, observe that to compute $c[i, j]$, all we need are the entries $c[i \u2212 1, j]$, $c[i \u2212 1, j \u2212 1]$, and $c[i, j \u2212 1]$. Thus, we can free up entry-by-entry those from the previous row which we will never need again, reducing the space requirement to $\\min(m, n)$. Computing the next entry from the three that it depends on takes $O(1)$ time and space. 15.4-5 Give an $O(n^2)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. Given a list of numbers $L$, make a copy of $L$ called $L'$ and then sort $L'$. PRINT - LCS ( c , X , Y ) n = c [ X . length , Y . length ] let s [ 1. . n ] be a new array i = X . length j = Y . length while i > 0 and j > 0 if x [ i ] == y [ j ] s [ n ] = x [ i ] n = n - 1 i = i - 1 j = j - 1 else if c [ i - 1 , j ] \u2265 c [ i , j - 1 ] i = i - 1 else j = j - 1 for i = 1 to s . length print s [ i ] MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) m = | X | n = | Y | if c [ m , n ] != 0 or m == 0 or n == 0 return if x [ m ] == y [ n ] b [ m , n ] = \u2196 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y [ 1. . n - 1 ], c , b ) + 1 else if MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) \u2265 MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) b [ m , n ] = \u2191 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) else b [ m , n ] = \u2190 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) MEMO - LCS - LENGTH ( X , Y ) let c [ 1. . | X | , 1. . | Y | ] and b [ 1. . | X | , 1. . | Y | ] be new tables MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) return c and b Then, just run the $\\text{LCS}$ algorithm on these two lists. The longest common subsequence must be monotone increasing because it is a subsequence of $L'$ which is sorted. It is also the longest monotone increasing subsequence because being a subsequence of $L'$ only adds the restriction that the subsequence must be monotone increasing. Since $|L| = |L'| = n$, and sorting $L$ can be done in $o(n^2)$ time, the final running time will be $O(|L||L'|) = O(n^2)$. 15.4-6 $\\star$ Give an $O(n\\lg n)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. ($\\textit{Hint:}$ Observe that the last element of a candidate subsequence of length $i$ is at least as large as the last element of a candidate subsequence of length $i - 1$. Maintain candidate subsequences by linking them through the input sequence.) The algorithm $\\text{LONG-MONOTONIC}(A)$ returns the longest monotonically increasing subsequence of $A$, where $A$ has length $n$. The algorithm works as follows: a new array B will be created such that $B[i]$ contains the last value of a longest monotonically increasing subsequence of length $i$. A new array $C$ will be such that $C[i]$ contains the monotonically increasing subsequence of length $i$ with smallest last element seen so far. To analyze the runtime, observe that the entries of $B$ are in sorted order, so we can execute line 9 in $O(\\lg n)$ time. Since every other line in the for-loop takes constant time, the total run-time is $O(n\\lg n)$. LONG - MONOTONIC ( A ) let B [ 1. . n ] be a new array where every value = \u221e let C [ 1. . n ] be a new array L = 1 for i = 1 to n if A [ i ] < B [ 1 ] B [ 1 ] = A [ i ] C [ 1 ]. head . key = A [ i ] else let j be the largest index of B such that B [ j ] < A [ i ] B [ j + 1 ] = A [ i ] C [ j + 1 ] = C [ j ] INSERT ( C [ j + 1 ], A [ i ]) if j + 1 > L L = L + 1 print C [ L ]","title":"15.4 Longest common subsequence"},{"location":"Chap15/15.4/#154-1","text":"Determine an $\\text{LCS}$ of $\\langle 1, 0, 0, 1, 0, 1, 0, 1 \\rangle$ and $\\langle 0, 1, 0, 1, 1, 0, 1, 1, 0 \\rangle$. $\\langle 1, 0, 0, 1, 1, 0 \\rangle$ or $\\langle 1, 0, 1, 0, 1, 0 \\rangle$.","title":"15.4-1"},{"location":"Chap15/15.4/#154-2","text":"Give pseudocode to reconstruct an $\\text{LCS}$ from the completed $c$ table and the original sequences $X = \\langle x_1, x_2, \\ldots, x_m \\rangle$ and $Y = \\langle y_1, y_2, \\ldots, y_n \\rangle$ in $O(m + n)$ time, without using the $b$ table. PRINT - LCS ( c , X , Y , i , j ) if c [ i , j ] == 0 return if X [ i ] == Y [ j ] PRINT - LCS ( c , X , Y , i - 1 , j - 1 ) print X [ i ] else if c [ i - 1 , j ] > c [ i , j - 1 ] PRINT - LCS ( c , X , Y , i - 1 , j ) else PRINT - LCS ( c , X , Y , i , j - 1 )","title":"15.4-2"},{"location":"Chap15/15.4/#154-3","text":"Give a memoized version of $\\text{LCS-LENGTH}$ that runs in $O(mn)$ time. MEMOIZED - LCS - LENGTH ( X , Y , i , j ) if c [ i , j ] > -1 return c [ i , j ] if i == 0 or j == 0 return c [ i , j ] = 0 if x [ i ] == y [ j ] return c [ i , j ] = LCS - LENGTH ( X , Y , i - 1 , j - 1 ) + 1 return c [ i , j ] = max ( LCS - LENGTH ( X , Y , i - 1 , j ), LCS - LENGTH ( X , Y , i , j - 1 ))","title":"15.4-3"},{"location":"Chap15/15.4/#154-4","text":"Show how to compute the length of an $\\text{LCS}$ using only $2 \\cdot \\min(m, n)$ entries in the $c$ table plus $O(1)$ additional space. Then show how to do the same thing, but using $\\min(m, n)$ entries plus $O(1)$ additional space. Since we only use the previous row of the $c$ table to compute the current row, we compute as normal, but when we go to compute row $k$, we free row $k - 2$ since we will never need it again to compute the length. To use even less space, observe that to compute $c[i, j]$, all we need are the entries $c[i \u2212 1, j]$, $c[i \u2212 1, j \u2212 1]$, and $c[i, j \u2212 1]$. Thus, we can free up entry-by-entry those from the previous row which we will never need again, reducing the space requirement to $\\min(m, n)$. Computing the next entry from the three that it depends on takes $O(1)$ time and space.","title":"15.4-4"},{"location":"Chap15/15.4/#154-5","text":"Give an $O(n^2)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. Given a list of numbers $L$, make a copy of $L$ called $L'$ and then sort $L'$. PRINT - LCS ( c , X , Y ) n = c [ X . length , Y . length ] let s [ 1. . n ] be a new array i = X . length j = Y . length while i > 0 and j > 0 if x [ i ] == y [ j ] s [ n ] = x [ i ] n = n - 1 i = i - 1 j = j - 1 else if c [ i - 1 , j ] \u2265 c [ i , j - 1 ] i = i - 1 else j = j - 1 for i = 1 to s . length print s [ i ] MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) m = | X | n = | Y | if c [ m , n ] != 0 or m == 0 or n == 0 return if x [ m ] == y [ n ] b [ m , n ] = \u2196 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y [ 1. . n - 1 ], c , b ) + 1 else if MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) \u2265 MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) b [ m , n ] = \u2191 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X [ 1. . m - 1 ], Y , c , b ) else b [ m , n ] = \u2190 c [ m , n ] = MEMO - LCS - LENGTH - AUX ( X , Y [ 1. . n - 1 ], c , b ) MEMO - LCS - LENGTH ( X , Y ) let c [ 1. . | X | , 1. . | Y | ] and b [ 1. . | X | , 1. . | Y | ] be new tables MEMO - LCS - LENGTH - AUX ( X , Y , c , b ) return c and b Then, just run the $\\text{LCS}$ algorithm on these two lists. The longest common subsequence must be monotone increasing because it is a subsequence of $L'$ which is sorted. It is also the longest monotone increasing subsequence because being a subsequence of $L'$ only adds the restriction that the subsequence must be monotone increasing. Since $|L| = |L'| = n$, and sorting $L$ can be done in $o(n^2)$ time, the final running time will be $O(|L||L'|) = O(n^2)$.","title":"15.4-5"},{"location":"Chap15/15.4/#154-6-star","text":"Give an $O(n\\lg n)$-time algorithm to find the longest monotonically increasing subsequence of a sequence of $n$ numbers. ($\\textit{Hint:}$ Observe that the last element of a candidate subsequence of length $i$ is at least as large as the last element of a candidate subsequence of length $i - 1$. Maintain candidate subsequences by linking them through the input sequence.) The algorithm $\\text{LONG-MONOTONIC}(A)$ returns the longest monotonically increasing subsequence of $A$, where $A$ has length $n$. The algorithm works as follows: a new array B will be created such that $B[i]$ contains the last value of a longest monotonically increasing subsequence of length $i$. A new array $C$ will be such that $C[i]$ contains the monotonically increasing subsequence of length $i$ with smallest last element seen so far. To analyze the runtime, observe that the entries of $B$ are in sorted order, so we can execute line 9 in $O(\\lg n)$ time. Since every other line in the for-loop takes constant time, the total run-time is $O(n\\lg n)$. LONG - MONOTONIC ( A ) let B [ 1. . n ] be a new array where every value = \u221e let C [ 1. . n ] be a new array L = 1 for i = 1 to n if A [ i ] < B [ 1 ] B [ 1 ] = A [ i ] C [ 1 ]. head . key = A [ i ] else let j be the largest index of B such that B [ j ] < A [ i ] B [ j + 1 ] = A [ i ] C [ j + 1 ] = C [ j ] INSERT ( C [ j + 1 ], A [ i ]) if j + 1 > L L = L + 1 print C [ L ]","title":"15.4-6 $\\star$"},{"location":"Chap15/15.5/","text":"15.5-1 Write pseudocode for the procedure $\\text{CONSTRUCT-OPTIMAL-BST}(root)$ which, given the table $root$, outputs the structure of an optimal binary search tree. For the example in Figure 15.10, your procedure should print out the structure $$ \\begin{aligned} & \\text{$k_2$ is the root} \\\\ & \\text{$k_1$ is the left child of $k_2$} \\\\ & \\text{$d_0$ is the left child of $k_1$} \\\\ & \\text{$d_1$ is the right child of $k_1$} \\\\ & \\text{$k_5$ is the right child of $k_2$} \\\\ & \\text{$k_4$ is the left child of $k_5$} \\\\ & \\text{$k_3$ is the left child of $k_4$} \\\\ & \\text{$d_2$ is the left child of $k_3$} \\\\ & \\text{$d_3$ is the right child of $k_3$} \\\\ & \\text{$d_4$ is the right child of $k_4$} \\\\ & \\text{$d_5$ is the right child of $k_5$} \\end{aligned} $$ corresponding to the optimal binary search tree shown in Figure 15.9(b). CONSTRUCT - OPTIMAL - BST ( root , i , j , last ) if i == j return if last == 0 print root [ i , j ] + \"is the root\" else if j < last print root [ i , j ] + \"is the left child of\" + last else print root [ i , j ] + \"is the right child of\" + last CONSTRUCT - OPTIMAL - BST ( root , i , root [ i , j ] - 1 , root [ i , j ]) CONSTRUCT - OPTIMAL - BST ( root , root [ i , j ] + 1 , j , root [ i , j ]) 15.5-2 Determine the cost and structure of an optimal binary search tree for a set of $n = 7$ keys with the following probabilities $$ \\begin{array}{c|cccccccc} i & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline p_i & & 0.04 & 0.06 & 0.08 & 0.02 & 0.10 & 0.12 & 0.14 \\\\ q_i & 0.06 & 0.06 & 0.06 & 0.06 & 0.05 & 0.05 & 0.05 & 0.05 \\end{array} $$ Cost is $3.12$. $$ \\begin{aligned} & \\text{$k_5$ is the root} \\\\ & \\text{$k_2$ is the left child of $k_5$} \\\\ & \\text{$k_1$ is the left child of $k_2$} \\\\ & \\text{$d_0$ is the left child of $k_1$} \\\\ & \\text{$d_1$ is the right child of $k_1$} \\\\ & \\text{$k_3$ is the right child of $k_2$} \\\\ & \\text{$d_2$ is the left child of $k_3$} \\\\ & \\text{$k_4$ is the right child of $k_3$} \\\\ & \\text{$d_3$ is the left child of $k_4$} \\\\ & \\text{$d_4$ is the right child of $k_4$} \\\\ & \\text{$k_7$ is the right child of $k_5$} \\\\ & \\text{$k_6$ is the left child of $k_7$} \\\\ & \\text{$d_5$ is the left child of $k_6$} \\\\ & \\text{$d_6$ is the right child of $k_6$} \\\\ & \\text{$d_7$ is the right child of $k_7$} \\end{aligned} $$ 15.5-3 Suppose that instead of maintaining the table $w[i, j]$, we computed the value of $w(i, j)$ directly from equation $\\text{(15.12)}$ in line 9 of $\\text{OPTIMAL-BST}$ and used this computed value in line 11. How would this change affect the asymptotic running time of $\\text{OPTIMAL-BST}$? Each of the $\\Theta(n^2)$ values of $w[i, j]$ would require computing those two sums, both of which can be of size $O(n)$, so, the asymptotic runtime would increase to $O(n^3)$. 15.5-4 $\\star$ Knuth [212] has shown that there are always roots of optimal subtrees such that $root[i, j - 1] \\le root[i, j] \\le root[i + 1, j]$ for all $1 \\le i < j \\le n$. Use this fact to modify the $\\text{OPTIMAL-BST}$ procedure to run in $\\Theta(n^2)$ time. Change the for loop of line 10 in $\\text{OPTIMAL-BST}$ to for r = r [ i , j - 1 ] to r [ i + 1 , j ] Knuth's result implies that it is sufficient to only check these values because optimal root found in this range is in fact the optimal root of some binary search tree. The time spent within the for loop of line 6 is now $\\Theta(n)$. This is because the bounds on $r$ in the new for loop of line 10 are nonoverlapping. To see this, suppose we have fixed $l$ and $i$. On one iteration of the for loop of line 6, the upper bound on $r$ is $$r[i + 1, j] = r[i + 1, i + l - 1].$$ When we increment $i$ by $1$ we increase $j$ by $1$. However, the lower bound on $r$ for the next iteration subtracts this, so the lower bound on the next iteration is $$r[i + 1, j + 1 - 1] = r[i + 1, j].$$ Thus, the total time spent in the for loop of line 6 is $\\Theta(n)$. Since we iterate the outer for loop of line 5 $n$ times, the total runtime is $\\Theta(n^2)$.","title":"15.5 Optimal binary search trees"},{"location":"Chap15/15.5/#155-1","text":"Write pseudocode for the procedure $\\text{CONSTRUCT-OPTIMAL-BST}(root)$ which, given the table $root$, outputs the structure of an optimal binary search tree. For the example in Figure 15.10, your procedure should print out the structure $$ \\begin{aligned} & \\text{$k_2$ is the root} \\\\ & \\text{$k_1$ is the left child of $k_2$} \\\\ & \\text{$d_0$ is the left child of $k_1$} \\\\ & \\text{$d_1$ is the right child of $k_1$} \\\\ & \\text{$k_5$ is the right child of $k_2$} \\\\ & \\text{$k_4$ is the left child of $k_5$} \\\\ & \\text{$k_3$ is the left child of $k_4$} \\\\ & \\text{$d_2$ is the left child of $k_3$} \\\\ & \\text{$d_3$ is the right child of $k_3$} \\\\ & \\text{$d_4$ is the right child of $k_4$} \\\\ & \\text{$d_5$ is the right child of $k_5$} \\end{aligned} $$ corresponding to the optimal binary search tree shown in Figure 15.9(b). CONSTRUCT - OPTIMAL - BST ( root , i , j , last ) if i == j return if last == 0 print root [ i , j ] + \"is the root\" else if j < last print root [ i , j ] + \"is the left child of\" + last else print root [ i , j ] + \"is the right child of\" + last CONSTRUCT - OPTIMAL - BST ( root , i , root [ i , j ] - 1 , root [ i , j ]) CONSTRUCT - OPTIMAL - BST ( root , root [ i , j ] + 1 , j , root [ i , j ])","title":"15.5-1"},{"location":"Chap15/15.5/#155-2","text":"Determine the cost and structure of an optimal binary search tree for a set of $n = 7$ keys with the following probabilities $$ \\begin{array}{c|cccccccc} i & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline p_i & & 0.04 & 0.06 & 0.08 & 0.02 & 0.10 & 0.12 & 0.14 \\\\ q_i & 0.06 & 0.06 & 0.06 & 0.06 & 0.05 & 0.05 & 0.05 & 0.05 \\end{array} $$ Cost is $3.12$. $$ \\begin{aligned} & \\text{$k_5$ is the root} \\\\ & \\text{$k_2$ is the left child of $k_5$} \\\\ & \\text{$k_1$ is the left child of $k_2$} \\\\ & \\text{$d_0$ is the left child of $k_1$} \\\\ & \\text{$d_1$ is the right child of $k_1$} \\\\ & \\text{$k_3$ is the right child of $k_2$} \\\\ & \\text{$d_2$ is the left child of $k_3$} \\\\ & \\text{$k_4$ is the right child of $k_3$} \\\\ & \\text{$d_3$ is the left child of $k_4$} \\\\ & \\text{$d_4$ is the right child of $k_4$} \\\\ & \\text{$k_7$ is the right child of $k_5$} \\\\ & \\text{$k_6$ is the left child of $k_7$} \\\\ & \\text{$d_5$ is the left child of $k_6$} \\\\ & \\text{$d_6$ is the right child of $k_6$} \\\\ & \\text{$d_7$ is the right child of $k_7$} \\end{aligned} $$","title":"15.5-2"},{"location":"Chap15/15.5/#155-3","text":"Suppose that instead of maintaining the table $w[i, j]$, we computed the value of $w(i, j)$ directly from equation $\\text{(15.12)}$ in line 9 of $\\text{OPTIMAL-BST}$ and used this computed value in line 11. How would this change affect the asymptotic running time of $\\text{OPTIMAL-BST}$? Each of the $\\Theta(n^2)$ values of $w[i, j]$ would require computing those two sums, both of which can be of size $O(n)$, so, the asymptotic runtime would increase to $O(n^3)$.","title":"15.5-3"},{"location":"Chap15/15.5/#155-4-star","text":"Knuth [212] has shown that there are always roots of optimal subtrees such that $root[i, j - 1] \\le root[i, j] \\le root[i + 1, j]$ for all $1 \\le i < j \\le n$. Use this fact to modify the $\\text{OPTIMAL-BST}$ procedure to run in $\\Theta(n^2)$ time. Change the for loop of line 10 in $\\text{OPTIMAL-BST}$ to for r = r [ i , j - 1 ] to r [ i + 1 , j ] Knuth's result implies that it is sufficient to only check these values because optimal root found in this range is in fact the optimal root of some binary search tree. The time spent within the for loop of line 6 is now $\\Theta(n)$. This is because the bounds on $r$ in the new for loop of line 10 are nonoverlapping. To see this, suppose we have fixed $l$ and $i$. On one iteration of the for loop of line 6, the upper bound on $r$ is $$r[i + 1, j] = r[i + 1, i + l - 1].$$ When we increment $i$ by $1$ we increase $j$ by $1$. However, the lower bound on $r$ for the next iteration subtracts this, so the lower bound on the next iteration is $$r[i + 1, j + 1 - 1] = r[i + 1, j].$$ Thus, the total time spent in the for loop of line 6 is $\\Theta(n)$. Since we iterate the outer for loop of line 5 $n$ times, the total runtime is $\\Theta(n^2)$.","title":"15.5-4 $\\star$"},{"location":"Chap15/Problems/15-1/","text":"Suppose that we are given a directed acyclic graph $G = (V, E)$ with real-valued edge weights and two distinguished vertices $s$ and $t$ . Describe a dynamic-programming approach for finding a longest weighted simple path from $s$ to $t$. What does the subproblem graph look like? What is the efficiency of your algorithm? Since any longest simple path must start by going through some edge out of $s$, and thereafter cannot pass through $s$ because it must be simple, that is, $$\\text{LONGEST}(G, s, t) = 1 + \\max_{s\u223cs'} \\text\\{LONGEST(G|_{V\\backslash\\{s\\}}, s', t)\\},$$ with the base case that if $s = t$ then we have a length of $0$. A naive bound would be to say that since the graph we are considering is a subset of the vertices, and the other two arguments to the substructure are distinguished vertices, then, the runtime will be $O(|V|^2 2^{|V|})$. We can see that we will actually have to consider this many possible subproblems by taking $|G|$ to be the complete graph on $|V|$ vertices.","title":"15-1 Longest simple path in a directed acyclic graph"},{"location":"Chap15/Problems/15-10/","text":"Your knowledge of algorithms helps you obtain an exciting job with the Acme Computer Company, along with a $\\$10,000$ signing bonus. You decide to invest this money with the goal of maximizing your return at the end of 10 years. You decide to use the Amalgamated Investment Company to manage your investments. Amalgamated Investments requires you to observe the following rules. It offers $n$ different investments, numbered $1$ through $n$. In each year $j$, investment $i$ provides a return rate of $r_{ij}$ . In other words, if you invest $d$ dollars in investment $i$ in year $j$, then at the end of year $j$ , you have $dr_{ij}$ dollars. The return rates are guaranteed, that is, you are given all the return rates for the next 10 years for each investment. You make investment decisions only once per year. At the end of each year, you can leave the money made in the previous year in the same investments, or you can shift money to other investments, by either shifting money between existing investments or moving money to a new investement. If you do not move your money between two consecutive years, you pay a fee of $f_1$ dollars, whereas if you switch your money, you pay a fee of $f_2$ dollars, where $f_2 > f_1$. a. The problem, as stated, allows you to invest your money inmultiple investments in each year. Prove that there exists an optimal investment strategy that, in each year, puts all the money into a single investment. (Recall that an optimal investment strategy maximizes the amount of money after 10 years and is not concerned with any other objectives, such as minimizing risk.) b. Prove that the problem of planning your optimal investment strategy exhibits optimal substructure. c. Design an algorithm that plans your optimal investment strategy. What is the running time of your algorithm? d. Suppose that Amalgamated Investments imposed the additional restriction that, at any point, you can have no more than $\\$15,000$ in any one investment. Show that the problem of maximizing your income at the end of 10 years no longer exhibits optimal substructure. a. Without loss of generality, suppose that there exists an optimal solution $S$ which involves investing $d_1$ dollars into investment $k$ and $d_2$ dollars into investement $m$ in year $1$. Further, suppose in this optimal solution, you don't move your money for the first $j$ years. If $r_{k1} + r_{k2} + \\ldots + r_{kj} > r_{m1} +r_{m2} + \\ldots + r_{mj}$ then we can perform the usual cut-and-paste maneuver and instead invest $d_1 + d_2$ dollars into investment $k$ for $j$ years. Keeping all other investments the same, this results in a strategy which is at least as profitable as $S$, but has reduced the number of different investments in a given span of years by $1$. Continuing in this way, we can reduce the optimal strategy to consist of only a single investment each year. b. If a particular investment strategy is the year-one-plan for a optimal investment strategy, then we must solve two kinds of optimal suproblem: either we maintain the strategy for an additional year, not incurring the moneymoving fee, or we move the money, which amounts to solving the problem where we ignore all information from year $1$. Thus, the problem exhibits optimal substructure. c. The algorithm works as follows: We build tables $I$ and $R$ of size $10$ such that $I[i]$ tells which investment should be made (with all money) in year $i$, and $R[i]$ gives the total return on the investment strategy in years $i$ through $10$. INVEST ( d , n ) let I [ 1..10 ] and R [ 1..10 ] be new tables for k = 10 downto 1 q = 1 for i = 1 to n if r [ i , k ] > r [ q , k ] // i now holds the investment which looks best for a given year q = i if R [ k + 1 ] + dr_ { I [ k + 1 ] k } - f [ 1 ] > R [ k + 1 ] + dr [ q , k ] - f [ 2 ] // If revenue is greater when money is not moved R [ k ] = R [ k + 1 ] + dr_ { I [ k + 1 ] k } - f [ 1 ] I [ k ] = I [ k + 1 ] else R [ k ] = R [ k + 1 ] + dr [ q , k ] - f [ 2 ] I [ k ] = q return I as an optimal stategy with return R [ 1 ] d. The previous investment strategy was independent of the amount of money you started with. When there is a cap on the amount you can invest, the amount you have to invest in the next year becomes relevant. If we know the year-one-strategy of an optimal investment, and we know that we need to move money after the first year, we're left with the problem of investing a different initial amount of money, so we'd have to solve a subproblem for every possible initial amount of money. Since there is no bound on the returns, there's also no bound on the number of subproblems we need to solve.","title":"15-10 Planning an investment strategy"},{"location":"Chap15/Problems/15-11/","text":"The Rinky Dink Company makes machines that resurface ice rinks. The demand for such products varies from month to month, and so the company needs to develop a strategy to plan its manufacturing given the fluctuating, but predictable, demand. The company wishes to design a plan for the next $n$ months. For each month $i$, the company knows the demand $d_i$, that is, the number of machines that it will sell. Let $D = \\sum_{i = 1}^n d_i$ be the total demand over the next $n$ months. The company keeps a full-time staff who provide labor to manufacture up to $m$ machines per month. If the company needs to make more than $m$ machines in a given month, it can hire additional, part-time labor, at a cost that works out to $c$ dollars per machine. Furthermore, if, at the end of a month, the company is holding any unsold machines, it must pay inventory costs. The cost for holding $j$ machines is given as a function $h(j)$ for $j = 1, 2, \\ldots, D$, where $h(j) \\ge 0$ for $1 \\le j \\le D$ and $h(j) \\le h(j + 1)$ for $1 \\le j \\le D - 1$. Give an algorithm that calculates a plan for the company that minimizes its costs while fulfilling all the demand. The running time should be polyomial in $n$ and $D$. Our subproblems will be indexed by and integer $i \\in [n]$ and another integer $j \\in [D]$. $i$ will indicate how many months have passed, that is, we will restrict ourselves to only caring about $(d_i, \\dots, d_n)$. $j$ will indicate how many machines we have in stock initially. Then, the recurrence we will use will try producing all possible numbers of machines from $1$ to $[D]$. Since the index space has size $O(nD)$ and we are only running through and taking the minimum cost from $D$ many options when computing a particular subproblem, the total runtime will be $O(nD^2)$.","title":"15-11 Inventory planning"},{"location":"Chap15/Problems/15-12/","text":"Suppose that you are the general manager for a major-league baseball team. During the off-season, you need to sign some free-agent players for your team. The team owner has given you a budget of $\\$X$ to spend on free agents. You are allowed to spend less than $\\$X$ altogether, but the owner will fire you if you spend any more than $\\$X$. You are considering $N$ different positions, and for each position, $P$ free-agent players who play that position are available. Because you do not want to overload your roster with too many players at any position, for each position you may sign at most one free agent who plays that position. (If you do not sign any players at a particular position, then you plan to stick with the players you already have at that position.) To determine how valuable a player is going to be, you decide to use a sabermetric statistic known as \"$\\text{VORP}$\", or \"value over replacement player\". A player with a higher $\\text{VORP}$ is more valuable than a player with a lower $\\text{VORP}$. A player with a higher $\\text{VORP}$ is not necessarily more expensive to sign than a player with a lower $\\text{VORP}$, because factors other than a player's value determine how much it costs to sign him. For each available free-agent player, you have three pieces of information: the player's position, the amount of money it will cost to sign the player, and the player's $\\text{VORP}$. Devise an algorithm that maximizes the total $\\text{VORP}$ of the players you sign while spending no more than $\\$X$ altogether. You may assume that each player signs for a multiple of $100,000$. Your algorithm should output the total $\\text{VORP}$ of the players you sign, the total amount of money you spend, and a list of which players you sign. Analyze the running time and space requirement of your algorithm. We will make an $N + 1$ by $X + 1$ by $P + 1$ table. The runtime of the algorithm is $O(NXP)$. BASEBALL ( N , X , P ) initialize a table B of size ( N + 1 ) by ( X + 1 ) initialize an array P of length N for i = 0 to N B [ i , 0 ] = 0 for j = 1 to X B [ 0 , j ] = 0 for i = 1 to N for j = 1 to X if j < i . cost B [ i , j ] = B [ i - 1 , j ] q = B [ i - 1 , j ] p = 0 for k = 1 to P if j >= i . cost t = B [ i - 1 , j - i . cost ] + i . value if t > q q = t p = k B [ i , j ] = q P [ i ] = p print ( \"The total VORP is\" , B [ N , X ], \"and the players are:\" ) i = N j = X C = 0 for k = 1 to N // prints the players from the table if B [ i , j ] != B [ i - 1 , j ] print ( P [ i ]) j = j - i . cost C = C + i . cost i = i - 1 print ( \"The total cost is\" , C )","title":"15-12 Signing free-agent baseball players"},{"location":"Chap15/Problems/15-2/","text":"A palindrome is a nonempty string over some alphabet that reads the same forward and backward. Examples of palindromes are all strings of length $1$, $\\text{civic}$, $\\text{racecar}$, and $\\text{aibohphobia}$ (fear of palindromes). Give an efficient algorithm to find the longest palindrome that is a subsequence of a given input string. For example, given the input $\\text{character}$, your algorithm should return $\\text{carac}$. What is the running time of your algorithm? Let $A[1..n]$ denote the array which contains the given word. First note that for a palindrome to be a subsequence we must be able to divide the input word at some position $i$, and then solve the longest common subsequence problem on $A[1..i]$ and $A[i + 1..n]$, possibly adding in an extra letter to account for palindromes with a central letter. Since there are $n$ places at which we could split the input word and the $\\text{LCS}$ problem takes time $O(n^2)$, we can solve the palindrome problem in time $O(n^3)$.","title":"15-2 Longest palindrome subsequence"},{"location":"Chap15/Problems/15-3/","text":"In the euclidean traveling-salesman problem , we are given a set of $n$ points in the plane, and we wish to find the shortest closed tour that connects all n points. Figure 15.11(a) shows the solution to a $7$-point problem. The general problem is NP-hard, and its solution is therefore believed to require more than polynomial time (see Chapter 34). J. L. Bentley has suggested that we simplify the problem by restricting our attention to bitonic tours , that is, tours that start at the leftmost point, go strictly rightward to the rightmost point, and then go strictly leftward back to the starting point. Figure 15.11(b) shows the shortest bitonic tour of the same $7$ points. In this case, a polynomial-time algorithm is possible. Describe an $O(n^2)$-time algorithm for determining an optimal bitonic tour. You may assume that no two points have the same $x$-coordinate and that all operations on real numbers take unit time. ($\\textit{Hint:}$ Scan left to right, maintaining optimal possibilities for the two parts of the tour.) First sort all the points based on their $x$ coordinate. To index our subproblem, we will give the rightmost point for both the path going to the left and the path going to the right. Then, we have that the desired result will be the subproblem indexed by $v$, where $v$ is the rightmost point. Suppose by symmetry that we are further along on the left-going path, that the leftmost path is going to the $i$th one and the right going path is going until the $j$th one. Then, if we have that $i > j + 1$, then we have that the cost must be the distance from the $i \u2212 1$st point to the ith plus the solution to the subproblem obtained where we replace $i$ with $i \u2212 1$. There can be at most $O(n^2)$ of these subproblem, but solving them only requires considering a constant number of cases. The other possibility for a subproblem is that $j \\le i \\le j + 1$. In this case, we consider for every $k$ from $1$ to $j$ the subproblem where we replace $i$ with $k$ plus the cost from $k$th point to the $i$th point and take the minimum over all of them. This case requires considering $O(n)$ things, but there are only $O(n)$ such cases. So, the final runtime is $O(n^2)$.","title":"15-3 Bitonic euclidean"},{"location":"Chap15/Problems/15-4/","text":"Consider the problem of neatly printing a paragraph with a monospaced font (all characters having the same width) on a printer. The input text is a sequence of $n$ words of lengths $l_1, l_2, \\ldots, l_n$, measured in characters. We want to print this paragraph neatly on a number of lines that hold a maximum of $M$ characters each. Our criterion of \"neatness\" is as follows. If a given line contains words $i$ through $j$, where $i \\le j$ , and we leave exactly one space between words, the number of extra space characters at the end of the line is $M - j + i - \\sum_{k = i}^j l_k$, which must be nonnegative so that the words fit on the line. We wish to minimize the sum, over all lines except the last, of the cubes of the numbers of extra space characters at the ends of lines. Give a dynamic-programming algorithm to print a paragraph of $n$ words neatly on a printer. Analyze the running time and space requirements of your algorithm. First observe that the problem exhibits optimal substructure in the following way: Suppose we know that an optimal solution has $k$ words on the first line. Then we must solve the subproblem of printing neatly words $l_{k + 1}, \\dots, l_n$. We build a table of optimal solutions solutions to solve the problem using dynamic programming. If $n \u2212 1 + \\sum_{k = 1}^n l_k < M$ then put all words on a single line for an optimal solution. In the following algorithm $\\text{PRINT-NEATLY}(n)$, $C[k]$ contains the cost of printing neatly words $l_k$ through $l_n$. We can determine the cost of an optimal solution upon termination by examining $C[1]$. The entry $P[k]$ contains the position of the last word which should appear on the first line of the optimal solution of words $l_k, \\dots, l_n$. Thus, to obtain the optimal way to place the words, we make $l_{P[1]}$ the last word on the first line, $l_{P[P[1]]}$ the last word on the second line, and so on. PRINT - NEATLY ( n ) let P [ 1. . n ] and C [ 1. . n ] be new tables for k = n downto 1 if sum_ { i = k } ^ n l_i + n - k < M C [ k ] = 0 q = \u221e for j = 1 to n - k cost = sum_ { m = 1 } ^ j l_ { k + m } + m - 1 if cost < M and ( M - cost ) ^ 3 + C [ k + m + 1 ] < q q = ( M - cost ) ^ 3 + C [ k + m + 1 ] P [ k ] = k + j C [ k ] = q","title":"15-4 Printing neatly"},{"location":"Chap15/Problems/15-5/","text":"In order to transform one source string of text $x[1..m]$ to a target string $y[1..n]$, we can perform various transformation operations. Our goal is, given $x$ and $y$, to produce a series of transformations that change $x$ to $y$. We use an array $z$\u2014assumed to be large enough to hold all the characters it will need\u2014to hold the intermediate results. Initially, $z$ is empty, and at termination, we should have $z[j] = y[j]$ for $j = 1, 2, \\ldots, n$. We maintain current indices $i$ into $x$ and $j$ into $z$, and the operations are allowed to alter $z$ and these indices. Initially, $i = j = 1$. We are required to examine every character in $x$ during the transformation, which means that at the end of the sequence of transformation operations, we must have $i = m + 1$. We may choose from among six transformation operations: Copy a character from $x$ to $z$ by setting $z[j] = x[i]$ and then incrementing both $i$ and $j$. This operation examines $x[i]$. Replace a character from $x$ by another character $c$, by setting $z[j] = c$, and then incrementing both $i$ and $j$. This operation examines $x[i]$. Delete a character from $x$ by incrementing $i$ but leaving $j$ alone. This operation examines $x[i]$. Insert the character $c$ into $z$ by setting $z[j] = c$ and then incrementing $j$, but leaving $i$ alone. This operation examines no characters of $x$. Twiddle (i.e., exchange) the next two characters by copying them from $x$ to $z$ but in the opposite order; we do so by setting $z[j] = x[i + 1]$ and $z[j + 1] = x[i]$ and then setting $i = i + 2$ and $j = j + 2$. This operation examines $x[i]$ and $x[i + 1]$. Kill the remainder of $x$ by setting $i = m + 1$. This operation examines all characters in $x$ that have not yet been examined. This operation, if performed, must be the final operation. As an example, one way to transform the source string $\\text{algorithm}$ to the target string $\\text{altruistic}$ is to use the following sequence of operations, where the underlined characters are $x[i]$ and $z[j]$ after the operation: $$ \\begin{array}{lll} \\text{Operation} & x & z \\\\ \\hline \\textit{initial strings} & \\underline algorithm & \\text{\\textunderscore} \\\\ \\text{copy} & a\\underline lgorithm & a\\text{\\textunderscore} \\\\ \\text{copy} & al\\underline gorithm & al\\text{\\textunderscore} \\\\ \\text{replace by $t$} & alg\\underline orithm & alt\\text{\\textunderscore} \\\\ \\text{delete} & algo\\underline rithm & alt\\text{\\textunderscore} \\\\ \\text{copy} & algor\\underline ithm & altr\\text{\\textunderscore} \\\\ \\text{insert $u$} & algor\\underline ithm & altru\\text{\\textunderscore} \\\\ \\text{insert $i$} & algor\\underline ithm & altrui\\text{\\textunderscore} \\\\ \\text{insert $s$} & algor\\underline ithm & altruis\\text{\\textunderscore} \\\\ \\text{twiddle} & algorit\\underline hm & altruisti\\text{\\textunderscore} \\\\ \\text{insert $c$} & algorit\\underline hm & altruistic\\text{\\textunderscore} \\\\ \\text{kill} & algorithm\\text{\\textunderscore} & altruistic\\text{\\textunderscore} \\end{array} $$ Note that there are several other sequences of transformation operations that transform $\\text{algorithm}$ to $\\text{altruistic}$. Each of the transformation operations has an associated cost. The cost of an operation depends on the specific application, but we assume that each operation's cost is a constant that is known to us. We also assume that the individual costs of the copy and replace operations are less than the combined costs of the delete and insert operations; otherwise, the copy and replace operations would not be used. The cost of a given sequence of transformation operations is the sum of the costs of the individual operations in the sequence. For the sequence above, the cost of transforming $\\text{algorithm}$ to $\\text{altruistic}$ is $$\\text{($3 \\cdot$ cost(copy)) + cost(replace) + cost(delete) + ($4 \\cdot$ cost(insert)) + cost(twiddle) + cost(kill)}.$$ a. Given two sequences $x[1..m]$ and $y[1..n]$ and set of transformation-operation costs, the edit distance from $x$ to $y$ is the cost of the least expensive operatoin sequence that transforms $x$ to $y$. Describe a dynamic-programming algorithm that finds the edit distance from $x[1..m]$ to $y[1..n]$ and prints an optimal opeartion sequence. Analyze the running time and space requirements of your algorithm. The edit-distance problem generalizes the problem of aligning two DNA sequences (see, for example, Setubal and Meidanis [310, Section 3.2]). There are several methods for measuring the similarity of two DNA sequences by aligning them. One such method to align two sequences $x$ and $y$ consists of inserting spaces at arbitrary locations in the two sequences (including at either end) so that the resulting sequences $x'$ and $y'$ have the same length but do not have a space in the same position (i.e., for no position $j$ are both $x'[j]$ and $y'[j]$ a space). Then we assign a \"score\" to each position. Position $j$ receives a score as follows: $+1$ if $x'[j] = y'[j]$ and neither is a space, $-1$ if $x'[j] \\ne y'[j]$ and neither is a space, $-2$ if either $x'[j]$ or $y'[j]$ is a space. The score for the alignment is the sum of the scores of the individual positions. For example, given the sequences $x = \\text{GATCGGCAT}$ and $y = \\text{CAATGTGAATC}$, one alignment is $$ \\begin{array}{cccccccccccc} \\text G & & \\text A & \\text T & \\text C & \\text G & & \\text G & \\text C & \\text A & \\text T & \\\\ \\text C & \\text A & \\text A & \\text T & & \\text G & \\text T & \\text G & \\text A & \\text A & \\text T & \\text C \\\\ - & * & + & + & * & + & * & + & - & + & + & * \\end{array} $$ A $+$ under a position indicates a score of $+1$ for that position, a $-$ indicates a score of $-1$, and a $*$ indicates a score of $-2$, so that this alignment has a total score of $6 \\cdot -2 \\cdot 1 - 4 \\cdot 2 = -4$. b. Explain how to cast the problem of finding an optimal alignment as an edit distance problem using a subset of the transformation operations copy, replace, delete, insert, twiddle, and kill. a. We will index our subproblems by two integers, $1 \\le i \\le m$ and $1 \\le j \\le n$. We will let $i$ indicate the rightmost element of $x$ we have not processed and $j$ indicate the rightmost element of $y$ we have not yet found matches for. For a solution, we call $\\text{EDIT}(x, y, i, j)$. b. We will set $$\\text{cost(delete)} = \\text{cost(insert)} = 2,$$ $$\\text{cost(copy)} = \u22121,$$ $$\\text{cost(replace)} = 1,$$ and $$\\text{cost(twiddle)} = \\text{cost(kill)} = \\infty.$$ Then a minimum cost translation of the first string into the second corresponds to an alignment. We view a $\\text{copy}$ or a $\\text{replace}$ as incrementing a pointer for both strings, a $\\text{insert}$ as putting a space at the current position of the pointer in the first string, and a $\\text{delete}$ operation means putting a space in the current position in the second string. Since $\\text{twiddle}$s and $\\text{kill}$s have infinite costs, we will have neither of them in a minimal cost solution. The final value for the alignment will be the negative of the minimum cost sequence of edits. EDIT ( x , y , i , j ) let m = x . length let n = y . length if i == m return ( n - j ) cost ( insert ) if j == n return min {( m - i ) cost ( delete ), cost ( kill )} initialize o1 , ..., o5 to \u221e if x [ i ] == y [ j ] o1 = cost ( copy ) + EDIT ( x , y , i + 1 , j + 1 ) o2 = cost ( replace ) + EDIT ( x , y , i + 1 , j + 1 ) o3 = cost ( delete ) + EDIT ( x , y , i + 1 , j ) o4 = cost ( insert ) + EDIT ( x , y , i , j + 1 ) if i < m - 1 and j < n - 1 if x [ i ] == y [ j + 1 ] and x [ i + 1 ] == y [ j ] o5 = cost ( twiddle ) + EDIT ( x , y , i + 2 , j + 2 ) return min_ { i \u2208 [ 5 ]}{ o_i }","title":"15-5 Edit distance"},{"location":"Chap15/Problems/15-6/","text":"Professor Stewart is consulting for the president of a corporation that is planning a company party. The company has a hierarchical structure; that is, the supervisor relation forms a tree rooted at the president. The personnel office has ranked each employee with a conviviality rating, which is a real number. In order to make the party fun for all attendees, the president does not want both an employee and his or her immediate supervisor to attend. Professor Stewart is given the tree that describes the structure of the corporation, using the left-child, right-sibling representation described in Section 10.4. Each node of the tree holds, in addition to the pointers, the name of an employee and that employee's conviviality ranking. Describe an algorithm to make up a guest list that maximizes the sum of the conviviality ratings of the guests. Analyze the running time of your algorithm. The problem exhibits optimal substructure in the following way: If the root $r$ is included in an optimal solution, then we must solve the optimal subproblems rooted at the grandchildren of $r$. If $r$ is not included, then we must solve the optimal subproblems on trees rooted at the children of $r$. The dynamic programming algorithm to solve this problem works as follows: We make a table $C$ indexed by vertices which tells us the optimal conviviality ranking of a guest list obtained from the subtree with root at that vertex. We also make a table $G$ such that $G[i]$ tells us the guest list we would use when vertex $i$ is at the root. Let $T$ be the tree of guests. To solve the problem, we need to examine the guest list stored at $G[T.root]$. First solve the problem at each leaf $L$. If the conviviality ranking at $L$ is positive, $G[L] = \\{L\\}$ and $C[L] = L.conviv$. Otherwise $G[L] = \\emptyset$ and $C[L] = 0$. Iteratively solve the subproblems located at parents of nodes at which the subproblem has been solved. In general for a node $x$, $$C[x] = \\min(\\sum_{y\\text{ is a child of } x} C[y], \\sum_{y\\text{ is a grandchild of } x} C[y]).$$ The runtime of the algorithm is $O(n^2)$ where $n$ is the number of vertices, because we solve $n$ subproblems, each in constant time, but the tree traversals required to find the appropriate next node to solve could take linear time.","title":"15-6 Planning a company party"},{"location":"Chap15/Problems/15-7/","text":"We can use dynamic programming on a directed graph $G = (V, E)$ for speech recognition. Each edge $(u, v) \\in E$ is labeled with a sound $\\sigma(u, v)$ from a finite set $\\Sigma$ of sounds. The labeled graph is a formal model of a person speaking a restricted language. Each path in the graph starting from a distinguished vertex $v_0 \\in V$ corresponds to a possible sequence of sounds producted by the model. We define the label of a directed path to be the concatenation of the labels of the edges on that path. a. Describe an efficient algorithm that, given an edge-labeled graph $G$ with distinguished vertex $v_0$ and a sequence $s = \\langle \\sigma_1, \\sigma_2, \\ldots, \\sigma_k \\rangle$ of sounds from $\\Sigma$, returns a path in $G$ that begins at $v_0$ and has $s$ as its label, if any such path exists. Otherwise, the algorithm should return $\\text{NO-SUCH-PATH}$. Analyze the running time of your algorithm. ($\\textit{Hint:}$ You may find concepts from Chapter 22 useful.) Now, suppose that every edge $(u, v) \\in E$ has an associated nonnegatve probability $p(u, v)$ of traversing the edge $(u, v)$ from vertex $u$ and thus producing the corresponding sound. The sum of the probabilities of the edges leaving any vertex equals $1$. The probability of a path is defined to the product of the probabilities of its edges. We can view the probability of a path beginning at $v_0$ as the probability that a \"random walk\" beginning at $v_0$ will follow the specified path, where we randomly choose which edge to take leaving a vertex $u$ according to the probabilities of the available edges leaving $u$. b. Extend your answer to part (a) so that if a path is returned, it is a most probable path starting at $v_0$ and having label $s$. Analyze the running time of your algorithm. a. Our substructure will consist of trying to find suffixes of s of length one less starting at all the edges leaving $v_0$ with label $\\sigma_0$. if any of them have a solution, then, there is a solution. If none do, then there is none. See the algorithm $\\text{VITERBI}$ for details. VITERBI ( G , s , v [ 0 ]) if s . length = 0 return v [ 0 ] for edges ( v [ 0 ], v [ 1 ]) in V for some v [ 1 ] if sigma ( v [ 0 ], v [ 1 ]) = sigma [ 1 ] res = VITERBI ( G , ( sigma [ 2 ], ..., sigma [ k ]), v [ 1 ]) if res != NO - SUCH - PATH return ( v [ 0 ], res ) return NO - SUCH - PATH Since the subproblems are indexed by a suffix of $s$ (of which there are only $k$) and a vertex in the graph, there are at most $O(k|V|)$ different possible arguments. Since each run may require testing a edge going to every other vertex, and each iteration of the for loop takes at most a constant amount of time other than the call to $\\text{PROB-VITERBI}$, the final runtime is $O(k|V|^2)$. b. For this modification, we will need to try all the possible edges leaving from $v_0$ instead of stopping as soon as we find one that works. The substructure is very similar. We'll make it so that instead of just returning the sequence, we'll have the algorithm also return the probability of that maximum probability sequence, calling the fields seq and prob respectively. See the algorithm $\\text{PROB-VITERBI}$. Since the runtime is indexed by the same things, we have that we will call it with at most $O(k|V|)$ different possible arguments. Since each run may require testing a edge going to every other vertex, and each iteration of the for loop takes at most a constant amount of time other than the call to $\\text{PROB-VITERBI}$, the final runtime is $O(k|V|^2)$. PROB - VITERBI ( G , s , v [ 0 ]) if s . length = 0 return v [ 0 ] sols . seq = NO - SUCH - PATH sols . prob = 0 for edges ( v [ 0 ], v [ 1 ]) in V for some v [ 1 ] if sigma ( v [ 0 ], v [ 1 ]) = sigma [ 1 ] res = PROB - VITERBI ( G , ( sigma [ 2 ], ..., sigma [ k ]), v [ 1 ]) if p ( v [ 0 ], v [ 1 ]) * res . prob \u2265 sols . prob sols . prob = p ( v [ 0 ], v [ 1 ]) * res . prob and sols . seq = v [ 0 ], res . seq return sols","title":"15-7 Viterbi algorithm"},{"location":"Chap15/Problems/15-8/","text":"We are given a color picture consisting of an $m \\times n$ array $A[1..m, 1..n]$ of pixels, where each pixel specifies a triple of red, green, and blue (RGB) intensities. Suppose that we wish to compress this picture slightly. Specifically, we wish to remove one pixel from each of the $m$ rows, so that the whole picture becomes one pixel narrower. To avoid disturbing visual effects, however, we require that the pixels removed in two adjacent rows be in the same or adjacent columns; the pixels removed form a \"seam\" from the top row to the bottom row where successive pixels in the seam are adjacent vertically or diagonally. a. Show that the number of such possible seams grows at least exponentially in $m$, assuming that $n > 1$. b. Suppose now that along with each pixel $A[i, j]$, we have calculated a real-valued disruption measure $d[i, j]$, indicating how disruptive it would be to remove pixel $A[i, j]$. Intuitively, the lower a pixel's disruption measure, the more similar the pixel is to its neighbors. Suppose further that we define the disruption measure of a seam to be the sum of the disruption measures of its pixels. Give an algorithm to find a seam with the lowest disruption measure. How efficient is your algorithm? a. If $n > 1$ then for every choice of pixel at a given row, we have at least $2$ choices of pixel in the next row to add to the seam ($3$ if we're not in column $1$ or $n$). Thus the total number of possibilities is bounded below by $2^m$. b. We create a table $D[1..m, 1..n]$ such that $D[i, j]$ stores the disruption of an optimal seam ending at position $[i, j]$, which started in row $1$. We also create a table $S[i, j]$ which stores the list of ordered pairs indicating which pixels were used to create the optimal seam ending at position $(i, j)$. To find the solution to the problem, we look for the minimum $k$ entry in row $m$ of table $D$, and use the list of pixels stored at $S[m, k]$ to determine the optimal seam. To simplify the algorithm $\\text{Seam}(A)$, let $\\text{MIN}(a, b, c)$ be the function which returns $\u22121$ if a is the minimum, $0$ if $b$ is the minimum, and $1$ if $c$ is the minimum value from among $a$, $b$, and $c$. The time complexity of the algorithm is $O(mn)$. SEAM ( A ) let D [ 1. . m , 1. . n ] be a table with zeros let S [ 1. . m , 1. . n ] be a table with empty lists for i = 1 to n S [ 1 , i ] = ( 1 , i ) D [ 1 , i ] = d_ { 1 i } for i = 2 to m for j = 1 to n if j == 1 // left-edge case if D [ i - 1 , j ] < D [ i - 1 , j + 1 ] D [ i , j ] = D [ i - 1 , j ] + d_ { ij } S [ i , j ] = S [ i - 1 , j ]. insert ( i , j ) else D [ i , j ] = D [ i - 1 , j + 1 ] + d_ { ij } S [ i , j ] = S [ i - 1 , j + 1 ]. insert ( i , j ) else if j == n // right-edge case if D [ i - 1 , j - 1 ] < D [ i - 1 , j ] D [ i , j ] = D [ i - 1 , j - 1 ] + d_ { ij } S [ i , j ] = S [ i - 1 , j - 1 ]. insert ( i , j ) else D [ i , j ] = D [ i - 1 , j ] + d_ { ij } S [ i , j ] = S [ i - 1 , j ]. insert ( i , j ) x = MIN ( D [ i - 1 , j - 1 ], D [ i - 1 , j ], D [ i - 1 , j + 1 ]) D [ i , j ] = D [ i - 1 , j + x ] S [ i , j ] = S [ i - 1 , j + x ]. insert ( i , j ) q = 1 for j = 1 to n if D [ m , j ] < D [ m , q ] q = j print ( S [ m , q ])","title":"15-8 Image compression by seam carving"},{"location":"Chap15/Problems/15-9/","text":"A certain string-processing language allows a programmer to break a string into two pieces. Because this operation copies the string, it costs $n$ time units to break a string of $n$ characters into two pieces. Suppose a programmer wants to break a string into many pieces. The order in which the breaks occur can affect the total amount of time used. For example, suppose that the programmer wants to break a $20$-character string after characters $2$, $8$, and $10$ (numbering the characters in ascending order from the left-hand end, starting from $1$). If she programs the breaks to occur in left-to-right order, then the first break costs $20$ time units, the second break costs $18$ time units (breaking the string from characters $3$ to $20$ at character $8$), and the third break costs $12$ time units, totaling $50$ time units. If she programs the breaks to occur in right-to-left order, however, then the first break costs $20$ time units, the second break costs $10$ time units, and the third break costs $8$ time units, totaling $38$ time units. In yet another order, she could break first at $8$ (costing $20$), then break the left piece at $2$ (costing $8$), and finally the right piece at $10$ (costing $12$), for a total cost of $40$. Design an algorithm that, given the numbers of characters after which to break, determines a least-cost way to sequence those breaks. More formally, given a string $S$ with $n$ characters and an array $L[1..m]$ containing the break points, com- pute the lowest cost for a sequence of breaks, along with a sequence of breaks that achieves this cost. The subproblems will be indexed by contiguous subarrays of the arrays of cuts needed to be made. We try making each possible cut, and take the one with cheapest cost. Since there are $m$ to try, and there are at most $m^2$ possible things to index the subproblems with, we have that the m dependence is that the solution is $O(m^3)$. Also, since each of the additions is of a number that is $O(n)$, each of the iterations of the for loop may take time $O(\\lg n + \\lg m)$, so, the final runtime is $O(m^3 \\lg n)$. The given algorithm will return $(cost, seq)$ where $cost$ is the cost of the cheapest sequence, $and$ seq is the sequence of cuts to make. CUT - STRING ( L , i , j , l , r ) if l == r return ( 0 , []) minCost = \u221e for k = i to j if l + r + CUT - STRING ( L , i , k , l , L [ k ]). cost + CUT - STRING ( L , k , j , L [ k ], j ). cost < minCost minCost = r - l + CUT - STRING ( L , i , k , l , L [ k ]). cost + CUT - STRING ( L , k + 1 , j , L [ k ], j ). cost minSeq = L [ k ] + CUT - STRING ( L , i , k , l , L [ k ]) + CUT - STRING ( L , i , k + 1 , l , L [ k ]) return ( minCost , minSeq ) Sample call: ``cpp L = [3, 8, 10] S = 20 CUT-STRING(L, 0, len(L), 0, s) ```","title":"15-9 Breaking a string"},{"location":"Chap16/16.1/","text":"16.1-1 Give a dynamic-programming algorithm for the activity-selection problem, based on recurrence $\\text{(16.2)}$. Have your algorithm compute the sizes $c[i, j]$ as defined above and also produce the maximum-size subset of mutually compatible activities. Assume that the inputs have been sorted as in equation $\\text{(16.1)}$. Compare the running time of your solution to the running time of $\\text{GREEDY-ACTIVITY-SELECTOR}$. DYNAMIC - ACTIVITY - SELECTOR ( s , f , n ) let c [ 0. . n + 1 , 0. . n + 1 ] and act [ 0. . n + 1 , 0. . n + 1 ] be new tables for i = 0 to n c [ i , i ] = 0 c [ i , i + 1 ] = 0 c [ n + 1 , n + 1 ] = 0 for l = 2 to n + 1 for i = 0 to n - l + 1 j = i + l c [ i , j ] = 0 k = j - 1 while f [ i ] < f [ k ] if f [ i ] \u2264 s [ k ] and f [ k ] \u2264 s [ j ] and c [ i , k ] + c [ k , j ] + 1 > c [ i , j ] c [ i , j ] = c [ i , k ] + c [ k , j ] + 1 act [ i , j ] = k k = k - 1 print \"A maximum size set of mutually compatible activities has size\" c [ 0 , n + 1 ] print \"The set contains\" PRINT - ACTIVITIES ( c , act , 0 , n + 1 ) PRINT - ACTIVITIES ( c , act , i , j ) if c [ i , j ] > 0 k = act [ i , j ] print k PRINT - ACTIVITIES ( c , act , i , k ) PRINT - ACTIVITIES ( c , act , k , j ) $\\text{GREEDY-ACTIVITY-SELECTOR}$ runs in $\\Theta(n)$ time and $\\text{DYNAMIC-ACTIVITY-SELECTOR}$ runs in $O(n^3)$ time. 16.1-2 Suppose that instead of always selecting the first activity to finish, we instead select the last activity to start that is compatible with all previously selected activities. Describe how this approach is a greedy algorithm, and prove that it yields an optimal solution. This becomes exactly the same as the original problem if we imagine time running in reverse, so it produces an optimal solution for essentially the same reasons. It is greedy because we make the best looking choice at each step. 16.1-3 Not just any greedy approach to the activity-selection problem produces a maximum-size set of mutually compatible activities. Give an example to show that the approach of selecting the activity of least duration from among those that are compatible with previously selected activities does not work. Do the same for the approaches of always selecting the compatible activity that overlaps the fewest other remaining activities and always selecting the compatible remaining activity with the earliest start time. As a counterexample to the optimality of greedily selecting the shortest, suppose our activity times are $\\{(1, 9), (8, 11), (10, 20)\\}$ then, picking the shortest first, we have to eliminate the other two, where if we picked the other two instead, we would have two tasks not one. As a counterexample to the optimality of greedily selecting the task that conflicts with the fewest remaining activities, suppose the activity times are $\\{(\u22121, 1), (2, 5), (0, 3), (0, 3), (0, 3), (4, 7), (6, 9), (8, 11), (8, 11), (8, 11), (10, 12)\\}$. Then, by this greedy strategy, we would first pick $(4, 7)$ since it only has a two conflicts. However, doing so would ean that we would not be able to pick the only optimal solution of $(\u22121, 1)$, $(2, 5)$, $(6, 9)$, $(10, 12)$. As a counterexample to the optimality of greedily selecting the earliest start times, suppose our activity times are $\\{(1, 10), (2, 3), (4, 5)\\}$. If we pick the earliest start time, we will only have a single activity, $(1, 10)$, whereas the optimal solution would be to pick the two other activities. 16.1-4 Suppose that we have a set of activities to schedule among a large number of lecture halls, where any activity can take place in any lecture hall. We wish to schedule all the activities using as few lecture halls as possible. Give an efficient greedy algorithm to determine which activity should use which lecture hall. (This problem is also known as the interval-graph coloring problem . We can create an interval graph whose vertices are the given activities and whose edges connect incompatible activities. The smallest number of colors required to color every vertex so that no two adjacent vertices have the same color corresponds to finding the fewest lecture halls needed to schedule all of the given activities.) Maintain a set of free (but already used) lecture halls $F$ and currently busy lecture halls $B$. Sort the classes by start time. For each new start time which you encounter, remove a lecture hall from $F$, schedule the class in that room, and add the lecture hall to $B$. If $F$ is empty, add a new, unused lecture hall to $F$. When a class finishes, remove its lecture hall from $B$ and add it to $F$. This is optimal for following reason, suppose we have just started using the mth lecture hall for the first time. This only happens when ever classroom ever used before is in $B$. But this means that there are $m$ classes occurring simultaneously, so it is necessary to have $m$ distinct lecture halls in use. 16.1-5 Consider a modification to the activity-selection problem in which each activity $a_i$ has, in addition to a start and finish time, a value $v_i$. The objective is no longer to maximize the number of activities scheduled, but instead to maximize the total value of the activities scheduled. That is, we wish to choose a set $A$ of compatible activities such that $\\sum_{a_k \\in A} v_k$ is maximized. Give a polynomial-time algorithm for this problem. Easy and straightforward solution is to run a dynamic programming solution based on the equation $\\text{(16.2)}$ where the second case has \"1\" replaced with \"$v_k$\". Since the subproblems are still indexed by a pair of activities, and each calculation requires taking the minimum over some set of size $\\le |S_{ij}| \\in O(n)$. The total runtime is bounded by $O(n^3)$. However, if we are cunning a little, we can be more efficient and give the algorithm which runs in $O(n\\log n)$. INPUT: $n$ activities with values. IDEA OF ALGORITHM: Sort input vector of activities according their finish times in ascending order. Let us denote the activities in this sorted vector by $(a_0, a_1, \\dots, a_{n - 1})$. For each $0 \\le i < n$ construct partial solution $S_i$. By a partial solution $S_i$, we mean a solution to the problem but considering only activities with indexes lower or equal to $i$. Remember value of each partial solution. Clearly $S_0 = \\{a_0\\}$. We can construct $S_{i + 1}$ as follows. Possible values of $S_{i + 1}$ is either $S_i$ or the solution obtained by joining the activity $a_{i + 1}$ with partial solution $S_j$ where $j < i + 1$ is the index of activity such that $a_j$ is compatible with $a_{i + 1}$ but $a_{j + 1}$ is not compatible with $a_{i + 1}$. Pick the one of these two possible solutions, which has greater value. Ties can be resolved arbitrarily. Therefore we can construct partial solutions in order $S_0, S_1, \\dots, S_{n - 1}$ using (3) for $S_0$ and (4) for all the others. Give $S_{n - 1}$ as the solution for problem. ANALYSIS OF TIME COMPLEXITY: Sorting of activities can be done in $O(n\\log n)$ time. Finding the value of $S_0$ is in $O(1)$. Any $S_{i + 1}$ can be found in $O(\\log n)$. It is thanks to the fact that we have properly sorted activities. Therefore we can for each $i + 1$ find the proper $j$ in $O(\\log n)$ using the binary search. If we have the proper $j$, the rest can be done in $O(1)$. Therefore, we have $O(n\\log n)$ time for constructing of all $S_i$'s. IMPLEMENTATION DETAILS: Use the dynamic programming. It is important not to remember too much for each $S_i$. Do not construct $S_i$'s directly (you can end up in $\\Omega(n^2)$ time if you do so). For each $S_i$ it is sufficient to remember: it's value whether or not it includes the activity $a_i$ the value of $j$ (from (4)). Using these information obtained by the run of described algorithm you can reconstruct the solution in $O(n)$ time, which does not violate final time complexity. PROOF OF CORRECTNESS: (sketched) Clearly, $S_0 = \\{a_0\\}.$ For $S_{i + 1}$ we argue by (4). Partial solution $S_{i + 1}$ either includes the activity $a_{i + 1}$ or doesn't include it, there is no third way. If it does not include $a_{i + 1}$, then clearly $S_{i + 1} = S_i$. If it includes $a_{i + 1}$, then $S_{i + 1}$ consists of $a_{i + 1}$ and partial solution which uses all activities compatible with $a_{i + 1}$ with indexes lower than $i + 1$. Since activities are sorted according their finish times, activities with indexes $j$ and lower are compatible and activities with index $j + 1$ and higher up to $i + 1$ are not compatible. We do not consider all the other activities for $S_{i + 1}$. Therefore setting $S_{i + 1} = \\{a_{i + 1}\\} \\cup S_j$ gives correct answer in this case. The fact that we need $S_j$ and not some other solution for activities with indexes up to $j$ can be easily shown by the standard cut-and-paste argument. Since for $S_{n - 1}$ we consider all of the activities, it is actually the solution of the problem.","title":"16.1 An activity-selection problem"},{"location":"Chap16/16.1/#161-1","text":"Give a dynamic-programming algorithm for the activity-selection problem, based on recurrence $\\text{(16.2)}$. Have your algorithm compute the sizes $c[i, j]$ as defined above and also produce the maximum-size subset of mutually compatible activities. Assume that the inputs have been sorted as in equation $\\text{(16.1)}$. Compare the running time of your solution to the running time of $\\text{GREEDY-ACTIVITY-SELECTOR}$. DYNAMIC - ACTIVITY - SELECTOR ( s , f , n ) let c [ 0. . n + 1 , 0. . n + 1 ] and act [ 0. . n + 1 , 0. . n + 1 ] be new tables for i = 0 to n c [ i , i ] = 0 c [ i , i + 1 ] = 0 c [ n + 1 , n + 1 ] = 0 for l = 2 to n + 1 for i = 0 to n - l + 1 j = i + l c [ i , j ] = 0 k = j - 1 while f [ i ] < f [ k ] if f [ i ] \u2264 s [ k ] and f [ k ] \u2264 s [ j ] and c [ i , k ] + c [ k , j ] + 1 > c [ i , j ] c [ i , j ] = c [ i , k ] + c [ k , j ] + 1 act [ i , j ] = k k = k - 1 print \"A maximum size set of mutually compatible activities has size\" c [ 0 , n + 1 ] print \"The set contains\" PRINT - ACTIVITIES ( c , act , 0 , n + 1 ) PRINT - ACTIVITIES ( c , act , i , j ) if c [ i , j ] > 0 k = act [ i , j ] print k PRINT - ACTIVITIES ( c , act , i , k ) PRINT - ACTIVITIES ( c , act , k , j ) $\\text{GREEDY-ACTIVITY-SELECTOR}$ runs in $\\Theta(n)$ time and $\\text{DYNAMIC-ACTIVITY-SELECTOR}$ runs in $O(n^3)$ time.","title":"16.1-1"},{"location":"Chap16/16.1/#161-2","text":"Suppose that instead of always selecting the first activity to finish, we instead select the last activity to start that is compatible with all previously selected activities. Describe how this approach is a greedy algorithm, and prove that it yields an optimal solution. This becomes exactly the same as the original problem if we imagine time running in reverse, so it produces an optimal solution for essentially the same reasons. It is greedy because we make the best looking choice at each step.","title":"16.1-2"},{"location":"Chap16/16.1/#161-3","text":"Not just any greedy approach to the activity-selection problem produces a maximum-size set of mutually compatible activities. Give an example to show that the approach of selecting the activity of least duration from among those that are compatible with previously selected activities does not work. Do the same for the approaches of always selecting the compatible activity that overlaps the fewest other remaining activities and always selecting the compatible remaining activity with the earliest start time. As a counterexample to the optimality of greedily selecting the shortest, suppose our activity times are $\\{(1, 9), (8, 11), (10, 20)\\}$ then, picking the shortest first, we have to eliminate the other two, where if we picked the other two instead, we would have two tasks not one. As a counterexample to the optimality of greedily selecting the task that conflicts with the fewest remaining activities, suppose the activity times are $\\{(\u22121, 1), (2, 5), (0, 3), (0, 3), (0, 3), (4, 7), (6, 9), (8, 11), (8, 11), (8, 11), (10, 12)\\}$. Then, by this greedy strategy, we would first pick $(4, 7)$ since it only has a two conflicts. However, doing so would ean that we would not be able to pick the only optimal solution of $(\u22121, 1)$, $(2, 5)$, $(6, 9)$, $(10, 12)$. As a counterexample to the optimality of greedily selecting the earliest start times, suppose our activity times are $\\{(1, 10), (2, 3), (4, 5)\\}$. If we pick the earliest start time, we will only have a single activity, $(1, 10)$, whereas the optimal solution would be to pick the two other activities.","title":"16.1-3"},{"location":"Chap16/16.1/#161-4","text":"Suppose that we have a set of activities to schedule among a large number of lecture halls, where any activity can take place in any lecture hall. We wish to schedule all the activities using as few lecture halls as possible. Give an efficient greedy algorithm to determine which activity should use which lecture hall. (This problem is also known as the interval-graph coloring problem . We can create an interval graph whose vertices are the given activities and whose edges connect incompatible activities. The smallest number of colors required to color every vertex so that no two adjacent vertices have the same color corresponds to finding the fewest lecture halls needed to schedule all of the given activities.) Maintain a set of free (but already used) lecture halls $F$ and currently busy lecture halls $B$. Sort the classes by start time. For each new start time which you encounter, remove a lecture hall from $F$, schedule the class in that room, and add the lecture hall to $B$. If $F$ is empty, add a new, unused lecture hall to $F$. When a class finishes, remove its lecture hall from $B$ and add it to $F$. This is optimal for following reason, suppose we have just started using the mth lecture hall for the first time. This only happens when ever classroom ever used before is in $B$. But this means that there are $m$ classes occurring simultaneously, so it is necessary to have $m$ distinct lecture halls in use.","title":"16.1-4"},{"location":"Chap16/16.1/#161-5","text":"Consider a modification to the activity-selection problem in which each activity $a_i$ has, in addition to a start and finish time, a value $v_i$. The objective is no longer to maximize the number of activities scheduled, but instead to maximize the total value of the activities scheduled. That is, we wish to choose a set $A$ of compatible activities such that $\\sum_{a_k \\in A} v_k$ is maximized. Give a polynomial-time algorithm for this problem. Easy and straightforward solution is to run a dynamic programming solution based on the equation $\\text{(16.2)}$ where the second case has \"1\" replaced with \"$v_k$\". Since the subproblems are still indexed by a pair of activities, and each calculation requires taking the minimum over some set of size $\\le |S_{ij}| \\in O(n)$. The total runtime is bounded by $O(n^3)$. However, if we are cunning a little, we can be more efficient and give the algorithm which runs in $O(n\\log n)$. INPUT: $n$ activities with values. IDEA OF ALGORITHM: Sort input vector of activities according their finish times in ascending order. Let us denote the activities in this sorted vector by $(a_0, a_1, \\dots, a_{n - 1})$. For each $0 \\le i < n$ construct partial solution $S_i$. By a partial solution $S_i$, we mean a solution to the problem but considering only activities with indexes lower or equal to $i$. Remember value of each partial solution. Clearly $S_0 = \\{a_0\\}$. We can construct $S_{i + 1}$ as follows. Possible values of $S_{i + 1}$ is either $S_i$ or the solution obtained by joining the activity $a_{i + 1}$ with partial solution $S_j$ where $j < i + 1$ is the index of activity such that $a_j$ is compatible with $a_{i + 1}$ but $a_{j + 1}$ is not compatible with $a_{i + 1}$. Pick the one of these two possible solutions, which has greater value. Ties can be resolved arbitrarily. Therefore we can construct partial solutions in order $S_0, S_1, \\dots, S_{n - 1}$ using (3) for $S_0$ and (4) for all the others. Give $S_{n - 1}$ as the solution for problem. ANALYSIS OF TIME COMPLEXITY: Sorting of activities can be done in $O(n\\log n)$ time. Finding the value of $S_0$ is in $O(1)$. Any $S_{i + 1}$ can be found in $O(\\log n)$. It is thanks to the fact that we have properly sorted activities. Therefore we can for each $i + 1$ find the proper $j$ in $O(\\log n)$ using the binary search. If we have the proper $j$, the rest can be done in $O(1)$. Therefore, we have $O(n\\log n)$ time for constructing of all $S_i$'s. IMPLEMENTATION DETAILS: Use the dynamic programming. It is important not to remember too much for each $S_i$. Do not construct $S_i$'s directly (you can end up in $\\Omega(n^2)$ time if you do so). For each $S_i$ it is sufficient to remember: it's value whether or not it includes the activity $a_i$ the value of $j$ (from (4)). Using these information obtained by the run of described algorithm you can reconstruct the solution in $O(n)$ time, which does not violate final time complexity. PROOF OF CORRECTNESS: (sketched) Clearly, $S_0 = \\{a_0\\}.$ For $S_{i + 1}$ we argue by (4). Partial solution $S_{i + 1}$ either includes the activity $a_{i + 1}$ or doesn't include it, there is no third way. If it does not include $a_{i + 1}$, then clearly $S_{i + 1} = S_i$. If it includes $a_{i + 1}$, then $S_{i + 1}$ consists of $a_{i + 1}$ and partial solution which uses all activities compatible with $a_{i + 1}$ with indexes lower than $i + 1$. Since activities are sorted according their finish times, activities with indexes $j$ and lower are compatible and activities with index $j + 1$ and higher up to $i + 1$ are not compatible. We do not consider all the other activities for $S_{i + 1}$. Therefore setting $S_{i + 1} = \\{a_{i + 1}\\} \\cup S_j$ gives correct answer in this case. The fact that we need $S_j$ and not some other solution for activities with indexes up to $j$ can be easily shown by the standard cut-and-paste argument. Since for $S_{n - 1}$ we consider all of the activities, it is actually the solution of the problem.","title":"16.1-5"},{"location":"Chap16/16.2/","text":"16.2-1 Prove that the fractional knapsack problem has the greedy-choice property. Let $I$ be the following instance of the knapsack problem: Let $n$ be the number of items, let $v_i$ be the value of the $i$th item, let $w_i$ be the weight of the $i$th item and let $W$ be the capacity. Assume the items have been ordered in increasing order by $v_i / w_i$ and that $W \\ge w_n$. Let $s = (s_1, s_2, \\ldots, s_n)$ be a solution. The greedy algorithm works by assigning $s_n = \\min(w_n, W)$, and then continuing by solving the subproblem $$I' = (n - 1, \\{v_1, v_2, \\ldots, v_{n - 1}\\}, \\{w_1, w_2, \\ldots, w_{n - 1}\\}, W - w_n)$$ until it either reaches the state $W = 0$ or $n = 0$. We need to show that this strategy always gives an optimal solution. We prove this by contradiction. Suppose the optimal solution to $I$ is $s_1, s_2, \\ldots, s_n$, where $s_n < \\min(w_n, W)$. Let $i$ be the smallest number such that $s_i > 0$. By decreasing $s_i$ to $\\max(0, W - w_n)$ and increasing $s_n$ by the same amount, we get a better solution. Since this a contradiction the assumption must be false. Hence the problem has the greedy-choice property. 16.2-2 Give a dynamic-programming solution to the $0$-$1$ knapsack problem that runs in $O(nW)$ time, where $n$ is the number of items and $W$ is the maximum weight of items that the thief can put in his knapsack. Suppose we know that a particular item of weight $w$ is in the solution. Then we must solve the subproblem on $n \u2212 1$ items with maximum weight $W \u2212 w$. Thus, to take a bottom-up approach we must solve the $0$-$1$ knapsack problem for all items and possible weights smaller than W. We'll build an $n + 1$ by $W + 1$ table of values where the rows are indexed by item and the columns are indexed by total weight. (The first row and column of the table will be a dummy row). For row $i$ column $j$, we decide whether or not it would be advantageous to include item i in the knapsack by comparing the total value of of a knapsack including items $1$ through $i \u2212 1$ with max weight $j$, and the total value of including items $1$ through $i \u2212 1$ with max weight $j \u2212 i.weight$ and also item $i$. To solve the problem, we simply examine the $n$, $W$ entry of the table to determine the maximum value we can achieve. To read off the items we include, start with entry $n$, $W$. In general, proceed as follows: if entry $i$, $j$ equals entry $i - 1$, $j$, don't include item $i$, and examine entry $i - 1$, $j$ next. If entry $i$, $j$ doesn't equal entry $i \u2212 1$, $j$, include item $i$ and examine entry $i \u2212 1$, $j \u2212 i$.weight next. See algorithm below for construction of table: 0-1 - KNAPSACK ( n , W ) Initialize an ( n + 1 ) by ( W + 1 ) table K for i = 1 to n K [ i , 0 ] = 0 for j = 1 to W K [ 0 , j ] = 0 for i = 1 to n for j = 1 to W if j < i . weight K [ i , j ] = K [ i - 1 , j ] else K [ i , j ] = max ( K [ i - 1 , j ], K [ i - 1 , j - i . weight ] + i . value ) 16.2-3 Suppose that in a $0$-$1$ knapsack problem, the order of the items when sorted by increasing weight is the same as their order when sorted by decreasing value. Give an efficient algorithm to find an optimal solution to this variant of the knapsack problem, and argue that your algorithm is correct. Suppose in an optimal solution we take an item with $v_1$, $w_1$, and drop an item with $v_2$, $w_2$, and $w_1 > w_2$, $v_1 < v_2$, we can substitude $1$ with $2$ and get a better solution. Therefore we should always choose the items with the greatest values. 16.2-4 Professor Gekko has always dreamed of inline skating across North Dakota. He plans to cross the state on highway U.S. 2, which runs from Grand Forks, on the eastern border with Minnesota, to Williston, near the western border with Montana. The professor can carry two liters of water, and he can skate $m$ miles before running out of water. (Because North Dakota is relatively flat, the professor does not have to worry about drinking water at a greater rate on uphill sections than on flat or downhill sections.) The professor will start in Grand Forks with two full liters of water. His official North Dakota state map shows all the places along U.S. 2 at which he can refill his water and the distances between these locations. The professor's goal is to minimize the number of water stops along his route across the state. Give an efficient method by which he can determine which water stops he should make. Prove that your strategy yields an optimal solution, and give its running time. The greedy solution solves this problem optimally, where we maximize distance we can cover from a particular point such that there still exists a place to get water before we run out. The first stop is at the furthest point from the starting position which is less than or equal to $m$ miles away. The problem exhibits optimal substructure, since once we have chosen a first stopping point $p$, we solve the subproblem assuming we are starting at $p$. Combining these two plans yields an optimal solution for the usual cut-and-paste reasons. Now we must show that this greedy approach in fact yields a first stopping point which is contained in some optimal solution. Let $O$ be any optimal solution which has the professor stop at positions $o_1, o_2, \\dots, o_k$. Let $g_1$ denote the furthest stopping point we can reach from the starting point. Then we may replace $o_1$ by $g_2$ to create a modified solution $G$, since $o_2 - o_1 < o_2 - g_1$. In other words, we can actually make it to the positions in $G$ without running out of water. Since $G$ has the same number of stops, we conclude that $g_1$ is contained in some optimal solution. Therefore the greedy strategy works. 16.2-5 Describe an efficient algorithm that, given a set $\\{x_1, x_2, \\ldots, x_n\\}$ of points on the real line, determines the smallest set of unit-length closed intervals that contains all of the given points. Argue that your algorithm is correct. Consider the leftmost interval. It will do no good if it extends any further left than the leftmost point, however, we know that it must contain the leftmost point. So, we know that it's left hand side is exactly the leftmost point. So, we just remove any point that is within a unit distance of the left most point since they are contained in this single interval. Then, we just repeat until all points are covered. Since at each step there is a clearly optimal choice for where to put the leftmost interval, this final solution is optimal. 16.2-6 $\\star$ Show how to solve the fractional knapsack problem in $O(n)$ time. First compute the value of each item, defined to be it's worth divided by its weight. We use a recursive approach as follows, find the item of median value, which can be done in linear time as shown in chapter 9. Then sum the weights of all items whose value exceeds the median and call it $M$. If $M$ exceeds $W$ then we know that the solution to the fractional knapsack problem lies in taking items from among this collection. In other words, we're now solving the fractional knapsack problem on input of size $n / 2$. On the other hand, if the weight doesn't exceed $W$, then we must solve the fractional knapsack problem on the input of $n / 2$ low-value items, with maximum weight $W \u2212 M$. Let $T(n)$ denote the runtime of the algorithm. Since we can solve the problem when there is only one item in constant time, the recursion for the runtime is $T(n) = T(n / 2) + cn$ and $T(1) = d$, which gives runtime of $O(n)$. 16.2-7 Suppose you are given two sets $A$ and $B$, each containing $n$ positive integers. You can choose to reorder each set however you like. After reordering, let $a_i$ be the $i$th element of set $A$, and let $b_i$ be the $i$th element of set $B$. You then receive a payoff of $\\prod_{i = 1}^n a_i^{b_i}$. Give an algorithm that will maximize your payoff. Prove that your algorithm maximizes the payoff, and state its running time. Since an idential permutation of both sets doesn't affect this product, suppose that $A$ is sorted in ascending order. Then, we will prove that the product is maximized when $B$ is also sorted in ascending order. To see this, suppose not, that is, there is some $i < j$ so that $a_i < a_j$ and $b_i > b_j$. Then, consider only the contribution to the product from the indices $i$ and $j$. That is, $a_i^{b_i}a_j^{b_j}$, then, if we were to swap the order of $b_1$ and $b_j$, we would have that contribution be $a_i^{b_j}a_j^{b_i}$. we can see that this is larger than the previous expression because it differs by a factor of $\\left(\\frac{a_j}{a_i}\\right)^{b_i - b_j}$ which is bigger than one. So, we couldn't of maximized the product with this ordering on $B$.","title":"16.2 Elements of the greedy strategy"},{"location":"Chap16/16.2/#162-1","text":"Prove that the fractional knapsack problem has the greedy-choice property. Let $I$ be the following instance of the knapsack problem: Let $n$ be the number of items, let $v_i$ be the value of the $i$th item, let $w_i$ be the weight of the $i$th item and let $W$ be the capacity. Assume the items have been ordered in increasing order by $v_i / w_i$ and that $W \\ge w_n$. Let $s = (s_1, s_2, \\ldots, s_n)$ be a solution. The greedy algorithm works by assigning $s_n = \\min(w_n, W)$, and then continuing by solving the subproblem $$I' = (n - 1, \\{v_1, v_2, \\ldots, v_{n - 1}\\}, \\{w_1, w_2, \\ldots, w_{n - 1}\\}, W - w_n)$$ until it either reaches the state $W = 0$ or $n = 0$. We need to show that this strategy always gives an optimal solution. We prove this by contradiction. Suppose the optimal solution to $I$ is $s_1, s_2, \\ldots, s_n$, where $s_n < \\min(w_n, W)$. Let $i$ be the smallest number such that $s_i > 0$. By decreasing $s_i$ to $\\max(0, W - w_n)$ and increasing $s_n$ by the same amount, we get a better solution. Since this a contradiction the assumption must be false. Hence the problem has the greedy-choice property.","title":"16.2-1"},{"location":"Chap16/16.2/#162-2","text":"Give a dynamic-programming solution to the $0$-$1$ knapsack problem that runs in $O(nW)$ time, where $n$ is the number of items and $W$ is the maximum weight of items that the thief can put in his knapsack. Suppose we know that a particular item of weight $w$ is in the solution. Then we must solve the subproblem on $n \u2212 1$ items with maximum weight $W \u2212 w$. Thus, to take a bottom-up approach we must solve the $0$-$1$ knapsack problem for all items and possible weights smaller than W. We'll build an $n + 1$ by $W + 1$ table of values where the rows are indexed by item and the columns are indexed by total weight. (The first row and column of the table will be a dummy row). For row $i$ column $j$, we decide whether or not it would be advantageous to include item i in the knapsack by comparing the total value of of a knapsack including items $1$ through $i \u2212 1$ with max weight $j$, and the total value of including items $1$ through $i \u2212 1$ with max weight $j \u2212 i.weight$ and also item $i$. To solve the problem, we simply examine the $n$, $W$ entry of the table to determine the maximum value we can achieve. To read off the items we include, start with entry $n$, $W$. In general, proceed as follows: if entry $i$, $j$ equals entry $i - 1$, $j$, don't include item $i$, and examine entry $i - 1$, $j$ next. If entry $i$, $j$ doesn't equal entry $i \u2212 1$, $j$, include item $i$ and examine entry $i \u2212 1$, $j \u2212 i$.weight next. See algorithm below for construction of table: 0-1 - KNAPSACK ( n , W ) Initialize an ( n + 1 ) by ( W + 1 ) table K for i = 1 to n K [ i , 0 ] = 0 for j = 1 to W K [ 0 , j ] = 0 for i = 1 to n for j = 1 to W if j < i . weight K [ i , j ] = K [ i - 1 , j ] else K [ i , j ] = max ( K [ i - 1 , j ], K [ i - 1 , j - i . weight ] + i . value )","title":"16.2-2"},{"location":"Chap16/16.2/#162-3","text":"Suppose that in a $0$-$1$ knapsack problem, the order of the items when sorted by increasing weight is the same as their order when sorted by decreasing value. Give an efficient algorithm to find an optimal solution to this variant of the knapsack problem, and argue that your algorithm is correct. Suppose in an optimal solution we take an item with $v_1$, $w_1$, and drop an item with $v_2$, $w_2$, and $w_1 > w_2$, $v_1 < v_2$, we can substitude $1$ with $2$ and get a better solution. Therefore we should always choose the items with the greatest values.","title":"16.2-3"},{"location":"Chap16/16.2/#162-4","text":"Professor Gekko has always dreamed of inline skating across North Dakota. He plans to cross the state on highway U.S. 2, which runs from Grand Forks, on the eastern border with Minnesota, to Williston, near the western border with Montana. The professor can carry two liters of water, and he can skate $m$ miles before running out of water. (Because North Dakota is relatively flat, the professor does not have to worry about drinking water at a greater rate on uphill sections than on flat or downhill sections.) The professor will start in Grand Forks with two full liters of water. His official North Dakota state map shows all the places along U.S. 2 at which he can refill his water and the distances between these locations. The professor's goal is to minimize the number of water stops along his route across the state. Give an efficient method by which he can determine which water stops he should make. Prove that your strategy yields an optimal solution, and give its running time. The greedy solution solves this problem optimally, where we maximize distance we can cover from a particular point such that there still exists a place to get water before we run out. The first stop is at the furthest point from the starting position which is less than or equal to $m$ miles away. The problem exhibits optimal substructure, since once we have chosen a first stopping point $p$, we solve the subproblem assuming we are starting at $p$. Combining these two plans yields an optimal solution for the usual cut-and-paste reasons. Now we must show that this greedy approach in fact yields a first stopping point which is contained in some optimal solution. Let $O$ be any optimal solution which has the professor stop at positions $o_1, o_2, \\dots, o_k$. Let $g_1$ denote the furthest stopping point we can reach from the starting point. Then we may replace $o_1$ by $g_2$ to create a modified solution $G$, since $o_2 - o_1 < o_2 - g_1$. In other words, we can actually make it to the positions in $G$ without running out of water. Since $G$ has the same number of stops, we conclude that $g_1$ is contained in some optimal solution. Therefore the greedy strategy works.","title":"16.2-4"},{"location":"Chap16/16.2/#162-5","text":"Describe an efficient algorithm that, given a set $\\{x_1, x_2, \\ldots, x_n\\}$ of points on the real line, determines the smallest set of unit-length closed intervals that contains all of the given points. Argue that your algorithm is correct. Consider the leftmost interval. It will do no good if it extends any further left than the leftmost point, however, we know that it must contain the leftmost point. So, we know that it's left hand side is exactly the leftmost point. So, we just remove any point that is within a unit distance of the left most point since they are contained in this single interval. Then, we just repeat until all points are covered. Since at each step there is a clearly optimal choice for where to put the leftmost interval, this final solution is optimal.","title":"16.2-5"},{"location":"Chap16/16.2/#162-6-star","text":"Show how to solve the fractional knapsack problem in $O(n)$ time. First compute the value of each item, defined to be it's worth divided by its weight. We use a recursive approach as follows, find the item of median value, which can be done in linear time as shown in chapter 9. Then sum the weights of all items whose value exceeds the median and call it $M$. If $M$ exceeds $W$ then we know that the solution to the fractional knapsack problem lies in taking items from among this collection. In other words, we're now solving the fractional knapsack problem on input of size $n / 2$. On the other hand, if the weight doesn't exceed $W$, then we must solve the fractional knapsack problem on the input of $n / 2$ low-value items, with maximum weight $W \u2212 M$. Let $T(n)$ denote the runtime of the algorithm. Since we can solve the problem when there is only one item in constant time, the recursion for the runtime is $T(n) = T(n / 2) + cn$ and $T(1) = d$, which gives runtime of $O(n)$.","title":"16.2-6 $\\star$"},{"location":"Chap16/16.2/#162-7","text":"Suppose you are given two sets $A$ and $B$, each containing $n$ positive integers. You can choose to reorder each set however you like. After reordering, let $a_i$ be the $i$th element of set $A$, and let $b_i$ be the $i$th element of set $B$. You then receive a payoff of $\\prod_{i = 1}^n a_i^{b_i}$. Give an algorithm that will maximize your payoff. Prove that your algorithm maximizes the payoff, and state its running time. Since an idential permutation of both sets doesn't affect this product, suppose that $A$ is sorted in ascending order. Then, we will prove that the product is maximized when $B$ is also sorted in ascending order. To see this, suppose not, that is, there is some $i < j$ so that $a_i < a_j$ and $b_i > b_j$. Then, consider only the contribution to the product from the indices $i$ and $j$. That is, $a_i^{b_i}a_j^{b_j}$, then, if we were to swap the order of $b_1$ and $b_j$, we would have that contribution be $a_i^{b_j}a_j^{b_i}$. we can see that this is larger than the previous expression because it differs by a factor of $\\left(\\frac{a_j}{a_i}\\right)^{b_i - b_j}$ which is bigger than one. So, we couldn't of maximized the product with this ordering on $B$.","title":"16.2-7"},{"location":"Chap16/16.3/","text":"16.3-1 Explain why, in the proof of Lemma 16.2, if $x.freq = b.freq$, then we must have $a.freq = b.freq = x.freq = y.freq$. If we have that $x.freq = b.freq$, then we know that $b$ is tied for lowest frequency. In particular, it means that there are at least two things with lowest frequency, so $y.freq = x.freq$. Also, since $x.freq \\le a.freq \\le b.freq = x.freq$, we must have $a.freq = x.freq$. 16.3-2 Prove that a binary tree that is not full cannot correspond to an optimal prefix code. Let $T$ be a binary tree that is not full. $T$ represents a binary prefix code for a file composed of characters from alphabet $C$, where $c \\in C$, $f(c)$ is th number of occurrences of $c$ in the file. The cost of tree $T$, or the number of bits in the encoding, is $\\sum_{c \\in C} d_T(c) \\cdot f(c)$, where $d_T(c)$ is the depth of character $c$ in tree $T$. Let $N$ be a node of greatest depth that has exactly one child. If $N$ is the root of $T$, $N$ can be removed and the deepth of each node reduced by one, yielding a tree representing the same alphabet with a lower cost. This mean the original code was not optimal. Otherwise, let $M$ be the parent of $N$, let $T_1$ be the (possibly non-existent) sibling of $N$, and let $T_2$ be the subtree rooted at the child of $N$. Replace $M$ by $N$, making the children of $N$ the roots of subtrees $T_1$ and $T_2$. If $T_1$ is empty, repeat the process. We have a new prefix code of lower cost, so the original was not optimal. 16.3-3 What is an optimal Huffman code for the following set of frequencies, based on the first $8$ Fibonacci numbers? $$a:1 \\quad b:1 \\quad c:2 \\quad d:3 \\quad e:5 \\quad f:8 \\quad g:13 \\quad h:21$$ Can you generalize your answer to find the optimal code when the frequencies are the first $n$ Fibonacci numbers? $$ \\begin{array}{c|l} a & 1111111 \\\\ b & 1111110 \\\\ c & 111110 \\\\ d & 11110 \\\\ e & 1110 \\\\ f & 110 \\\\ g & 10 \\\\ h & 0 \\end{array} $$ GENERALIZATION In what follows we use $a_i$ to denote $i$-th Fibonacci number. To avoid any confusiion we stress that we consider Fibonacci's sequence beginning $1$, $1$, i.e. $a_1 = a_2 = 1$. Let us consider a set of $n$ symbols $\\Sigma = \\{c_i ~|~ 1 \\le i \\le n \\}$ such that for each $i$ we have $c_i.freq = a_i$. We shall prove that the Huffman code for this set of symbols given by the run of algorithm HUFFMAN from CLRS is the following code: $code(c_n) = 0$ $code(c_{i - 1}) = 1code(c_i)$ for $2 \\le i \\le n - 1$ (i.e. we take a code for symbol $c_i$ and add $1$ to the beginning) $code(c_1) = 1^{n - 1}$ By $code(c)$ we mean the codeword assigned to the symbol $c_i$ by the run of HUFFMAN($\\Sigma$) for any $c \\in \\Sigma$. First we state two technical claims which can be easily proven using the proper induction. Following good manners of our field we leave the proofs to the reader :-) (HELPFUL CLAIM 1) $ (\\forall k \\in \\mathbb{N}) ~ \\sum\\limits_{i = 1}^{k} a_i = a_{k + 2} - 1$ (HELPFUL CLAIM 2) Let $z$ be an inner node of tree $T$ constructed by the algorithm HUFFMAN. Then $z.freq$ is sum of frequencies of all leafs of the subtree of $T$ rooted in $z$. Consider tree $T_n$ inductively defined by $T_2.left = c_2$, $T_2.right = c_1$ and $T_2.freq = c_1.freq + c_2.freq = 2$ $(\\forall i; 3 \\le i \\le n) ~ T_i.left = c_i$, $T_i.right = T_{i - 1}$ and $T_i.freq = c_i.freq + T_{i - 1}.freq$ We shall prove that $T_n$ is the tree produced by the run of HUFFMAN($\\Sigma$). KEY CLAIM: $T_{i + 1}$ is exactly the node $z$ constructed in $i$-th run of the for-cycle of HUFFMAN($\\Sigma$) and the content of the priority queue $Q$ just after $i$-th run of the for-cycle is exactly $Q = (a_{i + 2}, T_{i + 1}, a_{i + 3}, \\dots, a_n)$ with $a_{i + 2}$ being the minimal element for each $1 \\le i < n$. (Since we prefer not to overload our formal notation we just note that for $i = n - 1$ we claim that $Q = (T_n)$ and our notation grasp this fact in a sense.) PROOF OF KEY CLAIM by induction on $i$. for $i = 1$ we see that the characters with lowest frequencies are exactly $c_1$ and $c_2$, thus obviously the algorithm HUFFMAN($\\Sigma$) constructs $T_2$ in the first run of its for-cycle. Also it is obvious that just after this run of the for-cycle we have $Q = (a_3, T_{2}, a_4, \\dots, a_n)$. for $2 \\le i < n$ we suppose that our claim is true for all $j < i$ and prove the claim for $i$. Since the claim is true for $i - 1$, we know that just before $i-th$ execution of the for-cycle we have the following content of the priority queue $Q=(a_{i + 1}, T_i, a_{i + 2}, \\dots, a_n)$. Thus line 5 of HUFFMAN extracts $a_{i + 1}$ and sets $z.left = a_{i + 1}$ and line 6 of HUFFMAN extracts $T_i$ and sets $z.right = T_i$. Now we can see that indeed $z$ is exactly $T_{i + 1}$. Using (CLAIM 2) and observing the way $T_{i + 1}$ is defined we get that $z.freq = T_{i + 1}.freq = \\sum\\limits_{i=1}^{i + 1} a_i$. Thus using (CLAIM 1) one can see that $a_{i + 2} < T_{i + 1}.freq < a_{i + 3}$. Therefore for the content of the priority queue $Q$ just after the $i$-th execution of the for-cycle we have $Q=(a_{i + 2}, T_{i + 2}, a_{i + 3}, \\dots, a_n)$. KEY CLAIM tells us that just after the last execution of the for-cycle we have $Q = (T_n)$ and therefore the line 9 of HUFFMAN returns $T_n$ as the result. One can easily see that the code given in the beginning is exactly the code which corresponds to the code-tree $T_n$. 16.3-4 Prove that we can also express the total cost of a tree for a code as the sum, over all internal nodes, of the combined frequencies of the two children of the node. Let tree be a full binary tree with $n$ leaves. Apply induction hypothesis on the number of leaves in $T$. When $n = 2$ (the case $n = 1$ is trivially true), there are two leaves $x$ and $y$ with the same parent $z$, then the cost of $T$ is $$ \\begin{aligned} B(T) & = f(x) d_T(x) + f(y) d_T(y) \\\\ & = f(x) + f(y) & \\text{since $d_T(x) = d_T(y) = 1$} \\\\ & = f(\\text{child}_1\\text{ of }z) + f(\\text{child}_2\\text{ of }z). \\end{aligned} $$ Thus, the statement of theorem is true. Now suppose $n > 2$ and also suppose that theorem is true for trees on $n - 1$ leaves. Let $c_1$ and $c_2$ are two sibling leaves in $T$ such that they have the same parent $p$. Letting $T'$ be the tree obtained by deleting $c_1$ and $c_2$, by induction we know that $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves } l'\\in T'} f(l')d_T(l') \\\\ & = \\sum_{\\text{internal nodes } i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i'). \\end{aligned} $$ Using this information, calculates the cost of $T$. $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves }l \\in T} f(l)d_T(l) \\\\ & = \\sum_{l \\ne c_1, c_2} f(l)d_T(l) + f(c_1)d_T(c_1) - 1 + f(c_2)d_T(c_2) - 1 + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i') + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i\\in T} f(\\text{child}_1\\text{ of }i) + f(\\text{child}_1\\text{ of }i). \\end{aligned} $$ Thus the statement is true. 16.3-5 Prove that if we order the characters in an alphabet so that their frequencies are monotonically decreasing, then there exists an optimal code whose codeword lengths are monotonically increasing. Little formal-mathematical note here: We are required to prove existence of an optimal code with some property. Therefore we are required also to show, that some optimal code exists. It is trivial in this case, since we know that the code produced by a run of Huffman's algorithm produce one such code for us. However, it is good to be aware of this. Proving just the implication \"if a code is optimal then it has the desired property\" doesn't suffice. OK, now we are ready to prove the already mentioned implication \"if a code is optimal then it has the desired property\". Main idea of our proof is that if the code violates desired property, then we find two symbols which violate the property and 'fix the code'. For the formal proof we go as follows. Suppose that we have an alphabet $C = {a_1, \\ldots, a_n}$ where the characters are written in monotonically decreasing order, i.e. $a_1.freq \\ge a_2.freq \\ge \\ldots \\ge a_n$. Let us consider an optimal code $B$ for $C$. Let us denote the codeword for the character $c \\in C$ in the code $B$ by $cw_B(c)$. W.l.o.g. we can assume that for any $i$ such that $a_i.freq = a_{i + 1}.freq$ it holds that $|cw(a_i)| \\le |cw(a_{i + 1})|$. This assumption can be made since for any $a_i.freq = a_{i + 1}.freq$ for which $|cw(a_i)| > |cw(a_{i + 1})|$ we can simply swap codewords for $a_i$ and $a_{i + 1}$ and obtain a code with desired property and the same cost as is the cost of $B$. We prove that $B$ has the desired property, i.e., its codeword lengths are monotonically increasing. We proceed by contradiction. If lengths of the codewords are not monotonically increasing, then there exist an index $i$ such that $|cw_B(a_i)| > |cw_B(a_{i + 1})| $. Using our assumptions on $C$ and $B$ we get that $a_i.freq > a_{i + 1}.freq$. Define new code $B'$ for $C$ such that for $a_j$ such that $j \\ne i$ and $j \\ne i + 1$ we keep $cw_{B'}(a_j) = cw_B(a_j)$ and we swap codewords for $a_i$ and $a_{j + 1}$, i.e. we set $cw_{B'}(a_i) = cw_{B}(a_{i + 1})$ and $cw_{B'}(a_{i + 1}) = cw_{B}(a_{i})$. Now compare costs of the codes $B$ and $B'$. It holds that $$ \\begin{aligned} cost(B') &= cost(B) - (|cw_B(a_i)|(a_i.freq) + |cw_B(a_{i + 1})|(a_{i + 1}.freq)) \\\\ &+ (|cw_B(a_i)|(a_{i + 1}.freq) + |cw_B(a_{i + 1})|(a_{i}.freq)) \\\\ &= cost(B) + |cw_B(a_i)|(a_{i + 1}.freq - a_i.freq) + |cw_B(a_{i + 1})|(a_i.freq - a_{i + 1}.freq) \\end{aligned} $$ For better readability now denote $a_i.freq - a_{i + 1}.freq = \\phi$. Since $a_i.freq > a_{i + 1}.freq$, we get $\\phi > 0$ and we can write $$ cost(B') = cost(B) - \\phi|cw_B(a_i)| + \\phi|cw_B(a_{i + 1})| = cost(B) - \\phi(|cw_B(a_i)| - |cw_B(a_{i + 1})|) $$ Since $|cw_B(a_i)| > |cw_B(a_{i + 1})| $, we get $|cw_B(a_i)| - |cw_B(a_{i + 1})| > 0$. Thus $\\phi(|cw_B(a_i)| - |cw_B(a_{i + 1})|) > 0$ which imply $cost(B') < cost(B)$. Therefore the code $B$ is not optimal, a contradiction. Therefore, we conclude that codeword lengths of $B$ are monotonically increasing and the proof is complete. Note: For those not familiar with mathematical parlance, w.l.o.g means without loss of generality. 16.3-6 Suppose we have an optimal prefix code on a set $C = \\{0, 1, \\ldots, n - 1 \\}$ of characters and we wish to transmit this code using as few bits as possible. Show how to represent any optimal prefix code on $C$ using only $2n - 1 + n \\lceil \\lg n \\rceil$ bits. ($\\textit{Hint:}$ Use $2n - 1$ bits to specify the structure of the tree, as discovered by a walk of the tree.) First observe that any full binary tree has exactly $2n - 1$ nodes. We can encode the structure of our full binary tree by performing a preorder traversal of $T$. For each node that we record in the traversal, write a $0$ if it is an internal node and a $1$ if it is a leaf node. Since we know the tree to be full, this uniquely determines its structure. Next, note that we can encode any character of $C$ in $\\lceil \\lg n \\rceil$ bits. Since there are $n$ characters, we can encode them in order of appearance in our preorder traversal using $n\\left\\lceil \\lg n \\right\\rceil$ bits. 16.3-7 Generalize Huffman's algorithm to ternary codewords (i.e., codewords using the symbols $0$, $1$, and $2$), and prove that it yields optimal ternary codes. Instead of grouping together the two with lowest frequency into pairs that have the smallest total frequency, we will group together the three with lowest frequency in order to have a final result that is a ternary tree. The analysis of optimality is almost identical to the binary case. We are placing the symbols of lowest frequency lower down in the final tree and so they will have longer codewords than the more frequently occurring symbols. 16.3-8 Suppose that a data file contains a sequence of $8$-bit characters such that all $256$ characters are about equally common: the maximum character frequency is less than twice the minimum character frequency. Prove that Huffman coding in this case is no more efficient than using an ordinary $8$-bit fixed-length code. For any $2$ characters, the sum of their frequencies exceeds the frequency of any other character, so initially Huffman coding makes $128$ small trees with $2$ leaves each. At the next stage, no internal node has a label which is more than twice that of any other, so we are in the same setup as before. Continuing in this fashion, Huffman coding builds a complete binary tree of height $\\lg 256 = 8$, which is no more efficient than ordinary $8$-bit length codes. 16.3-9 Show that no compression scheme can expect to compress a file of randomly chosen $8$-bit characters by even a single bit. ($\\textit{Hint:}$ Compare the number of possible files with the number of possible encoded files.) If every possible character is equally likely, then, when constructing the Huffman code, we will end up with a complete binary tree of depth $7$. This means that every character, regardless of what it is will be represented using $7$ bits. This is exactly as many bits as was originally used to represent those characters, so the total length of the file will not decrease at all.","title":"16.3 Huffman codes"},{"location":"Chap16/16.3/#163-1","text":"Explain why, in the proof of Lemma 16.2, if $x.freq = b.freq$, then we must have $a.freq = b.freq = x.freq = y.freq$. If we have that $x.freq = b.freq$, then we know that $b$ is tied for lowest frequency. In particular, it means that there are at least two things with lowest frequency, so $y.freq = x.freq$. Also, since $x.freq \\le a.freq \\le b.freq = x.freq$, we must have $a.freq = x.freq$.","title":"16.3-1"},{"location":"Chap16/16.3/#163-2","text":"Prove that a binary tree that is not full cannot correspond to an optimal prefix code. Let $T$ be a binary tree that is not full. $T$ represents a binary prefix code for a file composed of characters from alphabet $C$, where $c \\in C$, $f(c)$ is th number of occurrences of $c$ in the file. The cost of tree $T$, or the number of bits in the encoding, is $\\sum_{c \\in C} d_T(c) \\cdot f(c)$, where $d_T(c)$ is the depth of character $c$ in tree $T$. Let $N$ be a node of greatest depth that has exactly one child. If $N$ is the root of $T$, $N$ can be removed and the deepth of each node reduced by one, yielding a tree representing the same alphabet with a lower cost. This mean the original code was not optimal. Otherwise, let $M$ be the parent of $N$, let $T_1$ be the (possibly non-existent) sibling of $N$, and let $T_2$ be the subtree rooted at the child of $N$. Replace $M$ by $N$, making the children of $N$ the roots of subtrees $T_1$ and $T_2$. If $T_1$ is empty, repeat the process. We have a new prefix code of lower cost, so the original was not optimal.","title":"16.3-2"},{"location":"Chap16/16.3/#163-3","text":"What is an optimal Huffman code for the following set of frequencies, based on the first $8$ Fibonacci numbers? $$a:1 \\quad b:1 \\quad c:2 \\quad d:3 \\quad e:5 \\quad f:8 \\quad g:13 \\quad h:21$$ Can you generalize your answer to find the optimal code when the frequencies are the first $n$ Fibonacci numbers? $$ \\begin{array}{c|l} a & 1111111 \\\\ b & 1111110 \\\\ c & 111110 \\\\ d & 11110 \\\\ e & 1110 \\\\ f & 110 \\\\ g & 10 \\\\ h & 0 \\end{array} $$ GENERALIZATION In what follows we use $a_i$ to denote $i$-th Fibonacci number. To avoid any confusiion we stress that we consider Fibonacci's sequence beginning $1$, $1$, i.e. $a_1 = a_2 = 1$. Let us consider a set of $n$ symbols $\\Sigma = \\{c_i ~|~ 1 \\le i \\le n \\}$ such that for each $i$ we have $c_i.freq = a_i$. We shall prove that the Huffman code for this set of symbols given by the run of algorithm HUFFMAN from CLRS is the following code: $code(c_n) = 0$ $code(c_{i - 1}) = 1code(c_i)$ for $2 \\le i \\le n - 1$ (i.e. we take a code for symbol $c_i$ and add $1$ to the beginning) $code(c_1) = 1^{n - 1}$ By $code(c)$ we mean the codeword assigned to the symbol $c_i$ by the run of HUFFMAN($\\Sigma$) for any $c \\in \\Sigma$. First we state two technical claims which can be easily proven using the proper induction. Following good manners of our field we leave the proofs to the reader :-) (HELPFUL CLAIM 1) $ (\\forall k \\in \\mathbb{N}) ~ \\sum\\limits_{i = 1}^{k} a_i = a_{k + 2} - 1$ (HELPFUL CLAIM 2) Let $z$ be an inner node of tree $T$ constructed by the algorithm HUFFMAN. Then $z.freq$ is sum of frequencies of all leafs of the subtree of $T$ rooted in $z$. Consider tree $T_n$ inductively defined by $T_2.left = c_2$, $T_2.right = c_1$ and $T_2.freq = c_1.freq + c_2.freq = 2$ $(\\forall i; 3 \\le i \\le n) ~ T_i.left = c_i$, $T_i.right = T_{i - 1}$ and $T_i.freq = c_i.freq + T_{i - 1}.freq$ We shall prove that $T_n$ is the tree produced by the run of HUFFMAN($\\Sigma$). KEY CLAIM: $T_{i + 1}$ is exactly the node $z$ constructed in $i$-th run of the for-cycle of HUFFMAN($\\Sigma$) and the content of the priority queue $Q$ just after $i$-th run of the for-cycle is exactly $Q = (a_{i + 2}, T_{i + 1}, a_{i + 3}, \\dots, a_n)$ with $a_{i + 2}$ being the minimal element for each $1 \\le i < n$. (Since we prefer not to overload our formal notation we just note that for $i = n - 1$ we claim that $Q = (T_n)$ and our notation grasp this fact in a sense.) PROOF OF KEY CLAIM by induction on $i$. for $i = 1$ we see that the characters with lowest frequencies are exactly $c_1$ and $c_2$, thus obviously the algorithm HUFFMAN($\\Sigma$) constructs $T_2$ in the first run of its for-cycle. Also it is obvious that just after this run of the for-cycle we have $Q = (a_3, T_{2}, a_4, \\dots, a_n)$. for $2 \\le i < n$ we suppose that our claim is true for all $j < i$ and prove the claim for $i$. Since the claim is true for $i - 1$, we know that just before $i-th$ execution of the for-cycle we have the following content of the priority queue $Q=(a_{i + 1}, T_i, a_{i + 2}, \\dots, a_n)$. Thus line 5 of HUFFMAN extracts $a_{i + 1}$ and sets $z.left = a_{i + 1}$ and line 6 of HUFFMAN extracts $T_i$ and sets $z.right = T_i$. Now we can see that indeed $z$ is exactly $T_{i + 1}$. Using (CLAIM 2) and observing the way $T_{i + 1}$ is defined we get that $z.freq = T_{i + 1}.freq = \\sum\\limits_{i=1}^{i + 1} a_i$. Thus using (CLAIM 1) one can see that $a_{i + 2} < T_{i + 1}.freq < a_{i + 3}$. Therefore for the content of the priority queue $Q$ just after the $i$-th execution of the for-cycle we have $Q=(a_{i + 2}, T_{i + 2}, a_{i + 3}, \\dots, a_n)$. KEY CLAIM tells us that just after the last execution of the for-cycle we have $Q = (T_n)$ and therefore the line 9 of HUFFMAN returns $T_n$ as the result. One can easily see that the code given in the beginning is exactly the code which corresponds to the code-tree $T_n$.","title":"16.3-3"},{"location":"Chap16/16.3/#163-4","text":"Prove that we can also express the total cost of a tree for a code as the sum, over all internal nodes, of the combined frequencies of the two children of the node. Let tree be a full binary tree with $n$ leaves. Apply induction hypothesis on the number of leaves in $T$. When $n = 2$ (the case $n = 1$ is trivially true), there are two leaves $x$ and $y$ with the same parent $z$, then the cost of $T$ is $$ \\begin{aligned} B(T) & = f(x) d_T(x) + f(y) d_T(y) \\\\ & = f(x) + f(y) & \\text{since $d_T(x) = d_T(y) = 1$} \\\\ & = f(\\text{child}_1\\text{ of }z) + f(\\text{child}_2\\text{ of }z). \\end{aligned} $$ Thus, the statement of theorem is true. Now suppose $n > 2$ and also suppose that theorem is true for trees on $n - 1$ leaves. Let $c_1$ and $c_2$ are two sibling leaves in $T$ such that they have the same parent $p$. Letting $T'$ be the tree obtained by deleting $c_1$ and $c_2$, by induction we know that $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves } l'\\in T'} f(l')d_T(l') \\\\ & = \\sum_{\\text{internal nodes } i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i'). \\end{aligned} $$ Using this information, calculates the cost of $T$. $$ \\begin{aligned} B(T) & = \\sum_{\\text{leaves }l \\in T} f(l)d_T(l) \\\\ & = \\sum_{l \\ne c_1, c_2} f(l)d_T(l) + f(c_1)d_T(c_1) - 1 + f(c_2)d_T(c_2) - 1 + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i'\\in T'} f(\\text{child}_1\\text{ of }i') + f(\\text{child}_2\\text{ of }i') + f(c_1) + f(c_2) \\\\ & = \\sum_{\\text{internal nodes }i\\in T} f(\\text{child}_1\\text{ of }i) + f(\\text{child}_1\\text{ of }i). \\end{aligned} $$ Thus the statement is true.","title":"16.3-4"},{"location":"Chap16/16.3/#163-5","text":"Prove that if we order the characters in an alphabet so that their frequencies are monotonically decreasing, then there exists an optimal code whose codeword lengths are monotonically increasing. Little formal-mathematical note here: We are required to prove existence of an optimal code with some property. Therefore we are required also to show, that some optimal code exists. It is trivial in this case, since we know that the code produced by a run of Huffman's algorithm produce one such code for us. However, it is good to be aware of this. Proving just the implication \"if a code is optimal then it has the desired property\" doesn't suffice. OK, now we are ready to prove the already mentioned implication \"if a code is optimal then it has the desired property\". Main idea of our proof is that if the code violates desired property, then we find two symbols which violate the property and 'fix the code'. For the formal proof we go as follows. Suppose that we have an alphabet $C = {a_1, \\ldots, a_n}$ where the characters are written in monotonically decreasing order, i.e. $a_1.freq \\ge a_2.freq \\ge \\ldots \\ge a_n$. Let us consider an optimal code $B$ for $C$. Let us denote the codeword for the character $c \\in C$ in the code $B$ by $cw_B(c)$. W.l.o.g. we can assume that for any $i$ such that $a_i.freq = a_{i + 1}.freq$ it holds that $|cw(a_i)| \\le |cw(a_{i + 1})|$. This assumption can be made since for any $a_i.freq = a_{i + 1}.freq$ for which $|cw(a_i)| > |cw(a_{i + 1})|$ we can simply swap codewords for $a_i$ and $a_{i + 1}$ and obtain a code with desired property and the same cost as is the cost of $B$. We prove that $B$ has the desired property, i.e., its codeword lengths are monotonically increasing. We proceed by contradiction. If lengths of the codewords are not monotonically increasing, then there exist an index $i$ such that $|cw_B(a_i)| > |cw_B(a_{i + 1})| $. Using our assumptions on $C$ and $B$ we get that $a_i.freq > a_{i + 1}.freq$. Define new code $B'$ for $C$ such that for $a_j$ such that $j \\ne i$ and $j \\ne i + 1$ we keep $cw_{B'}(a_j) = cw_B(a_j)$ and we swap codewords for $a_i$ and $a_{j + 1}$, i.e. we set $cw_{B'}(a_i) = cw_{B}(a_{i + 1})$ and $cw_{B'}(a_{i + 1}) = cw_{B}(a_{i})$. Now compare costs of the codes $B$ and $B'$. It holds that $$ \\begin{aligned} cost(B') &= cost(B) - (|cw_B(a_i)|(a_i.freq) + |cw_B(a_{i + 1})|(a_{i + 1}.freq)) \\\\ &+ (|cw_B(a_i)|(a_{i + 1}.freq) + |cw_B(a_{i + 1})|(a_{i}.freq)) \\\\ &= cost(B) + |cw_B(a_i)|(a_{i + 1}.freq - a_i.freq) + |cw_B(a_{i + 1})|(a_i.freq - a_{i + 1}.freq) \\end{aligned} $$ For better readability now denote $a_i.freq - a_{i + 1}.freq = \\phi$. Since $a_i.freq > a_{i + 1}.freq$, we get $\\phi > 0$ and we can write $$ cost(B') = cost(B) - \\phi|cw_B(a_i)| + \\phi|cw_B(a_{i + 1})| = cost(B) - \\phi(|cw_B(a_i)| - |cw_B(a_{i + 1})|) $$ Since $|cw_B(a_i)| > |cw_B(a_{i + 1})| $, we get $|cw_B(a_i)| - |cw_B(a_{i + 1})| > 0$. Thus $\\phi(|cw_B(a_i)| - |cw_B(a_{i + 1})|) > 0$ which imply $cost(B') < cost(B)$. Therefore the code $B$ is not optimal, a contradiction. Therefore, we conclude that codeword lengths of $B$ are monotonically increasing and the proof is complete. Note: For those not familiar with mathematical parlance, w.l.o.g means without loss of generality.","title":"16.3-5"},{"location":"Chap16/16.3/#163-6","text":"Suppose we have an optimal prefix code on a set $C = \\{0, 1, \\ldots, n - 1 \\}$ of characters and we wish to transmit this code using as few bits as possible. Show how to represent any optimal prefix code on $C$ using only $2n - 1 + n \\lceil \\lg n \\rceil$ bits. ($\\textit{Hint:}$ Use $2n - 1$ bits to specify the structure of the tree, as discovered by a walk of the tree.) First observe that any full binary tree has exactly $2n - 1$ nodes. We can encode the structure of our full binary tree by performing a preorder traversal of $T$. For each node that we record in the traversal, write a $0$ if it is an internal node and a $1$ if it is a leaf node. Since we know the tree to be full, this uniquely determines its structure. Next, note that we can encode any character of $C$ in $\\lceil \\lg n \\rceil$ bits. Since there are $n$ characters, we can encode them in order of appearance in our preorder traversal using $n\\left\\lceil \\lg n \\right\\rceil$ bits.","title":"16.3-6"},{"location":"Chap16/16.3/#163-7","text":"Generalize Huffman's algorithm to ternary codewords (i.e., codewords using the symbols $0$, $1$, and $2$), and prove that it yields optimal ternary codes. Instead of grouping together the two with lowest frequency into pairs that have the smallest total frequency, we will group together the three with lowest frequency in order to have a final result that is a ternary tree. The analysis of optimality is almost identical to the binary case. We are placing the symbols of lowest frequency lower down in the final tree and so they will have longer codewords than the more frequently occurring symbols.","title":"16.3-7"},{"location":"Chap16/16.3/#163-8","text":"Suppose that a data file contains a sequence of $8$-bit characters such that all $256$ characters are about equally common: the maximum character frequency is less than twice the minimum character frequency. Prove that Huffman coding in this case is no more efficient than using an ordinary $8$-bit fixed-length code. For any $2$ characters, the sum of their frequencies exceeds the frequency of any other character, so initially Huffman coding makes $128$ small trees with $2$ leaves each. At the next stage, no internal node has a label which is more than twice that of any other, so we are in the same setup as before. Continuing in this fashion, Huffman coding builds a complete binary tree of height $\\lg 256 = 8$, which is no more efficient than ordinary $8$-bit length codes.","title":"16.3-8"},{"location":"Chap16/16.3/#163-9","text":"Show that no compression scheme can expect to compress a file of randomly chosen $8$-bit characters by even a single bit. ($\\textit{Hint:}$ Compare the number of possible files with the number of possible encoded files.) If every possible character is equally likely, then, when constructing the Huffman code, we will end up with a complete binary tree of depth $7$. This means that every character, regardless of what it is will be represented using $7$ bits. This is exactly as many bits as was originally used to represent those characters, so the total length of the file will not decrease at all.","title":"16.3-9"},{"location":"Chap16/16.4/","text":"16.4-1 Show that $(S, \\mathcal I_k)$ is a matroid, where $S$ is any finite set and $\\mathcal I_k$ is the set of all subsets of $S$ of size at most $k$, where $k \\le |S|$. The first condition that $S$ is a finite set is a given. To prove the second condition we assume that $k \\ge 0$, this gets us that $\\mathcal I_k$ is nonempty. Also, to prove the hereditary property, suppose $A \\in \\mathcal I_k$ this means that $|A| \\le k$. Then, if $B \\subseteq A$, this means that $|B| \\le |A| \\le k$, so $B \\in \\mathcal I_k$. Lastly, we prove the exchange property by letting $A, B \\in \\mathcal I_k$ be such that $|A| < |B|$. Then, we can pick any element $x \\in B \\backslash A$, then, $$|A \\cup {x}| = |A| + 1 \\le |B| \\le k,$$ so, we can extend $A$ to $A \\cup \\{x\\} \\in \\mathcal I_k$. 16.4-2 $\\star$ Given an $m \\times n$ matrix $T$ over some field (such as the reals), show that $(S, \\mathcal I)$ is a matroid, where $S$ is the set of columns of $T$ and $A \\in \\mathcal I$ if and only if the columns in $A$ are linearly independent. Let $c_1, \\dots, c_m$ be the columns of $T$. Suppose $C = \\{c_{i1}, \\dots, c_{ik}\\}$ is dependent. Then there exist scalars $d_1, \\dots, d_k$ not all zero such that $\\sum_{j = 1}^k d_jc_{ij} = 0$. By adding columns to $C$ and assigning them to have coefficient $0$ in the sum, we see that any superset of $C$ is also dependent. By contrapositive, any subset of an independent set must be independent. Now suppose that $A$ and $B$ are two independent sets of columns with $|A| > |B|$. If we couldn't add any column of $A$ to be whilst preserving independence then it must be the case that every element of $A$ is a linear combination of elements of $B$. But this implies that $B$ spans a $|A|$-dimensional space, which is impossible. Therefore, our independence system must satisfy the exchange property, so it is in fact a matroid. 16.4-3 $\\star$ Show that if $(S, \\mathcal I)$ is a matroid, then $(S, \\mathcal I')$ is a matroid, where $\\mathcal I' = \\{A': S - A'$ contains some maximal $A \\in \\mathcal I\\}$. That is, the maximal independent sets of $(S, \\mathcal I')$ are just the complements of the maximal independent sets of $(S, \\mathcal I)$. Condition one of being a matroid is still satisfied because the base set hasn't changed. Next we show that $\\mathcal I'$ is nonempty. Let $A$ be any maximal element of $\\mathcal I$, then we have that $S - A \\in \\mathcal I'$ because $S - (S - A) = A \\subseteq A$ which is maximal in $\\mathcal I$. Next we show the hereditary property, suppose that $B \\subseteq A \\in \\mathcal I'$, then, there exists some $A' \\in \\mathcal I$ so that $S \u2212 A \\subseteq A'$, however, $S \u2212 B \\supseteq S \u2212 A \\subseteq A$ so $B \\in \\mathcal I'$. Last, we prove the exchange property. That is, if we have $B, A \\in \\mathcal I'$ and $|B| < |A|$, we can find an element $x$ in $A \u2212 B$ to add to $B$ so that it stays independent. We will split into two cases: The first case is that $|A - B| = 1$. Let $x \\in A-B$ be the only element in $A - B$. Since $|A| > |B|$ and $|A - B| = 1$, it follows in this case $B \\subset A$. We extend $B$ by $x$ and we have $B \\cup \\{x\\} = A \\in \\mathcal I'$. The second case is if the first case does not hold. Let $C$ be a maximal independent set of $\\mathcal I$ contained in $S \u2212 A$. Pick an aribitrary set of size $|C| \u2212 1$ from some maximal independent set contained in $S - B$, call it $$. Since $D$ is a subset of a maximal independent set, it is also independent, and so, by the exchange property, there is some $y \\in C \u2212 D$ so that $D \\cup \\{y\\}$ is a maximal independent set in $\\mathcal I$. Then, we select $x$ to be any element other than $y$ in $A \u2212 B$. Then, $S \u2212 (B \\cup \\{x\\})$ will still contain $D \\cup \\{y\\}$. This means that $B \\cup \\{x\\}$ is independent in $\\mathcal I'$. 16.4-4 $\\star$ Let $S$ be a finite set and let $S_1, S_2, \\ldots, S_k$ be a partition of $S$ into nonempty disjoint subsets. Define the structure $(S, \\mathcal I)$ by the condition that $\\mathcal I = \\{A: \\mid A \\cap S_i \\mid \\le 1$ for $i = 1, 2, \\ldots, k\\}$. Show that $(S, \\mathcal I)$ is a matroid. That is, the set of all sets $A$ that contain at most one member of each subset in the partition determines the independent sets of a matroid. Suppose $X \\subset Y$ and $Y \\in \\mathcal I$. Then $(X \\cap S_i) \\subset (Y \\cap S_i)$ for all $i$, so $$|X \\cap S_i| \\le |Y \\cap S_i| \\le 1$$ for all $1 \\le i \\le k$. Therefore $\\mathcal M$ is closed under inclusion. Now Let $A, B \\in \\mathcal I$ with $|A| > |B|$. Then there must exist some $j$ such that $|A \\cap S_j| = 1$ but $|B \\cap S_j| = 0$. Let $a \\in A \\cap S_j$. Then $a \\notin B$ and $|(B \\cup \\{a\\}) \\cap S_j| = 1$. Since $$|(B \\cup \\{a\\}) \\cap S_i| = |B \\cap S_i| \\le 1$$ for all $i \\ne j$, we must have $B \\cup \\{a\\} \\in \\mathcal I$. Therefore $\\mathcal M$ is a matroid. 16.4-5 Show how to transform the weight function of a weighted matroid problem, where the desired optimal solution is a minimum-weight maximal independent subset, to make it a standard weighted-matroid problem. Argue carefully that your transformation is correct. Suppose that $W$ is the largest weight that any one element takes. Then, define the new weight function $w_2(x) = 1 + W - w(x)$. This then assigns a strictly positive weight, and we will show that any independent set that that has maximum weight with respect to $w_2$ will have minimum weight with respect to $w$. Recall Theorem 16.6 since we will be using it, suppose that for our matriod, all maximal independent sets have size $S$. Then, suppose $M_1$ and $M_2$ are maximal independent sets so that $M_1$ is maximal with respect to $w_2$ and $M_2$ is minimal with respect to $w$. Then, we need to show that $w(M_1) = w(M_2)$. Suppose not to achieve a contradiction, then, by minimality of $M_2$, $w(M_1) > w(M_2)$. Rewriting both sides in terms of $w_2$, we have $$w_2(M_2) - (1 + W)S > w_2(M_1) - (1 + W)S,$$ so, $$w_2(M_2) > w_2(M_1).$$ This however contradicts maximality of $M_1$ with respect to $w_2$. So, we must have that $w(M_1) = w(M_2)$. So, a maximal independent set that has the largest weight with respect to $w_2$ also has the smallest weight with respect to $w$.","title":"16.4 Matroids and greedy methods"},{"location":"Chap16/16.4/#164-1","text":"Show that $(S, \\mathcal I_k)$ is a matroid, where $S$ is any finite set and $\\mathcal I_k$ is the set of all subsets of $S$ of size at most $k$, where $k \\le |S|$. The first condition that $S$ is a finite set is a given. To prove the second condition we assume that $k \\ge 0$, this gets us that $\\mathcal I_k$ is nonempty. Also, to prove the hereditary property, suppose $A \\in \\mathcal I_k$ this means that $|A| \\le k$. Then, if $B \\subseteq A$, this means that $|B| \\le |A| \\le k$, so $B \\in \\mathcal I_k$. Lastly, we prove the exchange property by letting $A, B \\in \\mathcal I_k$ be such that $|A| < |B|$. Then, we can pick any element $x \\in B \\backslash A$, then, $$|A \\cup {x}| = |A| + 1 \\le |B| \\le k,$$ so, we can extend $A$ to $A \\cup \\{x\\} \\in \\mathcal I_k$.","title":"16.4-1"},{"location":"Chap16/16.4/#164-2-star","text":"Given an $m \\times n$ matrix $T$ over some field (such as the reals), show that $(S, \\mathcal I)$ is a matroid, where $S$ is the set of columns of $T$ and $A \\in \\mathcal I$ if and only if the columns in $A$ are linearly independent. Let $c_1, \\dots, c_m$ be the columns of $T$. Suppose $C = \\{c_{i1}, \\dots, c_{ik}\\}$ is dependent. Then there exist scalars $d_1, \\dots, d_k$ not all zero such that $\\sum_{j = 1}^k d_jc_{ij} = 0$. By adding columns to $C$ and assigning them to have coefficient $0$ in the sum, we see that any superset of $C$ is also dependent. By contrapositive, any subset of an independent set must be independent. Now suppose that $A$ and $B$ are two independent sets of columns with $|A| > |B|$. If we couldn't add any column of $A$ to be whilst preserving independence then it must be the case that every element of $A$ is a linear combination of elements of $B$. But this implies that $B$ spans a $|A|$-dimensional space, which is impossible. Therefore, our independence system must satisfy the exchange property, so it is in fact a matroid.","title":"16.4-2 $\\star$"},{"location":"Chap16/16.4/#164-3-star","text":"Show that if $(S, \\mathcal I)$ is a matroid, then $(S, \\mathcal I')$ is a matroid, where $\\mathcal I' = \\{A': S - A'$ contains some maximal $A \\in \\mathcal I\\}$. That is, the maximal independent sets of $(S, \\mathcal I')$ are just the complements of the maximal independent sets of $(S, \\mathcal I)$. Condition one of being a matroid is still satisfied because the base set hasn't changed. Next we show that $\\mathcal I'$ is nonempty. Let $A$ be any maximal element of $\\mathcal I$, then we have that $S - A \\in \\mathcal I'$ because $S - (S - A) = A \\subseteq A$ which is maximal in $\\mathcal I$. Next we show the hereditary property, suppose that $B \\subseteq A \\in \\mathcal I'$, then, there exists some $A' \\in \\mathcal I$ so that $S \u2212 A \\subseteq A'$, however, $S \u2212 B \\supseteq S \u2212 A \\subseteq A$ so $B \\in \\mathcal I'$. Last, we prove the exchange property. That is, if we have $B, A \\in \\mathcal I'$ and $|B| < |A|$, we can find an element $x$ in $A \u2212 B$ to add to $B$ so that it stays independent. We will split into two cases: The first case is that $|A - B| = 1$. Let $x \\in A-B$ be the only element in $A - B$. Since $|A| > |B|$ and $|A - B| = 1$, it follows in this case $B \\subset A$. We extend $B$ by $x$ and we have $B \\cup \\{x\\} = A \\in \\mathcal I'$. The second case is if the first case does not hold. Let $C$ be a maximal independent set of $\\mathcal I$ contained in $S \u2212 A$. Pick an aribitrary set of size $|C| \u2212 1$ from some maximal independent set contained in $S - B$, call it $$. Since $D$ is a subset of a maximal independent set, it is also independent, and so, by the exchange property, there is some $y \\in C \u2212 D$ so that $D \\cup \\{y\\}$ is a maximal independent set in $\\mathcal I$. Then, we select $x$ to be any element other than $y$ in $A \u2212 B$. Then, $S \u2212 (B \\cup \\{x\\})$ will still contain $D \\cup \\{y\\}$. This means that $B \\cup \\{x\\}$ is independent in $\\mathcal I'$.","title":"16.4-3 $\\star$"},{"location":"Chap16/16.4/#164-4-star","text":"Let $S$ be a finite set and let $S_1, S_2, \\ldots, S_k$ be a partition of $S$ into nonempty disjoint subsets. Define the structure $(S, \\mathcal I)$ by the condition that $\\mathcal I = \\{A: \\mid A \\cap S_i \\mid \\le 1$ for $i = 1, 2, \\ldots, k\\}$. Show that $(S, \\mathcal I)$ is a matroid. That is, the set of all sets $A$ that contain at most one member of each subset in the partition determines the independent sets of a matroid. Suppose $X \\subset Y$ and $Y \\in \\mathcal I$. Then $(X \\cap S_i) \\subset (Y \\cap S_i)$ for all $i$, so $$|X \\cap S_i| \\le |Y \\cap S_i| \\le 1$$ for all $1 \\le i \\le k$. Therefore $\\mathcal M$ is closed under inclusion. Now Let $A, B \\in \\mathcal I$ with $|A| > |B|$. Then there must exist some $j$ such that $|A \\cap S_j| = 1$ but $|B \\cap S_j| = 0$. Let $a \\in A \\cap S_j$. Then $a \\notin B$ and $|(B \\cup \\{a\\}) \\cap S_j| = 1$. Since $$|(B \\cup \\{a\\}) \\cap S_i| = |B \\cap S_i| \\le 1$$ for all $i \\ne j$, we must have $B \\cup \\{a\\} \\in \\mathcal I$. Therefore $\\mathcal M$ is a matroid.","title":"16.4-4 $\\star$"},{"location":"Chap16/16.4/#164-5","text":"Show how to transform the weight function of a weighted matroid problem, where the desired optimal solution is a minimum-weight maximal independent subset, to make it a standard weighted-matroid problem. Argue carefully that your transformation is correct. Suppose that $W$ is the largest weight that any one element takes. Then, define the new weight function $w_2(x) = 1 + W - w(x)$. This then assigns a strictly positive weight, and we will show that any independent set that that has maximum weight with respect to $w_2$ will have minimum weight with respect to $w$. Recall Theorem 16.6 since we will be using it, suppose that for our matriod, all maximal independent sets have size $S$. Then, suppose $M_1$ and $M_2$ are maximal independent sets so that $M_1$ is maximal with respect to $w_2$ and $M_2$ is minimal with respect to $w$. Then, we need to show that $w(M_1) = w(M_2)$. Suppose not to achieve a contradiction, then, by minimality of $M_2$, $w(M_1) > w(M_2)$. Rewriting both sides in terms of $w_2$, we have $$w_2(M_2) - (1 + W)S > w_2(M_1) - (1 + W)S,$$ so, $$w_2(M_2) > w_2(M_1).$$ This however contradicts maximality of $M_1$ with respect to $w_2$. So, we must have that $w(M_1) = w(M_2)$. So, a maximal independent set that has the largest weight with respect to $w_2$ also has the smallest weight with respect to $w$.","title":"16.4-5"},{"location":"Chap16/16.5/","text":"16.5-1 Solve the instance of the scheduling problem given in Figure 16.7, but with each penalty $w_i$ replaced by $80 - w_i$. $$ \\begin{array}{c|ccccccc} a_i & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline d_i & 4 & 2 & 4 & 3 & 1 & 4 & 6 \\\\ w_i & 10 & 20 & 30 & 40 & 50 & 60 & 70 \\end{array} $$ We begin by just greedily constructing the matroid, adding the most costly to leave incomplete tasks first. So, we add tasks $7, 6, 5, 4, 3$. Then, in order to schedule tasks $1$ or $2$ we need to leave incomplete more important tasks. So, our final schedule is $\\langle 5, 3, 4, 6, 7, 1, 2 \\rangle$ to have a total penalty of only $w_1 + w_2 = 30$. 16.5-2 Show how to use property 2 of Lemma 16.12 to determine in time $O(|A|)$ whether or not a given set $A$ of tasks is independent. We provide a pseudocode which grasps main ideas of an algorithm. IS - INDEPENDENT ( A ) n = A . length let Nts [ 0. . n ] be an array filled with 0 s for each a in A if a . deadline >= n Nts [ n ] = Nts [ n ] + 1 else Nts [ d ] = Nts [ d ] + 1 for i = 1 to n Nts [ i ] = Nts [ i ] + Nts [ i - 1 ] // at this moment, Nts[i] holds value of N_i(A) for i = 1 to n if Nts [ i ] > i return false return true","title":"16.5 A task-scheduling problem as a matroid"},{"location":"Chap16/16.5/#165-1","text":"Solve the instance of the scheduling problem given in Figure 16.7, but with each penalty $w_i$ replaced by $80 - w_i$. $$ \\begin{array}{c|ccccccc} a_i & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline d_i & 4 & 2 & 4 & 3 & 1 & 4 & 6 \\\\ w_i & 10 & 20 & 30 & 40 & 50 & 60 & 70 \\end{array} $$ We begin by just greedily constructing the matroid, adding the most costly to leave incomplete tasks first. So, we add tasks $7, 6, 5, 4, 3$. Then, in order to schedule tasks $1$ or $2$ we need to leave incomplete more important tasks. So, our final schedule is $\\langle 5, 3, 4, 6, 7, 1, 2 \\rangle$ to have a total penalty of only $w_1 + w_2 = 30$.","title":"16.5-1"},{"location":"Chap16/16.5/#165-2","text":"Show how to use property 2 of Lemma 16.12 to determine in time $O(|A|)$ whether or not a given set $A$ of tasks is independent. We provide a pseudocode which grasps main ideas of an algorithm. IS - INDEPENDENT ( A ) n = A . length let Nts [ 0. . n ] be an array filled with 0 s for each a in A if a . deadline >= n Nts [ n ] = Nts [ n ] + 1 else Nts [ d ] = Nts [ d ] + 1 for i = 1 to n Nts [ i ] = Nts [ i ] + Nts [ i - 1 ] // at this moment, Nts[i] holds value of N_i(A) for i = 1 to n if Nts [ i ] > i return false return true","title":"16.5-2"},{"location":"Chap16/Problems/16-1/","text":"Consider the problem of making change for $n$ cents using the fewest number of coins. Assume that each coin's value is an integer. a. Describe a greedy algorithm to make change consisting of quarters, dimes, nickels, and pennies. Prove that your algorithm yields an optimal solution. b. Suppose that the available coins are in the denominations that are powers of $c$, i.e., the denominations are $c^0, c^1, \\ldots, c^k$ for some integers $c > 1$ and $k \\ge 1$. Show that the greedy algorithm always yields an optimal solution. c. Give a set of coin denominations for which the greedy algorithm does not yield an optimal solution. Your set should include a penny so that there is a solution for every value of $n$. d. Give an $O(nk)$-time algorithm that makes change for any set of $k$ different coin denominations, assuming that one of the coins is a penny. a. Always give the highest denomination coin that you can without going over. Then, repeat this process until the amount of remaining change drops to $0$. b. Given an optimal solution $(x_0, x_1, \\dots, x_k)$ where $x_i$ indicates the number of coins of denomination $c_i$ . We will first show that we must have $x_i < c$ for every $i < k$. Suppose that we had some $x_i \\ge c$, then, we could decrease $x_i$ by $c$ and increase $x_{i + 1}$ by $1$. This collection of coins has the same value and has $c \u2212 1$ fewer coins, so the original solution must of been non-optimal. This configuration of coins is exactly the same as you would get if you kept greedily picking the largest coin possible. This is because to get a total value of $V$, you would pick $x_k = \\lfloor V c^{\u2212k} \\rfloor$ and for $i < k$, $x_i\\lfloor (V\\mod c^{i + 1})c^{-i} \\rfloor$. This is the only solution that satisfies the property that there aren't more than $c$ of any but the largest denomination because the coin amounts are a base $c$ representation of $V\\mod c^k$. c. Let the coin denominations be $\\{1, 3, 4\\}$, and the value to make change for be $6$. The greedy solution would result in the collection of coins $\\{1, 1, 4\\}$ but the optimal solution would be $\\{3, 3\\}$. d. See algorithm $\\text{MAKE-CHANGE}(S, v)$ which does a dynamic programming solution. Since the first forloop runs $n$ times, and the inner for loop runs $k$ times, and the later while loop runs at most $n$ times, the total running time is $O(nk)$.","title":"16-1 Coin changing"},{"location":"Chap16/Problems/16-2/","text":"Suppose you are given a set $S = \\{a_1, a_2, \\ldots, a_n\\}$ of tasks, where task $a_i$ requires $p_i$ units of processing time to complete, once it has started. You have one computer on which to run these tasks, and the computer can run only one task at a time. Let $c_i$ be the completion time of task $a_i$ , that is, the time at which task $a_i$ completes processing. Your goal is to minimize the average completion time, that is, to minimize $(1 / n) \\sum_{i = 1}^n c_i$. For example, suppose there are two tasks, $a_1$ and $a_2$, with $p_1 = 3$ and $p_2 = 5$, and consider the schedule in which $a_2$ runs first, followed by $a_1$. Then $c_2 = 5$, $c_1 = 8$, and the average completion time is $(5 + 8) / 2 = 6.5$. If task $a_1$ runs first, however, then $c_1 = 3$, $c_2 = 8$, and the average completion time is $(3 + 8) / 2 = 5.5$. a. Give an algorithm that schedules the tasks so as to minimize the average completion time. Each task must run non-preemptively, that is, once task $a_i$ starts, it must run continuously for $p_i$ units of time. Prove that your algorithm minimizes the average completion time, and state the running time of your algorithm. b. Suppose now that the tasks are not all available at once. That is, each task cannot start until its release time $r_i$. Suppose also that we allow preemption , so that a task can be suspended and restarted at a later time. For example, a task $a_i$ with processing time $p_i = 6$ and release time $r_i = 1$ might start running at time $1$ and be preempted at time $4$. It might then resume at time $10$ but be preempted at time $11$, and it might finally resume at time $13$ and complete at time $15$. Task $a_i$ has run for a total of $6$ time units, but its running time has been divided into three pieces. In this scenario, $a_i$'s completion time is $15$. Give an algorithm that schedules the tasks so as to minimize the average completion time in this new scenario. Prove that your algorithm minimizes the average completion time, and state the running time of your algorithm. a. Order the tasks by processing time from smallest to largest and run them in that order. To see that this greedy solution is optimal, first observe that the problem exhibits optimal substructure: if we run the first task in an optimal solution, then we obtain an optimal solution by running the remaining tasks in a way which minimizes the average completion time. Let $O$ be an optimal solution. Let $a$ be the task which has the smallest processing time and let b be the first task run in $O$. Let $G$ be the solution obtained by switching the order in which we run $a$ and $b$ in $O$. This amounts reducing the completion times of a and the completion times of all tasks in $G$ between $a$ and $b$ by the difference in processing times of $a$ and $b$. Since all other completion times remain the same, the average completion time of $G$ is less than or equal to the average completion time of $O$, proving that the greedy solution gives an optimal solution. This has runtime $O(n\\lg n)$ because we must first sort the elements. b. Without loss of generality we my assume that every task is a unit time task. Apply the same strategy as in part (a), except this time if a task which we would like to add next to the schedule isn't allowed to run yet, we must skip over it. Since there could be many tasks of short processing time which have late release time, the runtime becomes $O(n^2)$ since we might have to spend $O(n)$ time deciding which task to add next at each step.","title":"16-2 Scheduling to minimize average completion time"},{"location":"Chap16/Problems/16-3/","text":"a. The incidence matrix for an undirected graph $G = (V, E)$ is a $|V| \\times |E|$ matrix $M$ such that $M_{ve} = 1$ if edge $e$ is incident on vertex $v$, and $M_{ve} = 0$ otherwise. Argue that a set of columns of $M$ is linearly independent over the field of integers modulo $2$ if and only if the corresponding set of edges is acyclic. Then, use the result of Exercise 16.4-2 to provide an alternate proof that $(E, \\mathcal I)$ of part (a) is a matroid. b. Suppose that we associate a nonnegative weight $w(e)$ with each edge in an undirected graph $G = (V, E)$. Give an efficient algorithm to find an acyclic subset of $E$ of maximum total weight. c. Let $G(V, E)$ be an arbitrary directed graph, and let $(E, \\mathcal I)$ be defined so that $A \\in \\mathcal I$ if and only if $A$ does not contain any directed cycles. Give an example of a directed graph $G$ such that the associated system $(E, \\mathcal I)$ is not a matroid. Specify which defining condition for a matroid fails to hold. d. The incidence matrix for a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $M$ such that $M_{ve} = -1$ if edge $e$ leaves vertex $v$, $M_{ve} = 1$ if edge $e$ enters vertex $v$, and $M_{ve} = 0$ otherwise. Argue that if a set of columns of $M$ is linearly independent, then the corresponding set of edges does not contain a directed cycle. e. Exercise 16.4-2 tells us that the set of linearly independent sets of columns of any matrix $M$ forms a matroid. Explain carefully why the results of parts (d) and (e) are not contradictory. How can there fail to be a perfect correspondence between the notion of a set of edges being acyclic and the notion of the associated set of columns of the incidence matrix being linearly independent? a. First, suppose that a set of columns is not linearly independent over $\\mathbb F_2$ then, there is some subset of those columns, say $S$ so that a linear combination of $S$ is $0$. However, over $\\mathbb F_2$, since the only two elements are $1$ and $0$, a linear combination is a sum over some subset. Suppose that this subset is $S'$, note that it has to be nonempty because of linear dependence. Now, consider the set of edges that these columns correspond to. Since the columns had their total incidence with each vertex $0$ in $\\mathbb F_2$, it is even. So, if we consider the subgraph on these edges, then every vertex has a even degree. Also, since our $S'$ was nonempty, some component has an edge. Restrict our attention to any such component. Since this component is connected and has all even vertex degrees, it contains an Euler Circuit, which is a cycle. Now, suppose that our graph had some subset of edges which was a cycle. Then, the degree of any vertex with respect to this set of edges is even, so, when we add the corresponding columns, we will get a zero column in $\\mathbb F_2$. Since sets of linear independent columns form a matroid, by problem 16.4-2, the acyclic sets of edges form a matroid as well. b. One simple approach is to take the highest weight edge that doesn't complete a cycle. Another way to phrase this is by running Kruskal's algorithm (see Chapter 23) on the graph with negated edge weights. c. Consider the digraph on [3] with the edges $(1, 2), (2, 1), (2, 3), (3, 2), (3, 1)$ where $(u, v)$ indicates there is an edge from $u$ to $v$. Then, consider the two acyclic subsets of edges $B = (3, 1), (3, 2), (2, 1)$ and $A = (1, 2), (2, 3)$. Then, adding any edge in $B - A$ to $A$ will create a cycle. So, the exchange property is violated. d. Suppose that the graph contained a directed cycle consisting of edges corresponding to columns $S$. Then, since each vertex that is involved in this cycle has exactly as many edges going out of it as going into it, the rows corresponding to each vertex will add up to zero, since the outgoing edges count negative and the incoming vertices count positive. This means that the sum of the columns in $S$ is zero, so, the columns were not linearly independent. e. There is not a perfect correspondence because we didn't show that not containing a directed cycle means that the columns are linearly independent, so there is not perfect correspondence between these sets of independent columns (which we know to be a matriod) and the acyclic sets of edges (which we know not to be a matroid).","title":"16-3 Acyclic subgraphs"},{"location":"Chap16/Problems/16-4/","text":"Consider the following algorithm for the problem from Section 16.5 of scheduling unit-time tasks with deadlines and penalties. Let all $n$ time slots be initially empty, where time slot $i$ is the unit-length slot of time that finishes at time $i$. We consider the tasks in order of monotonically decreasing penalty. When considering task $a_j$, if there exists a time slot at or before $a_j$'s deadline $d_j$ that is still empty, assign $a_j$ to the latest such slot, filling it. If there is no such slot, assign task $a_j$ to the latest of the as yet unfilled slots. a. Argue that this algorithm always gives an optimal answer. b. Use the fast disjoint-set forest presented in Section 21.3 to implement the algorithm efficiently. Assume that the set of input tasks has already been sorted into monotonically decreasing order by penalty. Analyze the running time of your implementation. a. Let $O$ be an optimal solution. If $a_j$ is scheduled before its deadline, we can always swap it with whichever activity is scheduled at its deadline without changing the penalty. If it is scheduled after its deadline but $a_j.deadline \\le j$ then there must exist a task from among the first $j$ with penalty less than that of $a_j$ . We can then swap aj with this task to reduce the overall penalty incurred. Since $O$ is optimal, this can't happen. Finally, if $a_j$ is scheduled after its deadline and $a_j.deadline > j$ we can swap $a_j$ with any other late task without increasing the penalty incurred. Since the problem exhibits the greedy choice property as well, this greedy strategy always yields on optimal solution. b. Assume that $\\text{MAKE-SET}(x)$ returns a pointer to the element $x$ which is now it its own set. Our disjoint sets will be collections of elements which have been scheduled at contiguous times. We'll use this structure to quickly find the next available time to schedule a task. Store attributes $x.low$ and $x.high$ at the representative $x$ of each disjoint set. This will give the earliest and latest time of a scheduled task in the block. Assume that $\\text{UNION}(x, y)$ maintains this attribute. This can be done in constant time, so it won't affect the asymptotics. Note that the attribute is well-defined under the union operation because we only union two blocks if they are contiguous. Without loss of generality we may assume that task $a_1$ has the greatest penalty, task $a_2$ has the second greatest penalty, and so on, and they are given to us in the form of an array $A$ where $A[i] = a_i$. We will maintain an array $D$ such that $D[i]$ contains a pointer to the task with deadline $i$. We may assume that the size of $D$ is at most $n$, since a task with deadline later than $n$ can't possibly be scheduled on time. There are at most $3n$ total $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, each of which occur at most $n$ times, so by Theorem 21.14 the runtime is $O(n\\alpha(n))$. SCHEDULING - VARIATIONS ( A ) let D [ 1. . n ] be a new array for i = 1 to n a [ i ]. time = a [ i ]. deadline if D [ a [ i ]. deadline ] != NIL y = FIND - SET ( D [ a [ i ]. deadline ]) a [ i ]. time = y . low - 1 x = MAKE - SET ( a [ i ]) D [ a [ i ]. time ] = x x . low = x . high = a [ i ]. time if D [ a [ i ]. time - 1 ] != NIL UNION ( D [ a [ i ]. time - 1 ], D [ a [ i ]. time ]) if D [ a [ i ]. time + 1 ] != NIL UNION ( D [ a [ i ]. time ], D [ a [ i ]. time + 1 ])","title":"16-4 Scheduling variations"},{"location":"Chap16/Problems/16-5/","text":"Modern computers use a cache to store a small amount of data in a fast memory. Even though a program may access large amounts of data, by storing a small subset of the main memory in the cache \u2014a small but faster memory\u2014overall access time can greatly decrease. When a computer program executes, it makes a sequence $\\langle r_1, r_2, \\ldots, r_n \\rangle$ of $n$ memory requests, where each request is for a particular data element. For example, a program that accesses 4 distinct elements $\\{a, b, c, d\\}$ might make the sequence of requests $\\langle d, b, d, b, d, a, c, d, b, a, c, b \\rangle$. Let $k$ be the size of the cache. When the cache contains $k$ elements and the program requests the $(k + 1)$st element, the system must decide, for this and each subsequent request, which $k$ elements to keep in the cache. More precisely, for each request $r_i$, the cache-management algorithm checks whether element $r_i$ is already in the cache. If it is, then we have a cache hit ; otherwise, we have a cache miss. Upon a cache miss , the system retrieves $r_i$ from the main memory, and the cache-management algorithm must decide whether to keep $r_i$ in the cache. If it decides to keep $r_i$ and the cache already holds $k$ elements, then it must evict one element to make room for $r_i$ . The cache-management algorithm evicts data with the goal of minimizing the number of cache misses over the entire sequence of requests. Typically, caching is an on-line problem. That is, we have to make decisions about which data to keep in the cache without knowing the future requests. Here, however, we consider the off-line version of this problem, in which we are given in advance the entire sequence of $n$ requests and the cache size $k$, and we wish to minimize the total number of cache misses. We can solve this off-line problem by a greedy strategy called furthest-in-future , which chooses to evict the item in the cache whose next access in the request sequence comes furthest in the future. a. Write pseudocode for a cache manager that uses the furthest-in-future strategy. The input should be a sequence $\\langle r_1, r_2, \\ldots, r_n \\rangle$ of requests and a cache size $k$, and the output should be a sequence of decisions about which data element (if any) to evict upon each request. What is the running time of your algorithm? b. Show that the off-line caching problem exhibits optimal substructure. c. Prove that furthest-in-future produces the minimum possible number of cache misses. a. Suppose there are $m$ distinct elements that could be requested. There may be some room for improvement in terms of keeping track of the furthest in future element at each position. If you maintain a (double circular) linked list with a node for each possible cache element and an array so that in index $i$ there is a pointer corresponding to the node in the linked list corresponding to the possible cache request $i$. Then, starting with the elements in an arbitrary order, process the sequence $\\langle r_1, \\dots, r_n \\rangle$ from right to left. Upon processing a request move the node corresponding to that request to the beginning of the linked list and make a note in some other array of length $n$ of the element at the end of the linked list. This element is tied for furthest-in-future. Then, just scan left to right through the sequence, each time just checking some set for which elements are currently in the cache. It can be done in constant time to check if an element is in the cache or not by a direct address table. If an element need be evicted, evict the furthest-in-future one noted earlier. This algorithm will take time $O(n + m)$ and use additional space $O(m + n)$. SCHEDULING - VARIATIONS ( A ) let D be an array of size n for i = 1 to n a_i . time = a_i . deadline if D [ a_i . deadline ] != NIL y = FIND - SET ( D [ a_i . deadline ]) a_i . time = y . low - 1 x = MAKE - SET ( a_i ) D [ a_i . time ] = x x . low = x . high = a_i . time if D [ a_i . time - 1 ] != NIL UNION ( D [ a_i . time - 1 ], D [ a_i . time ]) if D [ a_i . time + 1 ] != NIL UNION ( D [ a_i . time ], D [ a_i . time + 1 ]) If we were in the stupid case that $m > n$, we could restrict our attention to the possible cache requests that actually happen, so we have a solution that is $O(n)$ both in time and in additional space required. b. Index the subproblems $c[i, S]$ by a number $i \\in [n]$ and a subset $S \\in \\binom{[m]}{k}$. Which indicates the lowest number of misses that can be achieved with an initial cache of $S$ starting after index $i$. Then, $$c[i, S] = \\min_{x \\in \\{S\\}} (c[i + 1, \\{r_i\\} \\cup (S \u2212 \\{x\\})] + (1 \u2212 \\chi_{\\{r_i\\}}(x))),$$ which means that $x$ is the element that is removed from the cache unless it is the current element being accessed, in which case there is no cost of eviction. c. At each time we need to add something new, we can pick which entry to evict from the cache. We need to show the there is an exchange property. That is, if we are at round $i$ and need to evict someone, suppose we evict $x$. Then, if we were to instead evict the furthest in future element $y$, we would have no more evictions than before. To see this, since we evicted $x$, we will have to evict someone else once we get to $x$, whereas, if we had used the other strategy, we wouldn't of had to evict anyone until we got to $y$. This is a point later in time than when we had to evict someone to put $x$ back into the cache, so we could, at reloading $y$, just evict the person we would of evicted when we evicted someone to reload $x$. This causes the same number of misses unless there was an access to that element that wold of been evicted at reloading $x$ some point in between when $x$ any $y$ were needed, in which case furthest in future would be better.","title":"16-5 Off-line caching"},{"location":"Chap17/17.1/","text":"17.1-1 If the set of stack operations included a $\\text{MULTIPUSH}$ operation, which pushses $k$ items onto the stack, would the $O(1)$ bound on the amortized cost of stack operations continue to hold? No. The time complexity of such a series of operations depends on the number of pushes (pops vice versa) could be made. Since one $\\text{MULTIPUSH}$ needs $\\Theta(k)$ time, performing $n$ $\\text{MULTIPUSH}$ operations, each with $k$ elements, would take $\\Theta(kn)$ time, leading to amortized cost of $\\Theta(k)$. 17.1-2 Show that if a $\\text{DECREMENT}$ operation were included in the $k$-bit counter example, $n$ operations could cost as much as $\\Theta(nk)$ time. The logarithmic bit flipping predicate does not hold, and indeed a sequence of events could consist of the incrementation of all $1$s and decrementation of all $0$s; yielding $\\Theta(nk)$. 17.1-3 Suppose we perform a sequence of $n$ operations on a data structure in which the $i$th operation costs $i$ if $i$ is an exact power of $2$, and $1$ otherwise. Use aggregate analysis to determine the amortized cost per operation. Let $n$ be arbitrary, and have the cost of operation $i$ be $c(i)$. Then we have, $$ \\begin{aligned} \\sum_{i = 1}^n c(i) & = \\sum_{i = 1}^{\\left\\lceil\\lg n\\right\\rceil} 2^i + \\sum_{i \\le n \\text{ is not a power of } 2} 1 \\\\ & \\le \\sum_{i = 1}^{\\left\\lceil\\lg n\\right\\rceil} 2^i + n \\\\ & = 2^{1 + \\left\\lceil\\lg n\\right\\rceil} - 1 + n \\\\ & \\le 2n - 1 + n \\\\ & \\le 3n \\in O(n). \\end{aligned} $$ To find the average, we divide by $n$, and the amortized cost per operation is $O(1)$.","title":"17.1 Aggregate analysis"},{"location":"Chap17/17.1/#171-1","text":"If the set of stack operations included a $\\text{MULTIPUSH}$ operation, which pushses $k$ items onto the stack, would the $O(1)$ bound on the amortized cost of stack operations continue to hold? No. The time complexity of such a series of operations depends on the number of pushes (pops vice versa) could be made. Since one $\\text{MULTIPUSH}$ needs $\\Theta(k)$ time, performing $n$ $\\text{MULTIPUSH}$ operations, each with $k$ elements, would take $\\Theta(kn)$ time, leading to amortized cost of $\\Theta(k)$.","title":"17.1-1"},{"location":"Chap17/17.1/#171-2","text":"Show that if a $\\text{DECREMENT}$ operation were included in the $k$-bit counter example, $n$ operations could cost as much as $\\Theta(nk)$ time. The logarithmic bit flipping predicate does not hold, and indeed a sequence of events could consist of the incrementation of all $1$s and decrementation of all $0$s; yielding $\\Theta(nk)$.","title":"17.1-2"},{"location":"Chap17/17.1/#171-3","text":"Suppose we perform a sequence of $n$ operations on a data structure in which the $i$th operation costs $i$ if $i$ is an exact power of $2$, and $1$ otherwise. Use aggregate analysis to determine the amortized cost per operation. Let $n$ be arbitrary, and have the cost of operation $i$ be $c(i)$. Then we have, $$ \\begin{aligned} \\sum_{i = 1}^n c(i) & = \\sum_{i = 1}^{\\left\\lceil\\lg n\\right\\rceil} 2^i + \\sum_{i \\le n \\text{ is not a power of } 2} 1 \\\\ & \\le \\sum_{i = 1}^{\\left\\lceil\\lg n\\right\\rceil} 2^i + n \\\\ & = 2^{1 + \\left\\lceil\\lg n\\right\\rceil} - 1 + n \\\\ & \\le 2n - 1 + n \\\\ & \\le 3n \\in O(n). \\end{aligned} $$ To find the average, we divide by $n$, and the amortized cost per operation is $O(1)$.","title":"17.1-3"},{"location":"Chap17/17.2/","text":"17.2-1 Suppose we perform a sequence of stack operations on a stack whose size never exceeds $k$. After every $k$ operations, we make a copy of the entire stack for backup purposes. Show that the cost of $n$ stack operations, including copying the stack, is $O(n)$ by assigning suitable amortized costs to the various stack operations. For every stack operation, we charge twice. First, we charge the actual cost of the stack operation. Second, we charge the cost of copying an element later on. Since we have the size of the stack never exceed $k$, and there are always $k$ operations between backups, we always overpay by at least enough. Therefore, the amortized cost of the operation is constant, and the cost of the $n$ operation is $O(n)$. 17.2-2 Redo Exercise 17.1-3 using an accounting method of analysis. Let $c_i =$ cost of $i$th operation. $$ c_i = \\begin{cases} i & \\text{if $i$ is an exact power of $2$}, \\\\ 1 & \\text{otherwise}. \\end{cases} $$ Charge $3$ (amortized cost $\\hat c_i$) for each operation. If $i$ is not an exact power of $2$, pay $\\$1$, and store $\\$2$ as credit. If $i$ is an exact power of $2$, pay $\\$i$, using stored credit. $$ \\begin{array}{cccc} \\text{Operation} & \\text{Cost} & \\text{Actual cost} & \\text{Credit remaining} \\\\ \\hline 1 & 3 & 1 & 2 \\\\ 2 & 3 & 2 & 3 \\\\ 3 & 3 & 1 & 5 \\\\ 4 & 3 & 4 & 4 \\\\ 5 & 3 & 1 & 6 \\\\ 6 & 3 & 1 & 8 \\\\ 7 & 3 & 1 & 10 \\\\ 8 & 3 & 8 & 5 \\\\ 9 & 3 & 1 & 7 \\\\ 10 & 3 & 1 & 9 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{array} $$ Since the amortized cost is $\\$3$ per operation, $\\sum\\limits_{i = 1}^n \\hat c_i = 3n$. We know from Exercise 17.1-3 that $\\sum\\limits_{i = 1}^n \\hat c_i < 3n$. Then we have $$\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i \\Rightarrow \\text{credit} = \\text{amortized cose} - \\text{actual cost} \\ge 0.$$ Since the amortized cost of each operation is $O(1)$, and the amount of credit never goes negative, the total cost of $n$ operations is $O(n)$. 17.2-3 Suppose we wish not only to increment a counter but also to reset it to zero (i.e., make all bits in it $0$). Counting the time to examine or modify a bit as $\\Theta(1)$, show how to implement a counter as an array of bits so that any sequence of $n$ $\\text{INCREMENT}$ and $\\text{RESET}$ operations takes time $O(n)$ on an initially zero counter. ($\\textit{Hint:}$ Keep a pointer to the high-order $1$.) (Removed)","title":"17.2 The accounting method"},{"location":"Chap17/17.2/#172-1","text":"Suppose we perform a sequence of stack operations on a stack whose size never exceeds $k$. After every $k$ operations, we make a copy of the entire stack for backup purposes. Show that the cost of $n$ stack operations, including copying the stack, is $O(n)$ by assigning suitable amortized costs to the various stack operations. For every stack operation, we charge twice. First, we charge the actual cost of the stack operation. Second, we charge the cost of copying an element later on. Since we have the size of the stack never exceed $k$, and there are always $k$ operations between backups, we always overpay by at least enough. Therefore, the amortized cost of the operation is constant, and the cost of the $n$ operation is $O(n)$.","title":"17.2-1"},{"location":"Chap17/17.2/#172-2","text":"Redo Exercise 17.1-3 using an accounting method of analysis. Let $c_i =$ cost of $i$th operation. $$ c_i = \\begin{cases} i & \\text{if $i$ is an exact power of $2$}, \\\\ 1 & \\text{otherwise}. \\end{cases} $$ Charge $3$ (amortized cost $\\hat c_i$) for each operation. If $i$ is not an exact power of $2$, pay $\\$1$, and store $\\$2$ as credit. If $i$ is an exact power of $2$, pay $\\$i$, using stored credit. $$ \\begin{array}{cccc} \\text{Operation} & \\text{Cost} & \\text{Actual cost} & \\text{Credit remaining} \\\\ \\hline 1 & 3 & 1 & 2 \\\\ 2 & 3 & 2 & 3 \\\\ 3 & 3 & 1 & 5 \\\\ 4 & 3 & 4 & 4 \\\\ 5 & 3 & 1 & 6 \\\\ 6 & 3 & 1 & 8 \\\\ 7 & 3 & 1 & 10 \\\\ 8 & 3 & 8 & 5 \\\\ 9 & 3 & 1 & 7 \\\\ 10 & 3 & 1 & 9 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\end{array} $$ Since the amortized cost is $\\$3$ per operation, $\\sum\\limits_{i = 1}^n \\hat c_i = 3n$. We know from Exercise 17.1-3 that $\\sum\\limits_{i = 1}^n \\hat c_i < 3n$. Then we have $$\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i \\Rightarrow \\text{credit} = \\text{amortized cose} - \\text{actual cost} \\ge 0.$$ Since the amortized cost of each operation is $O(1)$, and the amount of credit never goes negative, the total cost of $n$ operations is $O(n)$.","title":"17.2-2"},{"location":"Chap17/17.2/#172-3","text":"Suppose we wish not only to increment a counter but also to reset it to zero (i.e., make all bits in it $0$). Counting the time to examine or modify a bit as $\\Theta(1)$, show how to implement a counter as an array of bits so that any sequence of $n$ $\\text{INCREMENT}$ and $\\text{RESET}$ operations takes time $O(n)$ on an initially zero counter. ($\\textit{Hint:}$ Keep a pointer to the high-order $1$.) (Removed)","title":"17.2-3"},{"location":"Chap17/17.3/","text":"17.3-1 Suppose we have a potential function $\\Phi$ such that $\\Phi(D_i) \\ge \\Phi(D_0)$ for all $i$, but $\\Phi(D_0) \\ne 0$. Show that there exists a potential fuction $\\Phi'$ such that $\\Phi'(D_0) = 0$, $\\Phi'(D_i) \\ge 0$ for all $i \\ge 1$, and the amortized costs using $\\Phi'$ are the same as the amortized costs using $\\Phi$. Define the potential function $\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0)$ for all $i \\ge 1$. Then $$\\Phi'(D_0) = \\Phi(D_0) - \\Phi(D_0) = 0,$$ and $$\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0) \\ge 0.$$ The amortized cost is $$ \\begin{aligned} \\hat c_i' & = c_i + \\Phi'(D_i) - \\Phi'(D_{i - 1}) \\\\ & = c_i + (\\Phi(D_i) - \\Phi(D_0)) - (\\Phi(D_{i - 1}) - \\Phi(D_0)) \\\\ & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & = \\hat c_i. \\end{aligned} $$ 17.3-2 Redo Exercise 17.1-3 using a potential method of analysis. Define the potential function $\\Phi(D_0) = 0$, and $\\Phi(D_i) = 2i - 2^{1 + \\lfloor \\lg i \\rfloor}$ for $i > 0$. For operation 1, $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1+ \\lfloor \\lg i \\rfloor} - 0 = 1.$$ For operation $i(i > 1)$, if $i$ is not a power of $2$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1 + \\lfloor \\lg 1 \\rfloor} - (2(i - 1) - 2^{1 + \\lfloor \\lg(i - 1) \\rfloor}) = 3.$$ If $i = 2^j$ for some $j \\in \\mathbb N$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = i + 2i - 2^{1 + j}-(2(i - 1) - 2^{1 + j - 1}) = i + 2i - 2i - 2i + 2 + i = 2.$$ Thus, the amortized cost is $3$ per operation. 17.3-3 Consider an ordinary binary min-heap data structure with $n$ elements supporting the instructions $\\text{INSERT}$ and $\\text{EXTRACT-MIN}$ in $O(\\lg n)$ worst-case time. Give a potential function $\\Phi$ such that the amortized cost of $\\text{INSERT}$ is $O(\\lg n)$ and the amortized cost of $\\text{EXTRACT-MIN}$ is $O(1)$, and show that it works. Make the potential function be equal to $\\sum_{i = 1}^n \\lg i$ where $n$ is the size of the min-heap. Then, there is still a cost of $O(\\lg n)$ to $\\text{INSERT}$, since only an amount of amortization that is $\\lg n$ was spent to increase the size of the heap by $1$. However, the amortized cost of $\\text{EXTRACT-MIN}$ is $0$, as all its actual cost is compensated. 17.3-4 What is the total cost of executing $n$ of the stack operations $\\text{PUSH}$, $\\text{POP}$, and $\\text{MULTIPOP}$, assuming that the stack begins with $s_0$ objects and finishes with $s_n$ objects? Let $\\Phi$ be the potential function that returns the number of elements in the stack. We know that for this potential function, we have amortized cost $2$ for $\\text{PUSH}$ operation and amortized cost $0$ for $\\text{POP}$ and $\\text{MULTIPOP}$ operations. The total amortized cost is $$\\sum_{i = 1}^n \\hat c_i = \\sum_{i = 1}^n c_i + \\Phi(D_n) - \\Phi(D_0).$$ Using the potential function and the known amortized costs, we can rewrite the equation as $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i + \\Phi(D_0) - \\Phi(D_n) \\\\ & = \\sum_{i = 1}^n \\hat c_i + s_0 - s_n \\\\ & \\le 2n + s_0 - s_n, \\end{aligned} $$ which gives us the total cost of $O(n + (s_0 - s_n))$. If $s_n \\ge s_0$, then this equals to $O(n)$, that is, if the stack grows, then the work done is limited by the number of operations. (Note that it does not matter here that the potential may go below the starting potential. The condition $\\Phi(D_n) \\ge \\Phi(D_0)$ for all $n$ is only required to have $\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i$, but we do not need for that to hold in this application.) 17.3-5 Suppose that a counter begins at a number with $b$ $1$s in its binary representation, rather than at $0$. Show that the cost of performing $n$ $\\text{INCREMENT}$ operations is $O(n)$ if $n = \\Omega(b)$. (Do not assume that $b$ is constant.) $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i - \\Phi(D_n) + \\Phi(D_0) \\\\ & = n - x + b \\\\ & \\le n - x + n \\\\ & = O(n). \\end{aligned} $$ 17.3-6 Show how to implement a queue with two ordinary stacks (Exercise 10.1-6) so that the amortized cost of each $\\text{ENQUEUE}$ and each $\\text{DEQUEUE}$ operation is $O(1)$. We'll use the accounting method for the analysis. Assign cost $3$ to the $\\text{ENQUEUE}$ operation and $0$ to the $\\text{DEQUEUE}$ operation. Recall the implementation of 10.1-6 where we enqueue by pushing on to the top of stack 1, and dequeue by popping from stack 2. If stack 2 is empty, then we must pop every element from stack 1 and push it onto stack 2 before popping the top element from stack 2. For each item that we enqueue we accumulate 2 credits. Before we can dequeue an element, it must be moved to stack 2. Note: this might happen prior to the time at which we wish to dequeue it, but it will happen only once overall. One of the 2 credits will be used for this move. Once an item is on stack 2 its pop only costs $1$ credit, which is exactly the remaining credit associated to the element. Since each operation's cost is $O(1)$, the amortized cost per operation is $O(1)$. 17.3-7 Design a data structure to support the following two operations for a dynamic multiset $S$ of integers, which allows duplicate values: $\\text{INSERT}(S, x)$ inserts $x$ into $S$. $\\text{DELETE-LARGER-HALF}(S)$ deletes the largest $\\lceil |S| / 2 \\rceil$ elements from $S$. Explain how to implement this data structure so that any sequence of $m$ $\\text{INSERT}$ and $\\text{DELETE-LARGER-HALF}$ operations runs in $O(m)$ time. Your implementation should also include a way to output the elements of $S$ in $O(|S|)$ time. We'll store all our elements in an array, and if ever it is too large, we will copy all the elements out into an array of twice the length. To delete the larger half, we first find the element $m$ with order statistic $\\lceil |S| / 2 \\rceil$ by the algorithm presented in section 9.3. Then, scan through the array and copy out the elements that are smaller or equal to $m$ into an array of half the size. Since the delete half operation takes time $O(|S|)$ and reduces the number of elements by $\\lfloor |S| / 2 \\rfloor \\in \\Omega(|S|)$, we can make these operations take ammortized constant time by selecting our potential function to be linear in $|S|$. Since the insert operation only increases $|S|$ by one, we have that there is only a constant amount of work going towards satisfying the potential, so the total ammortized cost of an insertion is still constant. To output all the elements just iterate through the array and output each.","title":"17.3 The potential method"},{"location":"Chap17/17.3/#173-1","text":"Suppose we have a potential function $\\Phi$ such that $\\Phi(D_i) \\ge \\Phi(D_0)$ for all $i$, but $\\Phi(D_0) \\ne 0$. Show that there exists a potential fuction $\\Phi'$ such that $\\Phi'(D_0) = 0$, $\\Phi'(D_i) \\ge 0$ for all $i \\ge 1$, and the amortized costs using $\\Phi'$ are the same as the amortized costs using $\\Phi$. Define the potential function $\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0)$ for all $i \\ge 1$. Then $$\\Phi'(D_0) = \\Phi(D_0) - \\Phi(D_0) = 0,$$ and $$\\Phi'(D_i) = \\Phi(D_i) - \\Phi(D_0) \\ge 0.$$ The amortized cost is $$ \\begin{aligned} \\hat c_i' & = c_i + \\Phi'(D_i) - \\Phi'(D_{i - 1}) \\\\ & = c_i + (\\Phi(D_i) - \\Phi(D_0)) - (\\Phi(D_{i - 1}) - \\Phi(D_0)) \\\\ & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ & = \\hat c_i. \\end{aligned} $$","title":"17.3-1"},{"location":"Chap17/17.3/#173-2","text":"Redo Exercise 17.1-3 using a potential method of analysis. Define the potential function $\\Phi(D_0) = 0$, and $\\Phi(D_i) = 2i - 2^{1 + \\lfloor \\lg i \\rfloor}$ for $i > 0$. For operation 1, $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1+ \\lfloor \\lg i \\rfloor} - 0 = 1.$$ For operation $i(i > 1)$, if $i$ is not a power of $2$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = 1 + 2i - 2^{1 + \\lfloor \\lg 1 \\rfloor} - (2(i - 1) - 2^{1 + \\lfloor \\lg(i - 1) \\rfloor}) = 3.$$ If $i = 2^j$ for some $j \\in \\mathbb N$, then $$\\hat c_i = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) = i + 2i - 2^{1 + j}-(2(i - 1) - 2^{1 + j - 1}) = i + 2i - 2i - 2i + 2 + i = 2.$$ Thus, the amortized cost is $3$ per operation.","title":"17.3-2"},{"location":"Chap17/17.3/#173-3","text":"Consider an ordinary binary min-heap data structure with $n$ elements supporting the instructions $\\text{INSERT}$ and $\\text{EXTRACT-MIN}$ in $O(\\lg n)$ worst-case time. Give a potential function $\\Phi$ such that the amortized cost of $\\text{INSERT}$ is $O(\\lg n)$ and the amortized cost of $\\text{EXTRACT-MIN}$ is $O(1)$, and show that it works. Make the potential function be equal to $\\sum_{i = 1}^n \\lg i$ where $n$ is the size of the min-heap. Then, there is still a cost of $O(\\lg n)$ to $\\text{INSERT}$, since only an amount of amortization that is $\\lg n$ was spent to increase the size of the heap by $1$. However, the amortized cost of $\\text{EXTRACT-MIN}$ is $0$, as all its actual cost is compensated.","title":"17.3-3"},{"location":"Chap17/17.3/#173-4","text":"What is the total cost of executing $n$ of the stack operations $\\text{PUSH}$, $\\text{POP}$, and $\\text{MULTIPOP}$, assuming that the stack begins with $s_0$ objects and finishes with $s_n$ objects? Let $\\Phi$ be the potential function that returns the number of elements in the stack. We know that for this potential function, we have amortized cost $2$ for $\\text{PUSH}$ operation and amortized cost $0$ for $\\text{POP}$ and $\\text{MULTIPOP}$ operations. The total amortized cost is $$\\sum_{i = 1}^n \\hat c_i = \\sum_{i = 1}^n c_i + \\Phi(D_n) - \\Phi(D_0).$$ Using the potential function and the known amortized costs, we can rewrite the equation as $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i + \\Phi(D_0) - \\Phi(D_n) \\\\ & = \\sum_{i = 1}^n \\hat c_i + s_0 - s_n \\\\ & \\le 2n + s_0 - s_n, \\end{aligned} $$ which gives us the total cost of $O(n + (s_0 - s_n))$. If $s_n \\ge s_0$, then this equals to $O(n)$, that is, if the stack grows, then the work done is limited by the number of operations. (Note that it does not matter here that the potential may go below the starting potential. The condition $\\Phi(D_n) \\ge \\Phi(D_0)$ for all $n$ is only required to have $\\sum_{i = 1}^n \\hat c_i \\ge \\sum_{i = 1}^n c_i$, but we do not need for that to hold in this application.)","title":"17.3-4"},{"location":"Chap17/17.3/#173-5","text":"Suppose that a counter begins at a number with $b$ $1$s in its binary representation, rather than at $0$. Show that the cost of performing $n$ $\\text{INCREMENT}$ operations is $O(n)$ if $n = \\Omega(b)$. (Do not assume that $b$ is constant.) $$ \\begin{aligned} \\sum_{i = 1}^n c_i & = \\sum_{i = 1}^n \\hat c_i - \\Phi(D_n) + \\Phi(D_0) \\\\ & = n - x + b \\\\ & \\le n - x + n \\\\ & = O(n). \\end{aligned} $$","title":"17.3-5"},{"location":"Chap17/17.3/#173-6","text":"Show how to implement a queue with two ordinary stacks (Exercise 10.1-6) so that the amortized cost of each $\\text{ENQUEUE}$ and each $\\text{DEQUEUE}$ operation is $O(1)$. We'll use the accounting method for the analysis. Assign cost $3$ to the $\\text{ENQUEUE}$ operation and $0$ to the $\\text{DEQUEUE}$ operation. Recall the implementation of 10.1-6 where we enqueue by pushing on to the top of stack 1, and dequeue by popping from stack 2. If stack 2 is empty, then we must pop every element from stack 1 and push it onto stack 2 before popping the top element from stack 2. For each item that we enqueue we accumulate 2 credits. Before we can dequeue an element, it must be moved to stack 2. Note: this might happen prior to the time at which we wish to dequeue it, but it will happen only once overall. One of the 2 credits will be used for this move. Once an item is on stack 2 its pop only costs $1$ credit, which is exactly the remaining credit associated to the element. Since each operation's cost is $O(1)$, the amortized cost per operation is $O(1)$.","title":"17.3-6"},{"location":"Chap17/17.3/#173-7","text":"Design a data structure to support the following two operations for a dynamic multiset $S$ of integers, which allows duplicate values: $\\text{INSERT}(S, x)$ inserts $x$ into $S$. $\\text{DELETE-LARGER-HALF}(S)$ deletes the largest $\\lceil |S| / 2 \\rceil$ elements from $S$. Explain how to implement this data structure so that any sequence of $m$ $\\text{INSERT}$ and $\\text{DELETE-LARGER-HALF}$ operations runs in $O(m)$ time. Your implementation should also include a way to output the elements of $S$ in $O(|S|)$ time. We'll store all our elements in an array, and if ever it is too large, we will copy all the elements out into an array of twice the length. To delete the larger half, we first find the element $m$ with order statistic $\\lceil |S| / 2 \\rceil$ by the algorithm presented in section 9.3. Then, scan through the array and copy out the elements that are smaller or equal to $m$ into an array of half the size. Since the delete half operation takes time $O(|S|)$ and reduces the number of elements by $\\lfloor |S| / 2 \\rfloor \\in \\Omega(|S|)$, we can make these operations take ammortized constant time by selecting our potential function to be linear in $|S|$. Since the insert operation only increases $|S|$ by one, we have that there is only a constant amount of work going towards satisfying the potential, so the total ammortized cost of an insertion is still constant. To output all the elements just iterate through the array and output each.","title":"17.3-7"},{"location":"Chap17/17.4/","text":"17.4-1 Suppose that we wish to implement a dynamic, open-address hash table. Why might we consider the table to be full when its load factor reaches some value $\\alpha$ that is strictly less than $1$? Describe briefly how to make insertion into a dynamic, open-address hash table run in such a way that the expected value of the amortized cost per insertion is $O(1)$. Why is the expected value of the actual cost per insertion not necessarily $O(1)$ for all insertions? By theorems 11.6-11.8, the expected cost of performing insertions and searches in an open address hash table approaches infinity as the load factor approaches one, for any load factor fixed away from $1$, the expected time is bounded by a constant though. The expected value of the actual cost my not be $O(1)$ for every insertion because the actual cost may include copying out the current values from the current table into a larger table because it became too full. This would take time that is linear in the number of elements stored. 17.4-2 Show that if $\\alpha_{i - 1} \\ge 1 / 2$ and the $i$th operation on a dynamic table is $\\text{TABLE-DELETE}$, then the amortized cost of the operation with respect to the potential function $\\text{(17.6)}$ is bounded above by a constant. If $\\alpha_{i - 1} \\ge 1 / 2$, $\\text{TABLE-DELETE}$ cannot contract , so $c_i = 1$ and $size_i = size_{i - 1}$. Case 1: if $\\alpha_i \\ge 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (2 \\cdot num_i - size_i) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = 1 + (2 \\cdot (num_{i - 1} - 1) - size_{i - 1}) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = -1. \\end{aligned} $$ Case 2: if $\\alpha_i < 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (size_i / 2 - num_i) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = 1 + (size_{i - 1} / 2 - (num_{i - 1} - 1)) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = 2 + \\frac{3}{2} size_{i - 1} - 3 \\cdot num_{i - 1} \\\\ & = 2 + \\frac{3}{2} size_{i - 1} - 3\\alpha_{i - 1} size_{i - 1} \\\\ & \\le 2 + \\frac{3}{2} size_{i - 1} - \\frac{3}{2} size_{i - 1} \\\\ & = 2. \\end{aligned} $$ 17.4-3 Suppose that instead of contracting a table by halving its size when its load factor drops below $1 / 4$, we contract it by multiplying its size by $2 / 3$ when its load factor drops below $1 / 3$. Using the potential function $$\\Phi(T) = | 2 \\cdot T.num - T.size |,$$ show that the amortized cost of a $\\text{TABLE-DELETE}$ that uses this strategy is bounded above by a constant. If $1 / 3 < \\alpha_i \\le 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (size_i - 2 \\cdot num_i) - (size_i - 2 \\cdot (num_i + 1)) \\\\ & = 3. \\end{aligned} $$ If the $i$th operation does trigger a contraction, $$ \\begin{aligned} \\frac{1}{3} size_{i - 1} & = num_i + 1 \\\\ size_{i - 1} & = 3 (num_i + 1) \\\\ size_{i} & = \\frac{2}{3} size_{i - 1} = 2 (num_i + 1). \\end{aligned} $$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = (num_i + 1) + [2 \\cdot (num_i + 1) - 2 \\cdot num_i] - [3 \\cdot (num_i + 1) - 2 \\cdot (num_i + 1)] \\\\ & = 2. \\end{aligned} $$","title":"17.4 Dynamic tables"},{"location":"Chap17/17.4/#174-1","text":"Suppose that we wish to implement a dynamic, open-address hash table. Why might we consider the table to be full when its load factor reaches some value $\\alpha$ that is strictly less than $1$? Describe briefly how to make insertion into a dynamic, open-address hash table run in such a way that the expected value of the amortized cost per insertion is $O(1)$. Why is the expected value of the actual cost per insertion not necessarily $O(1)$ for all insertions? By theorems 11.6-11.8, the expected cost of performing insertions and searches in an open address hash table approaches infinity as the load factor approaches one, for any load factor fixed away from $1$, the expected time is bounded by a constant though. The expected value of the actual cost my not be $O(1)$ for every insertion because the actual cost may include copying out the current values from the current table into a larger table because it became too full. This would take time that is linear in the number of elements stored.","title":"17.4-1"},{"location":"Chap17/17.4/#174-2","text":"Show that if $\\alpha_{i - 1} \\ge 1 / 2$ and the $i$th operation on a dynamic table is $\\text{TABLE-DELETE}$, then the amortized cost of the operation with respect to the potential function $\\text{(17.6)}$ is bounded above by a constant. If $\\alpha_{i - 1} \\ge 1 / 2$, $\\text{TABLE-DELETE}$ cannot contract , so $c_i = 1$ and $size_i = size_{i - 1}$. Case 1: if $\\alpha_i \\ge 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (2 \\cdot num_i - size_i) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = 1 + (2 \\cdot (num_{i - 1} - 1) - size_{i - 1}) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = -1. \\end{aligned} $$ Case 2: if $\\alpha_i < 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (size_i / 2 - num_i) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = 1 + (size_{i - 1} / 2 - (num_{i - 1} - 1)) - (2 \\cdot num_{i - 1} - size_{i - 1}) \\\\ & = 2 + \\frac{3}{2} size_{i - 1} - 3 \\cdot num_{i - 1} \\\\ & = 2 + \\frac{3}{2} size_{i - 1} - 3\\alpha_{i - 1} size_{i - 1} \\\\ & \\le 2 + \\frac{3}{2} size_{i - 1} - \\frac{3}{2} size_{i - 1} \\\\ & = 2. \\end{aligned} $$","title":"17.4-2"},{"location":"Chap17/17.4/#174-3","text":"Suppose that instead of contracting a table by halving its size when its load factor drops below $1 / 4$, we contract it by multiplying its size by $2 / 3$ when its load factor drops below $1 / 3$. Using the potential function $$\\Phi(T) = | 2 \\cdot T.num - T.size |,$$ show that the amortized cost of a $\\text{TABLE-DELETE}$ that uses this strategy is bounded above by a constant. If $1 / 3 < \\alpha_i \\le 1 / 2$, $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = 1 + (size_i - 2 \\cdot num_i) - (size_i - 2 \\cdot (num_i + 1)) \\\\ & = 3. \\end{aligned} $$ If the $i$th operation does trigger a contraction, $$ \\begin{aligned} \\frac{1}{3} size_{i - 1} & = num_i + 1 \\\\ size_{i - 1} & = 3 (num_i + 1) \\\\ size_{i} & = \\frac{2}{3} size_{i - 1} = 2 (num_i + 1). \\end{aligned} $$ $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi_i - \\Phi_{i - 1} \\\\ & = (num_i + 1) + [2 \\cdot (num_i + 1) - 2 \\cdot num_i] - [3 \\cdot (num_i + 1) - 2 \\cdot (num_i + 1)] \\\\ & = 2. \\end{aligned} $$","title":"17.4-3"},{"location":"Chap17/Problems/17-1/","text":"Chapter 30 examines an important algorithm called the fast Fourier transform, or $\\text{FFT}$. The first step of the $\\text{FFT}$ algorithm performs a bit-reversal permutation on an input array $A[0..n - 1]$ whose length is $n = 2^k$ for some nonnegative integer $k$. This permutation swaps elements whose indices have binary representations that are the reverse of each other. We can express each index $a$ as a $k$-bit sequence $\\langle a_{k - 1}, a_{k - 2}, \\ldots, a_0 \\rangle$, where $a = \\sum_{i = 0}^{k - 1} a_i 2^i$. We define $$\\text{rev}_k(\\langle a_{k - 1}, a_{k - 2}, \\ldots, a_0 \\rangle) = \\langle a_0, a_1, \\ldots, a_{k - 1} \\rangle;$$ thus, $$\\text{rev}_k(a) = \\sum_{i = 0}^{k - 1} a_{k - i - 1} 2^i.$$ For example, if $n = 16$ (or, equivalently, $k = 4$), then $\\text{rev}_k(3) = 12$, since the $4$-bit representation of $3$ is $0011$, which when reversed gives $1100$, the $4$-bit representation of $12$. a. Given a function $\\text{rev}_k$ that runs in $\\Theta(k)$ time, write an algorithm to perform the bit-reversal permutation on an array of length $n = 2^k$ in $O(nk)$ time. We can use an algorithm based on an amortized analysis to improve the running time of the bit-reversal permutation. We maintain a \"bit-reversed counter\" and a procedure $\\text{BIT-REVERSED-INCREMENT}$ that, when given a bit-reversed-counter value $a$, produces $\\text{rev}_k(\\text{rev}_k(a) + 1)$. If $k = 4$, for example, and the bit-reversed counter starts at $0$, then successive calls to $\\text{BIT-REVERSED-INCREMENT}$ produce the sequence $$0000, 1000, 0100, 1100, 0010, 1010, \\ldots = 0, 8, 4, 12, 2, 10, \\ldots.$$ b. Assume that the words in your computer store $k$-bit values and that in unit time, your computer can manipulate the binary values with operations such as shifting left or right by arbitrary amounts, bitwise-$\\text{AND}$, bitwise-$\\text{OR}$, etc. Describe an implementation of the $\\text{BIT-REVERSED-INCREMENT}$ procedure that allows the bit-reversal permutation on an $n$-element array to be performed in a total of $O(n)$ time. c. Suppose that you can shift a word left or right by only one bit in unit time. Is it still possible to implement an $O(n)$-time bit-reversal permutation? a. Initialize a second array of length $n$ to all trues, then, going through the indices of the original array in any order, if the corresponding entry in the second array is true, then swap the element at the current index with the element at the bit-reversed position, and set the entry in the second array corresponding to the bit-reversed index equal to false. Since we are running $rev_k < n$ times, the total runtime is $O(nk)$. b. Doing a bit reversed increment is the same thing as adding a one to the leftmost position where all carries are going to the left instead of the right. BIT - REVERSED - INCREMENT ( a ) let m be a 1 followed by k - 1 0 s while m bitwise - AND is not zero a = a bitwise - XOR m shift m right by 1 m bitwise - OR a By a similar analysis to the binary counter (just look at the problem in a mirror), this $\\text{BIT-REVERSED-INCREMENT}$ will take constant ammortized time. So, to perform the bit-reversed permutation, have a normal binary counter and a bit reversed counter, then, swap the values of the two counters and increment. Do not swap however if those pairs of elements have already been swapped, which can be kept track of in a auxiliary array. c. The $\\text{BIT-REVERSED-INCREMENT}$ procedure given in the previous part only uses single shifts to the right, not arbitrary shifts.","title":"17-1 Bit-reversed binary counter"},{"location":"Chap17/Problems/17-2/","text":"Binary search of a sorted array takes logarithmic search time, but the time to insert a new element is linear in the size of the array. We can improve the time for insertion by keeping several sorted arrays. Specifically, suppose that we wish to support $\\text{SEARCH}$ and $\\text{INSERT}$ on a set of $n$ elements. Let $k = \\lceil \\lg(n + 1) \\rceil$, and let the binary representation of $n$ be $\\langle n_{k - 1}, n_{k - 2}, \\ldots, n_0 \\rangle$. We have $k$ sorted arrays $A_0, A_1, \\ldots, A_{k - 1}$, where for $i = 0, 1, \\ldots, k - 1$, the length of array $A_i$ is $2^i$. Each array is either full or empty, depending on whether $n_i = 1$ or $n_i = 0$, respectively. The total number of elements held in all $k$ arrays is therefore $\\sum_{i = 0}^{k - 1} n_i 2^i = n$. Although each individual array is sorted, elements in different arrays bear no particular relationship to each other. a. Describe how to perform the $\\text{SEARCH}$ operation for this data structure. Analyze its worst-case running time. b. Describe how to perform the $\\text{INSERT}$ operation. Analyze its worst-case and amortized running times. c. Discuss how to implement $\\text{DELETE}$. a. We linearly go through the lists and binary search each one since we don't know the relationship between one list an another. In the worst case, every list is actually used. Since list $i$ has length $2^i$ and it's sorted, we can search it in $O(i)$ time. Since $i$ varies from $0$ to $O(\\lg n)$, the runtime of SEARCH is $O(\\lg^2 n)$. b. To insert, we put the new element into $A_0$ and update the lists accordingly. In the worst case, we must combine lists $A_0, A_1, \\dots, A_{m \u2212 1}$ into list Am. Since merging two sorted lists can be done linearly in the total length of the lists, the time this takes is $O(2^m)$. In the worst case, this takes time $O(n)$ since $m$ could equal $k$. We'll use the accounting method to analyse the amoritized cost. Assign a cost of $\\lg n$ to each insertion. Thus, each item carries $\\lg n$ credit to pay for its later merges as additional items are inserted. Since an individual item can only be merged into a larger list and there are only $\\lg n$ lists, the credit pays for all future costs the item might incur. Thus, the amortized cost is $O(\\lg n)$. c. Find the smallest $m$ such that $n_m \\ne 0$ in the binary representation of $n$. If the item to be deleted is not in list $A_m$, remove it from its list and swap in an item from Am, arbitrarily. This can be done in $O(\\lg n)$ time since we may need to search list $A_k$ to find the element to be deleted. Now simply break list $A_m$ into lists $A_0, A_1, \\dots, A_{m \u2212 1}$ by index. Since the lists are already sorted, the runtime comes entirely from making the splits, which takes $O(m)$ time. In the worst case, this is $O(\\lg n)$.","title":"17-2 Making binary search dynamic"},{"location":"Chap17/Problems/17-3/","text":"Consider an ordinary binary search tree augmented by adding to each node $x$ the attribute $x.size$ giving the number of keys stored in the subtree rooted at $x$. Let $\\alpha$ be a constant in the range $1 / 2 \\le \\alpha < 1$. We say that a given node $x$ is $\\alpha$-balanced if $x.left.size \\le \\alpha \\cdot x.size$ and $x.right.size \\le \\alpha \\cdot x.size$. The tree as a whole is $\\alpha$-balanced if every node in the tree is $\\alpha$-balanced. The following amortized approach to maintaining weight-balanced trees was suggested by G. Varghese. a. A $1 / 2$-balanced tree is, in a sense, as balanced as it can be. Given a node $x$ in an arbitrary binary search tree, show how to rebuild the subtree rooted at $x$ so that it becomes $1 / 2$-balanced. Your algorithm should run in time $\\Theta(x.size)$, and it can use $O(x.size)$ auxiliary storage. b. Show that performing a search in an $n$-node $\\alpha$-balanced binary search tree takes $O(\\lg n)$ worst-case time. For the remainder of this problem, assume that the constant $\\alpha$ is strictly greater than $1 / 2$. Suppose that we implement $\\text{INSERT}$ and $\\text{DELETE}$ as usual for an $n$-node binary search tree, except that after every such operation, if any node in the tree is no longer $\\alpha$-balanced, then we \"rebuild\" the subtree rooted at the highest such node in the tree so that it becomes $1 / 2$-balanced. We shall analyze this rebuilding scheme using the potential method. For a node $x$ in a binary search tree $T$, we define $$\\Delta(x) = |x.left.size - x.right.size|,$$ and we define the potential of $T$ as $$\\Phi(T) = c \\sum_{x \\in T: \\Delta(x) \\ge 2} \\Delta(x),$$ where $c$ is a sufficiently large constant that depends on $\\alpha$. c. Argue that any binary search tree has nonnegative potential and that a $1 / 2$-balanced tree has potential $0$. d. Suppose that $m$ units of potential can pay for rebuilding an $m$-node subtree. How large must $c$ be in terms of $\\alpha$ in order for it to take $O(1)$ amortized time to rebuild a subtree that is not $\\alpha$-balanced? e. Show that inserting a node into or deleting a node from an $n$-node $\\alpha$-balanced tree costs $O(\\lg n)$ amortized time. a. Since we have $O(x.size)$ auxiliary space, we will take the tree rooted at $x$ and write down an inorder traversal of the tree into the extra space. This will only take linear time to do because it will visit each node thrice, once when passing to its left child, once when the nodes value is output and passing to the right child, and once when passing to the parent. Then, once the inorder traversal is written down, we can convert it back to a binary tree by selecting the median of the list to be the root, and recursing on the two halves of the list that remain on both sides. Since we can index into the middle element of a list in constant time, we will have the recurrence $$T(n) = 2T(n / 2) + 1,$$ which has solution that is linear. Since both trees come from the same underlying inorder traversal, the result is a $\\text{BST}$ since the original was. Also, since the root at each point was selected so that half the elements are larger and half the elements are smaller, it is a $1 / 2$-balanced tree. b. We will show by induction that any tree with $\\le \\alpha^{-d} + d$ elements has a depth of at most $d$. This is clearly true for $d = 0$ because any tree with a single node has depth $0$, and since $\\alpha^0 = 1$, we have that our restriction on the number of elements requires there to only be one. Now, suppose that in some inductive step we had a contradiction, that is, some tree of depth $d$ that is $\\alpha$ balanced but has more than $\\alpha - d$ elements. We know that both of the subtrees are alpha balanced, and by being alpha balanced at the root, we have $$root.left.size \\le \\alpha \\cdot root.size,$$ which implies $$root.right.size > root.size - \\alpha \\cdot root.size - 1.$$ So, $$ \\begin{aligned} root.right.size & > (1 - \\alpha)root.size - 1 \\\\ & > (1 - \\alpha)\\alpha - d + d - 1 \\\\ & = (\\alpha - 1 - 1)\\alpha - d + 1 + d - 1 \\\\ & \\ge \\alpha - d + 1 + d - 1, \\end{aligned} $$ which is a contradiction to the fact that it held for all smaller values of $d$ because any child of a tree of depth d has depth $d - 1$. c. The potential function is a sum of $\\Delta(x)$ each of which is the absolute value of a quantity, so, since it is a sum of nonnegative values, it is nonnegative regardless of the input $\\text{BST}$. If we suppose that our tree is $1 / 2$-balanced, then, for every node $x$, we'll have that $\\Delta(x) \\le 1$, so, the sum we compute to find the potential will be over no nonzero terms. d. $$ \\begin{aligned} \\hat c_i & = c_i + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ O(1) & = m + \\Phi(D_i) - \\Phi(D_{i - 1}) \\\\ \\Phi(D_{i - 1}) & = m + \\Phi(D_i) \\\\ \\Phi(D_{i - 1}) & \\ge m. \\end{aligned} $$ $$ \\begin{aligned} \\Delta(x) & = x.left.size - x.right.size \\\\ & \\ge \\alpha \\cdot m - ((1 - \\alpha) m - 1) \\\\ & = (2\\alpha - 1)m + 1. \\end{aligned} $$ $$ \\begin{aligned} m & \\le c((2\\alpha - 1)m + 1) \\\\ c & \\ge \\frac{m}{(2\\alpha - 1)m + 1} \\\\ & \\ge \\frac{1}{2\\alpha}. \\end{aligned} $$ e. Suppose that our tree is $\\alpha$ balanced. Then, we know that performing a search takes time $O(\\lg(n))$. So, we perform that search and insert the element that we need to insert or delete the element we found. Then, we may have made the tree become unbalanced. However, we know that since we only changed one position, we have only changed the $\\Delta$ value for all of the parents of the node that we either inserted or deleted. Therefore, we can rebuild the balanced properties starting at the lowest such unbalanced node and working up. Since each one only takes ammortized constant time, and there are $O(\\lg(n))$ many trees made unbalanced, tot total time to rebalanced every subtree is $O(\\lg(n))$ ammortized time.","title":"17-3 Amortized weight-balanced trees"},{"location":"Chap17/Problems/17-4/","text":"There are four basic operations on red-black trees that perform structural modifications : node insertions, node deletions, rotations, and color changes. We have seen that $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ use only $O(1)$ rotations, node insertions, and node deletions to maintain the red-black properties, but they may make many more color changes. a. Describe a legal red-black tree with $n$ nodes such that calling $\\text{RB-INSERT}$ to add the $(n + 1)$st node causes $\\Omega(\\lg n)$ color changes. Then describe a legal red-black tree with $n$ nodes for which calling $\\text{RB-DELETE}$ on a particular node causes $\\Omega(\\lg n)$ color changes. Although the worst-case number of color changes per operation can be logarithmic, we shall prove that any sequence of $m$ $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ operations on an initially empty red-black tree causes $O(m)$ structural modifications in the worst case. Note that we count each color change as a structural modification. b. Some of the cases handled by the main loop of the code of both $\\text{RB-INSERT-FIXUP}$ and $\\text{RB-DELETE-FIXUP}$ are terminating : once encountered, they cause the loop to terminate after a constant number of additional operations. For each of the cases of $\\text{RB-INSERT-FIXUP}$ and $\\text{RB-DELETE-FIXUP}$, specify which are terminating and which are not. ($\\textit{Hint:}$ Look at Figures 13.5, 13.6, and 13.7.) We shall first analyze the structural modifications when only insertions are performed. Let $T$ be a red-black tree, and define $\\Phi(T)$ to be the number of red nodes in $T$. Assume that $1$ unit of potential can pay for the structural modifications performed by any of the three cases of $\\text{RB-INSERT-FIXUP}$. c. Let $T'$ be the result of applying Case 1 of $\\text{RB-INSERT-FIXUP}$ to $T$. Argue that $\\Phi(T') = \\Phi(T) - 1$. d. When we insert a node into a red-black tree using $\\text{RB-INSERT}$, we can break the operation into three parts. List the structural modifications and potential changes resulting from lines 1\u201316 of $\\text{RB-INSERT}$, from nonterminating cases of $\\text{RB-INSERT-FIXUP}$, and from terminating cases of $\\text{RB-INSERT-FIXUP}$. e. Using part (d), argue that the amortized number of structural modifications performed by any call of $\\text{RB-INSERT}$ is $O(1)$. We now wish to prove that there are $O(m)$ structural modifications when there are both insertions and deletions. Let us define, for each node $x$, $$ w(x) = \\begin{cases} 0 & \\text{ if } x \\text{ is red}, \\\\ 1 & \\text{ if } x \\text{ is black and has no red children}, \\\\ 0 & \\text{ if } x \\text{ is black and has one red children}, \\\\ 2 & \\text{ if } x \\text{ is black and has two red children}. \\end{cases} $$ Now we redefine the potential of a red-black tree $T$ as $$\\Phi(T) = \\sum_{x \\in T} w(x),$$ and let $T'$ be the tree that results from applying any nonterminating case of $\\text{RB-INSERT-FIXUP}$ or $\\text{RB-DELETE-FIXUP}$ to $T$. f. Show that $\\Phi(T') \\le \\Phi(T) - 1$ for all nonterminating cases of $\\text{RB-INSERT-FIXUP}$. Argue that the amortized number of structural modifications performed by any call of $\\text{RB-INSERT-FIXUP}$ is $O(1)$. g. Show that $\\Phi(T') \\le \\Phi(T) - 1$ for all nonterminating cases of $\\text{RB-DELETE-FIXUP}$. Argue that the amortized number of structural modifications performed by any call of $\\text{RB-DELETE-FIXUP}$ is $O(1)$. h. Complete the proof that in the worst case, any sequence of $m$ $\\text{RB-INSERT}$ and $\\text{RB-DELETE}$ operations performs $O(m)$ structural modifications. a. If we insert a node into a complete binary search tree whose lowest level is all red, then there will be $\\Omega(\\lg n)$ instances of case 1 required to switch the colors all the way up the tree. If we delete a node from an all-black, complete binary tree then this also requires $\\Omega(\\lg n)$ time because there will be instances of case 2 at each iteration of the while loop. b. For $\\text{RB-INSERT}$, cases 2 and 3 are terminating. For $\\text{RB-DELETE}$, cases 1 and 3 are terminating. c. After applying case 1, $z$'s parent and uncle have been changed to black and $z$'s grandparent is changed to red. Thus, there is a ned loss of one red node, so $\\Phi(T') = \\Phi(T) \u2212 1$. d. For case 1, there is a single decrease in the number of red nodes, and thus a decrease in the potential function. However, a single call to $\\text{RB-INSERTFIXUP}$ could result in $\\Omega(\\lg n)$ instances of case 1. For cases 2 and 3, the colors stay the same and each performs a rotation. e. Since each instance of case 1 requires a specific node to be red, it can't decrease the number of red nodes by more than $\\Phi(T)$. Therefore the potential function is always non-negative. Any insert can increase the number of red nodes by at most $1$, and one unit of potential can pay for any structural modifications of any of the 3 cases. Note that in the worst case, the call to $\\text{RB-INSERT}$ has to perform $k$ case-1 operations, where $k$ is equal to $\\Phi(T_i) \u2212 \\Phi(T_{i \u2212 1})$. Thus, the total amortized cost is bounded above by $2(\\Phi(T_n) \u2212 \\Phi(T_0)) \\le n$, so the amortized cost of each insert is $O(1)$. f. In case 1 of $\\text{RB-INSERT}$, we reduce the number of black nodes with two red children by $1$ and we at most increase the number of black nodes with no red children by $1$, leaving a net loss of at most $1$ to the potential function. In our new potential function, $\\Phi(T_n) \u2212 \\Phi(T_0) \\le n$. Since one unit of potential pays for each operation and the terminating cases cause constant structural changes, the total amortized cost is $O(n)$ making the amortized cost of each $\\text{RB-INSERT-FIXUP}$ $O(1)$. g. In case 2 of $\\text{RB-DELETE}$, we reduce the number of black nodes with two red children by $1$, thereby reducing the potential function by $2$. Since the change in potential is at least negative $1$, it pays for the structural modifications. Since the other cases cause constant structural changes, the total amortized cost is $O(n)$ making the amortized cost of each $\\text{RB-DELETE-FIXUP}$ $O(1)$. h. As described above, whether we insert or delete in any of the cases, the potential function always pays for the changes made if they're nonterminating. If they're terminating then they already take constant time, so the amortized cost of any operation in a sequence of $m$ inserts and deletes is $O(1)$, making the toal amortized cost $O(m)$.","title":"17-4 The cost of restructuring red-black trees"},{"location":"Chap17/Problems/17-5/","text":"A self-organizing list is a linked list of $n$ elements, in which each element has a unique key. When we search for an element in the list, we are given a key, and we want to find an element with that key. A self-organizing list has two important properties: To find an element in the list, given its key, we must traverse the list from the beginning until we encounter the element with the given key. If that element is the $k$th element from the start of the list, then the cost to find the element is $k$. We may reorder the list elements after any operation, according to a given rule with a given cost. We may choose any heuristic we like to decide how to reorder the list. Assume that we start with a given list of $n$ elements, and we are given an access sequence $\\sigma = \\langle \\sigma_1, \\sigma_2, \\ldots, \\sigma_m \\rangle$ of keys to find, in order. The cost of the sequence is the sum of the costs of the individual accesses in the sequence. Out of the various possible ways to reorder the list after an operation, this problem focuses on transposing adjacent list elements-switching their positions in the list\u2014with a unit cost for each transpose operation. You will show, by means of a potential function, that a particular heuristic for reordering the list, move-to-front, entails a total cost no worse than $4$ times that of any other heuristic for maintaining the list order\u2014even if the other heuristic knows the access sequence in advance! We call this type of analysis a competitive analysis . For a heuristic $H$ and a given initial ordering of the list, denote the access cost of sequence $\\sigma$ by $C_H(\\sigma)$ Let $m$ be the number of accesses in $\\sigma$. a. Argue that if heuristic $\\text H$ does not know the access sequence in advance, then the worst-case cost for $\\text H$ on an access sequence $\\sigma$ is $C_H(\\sigma) = \\Omega(mn)$. With the move-to-front heuristic, immediately after searching for an element $x$, we move $x$ to the first position on the list (i.e., the front of the list). Let $\\text{rank}_L(x)$ denote the rank of element $x$ in list $L$, that is, the position of $x$ in list $L$. For example, if $x$ is the fourth element in $L$, then $\\text{rank}_L(x) = 4$. Let $c_i$ denote the cost of access $\\sigma_i$ using the move-to-front heuristic, which includes the cost of finding the element in the list and the cost of moving it to the front of the list by a series of transpositions of adjacent list elements. b. Show that if $\\sigma_i$ accesses element $x$ in list $L$ using the move-to-front heuristic, then $c_i = 2 \\cdot \\text{rank}_L(x) - 1$. Now we compare move-to-front with any other heuristic $\\text H$ that processes an access sequence according to the two properties above. Heuristic $\\text H$ may transpose elements in the list in any way it wants, and it might even know the entire access sequence in advance. Let $L_i$ be the list after access $\\sigma_i$ using move-to-front, and let $L_i^*$ be the list after access $\\sigma_i$ using heuristic $\\text H$. We denote the cost of access $\\sigma_i$ by $c_i$ for move-to-front and by $c_i^*$ for heuristic $\\text H$. Suppose that heuristic $\\text H$ performs $t_i^*$ transpositions during access $\\sigma_i$. c. In part (b), you showed that $c_i = 2 \\cdot \\text{rank}_{L_{i - 1}}(x) - 1$. Now show that $c_i^* = \\text{rank}_{L_{i - 1}^*}(x) + t_i^*$. We define an inversion in list $L_i$ as a pair of elements $y$ and $z$ such that $y$ precedes $z$ in $L_i$ and $z$ precedes $y$ in list $L_i^*$. Suppose that list $L_i$ has $q_i$ inversions after processing the access sequence $\\langle \\sigma_1, \\sigma_2, \\ldots, \\sigma_i \\rangle$. Then, we define a potential function $\\Phi$ that maps $L_i$ to a real number by $\\Phi(L_i) = 2q_i$. For example, if $L_i$ has the elements $\\langle e, c, a, d, b \\rangle$ and $L_i^*$ has the elements $\\langle c, a, b, d, e \\rangle$, then $L_i$ has 5 inversions $((e, c), (e, a), (e, d), (e, b), (d, b))$, and so $\\Phi(L_i) = 10$. Observe that $\\Phi(L_i) \\ge 0$ for all $i$ and that, if move-to-front and heuristic $\\text H$ start with the same list $L_0$, then $\\Phi(L_0) = 0$. d. Argue that a transposition either increases the potential by $2$ or decreases the potential by $2$. Suppose that access $\\sigma_i$ finds the element $x$. To understand how the potential changes due to $\\sigma_i$, let us partition the elements other than $x$ into four sets, depending on where they are in the lists just before the $i$th access: Set $A$ consists of elements that precede $x$ in both $L_{i - 1}$ and $L_{i - 1}^*$. Set $B$ consists of elements that precede $x$ in $L_{i - 1}$ and follow $x$ in $L_{i - 1}^*$. Set $C$ consists of elements that follow $x$ in $L_{i - 1}$ and precede $x$ in $L_{i - 1}^*$. Set $D$ consists of elements that follow $x$ in both $L_{i - 1}$ and $L_{i - 1}^*$. e. Argue that $\\text{rank}_{L_{i - 1}}(x) = |A| + |B| + 1$ and $\\text{rank}_{L_{i - 1}^*}(x) = |A| + |C| + 1$. f. Show that access $\\sigma_i$ causes a change in potential of $$\\Phi(L_i) - \\Phi(L_{i - 1}) \\le 2(|A| - |B| + t_i^*),$$ where, as before, heuristic $\\text H$ performs $t_i^*$ transpositions during access $\\sigma_i$. Define the amortized cost $\\hat c_i$ of access $\\sigma_i$ by $\\hat c_i = c_i + \\Phi(L_i) - \\Phi(L_{i - 1})$. g. Show that the amortized cost $\\hat c_i$ of access $\\sigma_i$ is bounded from above by $4c_i^*$. h. Conclude that the cost $C_{\\text{MTF}}(\\sigma)$ of access sequence $\\sigma$ with move-to-front is at most $4$ times the cost $C_H(\\sigma)$ of $\\sigma$ withany other heuristic $\\text H$, assuming that both heuristics start with the same list. a. Since the heuristic is picked in advance, given any sequence of requests given so far, we can simulate what ordering the heuristic will call for, then, we will pick our next request to be whatever element will of been in the last position of the list. Continuing until all the requests have been made, we have that the cost of this sequence of accesses is $= mn$. b. The cost of finding an element is $= \\text{rank}_L(x)$ and since it needs to be swapped with all the elements before it, of which there are $\\text{rank}_L(x) - 1$, the total cost is $2 \\cdot \\text{rank}_L(x) - 1$. c. Regardless of the heuristic used, we first need to locate the element, which is left where ever it was after the previous step, so, needs $\\text{rank}_{L_{i - 1}}(x)$. After that, by definition, there are $t_i$ transpositions made, so, $c^*_i = \\text{rank}_{L_{i - 1}}(x) + t_i^*$. d. If we perform a transposition of elements $y$ and $z$, where $y$ is towards the left. Then there are two cases. The first is that the final ordering of the list in $L_i^*$ is with $y$ in front of $z$, in which case we have just increased the number of inversions by $1$, so the potential increases by $2$. The second is that in $L_I^*z$ occurs before $y$, in which case, we have just reduced the number of inversions by one, reducing the potential by $2$. In both cases, whether or not there is an inversion between $y$ or $z$ and any other element has not changed, since the transposition only changed the relative ordering of those two elements. e. By definition, $A$ and $B$ are the only two of the four categories to place elements that precede $x$ in $L_{i - 1}$, since there are $|A| + |B|$ elements preceding it, it's rank in $L_{i - 1}$ is $|A| + |B| + 1$. Similarly, the two categories in which an element can be if it precedes $x$ in $L^*_{i - 1}$ are $A$ and $C$, so, in $L^*_{i - 1}$, $x$ has $\\text{rank} |A| + |C| + 1$. f. We have from part d that the potential increases by $2$ if we transpose two elements that are being swapped so that their relative order in the final ordering is being screwed up, and decreases by two if they are begin placed into their correct order in $L^*_i$. In particular, they increase it by at most $2$, since we are keeping track of the number of inversions that may not be the direct effect of the transpositions that heuristic $H$ made, we see which ones the Move to front heuristic may of added. In particular, since the move to front heuristic only changed the relative order of $x$ with respect to the other elements, moving it in front of the elements that preceded it in $L_{i - 1}$, we only care about sets $A$ and $B$. For an element in $A$, moving it to be behind $A$ created an inversion, since that element preceded $x$ in $L^*_i$. However, if the element were in $B$, we are removing an inversion by placing $x$ in front of it. g. $$ \\begin{aligned} \\hat c_i & \\le 2(|A| + |B| + 1) - 1 + 2(|A| - |B| + t_i^*) \\\\ & = 4|A| + 1 + 2 t_i^* \\\\ & \\le 4(|A| + |C| + 1 + t_i^*) \\\\ & = 4 c_i^*. \\end{aligned} $$ h. We showed that the amortized cost of each operation under the move to front heuristic was at most four times the cost of the operation using any other heuristic. Since the amortized cost added up over all these operation is at most the total (real) cost, so we have that the total cost with movetofront is at most four times the total cost with an arbitrary other heuristic.","title":"17-5 Competitive analysis of self-organizing lists with move-to-front"},{"location":"Chap18/18.1/","text":"18.1-1 Why don't we allow a minimum degree of $t = 1$? According to the definition, minimum degree $t$ means every node other than the root must have at least $t - 1$ keys, and every internal node other than the root thus has at least $t$ children. So, when $t = 1$, it means every node other than the root must have at least $t - 1 = 0$ key, and every internal node other than the root thus has at least $t = 1$ child. Thus, we can see that the minimum case doesn't exist, because no node exists with $0$ key, and no node exists with only $1$ child in a B-tree. 18.1-2 For what values of $t$ is the tree of Figure 18.1 a legal B-tree? According to property 5 of B-tree, every node other than the root must have at least $t - 1$ keys and may contain at most $2t - 1$ keys. In Figure 18.1, the number of keys of each node (except the root) is either $2$ or $3$. So to make it a legal B-tree, we need to guarantee that $t - 1 \\le 2 \\text{ and } 2 t - 1 \\ge 3$, which yields $2 \\le t \\le 3$. So $t$ can be $2$ or $3$. 18.1-3 Show all legal B-trees of minimum degree $2$ that represent $\\{1, 2, 3, 4, 5\\}$. We know that every node except the root must have at least $t - 1 = 1$ keys, and at most $2t - 1 = 3$ keys. Also remember that the leaves stay in the same depth. Thus, there are $2$ possible legal B-trees: $$| 1, 2, 3, 4, 5 |$$ $$| 3 |$$ $$\\swarrow \\quad \\searrow$$ $$| 1, 2 | \\qquad\\qquad | 4, 5 |$$ 18.1-4 As a function of the minimum degree $t$, what is the maximum number of keys that can be stored in a B-tree of height $h$? $$ \\begin{aligned} n & = (1 + 2t + (2t) ^ 2 + \\cdots + (2t) ^ {h}) \\cdot (2t - 1) \\\\ & = (2t)^{h + 1} - 1. \\end{aligned} $$ 18.1-5 Describe the data structure that would result if each black node in a red-black tree were to absorb its red children, incorporating their children with its own. After absorbing each red node into its black parent, each black node may contain $1, 2$ ($1$ red child), or $3$ ($2$ red children) keys, and all leaves of the resulting tree have the same depth, according to property 5 of red-black tree (For each node, all paths from the node to descendant leaves contain the same number of black nodes). Therefore, a red-black tree will become a Btree with minimum degree $t = 2$, i.e., a 2-3-4 tree.","title":"18.1 Definition of B-trees"},{"location":"Chap18/18.1/#181-1","text":"Why don't we allow a minimum degree of $t = 1$? According to the definition, minimum degree $t$ means every node other than the root must have at least $t - 1$ keys, and every internal node other than the root thus has at least $t$ children. So, when $t = 1$, it means every node other than the root must have at least $t - 1 = 0$ key, and every internal node other than the root thus has at least $t = 1$ child. Thus, we can see that the minimum case doesn't exist, because no node exists with $0$ key, and no node exists with only $1$ child in a B-tree.","title":"18.1-1"},{"location":"Chap18/18.1/#181-2","text":"For what values of $t$ is the tree of Figure 18.1 a legal B-tree? According to property 5 of B-tree, every node other than the root must have at least $t - 1$ keys and may contain at most $2t - 1$ keys. In Figure 18.1, the number of keys of each node (except the root) is either $2$ or $3$. So to make it a legal B-tree, we need to guarantee that $t - 1 \\le 2 \\text{ and } 2 t - 1 \\ge 3$, which yields $2 \\le t \\le 3$. So $t$ can be $2$ or $3$.","title":"18.1-2"},{"location":"Chap18/18.1/#181-3","text":"Show all legal B-trees of minimum degree $2$ that represent $\\{1, 2, 3, 4, 5\\}$. We know that every node except the root must have at least $t - 1 = 1$ keys, and at most $2t - 1 = 3$ keys. Also remember that the leaves stay in the same depth. Thus, there are $2$ possible legal B-trees: $$| 1, 2, 3, 4, 5 |$$ $$| 3 |$$ $$\\swarrow \\quad \\searrow$$ $$| 1, 2 | \\qquad\\qquad | 4, 5 |$$","title":"18.1-3"},{"location":"Chap18/18.1/#181-4","text":"As a function of the minimum degree $t$, what is the maximum number of keys that can be stored in a B-tree of height $h$? $$ \\begin{aligned} n & = (1 + 2t + (2t) ^ 2 + \\cdots + (2t) ^ {h}) \\cdot (2t - 1) \\\\ & = (2t)^{h + 1} - 1. \\end{aligned} $$","title":"18.1-4"},{"location":"Chap18/18.1/#181-5","text":"Describe the data structure that would result if each black node in a red-black tree were to absorb its red children, incorporating their children with its own. After absorbing each red node into its black parent, each black node may contain $1, 2$ ($1$ red child), or $3$ ($2$ red children) keys, and all leaves of the resulting tree have the same depth, according to property 5 of red-black tree (For each node, all paths from the node to descendant leaves contain the same number of black nodes). Therefore, a red-black tree will become a Btree with minimum degree $t = 2$, i.e., a 2-3-4 tree.","title":"18.1-5"},{"location":"Chap18/18.2/","text":"18.2-1 Show the results of inserting the keys $$F, S, Q, K, C, L, H, T, V, W, M, R, N, P, A, B, X, Y, D, Z, E$$ in order into an empty B-tree with minimum degree $2$. Draw only the configurations of the tree just before some node must split, and also draw the final configuration. (Omit!) 18.2-2 Explain under what circumstances, if any, redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ operations occur during the course of executing a call to $\\text{B-TREE-INSERT}$. (A redundant $\\text{DISK-READ}$ is a $\\text{DISK-READ}$ for a page that is already in memory. A redundant $\\text{DISK-WRITE}$ writes to disk a page of information that is identical to what is already stored there.) In order to insert the key into a full child node but without its parent being full, we need the following operations: $\\text{DISK-READ}$: Key placement $\\text{DISK-WRITE}$: Split nodes $\\text{DISK-READ}$: Get to the parent $\\text{DISK-WRITE}$: Fill parent If both were full, we'd have to do the same, but instead of the final step, repeat the above to split the parent node and write into the child nodes. With both considerations in mind, there should never be a redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ on a $\\text{B-TREE-INSERT}$. 18.2-3 Explain how to find the minimum key stored in a B-tree and how to find the predecessor of a given key stored in a B-tree. Finding the minimum in a B-tree is quite similar to finding a minimum in a binary search tree. We need to find the left most leaf for the given root, and return the first key. PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MIN}(T.root)$. POST: $\\text{FCTVAL}$ is the minimum key stored in the subtree rooted at $x$. B - TREE - FIND - MIN ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x . key [ 1 ] // return the minimum key of x else DISK - READ ( x . c [ 1 ]) return B - TREE - FIND - MIN ( x . c [ 1 ]) Finding the predecessor of a given key $x.key_i$ is according to the following rules: If $x$ is not a leaf, return the maximum key in the $i$-th child of $x$, which is also the maximum key of the subtree rooted at $x.c_i$. If $x$ is a leaf and $i > 1$, return the $(i - 1)$st key of $x$, i.e., $x.key_{i - 1}$. Otherwise, look for the last node y (from the bottom up) and $j > 0$, such that $x.key_i$ is the leftmost key in $y.c_j$; if $j = 1$, return $\\text{NIL}$ since $x.key_i$ is the minimum key in the tree; otherwise we return $y.key_{j - 1}$. PRE: $x$ is a node on the B-tree $T$. $i$ is the index of the key. POST: $\\text{FCTVAL}$ is the predecessor of $x.key_i$. B - TREE - FIND - PREDECESSOR ( x , i ) if ! x . leaf DISK - READ ( x . c [ i ]) return B - TREE - FIND - MAX ( x . c [ i ]) else if i > 1 // x is a leaf and i > 1 return x . key [ i - 1 ] else z = x while true if z . p == NIL // z is root return NIL // z.key[i] is the minimum key in T; no predecessor y = z . p j = 1 DISK - READ ( y . c [ 1 ]) while y . c [ j ] != x j = j + 1 DISK - READ ( y . c [ j ]) if j == 1 z = y else return y . key [ j - 1 ] PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MAX}(T.root)$. POST: $\\text{FCTVAL}$ is the maximum key stored in the subtree rooted at $x$. B - TREE - FIND - MAX ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x .[ x . n ] // return the maximum key of x else DISK - READ ( x . c [ x . n + 1 ]) return B - TREE - FIND - MIN ( x . c [ x . n + 1 ]) 18.2-4 $\\star$ Suppose that we insert the keys $\\{1, 2, \\ldots, n\\}$ into an empty B-tree with minimum degree 2. How many nodes does the final B-tree have? The final tree can have as many as $n - 1$ nodes. Unless $n = 1$ there cannot ever be $n$ nodes since we only ever insert a key into a non-empty node, so there will always be at least one node with $2$ keys. Next observe that we will never have more than one key in a node which is not a right spine of our B-tree. This is because every key we insert is larger than all keys stored in the tree, so it will be inserted into the right spine of the tree. Nodes not in the right spine are a result of splits, and since $t = 2$, every split results in child nodes with one key each. The fewest possible number of nodes occurs when every node in the right spine has $3$ keys. In this case, $n = 2h + 2^{h + 1} - 1$ where $h$ is the height of the B-tree, and the number of nodes is $2^{h + 1} - 1$. Asymptotically these are the same, so the number of nodes is $\\Theta(n)$. 18.2-5 Since leaf nodes require no pointers to children, they could conceivably use a different (larger) $t$ value than internal nodes for the same disk page size. Show how to modify the procedures for creating and inserting into a B-tree to handle this variation. You would modify the insertion procedure by, in $\\text{B-TREE-INSERT}$, check if the node is a leaf, and if it is, only split it if there twice as many keys stored as expected. Also, if an element needs to be inserted into a full leaf, we would split the leaf into two separate leaves, each of which doesn't have too many keys stored in it. 18.2-6 Suppose that we were to implement $\\text{B-TREE-SEARCH}$ to use binary search rather than linear search within each node. Show that this change makes the CPU time required $O(\\lg n)$, independently of how $t$ might be chosen as a function of $n$. As in the $\\text{TREE-SEARCH}$ procedure for binary search trees, the nodes encountered during the recursion form a simple path downward from the root of the tree. Thus, the $\\text{B-TREE-SEARCH}$ procedure needs $O(h) = O(\\log_t n)$ CPU time to search along the path, where $h$ is the height of the B-tree and $n$ is the number of keys in the B-tree, and we know that $h \\le \\log_t \\frac{n + 1}{2}$. Since the number of keys in each nodeis less than $2t - 1$, a binary search within each node is $O(\\lg t)$. So the total time is: $$ \\begin{aligned} O(\\lg t \\cdot \\log_t n) & = O(\\lg t \\cdot \\frac{\\lg n}{\\lg t}) & \\text{by changing the base of the logarithm.} \\\\ & = O(\\lg n). \\end{aligned} $$ Thus, the CPU time required is $O(\\lg n)$. 18.2-7 Suppose that disk hardware allows us to choose the size of a disk page arbitrarily, but that the time it takes to read the disk page is $a + bt$, where $a$ and $b$ are specified constants and $t$ is the minimum degree for a B-tree using pages of the selected size. Describe how to choose $t$ so as to minimize (approximately) the B-tree search time. Suggest an optimal value of $t$ for the case in which $a = 5$ milliseconds and $b = 10$ microseconds. $$\\min \\log_t n \\cdot (a + bt) = \\min \\frac{a + bt}{\\ln t}$$ $$\\frac{\\partial}{\\partial t} (\\frac{a + bt}{\\ln t}) = - \\frac{a + bt - bt \\ln t}{t \\ln^2 t}$$ $$ \\begin{aligned} a + bt & = bt \\ln t \\\\ 5 + 10t & = 10t \\ln t \\\\ t & = e^{W \\left(\\frac{1}{2e} \\right) + 1}, \\\\ \\end{aligned} $$ where $W$ is the LambertW function, and we should choose $t = 3$.","title":"18.2 Basic operations on B-trees"},{"location":"Chap18/18.2/#182-1","text":"Show the results of inserting the keys $$F, S, Q, K, C, L, H, T, V, W, M, R, N, P, A, B, X, Y, D, Z, E$$ in order into an empty B-tree with minimum degree $2$. Draw only the configurations of the tree just before some node must split, and also draw the final configuration. (Omit!)","title":"18.2-1"},{"location":"Chap18/18.2/#182-2","text":"Explain under what circumstances, if any, redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ operations occur during the course of executing a call to $\\text{B-TREE-INSERT}$. (A redundant $\\text{DISK-READ}$ is a $\\text{DISK-READ}$ for a page that is already in memory. A redundant $\\text{DISK-WRITE}$ writes to disk a page of information that is identical to what is already stored there.) In order to insert the key into a full child node but without its parent being full, we need the following operations: $\\text{DISK-READ}$: Key placement $\\text{DISK-WRITE}$: Split nodes $\\text{DISK-READ}$: Get to the parent $\\text{DISK-WRITE}$: Fill parent If both were full, we'd have to do the same, but instead of the final step, repeat the above to split the parent node and write into the child nodes. With both considerations in mind, there should never be a redundant $\\text{DISK-READ}$ or $\\text{DISK-WRITE}$ on a $\\text{B-TREE-INSERT}$.","title":"18.2-2"},{"location":"Chap18/18.2/#182-3","text":"Explain how to find the minimum key stored in a B-tree and how to find the predecessor of a given key stored in a B-tree. Finding the minimum in a B-tree is quite similar to finding a minimum in a binary search tree. We need to find the left most leaf for the given root, and return the first key. PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MIN}(T.root)$. POST: $\\text{FCTVAL}$ is the minimum key stored in the subtree rooted at $x$. B - TREE - FIND - MIN ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x . key [ 1 ] // return the minimum key of x else DISK - READ ( x . c [ 1 ]) return B - TREE - FIND - MIN ( x . c [ 1 ]) Finding the predecessor of a given key $x.key_i$ is according to the following rules: If $x$ is not a leaf, return the maximum key in the $i$-th child of $x$, which is also the maximum key of the subtree rooted at $x.c_i$. If $x$ is a leaf and $i > 1$, return the $(i - 1)$st key of $x$, i.e., $x.key_{i - 1}$. Otherwise, look for the last node y (from the bottom up) and $j > 0$, such that $x.key_i$ is the leftmost key in $y.c_j$; if $j = 1$, return $\\text{NIL}$ since $x.key_i$ is the minimum key in the tree; otherwise we return $y.key_{j - 1}$. PRE: $x$ is a node on the B-tree $T$. $i$ is the index of the key. POST: $\\text{FCTVAL}$ is the predecessor of $x.key_i$. B - TREE - FIND - PREDECESSOR ( x , i ) if ! x . leaf DISK - READ ( x . c [ i ]) return B - TREE - FIND - MAX ( x . c [ i ]) else if i > 1 // x is a leaf and i > 1 return x . key [ i - 1 ] else z = x while true if z . p == NIL // z is root return NIL // z.key[i] is the minimum key in T; no predecessor y = z . p j = 1 DISK - READ ( y . c [ 1 ]) while y . c [ j ] != x j = j + 1 DISK - READ ( y . c [ j ]) if j == 1 z = y else return y . key [ j - 1 ] PRE: $x$ is a node on the B-tree $T$. The top level call is $\\text{B-TREE-FIND-MAX}(T.root)$. POST: $\\text{FCTVAL}$ is the maximum key stored in the subtree rooted at $x$. B - TREE - FIND - MAX ( x ) if x == NIL // T is empty return NIL else if x . leaf // x is leaf return x .[ x . n ] // return the maximum key of x else DISK - READ ( x . c [ x . n + 1 ]) return B - TREE - FIND - MIN ( x . c [ x . n + 1 ])","title":"18.2-3"},{"location":"Chap18/18.2/#182-4-star","text":"Suppose that we insert the keys $\\{1, 2, \\ldots, n\\}$ into an empty B-tree with minimum degree 2. How many nodes does the final B-tree have? The final tree can have as many as $n - 1$ nodes. Unless $n = 1$ there cannot ever be $n$ nodes since we only ever insert a key into a non-empty node, so there will always be at least one node with $2$ keys. Next observe that we will never have more than one key in a node which is not a right spine of our B-tree. This is because every key we insert is larger than all keys stored in the tree, so it will be inserted into the right spine of the tree. Nodes not in the right spine are a result of splits, and since $t = 2$, every split results in child nodes with one key each. The fewest possible number of nodes occurs when every node in the right spine has $3$ keys. In this case, $n = 2h + 2^{h + 1} - 1$ where $h$ is the height of the B-tree, and the number of nodes is $2^{h + 1} - 1$. Asymptotically these are the same, so the number of nodes is $\\Theta(n)$.","title":"18.2-4 $\\star$"},{"location":"Chap18/18.2/#182-5","text":"Since leaf nodes require no pointers to children, they could conceivably use a different (larger) $t$ value than internal nodes for the same disk page size. Show how to modify the procedures for creating and inserting into a B-tree to handle this variation. You would modify the insertion procedure by, in $\\text{B-TREE-INSERT}$, check if the node is a leaf, and if it is, only split it if there twice as many keys stored as expected. Also, if an element needs to be inserted into a full leaf, we would split the leaf into two separate leaves, each of which doesn't have too many keys stored in it.","title":"18.2-5"},{"location":"Chap18/18.2/#182-6","text":"Suppose that we were to implement $\\text{B-TREE-SEARCH}$ to use binary search rather than linear search within each node. Show that this change makes the CPU time required $O(\\lg n)$, independently of how $t$ might be chosen as a function of $n$. As in the $\\text{TREE-SEARCH}$ procedure for binary search trees, the nodes encountered during the recursion form a simple path downward from the root of the tree. Thus, the $\\text{B-TREE-SEARCH}$ procedure needs $O(h) = O(\\log_t n)$ CPU time to search along the path, where $h$ is the height of the B-tree and $n$ is the number of keys in the B-tree, and we know that $h \\le \\log_t \\frac{n + 1}{2}$. Since the number of keys in each nodeis less than $2t - 1$, a binary search within each node is $O(\\lg t)$. So the total time is: $$ \\begin{aligned} O(\\lg t \\cdot \\log_t n) & = O(\\lg t \\cdot \\frac{\\lg n}{\\lg t}) & \\text{by changing the base of the logarithm.} \\\\ & = O(\\lg n). \\end{aligned} $$ Thus, the CPU time required is $O(\\lg n)$.","title":"18.2-6"},{"location":"Chap18/18.2/#182-7","text":"Suppose that disk hardware allows us to choose the size of a disk page arbitrarily, but that the time it takes to read the disk page is $a + bt$, where $a$ and $b$ are specified constants and $t$ is the minimum degree for a B-tree using pages of the selected size. Describe how to choose $t$ so as to minimize (approximately) the B-tree search time. Suggest an optimal value of $t$ for the case in which $a = 5$ milliseconds and $b = 10$ microseconds. $$\\min \\log_t n \\cdot (a + bt) = \\min \\frac{a + bt}{\\ln t}$$ $$\\frac{\\partial}{\\partial t} (\\frac{a + bt}{\\ln t}) = - \\frac{a + bt - bt \\ln t}{t \\ln^2 t}$$ $$ \\begin{aligned} a + bt & = bt \\ln t \\\\ 5 + 10t & = 10t \\ln t \\\\ t & = e^{W \\left(\\frac{1}{2e} \\right) + 1}, \\\\ \\end{aligned} $$ where $W$ is the LambertW function, and we should choose $t = 3$.","title":"18.2-7"},{"location":"Chap18/18.3/","text":"18.3-1 Show the results of deleting $C$, $P$, and $V$, in order, from the tree of Figure 18.8(f). Figure 18.8(f) delete $C$ delete $P$ delete $V$ 18.3-2 Write pseudocode for $\\text{B-TREE-DELETE}$. The algorithm $\\text{B-TREE-DELETE}(x, k)$ is a recursive procedure which deletes key $k$ from the B-tree rooted at node $x$. The functions $\\text{PREDECESSOR}(k, x)$ and $\\text{SUCCESSOR}(k, x)$ return the predecessor and successor of $k$ in the B-tree rooted at $x$ respectively. The cases where $k$ is the last key in a node have been omitted because the pseudocode is already unwieldy. For these, we simply use the left sibling as opposed to the right sibling, making the appropriate modifications to the indexing in the for loops.","title":"18.3 Deleting a key from a B-tree"},{"location":"Chap18/18.3/#183-1","text":"Show the results of deleting $C$, $P$, and $V$, in order, from the tree of Figure 18.8(f). Figure 18.8(f) delete $C$ delete $P$ delete $V$","title":"18.3-1"},{"location":"Chap18/18.3/#183-2","text":"Write pseudocode for $\\text{B-TREE-DELETE}$. The algorithm $\\text{B-TREE-DELETE}(x, k)$ is a recursive procedure which deletes key $k$ from the B-tree rooted at node $x$. The functions $\\text{PREDECESSOR}(k, x)$ and $\\text{SUCCESSOR}(k, x)$ return the predecessor and successor of $k$ in the B-tree rooted at $x$ respectively. The cases where $k$ is the last key in a node have been omitted because the pseudocode is already unwieldy. For these, we simply use the left sibling as opposed to the right sibling, making the appropriate modifications to the indexing in the for loops.","title":"18.3-2"},{"location":"Chap18/Problems/18-1/","text":"Consider implementing a stack in a computer that has a relatively small amount of fast primary memory and a relatively large amount of slower disk storage. The operations $\\text{PUSH}$ and $\\text{POP}$ work on single-word values. The stack we wish to support can grow to be much larger than can fit in memory, and thus most of it must be stored on disk. A simple, but inefficient, stack implementation keeps the entire stack on disk. We maintain in - memory a stack pointer, which is the disk address of the top element on the stack. If the pointer has value $p$, the top element is the $(p \\mod m)$th word on page $\\lfloor p / m \\rfloor$ of the disk, where $m$ is the number of words per page. To implement the $\\text{PUSH}$ operation, we increment the stack pointer, read the appropriate page into memory from disk, copy the element to be pushed to the appropriate word on the page, and write the page back to disk. A $\\text{POP}$ operation is similar. We decrement the stack pointer, read in the appropriate page from disk, and return the top of the stack. We need not write back the page, since it was not modified. Because disk operations are relatively expensive, we count two costs for any implementation: the total number of disk accesses and the total CPU time. Any disk access to a page of $m$ words incurs charges of one disk access and $\\Theta(m)$ CPU time. a. Asymptotically, what is the worst-case number of disk accesses for $n$ stack operations using this simple implementation? What is the CPU time for $n$ stack operations? (Express your answer in terms of $m$ and $n$ for this and subsequent parts.) Now consider a stack implementation in which we keep one page of the stack in memory. (We also maintain a small amount of memory to keep track of which page is currently in memory.) We can perform a stack operation only if the relevant disk page resides in memory. If necessary, we can write the page currently in memory to the disk and read in the new page from the disk to memory. If the relevant disk page is already in memory, then no disk accesses are required. b. What is the worst-case number of disk accesses required for $n$ $\\text{PUSH}$ operations? What is the CPU time? c. What is the worst-case number of disk accesses required for $n$ stack operations? What is the CPU time? Suppose that we now implement the stack by keeping two pages in memory (in addition to a small number of words for bookkeeping). d. Describe how to manage the stack pages so that the amortized number of disk accesses for any stack operation is $O(1 / m)$ and the amortized CPU time for any stack operation is $O(1)$. a. We will have to make a disk access for each stack operation. Since each of these disk operations takes time $\\Theta(m)$, the CPU time is $\\Theta(mn)$. b. Since only every mth push starts a new page, the number of disk operations is approximately $n / m$, and the CPU runtime is $\\Theta(n)$, since both the contribution from the cost of the disk access and the actual running of the push operations are both $\\Theta(n)$. c. If we make a sequence of pushes until it just spills over onto the second page, then alternate popping and pulling many times, the asymptotic number of disk accesses and CPU time is of the same order as in part a. This is because when we are doing that alternating of pops and pushes, each one triggers a disk access. d. We define the potential of the stack to be the absolute value of the difference between the current size of the stack and the most recently passed multiple of $m$. This potential function means that the initial stack which has size $0$, is also a multiple of $m$, so the potential is zero. Also, as we do a stack operation we either increase or decrease the potential by one. For us to have to load a new page from disk and write an old one to disk, we would need to be at least $m$ positions away from the most recently visited multiple of $m$, because we would have had to just cross a page boundary. This cost of loading and storing a page takes (real) cpu time of $\\Theta(m)$. However, we just had a drop in the potential function of order $\\Theta(m)$. So, the amortized cost of this operation is $O(1)$.","title":"18-1 Stacks on secondary storage"},{"location":"Chap18/Problems/18-2/","text":"The join operation takes two dynamic sets $S'$ and $S''$ and an element $x$ such that for any $x' \\in S'$ and $x'' \\in S''$, we have $x'.key < x.key < x''.key$. It returns a set $S = S' \\cup \\{x\\} \\cup S''$. The split operation is like an \"inverse\" join: given a dynamic set $S$ and an element $x \\in S$, it creates a set $S'$ that consists of all elements in set $S$ and an element $x \\in S$, it creates a set $S'$ that consists of all elements in $S - \\{x\\}$ whose keys are less than $x.key$ and a set $S''$ that consists of all elements in $S - \\{x\\}$ whose keys are greater than $x.key$. In this problem, we investigate how to implement these operations on 2-3-4 trees. We assume for convenience that elements consist only of keys and that all key values are distinct. a. Show how to maintain, for every node $x$ of a 2-3-4 tree, the height of the subtree rooted at $x$ as an attribute $x.height$. Make sure that your implementation does not affect the asymptotic running times of searching, insertion, and deletion. b. Show how to implement the join operation. Given two 2-3-4 trees $T'$ and $T''$ and a key $k$, the join operation should run in $O(1 + |h' - h''|)$ time, where $h'$ and $h''$ are the heights of $T'$ and $T''$, respectively. c. Consider the simple path $p$ from the root of a 2-3-4 tree $T$ to a given key $k$, the set $S'$ of keys in $T$ that are less than $k$, and the set $S''$ of keys in $T$ that are greater than $k$. Show that $p$ breaks $S'$ into a set of trees $\\{T'_0, T'_1, \\ldots, T'_m\\}$ and a set of keys $\\{k'_1, k'_2, \\ldots, k'_m\\}$, where, for $i = 1, 2, \\ldots, m$, we have $y < k'_i < z$ for any keys $y \\in T'_{i - 1}$ and $z \\in T'_i$. What is the relationship between the heights of $T'_{i - 1}$ and $T'_i$? Describe how $p$ breaks $S''$ into sets of trees and keys. d. Show how to implement the split operation on $T$. Use the join operation to assemble the keys in $S'$ into a single 2-3-4 tree $T'$ and the keys in $S''$ into a single 2-3-4 tree $T''$. The running time of the split operation should be $O(\\lg n)$, where $n$ is the number of keys in $T$. ($\\textit{Hint:}$ The costs for joining should telescope.) a. For insertion it will suffice to explain how to update height when we split a node. Suppose node $x$ is split into nodes $y$ and $z$, and the median of $x$ is merged into node $w$. The height of $w$ remains unchanged unless $x$ was the root (in which case $w.height = x.height + 1$). The height of $y$ or $z$ will often change. We set $$y.height = \\max_i y.c_i .height + 1$$ and $$z.height = \\max_i z.c_i.height + 1.$$ Each update takes $O(t)$. Since a call to $\\text{B-TREE-INSERT}$ makes at most $h$ splits where $h$ is the height of the tree, the total time it takes to update heights is $O(th)$, preserving the asymptotic running time of insert. For deletion the situation is even simple. The only time the height changes is when the root has a single node and it is merged with its subtree nodes, leaving an empty root node to be deleted. In this case, we update the height of the new node to be the (old) height of the root minus $1$. b. Without loss of generality, assume $h' \\ge h''$. We essentially wish to merge $T''$ into $T'$ at a node of height $h''$ using node $x$. To do this, find the node at depth $h' - h''$ on the right spine of $T'$. Add $x$ as a key to this node, and $T''$ as the additional child. If it should happen that the node was already full, perform a split operation. c. Let $x_i$ be the node encountered after $i$ steps on path $p$. Let $l_i$ be the index of the largest key stored in $x_i$ which is less than or equal to $k$. We take $k_i' = x_i.key_{l_i}$ and $T'_{i - 1}$ to be the tree whose root node consists of the keys in $x_i$ which are less than $x_i.key_{l_i}$, and all of their children. In general, $T'_{i - 1}.height \\ge T'_i.height$. For $S''$, we take a similar approach. They keys will be those in nodes passed on $p$ which are immediately greater than $k$, and the trees will be rooted at a node consisting of the larger keys, with the associated subtrees. When we reach the node which contains $k$, we don't assign a key, but we do assign a tree. d. Let $T_1$ and $T_2$ be empty trees. Consider the path $p$ from the root of $T$ to $k$. Suppose we have reached node $x_i$. We join tree $T'_{i - 1}$ to $T_1$, then insert $k' i$ into $T_1$. We join $T''_{i - 1}$ to $T_2$ and insert $k''_i$ into $T_2$. Once we have encountered the node which contains $k$ at $x_m.key_k$, join $x_m.c_k$ with $T_1$ and $x_m.c_{k + 1}$ with $T_2$. We will perform at most $2$ join operations and $1$ insert operation for each level of the tree. Using the runtime determined in part (b), and the fact that when we join a tree $T'$ to $T_1$ (or $T''$ to $T_2$ respectively) the height difference is $$T'.height - T_1.height.$$ Since the heights are nondecreasing of successive tree that are joined, we get a telescoping sum of heights. The first tree has height $h$, where $h$ is the height of $T$, and the last tree has height $0$. Thus, the runtime is $$O(2(h + h)) = O(\\lg n).$$","title":"18-2 Joining and splitting 2-3-4 trees"},{"location":"Chap19/19.1/","text":"There is no exercise in this section.","title":"19.1 Structure of Fibonacci heaps"},{"location":"Chap19/19.2/","text":"19.2-1 Show the Fibonacci heap that results from calling $\\text{FIB-HEAP-EXTRACT-MIN}$ on the Fibonacci heap shown in Figure 19.4(m). (Omit!)","title":"19.2 Mergeable-heap operations"},{"location":"Chap19/19.2/#192-1","text":"Show the Fibonacci heap that results from calling $\\text{FIB-HEAP-EXTRACT-MIN}$ on the Fibonacci heap shown in Figure 19.4(m). (Omit!)","title":"19.2-1"},{"location":"Chap19/19.3/","text":"19.3-1 Suppose that a root $x$ in a Fibonacci heap is marked. Explain how $x$ came to be a marked root. Argue that it doesn't matter to the analysis that $x$ is marked, even though it is not a root that was first linked to another node and then lost one child. $x$ came to be a marked root because at some point it had been a marked child of $H.min$ which had been removed in $\\text{FIB-HEAP-EXTRACT-MIN}$ operation. See figure 19.4 for an example, where the node with key $18$ became a marked root. It doesn't add the potential for having to do any more actual work for it to be marked. This is because the only time that markedness is checked is in line 3 of cascading cut. This however is only ever run on nodes whose parent is non $\\text{NIL}$. Since every root has $\\text{NIL}$ as it parent, line 3 of cascading cut will never be run on this marked root. It will still cause the potential function to be larger than needed, but that extra computation that was paid in to get the potential function higher will never be used up later. 19.3-2 Justify the $O(1)$ amortized time of $\\text{FIB-HEAP-DECREASE-KEY}$ as an average cost per operation by using aggregate analysis. Recall that the actual cost of $\\text{FIB-HEAP-DECREASE-KEY}$ is $O(c)$, where $c$ is the number of calls made to $\\text{CASCADING-CUT}$. If $c_i$ is the number of calls made on the $i$th key decrease, then the total time of $n$ calls to $\\text{FIB-HEAPDECREASE-KEY}$ is $\\sum_{i = 1}^n O(c_i)$. Next observe that every call to $\\text{CASCADING-CUT}$ moves a node to the root, and every call to a root node takes $O(1)$. Since no roots ever become children during the course of these calls, we must have that $\\sum_{i = 1}^n c_i = O(n)$. Therefore the aggregate cost is $O(n)$, so the average, or amortized, cost is $O(1)$.","title":"19.3 Decreasing a key and deleting a node"},{"location":"Chap19/19.3/#193-1","text":"Suppose that a root $x$ in a Fibonacci heap is marked. Explain how $x$ came to be a marked root. Argue that it doesn't matter to the analysis that $x$ is marked, even though it is not a root that was first linked to another node and then lost one child. $x$ came to be a marked root because at some point it had been a marked child of $H.min$ which had been removed in $\\text{FIB-HEAP-EXTRACT-MIN}$ operation. See figure 19.4 for an example, where the node with key $18$ became a marked root. It doesn't add the potential for having to do any more actual work for it to be marked. This is because the only time that markedness is checked is in line 3 of cascading cut. This however is only ever run on nodes whose parent is non $\\text{NIL}$. Since every root has $\\text{NIL}$ as it parent, line 3 of cascading cut will never be run on this marked root. It will still cause the potential function to be larger than needed, but that extra computation that was paid in to get the potential function higher will never be used up later.","title":"19.3-1"},{"location":"Chap19/19.3/#193-2","text":"Justify the $O(1)$ amortized time of $\\text{FIB-HEAP-DECREASE-KEY}$ as an average cost per operation by using aggregate analysis. Recall that the actual cost of $\\text{FIB-HEAP-DECREASE-KEY}$ is $O(c)$, where $c$ is the number of calls made to $\\text{CASCADING-CUT}$. If $c_i$ is the number of calls made on the $i$th key decrease, then the total time of $n$ calls to $\\text{FIB-HEAPDECREASE-KEY}$ is $\\sum_{i = 1}^n O(c_i)$. Next observe that every call to $\\text{CASCADING-CUT}$ moves a node to the root, and every call to a root node takes $O(1)$. Since no roots ever become children during the course of these calls, we must have that $\\sum_{i = 1}^n c_i = O(n)$. Therefore the aggregate cost is $O(n)$, so the average, or amortized, cost is $O(1)$.","title":"19.3-2"},{"location":"Chap19/19.4/","text":"19.4-1 Professor Pinocchio claims that the height of an $n$-node Fibonacci heap is $O(\\lg n)$. Show that the professor is mistaken by exhibiting, for any positive integer $n$, a sequence of Fibonacci-heap operations that creates a Fibonacci heap consisting of just one tree that is a linear chain of $n$ nodes. Initialize: insert $3$ numbers then extract-min. Iteration: insert $3$ numbers, in which at least two numbers are less than the root of chain, then extract-min. The smallest newly inserted number will be extracted and the remaining two numbers will form a heap whose degree of root is $1$, and since the root of the heap is less than the old chain, the chain will be merged into the newly created heap. Finally we should delete the node which contains the largest number of the 3 inserted numbers. 19.4-2 Suppose we generalize the cascading-cut rule to cut a node $x$ from its parent as soon as it loses its $k$th child, for some integer constant $k$. (The rule in Section 19.3 uses $k = 2$.) For what values of $k$ is $D(n) = O(\\lg n)$? Following the proof of lemma 19.1, if $x$ is any node if a Fibonacci heap, $x.degree = m$, and $x$ has children $y_1, y_2, \\ldots, y_m$, then $y_1.degree \\ge 0$ and $y_i.degree \\ge i - k$. Thus, if $s_m$ denotes the fewest nodes possible in a node of degree $m$, then we have $s_0 = 1, s_1 = 2, \\ldots, s_{k - 1} = k$ and in general, $s_m = k + \\sum_{i = 0}^{m - k} s_i$. Thus, the difference between $s_m$ and $s_{m - 1}$ is $s_{m - k}$. Let $\\{f_m\\}$ be the sequence such that $f_m = m + 1$ for $0 \\le m < k$ and $f_m = f_{m - 1} + f_{m - k}$ for $m \\ge k$. If $F(x)$ is the generating function for $f_m$ then we have $F(x) = \\frac{1 - x^k}{(1 - x)(1 - x - x^k)}$. Let $\\alpha$ be a root of $x^k = x^{k - 1} + 1$. We'll show by induction that $f_{m + k} \\ge \\alpha^m$. For the base cases: $$ \\begin{aligned} f_k & = k + 1 \\ge 1 = \\alpha^0 \\\\ f_{k + 1} & = k + 3 \\ge \\alpha^1 \\\\ & \\vdots \\\\ f_{k + k} & = k + \\frac{(k + 1)(k + 2)}{2} = k + k + 1 + \\frac{k(k + 1)}{2} \\ge 2k + 1+\\alpha^{k - 1} \\ge \\alpha^k. \\end{aligned} $$ In general, we have $$f_{m + k} = f_{m + k - 1} + f_m \\ge \\alpha^{m - 1} + \\alpha^{m - k} = \\alpha^{m - k}(\\alpha^{k - 1} + 1) = \\alpha^m.$$ Next we show that $f_{m + k} = k + \\sum_{i = 0}^m f_i$. The base case is clear, since $f_k = f_0 + k = k + 1$. For the induction step, we have $$f_{m + k} = f_{m - 1 - k} + f_m = k \\sum_{i = 0}^{m - 1} f_i + f_m = k + \\sum_{i = 0}^m f_i.$$ Observe that $s_i \\ge f_{i + k}$ for $0 \\le i < k$. Again, by induction, for $m \\ge k$ we have $$s_m = k + \\sum_{i = 0}^{m - k} s_i \\ge k + \\sum_{i = 0}^{m - k} f_{i + k} \\ge k + \\sum_{i = 0}^m f_i = f_{m + k}.$$ So in general, $s_m \\ge f_{m + k}$. Putting it all together, we have $$ \\begin{aligned} size(x) & \\ge s_m \\\\ & \\ge k + \\sum_{i = k}^m s_{i - k} \\\\ & \\ge k + \\sum_{i = k}^m f_i \\\\ & \\ge f_{m + k} \\\\ & \\ge \\alpha^m. \\end{aligned} $$ Taking logs on both sides, we have $$\\log_\\alpha n \\ge m.$$ In other words, provided that $\\alpha$ is a constant, we have a logarithmic bound on the maximum degree.","title":"19.4 Bounding the maximum degree"},{"location":"Chap19/19.4/#194-1","text":"Professor Pinocchio claims that the height of an $n$-node Fibonacci heap is $O(\\lg n)$. Show that the professor is mistaken by exhibiting, for any positive integer $n$, a sequence of Fibonacci-heap operations that creates a Fibonacci heap consisting of just one tree that is a linear chain of $n$ nodes. Initialize: insert $3$ numbers then extract-min. Iteration: insert $3$ numbers, in which at least two numbers are less than the root of chain, then extract-min. The smallest newly inserted number will be extracted and the remaining two numbers will form a heap whose degree of root is $1$, and since the root of the heap is less than the old chain, the chain will be merged into the newly created heap. Finally we should delete the node which contains the largest number of the 3 inserted numbers.","title":"19.4-1"},{"location":"Chap19/19.4/#194-2","text":"Suppose we generalize the cascading-cut rule to cut a node $x$ from its parent as soon as it loses its $k$th child, for some integer constant $k$. (The rule in Section 19.3 uses $k = 2$.) For what values of $k$ is $D(n) = O(\\lg n)$? Following the proof of lemma 19.1, if $x$ is any node if a Fibonacci heap, $x.degree = m$, and $x$ has children $y_1, y_2, \\ldots, y_m$, then $y_1.degree \\ge 0$ and $y_i.degree \\ge i - k$. Thus, if $s_m$ denotes the fewest nodes possible in a node of degree $m$, then we have $s_0 = 1, s_1 = 2, \\ldots, s_{k - 1} = k$ and in general, $s_m = k + \\sum_{i = 0}^{m - k} s_i$. Thus, the difference between $s_m$ and $s_{m - 1}$ is $s_{m - k}$. Let $\\{f_m\\}$ be the sequence such that $f_m = m + 1$ for $0 \\le m < k$ and $f_m = f_{m - 1} + f_{m - k}$ for $m \\ge k$. If $F(x)$ is the generating function for $f_m$ then we have $F(x) = \\frac{1 - x^k}{(1 - x)(1 - x - x^k)}$. Let $\\alpha$ be a root of $x^k = x^{k - 1} + 1$. We'll show by induction that $f_{m + k} \\ge \\alpha^m$. For the base cases: $$ \\begin{aligned} f_k & = k + 1 \\ge 1 = \\alpha^0 \\\\ f_{k + 1} & = k + 3 \\ge \\alpha^1 \\\\ & \\vdots \\\\ f_{k + k} & = k + \\frac{(k + 1)(k + 2)}{2} = k + k + 1 + \\frac{k(k + 1)}{2} \\ge 2k + 1+\\alpha^{k - 1} \\ge \\alpha^k. \\end{aligned} $$ In general, we have $$f_{m + k} = f_{m + k - 1} + f_m \\ge \\alpha^{m - 1} + \\alpha^{m - k} = \\alpha^{m - k}(\\alpha^{k - 1} + 1) = \\alpha^m.$$ Next we show that $f_{m + k} = k + \\sum_{i = 0}^m f_i$. The base case is clear, since $f_k = f_0 + k = k + 1$. For the induction step, we have $$f_{m + k} = f_{m - 1 - k} + f_m = k \\sum_{i = 0}^{m - 1} f_i + f_m = k + \\sum_{i = 0}^m f_i.$$ Observe that $s_i \\ge f_{i + k}$ for $0 \\le i < k$. Again, by induction, for $m \\ge k$ we have $$s_m = k + \\sum_{i = 0}^{m - k} s_i \\ge k + \\sum_{i = 0}^{m - k} f_{i + k} \\ge k + \\sum_{i = 0}^m f_i = f_{m + k}.$$ So in general, $s_m \\ge f_{m + k}$. Putting it all together, we have $$ \\begin{aligned} size(x) & \\ge s_m \\\\ & \\ge k + \\sum_{i = k}^m s_{i - k} \\\\ & \\ge k + \\sum_{i = k}^m f_i \\\\ & \\ge f_{m + k} \\\\ & \\ge \\alpha^m. \\end{aligned} $$ Taking logs on both sides, we have $$\\log_\\alpha n \\ge m.$$ In other words, provided that $\\alpha$ is a constant, we have a logarithmic bound on the maximum degree.","title":"19.4-2"},{"location":"Chap19/Problems/19-1/","text":"Professor Pisano has proposed the following variant of the $\\text{FIB-HEAP-DELETE}$ procedure, claiming that it runs faster when the node being deleted is not the node pointed to by $H.min$. PISANO - DELETE ( H , x ) if x == H . min FIB - HEAP - EXTRACT - MIN ( H ) else y = x . p if y != NIL CUT ( H , x , y ) CASCADING - CUT ( H , y ) add x ' s child list to the root list of H remove x from the root list of H a. The professor's claim that this procedure runs faster is based partly on the assumption that line 7 can be performed in $O(1)$ actual time. What is wrong with this assumption? b. Give a good upper bound on the actual time of $\\text{PISANO-DELETE}$ when $x$ is not $H.min$. Your bound should be in terms of $x.degree$ and the number $c$ of calls to the $\\text{CASCADING-CUT}$ procedure. c. Suppose that we call $\\text{PISANO-DELETE}(H, x)$, and let $H'$ be the Fibonacci heap that results. Assuming that node $x$ is not a root, bound the potential of $H'$ in terms of $x.degree$, $c$, $t(H)$, and $m(H)$. d. Conclude that the amortized time for $\\text{PISANO-DELETE}$ is asymptotically no better than for $\\text{FIB-HEAP-DELETE}$, evenwhen $x \\ne H.min$. a. It can take actual time proportional to the number of children that $x$ had because for each child, when placing it in the root list, their parent pointer needs to be updated to be $\\text{NIL}$ instead of $x$. b. Line 7 takes actual time bounded by $x.degree$ since updating each of the children of $x$ only takes constant time. So, if $c$ is the number of cascading cuts that are done, the actual cost is $O(c + x.degree)$. c. We examine the number of trees in the root list $t(H)$ and the number of marked nodes $m(H)$ of the resulting Fibonacci heap $H'$ to upper-bound its potential. The number of trees $t(H)$ increases by the number of children $x$ had ($=x.degree$), due to line 7 of $\\text{PISANO-DELETE}(H, x)$. The number of marked nodes in $H'$ is calculated as follows. The first $c - 1$ recursive calls out of the $c$ calls to $\\text{CASCADING-CUT}$ unmarks a marked node (line 4 of $\\text{CUT}$ invoked by line 5 of $\\text{CASCADING-CUT}$). The final $c$th call to $\\text{CASCADING-CUT}$ marks an unmarked node (line 4 of $\\text{CASCADING-CUT}$), and therefore, the total change in marked nodes is $-(c - 1) + 1 = -c + 2$. Therefore, the potential of H' is $$\\Phi(H') \\le t(H) + x.degree + 2(m(H) - c + 2).$$ d. The asymptotic time is $$\\Theta(x.degree) = \\Theta(\\lg(n)),$$ which is the same asyptotic time that was required for the original deletion method.","title":"19-1 Alternative implementation of deletion"},{"location":"Chap19/Problems/19-2/","text":"The binomial tree $B_k$ is an ordered tree (see Section B.5.2) defined recursively. As shown in Figure 19.6(a), the binomial tree $B_0$ consists of a single node. The binomial tree $B_k$ consists of two binomial trees $B_{k - 1}$ that are linked together so that the root of one is the leftmost child of the root of the other. Figure 19.6(b) shows the binomial trees $B_0$ through $B_4$. a. Show that for the binomial tree $B_k$, there are $2^k$ nodes, the height of the tree is $k$, there are exactly $\\binom{k}{i}$ nodes at depth $i$ for $i = 0, 1, \\ldots, k$, and the root has degree $k$, which is greater than that of any other node; moreover, as Figure 19.6(c) shows, if we number the children of the root from left to right by $k - 1, k - 2, \\ldots, 0$, then child $i$ is the root of a subtree $B_i$. A binomial heap $H$ is a set of binomial trees that satisfies the following properties: Each node has a $key$ (like a Fibonacci heap). Each binomial tree in $H$ obeys the min-heap property. For any nonnegative integer $k$, there is at most one binomial tree in $H$ whose root has degree $k$. b. Suppose that a binomial heap $H$ has a total of $n$ nodes. Discuss the relationship between the binomial trees that $H$ contains and the binary representation of $n$. Conclude that $H$ consists of at most $\\lfloor \\lg n \\rfloor + 1$ binomial trees. Suppose that we represent a binomial heap as follows. The left-child, right-sibling scheme of Section 10.4 represents each binomial tree within a binomial heap. Each node contains its key; pointers to its parent, to its leftmost child, and to the sibling immediately to its right (these pointers are $\\text{NIL}$ when appropriate); and its degree (as in Fibonacci heaps, how many children it has). The roots form a singly linked root list, ordered by the degrees of the roots (from low to high), and we access the binomial heap by a pointer to the first node on the root list. c. Complete the description of how to represent a binomial heap (i.e., name the attributes, describe when attributes have the value $\\text{NIL}$, and define how the root list is organized), and show how to implement the same seven operations on binomial heaps as this chapter implemented on Fibonacci heaps. Each operation should run in $O(\\lg n)$ worst-case time, where $n$ is the number of nodes in the binomial heap (or in the case of the $\\text{UNION}$ operation, in the two binomial heaps that are being united). The $\\text{MAKE-HEAP}$ operation should take constant time. d. Suppose that we were to implement only the mergeable-heap operations on a Fibonacci heap (i.e., we do not implement the $\\text{DECREASE-KEY}$ or $\\text{DELETE}$ operations). How would the trees in a Fibonacci heap resemble those in a binomial heap? How would they differ? Show that the maximum degree in an $n$-node Fibonacci heap would be at most $\\lfloor \\lg n\\rfloor$. e. Professor McGee has devised a new data structure based on Fibonacci heaps. A McGee heap has the same structure as a Fibonacci heap and supports just the mergeable-heap operations. The implementations of the operations are the same as for Fibonacci heaps, except that insertion and union consolidate the root list as their last step. What are the worst-case running times of operations on McGee heaps? a. $B_k$ consists of two binomial trees $B_{k - 1}$. The height of one $B_{k - 1}$ is increased by $1$. For $i = 0$, $\\binom{k}{0} = 1$ and only root is at depth $0$. Suppose in $B_{k - 1}$, the number of nodes at depth $i$ is $\\binom{k - 1}{i}$, in $B_k$, the number of nodes at depth $i$ is $\\binom{k - 1}{i} + \\binom{k - 1}{i - 1} = \\binom{k}{i}$. The degree of the root increase by $1$. b. Let $n.b$ denote the binary expansion of $n$. The fact that we can have at most one of each binomial tree corresponds to the fact that we can have at most $1$ as any digit of $n.b$. Since each binomial tree has a size which is a power of $2$, the binomial trees required to represent n nodes are uniquely determined. We include $B_k$ if and only if the $k$th position of $n.b$ is $1$. Since the binary representation of $n$ has at most $\\lfloor \\lg n \\rfloor+ 1$ digits, this also bounds the number of trees which can be used to represent $n$ nodes. c. Given a node $x$, let $x.key$, $x.p$, $x.c$, and $x.s$ represent the attributes key, parent, left-most child, and sibling to the right, respectively. The pointer attributes have value $\\text{NIL}$ when no such node exists. The root list will be stored in a singly linked list. MAKE-HEAP initialize an empty list for the root list and return a pointer to the head of the list, which contains $\\text{NIL}$. This takes constant time. To insert: Let $x$ be a node with key $k$, to be inserted. Scan the root list to find the first $m$ such that $B_m$ is not one of the trees in the binomial heap. If there is no $B_0$, simply create a single root node $x$. Otherwise, union $x, B_0, B_1, \\ldots, B_{m - 1}$ into a $B_m$ tree. Remove all root nodes of the unioned trees from the root list, and update it with the new root. Since each join operation is logarithmic in the height of the tree, the total time is $O(\\lg n)$. $\\text{MINIMUM}$ just scans the root list and returns the minimum in $O(\\lg n)$, since the root list has size at most $O(\\lg n)$. EXTRACT-MIN: finds and deletes the minimum, then splits the tree Bm which contained the minimum into its component binomial trees $B_0, B_1, \\ldots, B_{m - 1}$ in $O(\\lg n)$ time. Finally, it unions each of these with any existing trees of the same size in $O(\\lg n)$ time. UNION: suppose we have two binomial heaps consisting of trees $B_{i_1}, B_{i_2}, \\ldots, B_{i_k}$ and $B_{j_1}, B_{j_2}, \\ldots, B_{j_m}$ respectively. Simply union orresponding trees of the same size between the two heaps, then do another check and join any newly created trees which have caused additional duplicates. Note: we will perform at most one union on any fixed size of binomial tree so the total running time is still logarithmic in $n$, where we assume that $n$ is sum of the sizes of the trees which we are unioning. DECREASE-KEY: simply swap the node whose key was decreased up the tree until it satisfies the min-heap property. This method requires that we swap the node with its parent along with all their satellite data in a brute-force manner to avoid updating $p$ attributes of the siblings of the node. When the data stored in each node is large, we may want to update $p$ instead, which, however, will increase the running time bound to $O(\\lg^2 n)$. DELETE: note that every binomial tree consists of two copies of a smaller binomial tree, so we can write the procedure recursively. If the tree is a single node, simply delete it. If we wish to delete from $B_k$, first split the tree into its constituent copies of $B_{k - 1}$, and recursively call delete on the copy of $B_{k - 1}$ which contains $x$. If this results in two binomial trees of the same size, simply union them. d. The Fibonacci heap will look like a binomial heap, except that multiple copies of a given binomial tree will be allowed. Since the only trees which will appear are binomial trees and $B_k$ has $2k$ nodes, we must have $2k \\le n$, which implies $k \\le \\lfloor \\lg n \\rfloor$. Since the largest root of any binomial tree occurs at the root, and on $B_k$ it is degree $k$, this also bounds the largest degree of a node. e. $\\text{INSERT}$ and $\\text{UNION}$ will no longer have amortized $O(1)$ running time because $\\text{CONSOLIDATE}$ has runtime $O(\\lg n)$. Even if no nodes are consolidated, the runtime is dominated by the check that all degrees are distinct. Since calling $\\text{UNION}$ on a heap and a single node is the same as insertion, it must also have runtime $O(\\lg n)$. The other operations remain unchanged.","title":"19-2 Binomial trees and binomial heaps"},{"location":"Chap19/Problems/19-3/","text":"We wish to augment a Fibonacci heap $H$ to support two new operations without changing the amortized running time of any other Fibonacci-heap operations. a. The operation $\\text{FIB-HEAP-CHANGE-KEY}(H, x, k)$ changes the key of node $x$ to the value $k$. Give an efficient implementation of $\\text{FIB-HEAP-CHANGE-KEY}$, and analyze the amortized running time of your implementation for the cases in which $k$ is greater than, less than, or equal to $x.key$. b. Give an efficient implementation of $\\text{FIB-HEAP-PRUNE}(H, r)$, which deletes $q = \\min(r, H.n)$ nodes from $H$. You may choose any $q$ nodes to delete. Analyze the amortized running time of your implementation. ($\\textit{Hint:}$ You may need to modify the data structure and potential function.) a. If $k < x.key$ just run the decrease key procedure. If $k > x.key$, delete the current value $x$ and insert $x$ again with a new key. For the first case, the amortized time is $O(1)$, and for the last case the amortized time is $O(\\lg n)$. b. Suppose that we also had an additional cost to the potential function that was proportional to the size of the structure. This would only increase when we do an insertion, and then only by a constant amount, so there aren't any worries concerning this increased potential function raising the amortized cost of any operations. Once we've made this modification, to the potential function, we also modify the heap itself by having a doubly linked list along all of the leaf nodes in the heap. To prune we then pick any leaf node, remove it from it's parent's child list, and remove it from the list of leaves. We repeat this $\\min(r, H.n)$ times. This causes the potential to drop by an amount proportional to $r$ which is on the order of the actual cost of what just happened since the deletions from the linked list take only constant amounts of time each. So, the amortized time is constant.","title":"19-3 More Fibonacci-heap operations"},{"location":"Chap19/Problems/19-4/","text":"Chapter 18 introduced the 2-3-4 tree, in which every internal node (other than possibly the root) has two, three, or four children and all leaves have the same depth. In this problem, we shall implement 2-3-4 heaps , which support the mergeable-heap operations. The 2-3-4 heaps differ from 2-3-4 trees in the following ways. In 2-3-4 heaps, only leaves store keys, and each leaf $x$ stores exactly one key in the attribute $x.key$. The keys in the leaves may appear in any order. Each internal node $x$ contains a value $x.small$ that is equal to the smallest key stored in any leaf in the subtree rooted at $x$. The root $r$ contains an attribute $r.height$ that gives the height of the tree. Finally, 2-3-4 heaps are designed to be kept in main memory, so that disk reads and writes are not needed. Implement the following 2-3-4 heap operations. In parts (a)\u2013(e), each operation should run in $O(\\lg n)$ time on a 2-3-4 heap with $n$ elements. The $\\text{UNION}$ operation in part (f) should run in $O(\\lg n)$ time, where $n$ is the number of elements in the two input heaps. a. $\\text{MINIMUM}$, which returns a pointer to the leaf with the smallest key. b. $\\text{DECREASE-KEY}$, which decreases the key of a given leaf $x$ to a given value $k \\le x.key$. c. $\\text{INSERT}$, which inserts leaf $x$ with key $k$. d. $\\text{DELETE}$, which deletes a given leaf $x$. e. $\\text{EXTRACT-MIN}$, which extracts the leaf with the smallest key. f. $\\text{UNION}$, which unites two 2-3-4 heaps, returning a single 2-3-4 heap and destroying the input heaps. a. Traverse a path from root to leaf as follows: At a given node, examine the attribute $x.small$ in each child-node of the current node. Proceed to the child node which minimizes this attribute. If the children of the current node are leaves, then simply return a pointer to the child node with smallest key. Since the height of the tree is $O(\\lg n)$ and the number of children of any node is at most $4$, this has runtime $O(\\lg n)$. b. Decrease the key of $x$, then traverse the simple path from $x$ to the root by following the parent pointers. At each node $y$ encountered, check the attribute $y.small$. If $k < y.small$, set $y.small = k$. Otherwise do nothing and continue on the path. c. Insert works the same as in a B-tree, except that at each node it is assumed that the node to be inserted is 'smaller' than every key stored at that node, so the runtime is inherited. If the root is split, we update the height of the tree. When we reach the final node before the leaves, simply insert the new node as the leftmost child of that node. d. As with $\\text{B-TREE-DELETE}$, we'll want to ensure that the tree satisfies the properties of being a 2-3-4 tree after deletion, so we'll need to check that we're never deleting a leaf which only has a single sibling. This is handled in much the same way as in chapter 18. We can imagine that dummy keys are stored in all the internal nodes, and carry out the deletion process in exactly the same way as done in exercise 18.3-2, with the added requirement that we update the height stored in the root if we merge the root with its child nodes. e. $\\text{EXTRACT-MIN}$ simply locates the minimum as done in part (a), then deletes it as in part (d). f. This can be done by implementing the join operation, as in Problem 18-2 (b).","title":"19-4 2-3-4 heaps"},{"location":"Chap20/20.1/","text":"20.1-1 Modify the data structures in this section to support duplicate keys. To modify these structure to allow for multiple elements, instead of just storing a bit in each of the entries, we can store the head of a linked list representing how many elements of that value that are contained in the structure, with a $\\text{NIL}$ value to represent having no elements of that value. 20.1-2 Modify the data structures in this section to support keys that have associated satellite data. All operations will remain the same, except instead of the leaves of the tree being an array of integers, they will be an array of nodes, each of which stores $x.key$ in addition to whatever additional satellite data you wish. 20.1-3 Observe that, using the structures in this section, the way we find the successor and predecessor of a value $x$ does not depend on whether $x$ is in the set at the time. Show how to find the successor of $x$ in a binary search tree when $x$ is not stored in the tree. To find the successor of a given key $k$ from a binary tree, call the procedure $\\text{SUCC}(x, T.root)$. Note that this will return $\\text{NIL}$ if there is no entry in the tree with a larger key. 20.1-4 Suppose that instead of superimposing a tree of degree $\\sqrt u$, we were to superimpose a tree of degree $u^{1 / k}$, where $k > 1$ is a constant. What would be the height of such a tree, and how long would each of the operations take? The new tree would have height $k$. $\\text{INSERT}$ would take $O(k)$, $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, $\\text{PREDECESSOR}$, and $\\text{DELETE}$ would take $O(ku^{1 / k})$.","title":"20.1 Preliminary approaches"},{"location":"Chap20/20.1/#201-1","text":"Modify the data structures in this section to support duplicate keys. To modify these structure to allow for multiple elements, instead of just storing a bit in each of the entries, we can store the head of a linked list representing how many elements of that value that are contained in the structure, with a $\\text{NIL}$ value to represent having no elements of that value.","title":"20.1-1"},{"location":"Chap20/20.1/#201-2","text":"Modify the data structures in this section to support keys that have associated satellite data. All operations will remain the same, except instead of the leaves of the tree being an array of integers, they will be an array of nodes, each of which stores $x.key$ in addition to whatever additional satellite data you wish.","title":"20.1-2"},{"location":"Chap20/20.1/#201-3","text":"Observe that, using the structures in this section, the way we find the successor and predecessor of a value $x$ does not depend on whether $x$ is in the set at the time. Show how to find the successor of $x$ in a binary search tree when $x$ is not stored in the tree. To find the successor of a given key $k$ from a binary tree, call the procedure $\\text{SUCC}(x, T.root)$. Note that this will return $\\text{NIL}$ if there is no entry in the tree with a larger key.","title":"20.1-3"},{"location":"Chap20/20.1/#201-4","text":"Suppose that instead of superimposing a tree of degree $\\sqrt u$, we were to superimpose a tree of degree $u^{1 / k}$, where $k > 1$ is a constant. What would be the height of such a tree, and how long would each of the operations take? The new tree would have height $k$. $\\text{INSERT}$ would take $O(k)$, $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{SUCCESSOR}$, $\\text{PREDECESSOR}$, and $\\text{DELETE}$ would take $O(ku^{1 / k})$.","title":"20.1-4"},{"location":"Chap20/20.2/","text":"20.2-1 Write pseudocode for the procedures $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$. See the two algorithms, $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$. 20.2-2 Write pseudocode for $\\text{PROTO-vEB-DELETE}$. It should update the appropriate summary bit by scanning the related bits within the cluster. What is the worst-case running time of your procedure? PROTO - vEB - DELETE ( V , x ) if V . u == 2 V . A [ x ] = 0 else PROTO - vEB - DELETE ( V . cluster [ high ( x )], low ( x )) inCluster = false for i = 0 to sqrt ( u ) - 1 if PROTO - vEB - MEMBER ( V . cluster [ high ( x )], low ( i )) inCluster = true break if inCluster == false PROTO - vEB - DELETE ( V . summary , high ( x )) When we delete a key, we need to check membership of all keys of that cluster to know how to update the summary structure. There are $\\sqrt u$ of these, and each membership takes $O(\\lg\\lg u)$ time to check. With the recursive calls, recurrence for running time is $$T(u) = T(\\sqrt u) + O(\\sqrt u\\lg\\lg u).$$ We make the substitution $m = \\lg u$ and $S(m) = T(2^m)$. Then we apply the Master Theorem, using case 3, to solve the recurrence. Substituting back, we find that the runtime is $T(u) = O(\\sqrt u\\lg\\lg u)$. 20.2-3 Add the attribute $n$ to each $\\text{proto-vEB}$ structure, giving the number of elements currently in the set it represents, and write pseudocode for $\\text{PROTO-vEB-DELETE}$ that uses the attribute $n$ to decide when to reset summary bits to $0$. What is the worst-case running time of your procedure? What other procedures need to change because of the new attribute? Do these changes affect their running times? We would keep the same as before, but insert immediately after the else, a check of whether $n = 1$. If it doesn't continue as usual, but if it does, then we can just immediately set the summary bit to $0$, null out the pointer in the table, and be done immediately. This has the upside that it can sometimes save up to $\\lg\\lg u$. The procedure has the big downside that the number of elements that are in the set could be as high as $\\lg(\\lg u)$, in which case $\\lg u$ many bits are needed to store $n$. 20.2-4 Modify the $\\text{proto-vEB}$ structure to support duplicate keys. The array $A$ found in a proto van Emde Boas structure of size $2$ should now support integers, instead of just bits. All other pats of the structure will remain the same. The integer will store the number of duplicates at that position. The modifications to insert, delete, minimum, successor, etc will be minor. Only the base cases will need to be updated. 20.2-5 Modify the $\\text{proto-vEB}$ structure to support keys that have associated satellite data. The only modification necessary would be for the $u = 2$ trees. They would need to also include a length two array that had pointers to the corresponding satellite data which would be populated in case the corresponding entry in $A$ were $1$. 20.2-6 Write pseudocode for a procedure that creates a $\\text{proto-vEB}(u)$ structure. This algorithm recursively allocates proper space and appropriately initializes attributes for a proto van Emde Boas structure of size $u$. MAKE - PROTO - vEB ( u ) allocate a new vEB tree V V . u = u if u == 2 let A be an array of size 2 V . A [ 1 ] = V . A [ 0 ] = 0 else V . summary = MAKE - PROTO - vEB ( sqrt ( u )) for i = 0 to sqrt ( u ) - 1 V . cluster [ i ] = MAKE - PROTO - vEB ( sqrt ( u )) 20.2-7 Argue that if line 9 of $\\text{PROTO-vEB-MINIMUM}$ is executed, then the $\\text{proto-vEB}$ structure is empty. For line 9 to be executed, we would need that in the summary data, we also had a $\\text{NIL}$ returned. This could of either happened through line 9, or 6. Eventually though, it would need to happen in line 6, so, there must be some number of summarizations that happened of $V$ that caused us to get an empty $u = 2$ $\\text{vEB}$. However, a summarization has an entry of one if any of the corresponding entries in the data structure are one. This means that there are no entries in $V$, and so, we have that $V$ is empty. 20.2-8 Suppose that we designed a $\\text{proto-vEB}$ structure in which each cluster array had only $u^{1 / 4}$ elements. What would the running times of each operation be? There are $u^{3 / 4}$ clusters in each $\\text{proto-vEB}$. MEMBER/INSERT: $$T(u) = T(u^{1 / 4}) + O(1) = \\Theta(\\lg\\log_4 u) = \\Theta(\\lg\\lg u).$$ MINIMUM/MAXIMUM: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + O(1) = \\Theta(\\lg u).$$ SUCCESSOR/PREDECESSOR/DELETE: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + \\Theta(\\lg u^{1 / 4}) = \\Theta(\\lg u \\lg\\lg u).$$","title":"20.2 A recursive structure"},{"location":"Chap20/20.2/#202-1","text":"Write pseudocode for the procedures $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$. See the two algorithms, $\\text{PROTO-vEB-MAXIMUM}$ and $\\text{PROTO-vEB-PREDECESSOR}$.","title":"20.2-1"},{"location":"Chap20/20.2/#202-2","text":"Write pseudocode for $\\text{PROTO-vEB-DELETE}$. It should update the appropriate summary bit by scanning the related bits within the cluster. What is the worst-case running time of your procedure? PROTO - vEB - DELETE ( V , x ) if V . u == 2 V . A [ x ] = 0 else PROTO - vEB - DELETE ( V . cluster [ high ( x )], low ( x )) inCluster = false for i = 0 to sqrt ( u ) - 1 if PROTO - vEB - MEMBER ( V . cluster [ high ( x )], low ( i )) inCluster = true break if inCluster == false PROTO - vEB - DELETE ( V . summary , high ( x )) When we delete a key, we need to check membership of all keys of that cluster to know how to update the summary structure. There are $\\sqrt u$ of these, and each membership takes $O(\\lg\\lg u)$ time to check. With the recursive calls, recurrence for running time is $$T(u) = T(\\sqrt u) + O(\\sqrt u\\lg\\lg u).$$ We make the substitution $m = \\lg u$ and $S(m) = T(2^m)$. Then we apply the Master Theorem, using case 3, to solve the recurrence. Substituting back, we find that the runtime is $T(u) = O(\\sqrt u\\lg\\lg u)$.","title":"20.2-2"},{"location":"Chap20/20.2/#202-3","text":"Add the attribute $n$ to each $\\text{proto-vEB}$ structure, giving the number of elements currently in the set it represents, and write pseudocode for $\\text{PROTO-vEB-DELETE}$ that uses the attribute $n$ to decide when to reset summary bits to $0$. What is the worst-case running time of your procedure? What other procedures need to change because of the new attribute? Do these changes affect their running times? We would keep the same as before, but insert immediately after the else, a check of whether $n = 1$. If it doesn't continue as usual, but if it does, then we can just immediately set the summary bit to $0$, null out the pointer in the table, and be done immediately. This has the upside that it can sometimes save up to $\\lg\\lg u$. The procedure has the big downside that the number of elements that are in the set could be as high as $\\lg(\\lg u)$, in which case $\\lg u$ many bits are needed to store $n$.","title":"20.2-3"},{"location":"Chap20/20.2/#202-4","text":"Modify the $\\text{proto-vEB}$ structure to support duplicate keys. The array $A$ found in a proto van Emde Boas structure of size $2$ should now support integers, instead of just bits. All other pats of the structure will remain the same. The integer will store the number of duplicates at that position. The modifications to insert, delete, minimum, successor, etc will be minor. Only the base cases will need to be updated.","title":"20.2-4"},{"location":"Chap20/20.2/#202-5","text":"Modify the $\\text{proto-vEB}$ structure to support keys that have associated satellite data. The only modification necessary would be for the $u = 2$ trees. They would need to also include a length two array that had pointers to the corresponding satellite data which would be populated in case the corresponding entry in $A$ were $1$.","title":"20.2-5"},{"location":"Chap20/20.2/#202-6","text":"Write pseudocode for a procedure that creates a $\\text{proto-vEB}(u)$ structure. This algorithm recursively allocates proper space and appropriately initializes attributes for a proto van Emde Boas structure of size $u$. MAKE - PROTO - vEB ( u ) allocate a new vEB tree V V . u = u if u == 2 let A be an array of size 2 V . A [ 1 ] = V . A [ 0 ] = 0 else V . summary = MAKE - PROTO - vEB ( sqrt ( u )) for i = 0 to sqrt ( u ) - 1 V . cluster [ i ] = MAKE - PROTO - vEB ( sqrt ( u ))","title":"20.2-6"},{"location":"Chap20/20.2/#202-7","text":"Argue that if line 9 of $\\text{PROTO-vEB-MINIMUM}$ is executed, then the $\\text{proto-vEB}$ structure is empty. For line 9 to be executed, we would need that in the summary data, we also had a $\\text{NIL}$ returned. This could of either happened through line 9, or 6. Eventually though, it would need to happen in line 6, so, there must be some number of summarizations that happened of $V$ that caused us to get an empty $u = 2$ $\\text{vEB}$. However, a summarization has an entry of one if any of the corresponding entries in the data structure are one. This means that there are no entries in $V$, and so, we have that $V$ is empty.","title":"20.2-7"},{"location":"Chap20/20.2/#202-8","text":"Suppose that we designed a $\\text{proto-vEB}$ structure in which each cluster array had only $u^{1 / 4}$ elements. What would the running times of each operation be? There are $u^{3 / 4}$ clusters in each $\\text{proto-vEB}$. MEMBER/INSERT: $$T(u) = T(u^{1 / 4}) + O(1) = \\Theta(\\lg\\log_4 u) = \\Theta(\\lg\\lg u).$$ MINIMUM/MAXIMUM: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + O(1) = \\Theta(\\lg u).$$ SUCCESSOR/PREDECESSOR/DELETE: $$T(u) = T(u^{1 / 4}) + T(u^{3 / 4}) + \\Theta(\\lg u^{1 / 4}) = \\Theta(\\lg u \\lg\\lg u).$$","title":"20.2-8"},{"location":"Chap20/20.3/","text":"20.3-1 Modify vEB trees to support duplicate keys. To support duplicate keys, for each $u = 2$ vEB tree, instead of storing just a bit in each of the entries of its array, it should store an integer representing how many elements of that value the vEB contains. 20.3-2 Modify vEB trees to support keys that have associated satellite data. For any key which is a minimum on some vEB, we'll need to store its satellite data with the min value since the key doesn't appear in the subtree. The rest of the satellite data will be stored alongside the keys of the vEB trees of size $2$. Explicitly, for each non-summary vEB tree, store a pointer in addition to min. If min is $\\text{NIL}$, the pointer should also point to $\\text{NIL}$. Otherwise, the pointer should point to the satellite data associated with that minimum. In a size $2$ vEB tree, we'll have two additional pointers, which will each point to the minimum's and maximum's satellite data, or $\\text{NIL}$ if these don't exist. In the case where $\\min = \\max$, the pointers will point to the same data. 20.3-3 Write pseudocode for a procedure that creates an empty van Emde Boas tree. We define the procedure for any $u$ that is a power of $2$. If $u = 2$, then, just slap that fact together with an array of length $2$ that contains $0$ in both entries. If $u = 2k > 2$, then, we create an empty vEB tree called Summary with $u = 2^{\\lceil k / 2 \\rceil}$. We also make an array called cluster of length $2^{\\lceil k / 2 \\rceil}$ with each entry initialized to an empty vEB tree with $u = 2^{\\lfloor k / 2 \\rfloor}$. Lastly, we create a min and max element, both initialized to $\\text{NIL}$. 20.3-4 What happens if you call $\\text{VEB-TREE-INSERT}$ with an element that is already in the vEB tree? What happens if you call $\\text{VEB-TREE-DELETE}$ with an element that is not in the vEB tree? Explain why the procedures exhibit the behavior that they do. Show how to modify vEB trees and their operations so that we can check in constant time whether an element is present. Suppose that $x$ is already in $V$ and we call $\\text{INSERT}$. Then we can't satisfy lines 1, 3, 6, or 10, so we will enter the else case on line 9 every time until we reach the base case. If $x$ is already in the base-case tree, then we won't change anything. If $x$ is stored in a min attribute of a vEB tree that isn't base-case, however, we will insert a duplicate of it in some base-case tree. Now suppose we call $\\text{DELETE}$ when $x$ isn't in $V$ . If there is only a single element in $V$, lines 1 through 3 will delete it, regardless of what element it is. To enter the elseif of line 4, $x$ can't be equal to $0$ or $1$ and the vEB tree must be of size $2$. In this case, we delete the max element, regardless of what it is. Since the recursive call always puts us in this case, we always delete an element we shouldn't. To avoid these issue, keep and updated auxiliary array $A$ with $u$ elements. Set $A[i] = 0$ if $i$ is not in the tree, and $1$ if it is. Since we can perform constant time updates to this array, it won't affect the runtime of any of our operations. When inserting $x$, check first to be sure $A[x] = 0$. If it's not, simply return. If it is, set $A[x] = 1$ and proceed with insert as usual. When deleting $x$, check if $A[x] = 1$. If it isn't, simply return. If it is, set $A[x] = 0$ and proceed with delete as usual. 20.3-5 Suppose that instead of $\\sqrt[\\uparrow]u$ clusters, each with universe size $\\sqrt[\\downarrow]u$, we constructed vEB trees to have $u^{1 / k}$ clusters, each with universe size $u^{1 - 1 / k}$, where $k > 1$ is a constant. If we were to modify the operations appropriately, what would be their running times? For the purpose of analysis, assume that $u^{1 / k}$ and $u^{1 - 1 / k}$ are always integers. Similar to the analysis of $\\text{(20.4)}$, we will analyze $$T(u) \\le T(u^{1 - 1 / k}) + T(u^{1 / k}) + O(1).$$ This is a good choice for analysis because for many operations we first check the summary vEB tree, which will have size $u^{1 / k}$ (the second term). And then possible have to check a vEB tree somewhere in cluster, which will have size $u^{1 - 1/k}$ (the first term). We let $T(2^m) = S(m)$, so the equation becomes $$S(m) \\le S(m(1 - 1/k)) + S(m/k) + O(1).$$ If $k > 2$ the first term dominates, so by master theorem, we'll have that $S(m)$ is $O(\\lg m)$, this means that T will be $O(\\lg(\\lg u))$ just as in the original case where we took squareroots. 20.3-6 Creating a vEB tree with universe size $u$ requires $O(u)$ time. Suppose we wish to explicitly account for that time. What is the smallest number of operations $n$ for which the amortized time of each operation in a vEB tree is $O(\\lg\\lg u)$? Set $n = u / \\lg\\lg u$. Then performing $n$ operations takes $c(u + n\\lg\\lg u)$ time for some constant $c$. Using the aggregate amortized analysis, we divide by $n$ to see that the amortized cost of each operations is $c(\\lg\\lg u + \\lg\\lg u) = O(\\lg\\lg u)$ per operation. Thus we need $n \\ge u/ \\lg \\lg u$.","title":"20.3 The van Emde Boas tree"},{"location":"Chap20/20.3/#203-1","text":"Modify vEB trees to support duplicate keys. To support duplicate keys, for each $u = 2$ vEB tree, instead of storing just a bit in each of the entries of its array, it should store an integer representing how many elements of that value the vEB contains.","title":"20.3-1"},{"location":"Chap20/20.3/#203-2","text":"Modify vEB trees to support keys that have associated satellite data. For any key which is a minimum on some vEB, we'll need to store its satellite data with the min value since the key doesn't appear in the subtree. The rest of the satellite data will be stored alongside the keys of the vEB trees of size $2$. Explicitly, for each non-summary vEB tree, store a pointer in addition to min. If min is $\\text{NIL}$, the pointer should also point to $\\text{NIL}$. Otherwise, the pointer should point to the satellite data associated with that minimum. In a size $2$ vEB tree, we'll have two additional pointers, which will each point to the minimum's and maximum's satellite data, or $\\text{NIL}$ if these don't exist. In the case where $\\min = \\max$, the pointers will point to the same data.","title":"20.3-2"},{"location":"Chap20/20.3/#203-3","text":"Write pseudocode for a procedure that creates an empty van Emde Boas tree. We define the procedure for any $u$ that is a power of $2$. If $u = 2$, then, just slap that fact together with an array of length $2$ that contains $0$ in both entries. If $u = 2k > 2$, then, we create an empty vEB tree called Summary with $u = 2^{\\lceil k / 2 \\rceil}$. We also make an array called cluster of length $2^{\\lceil k / 2 \\rceil}$ with each entry initialized to an empty vEB tree with $u = 2^{\\lfloor k / 2 \\rfloor}$. Lastly, we create a min and max element, both initialized to $\\text{NIL}$.","title":"20.3-3"},{"location":"Chap20/20.3/#203-4","text":"What happens if you call $\\text{VEB-TREE-INSERT}$ with an element that is already in the vEB tree? What happens if you call $\\text{VEB-TREE-DELETE}$ with an element that is not in the vEB tree? Explain why the procedures exhibit the behavior that they do. Show how to modify vEB trees and their operations so that we can check in constant time whether an element is present. Suppose that $x$ is already in $V$ and we call $\\text{INSERT}$. Then we can't satisfy lines 1, 3, 6, or 10, so we will enter the else case on line 9 every time until we reach the base case. If $x$ is already in the base-case tree, then we won't change anything. If $x$ is stored in a min attribute of a vEB tree that isn't base-case, however, we will insert a duplicate of it in some base-case tree. Now suppose we call $\\text{DELETE}$ when $x$ isn't in $V$ . If there is only a single element in $V$, lines 1 through 3 will delete it, regardless of what element it is. To enter the elseif of line 4, $x$ can't be equal to $0$ or $1$ and the vEB tree must be of size $2$. In this case, we delete the max element, regardless of what it is. Since the recursive call always puts us in this case, we always delete an element we shouldn't. To avoid these issue, keep and updated auxiliary array $A$ with $u$ elements. Set $A[i] = 0$ if $i$ is not in the tree, and $1$ if it is. Since we can perform constant time updates to this array, it won't affect the runtime of any of our operations. When inserting $x$, check first to be sure $A[x] = 0$. If it's not, simply return. If it is, set $A[x] = 1$ and proceed with insert as usual. When deleting $x$, check if $A[x] = 1$. If it isn't, simply return. If it is, set $A[x] = 0$ and proceed with delete as usual.","title":"20.3-4"},{"location":"Chap20/20.3/#203-5","text":"Suppose that instead of $\\sqrt[\\uparrow]u$ clusters, each with universe size $\\sqrt[\\downarrow]u$, we constructed vEB trees to have $u^{1 / k}$ clusters, each with universe size $u^{1 - 1 / k}$, where $k > 1$ is a constant. If we were to modify the operations appropriately, what would be their running times? For the purpose of analysis, assume that $u^{1 / k}$ and $u^{1 - 1 / k}$ are always integers. Similar to the analysis of $\\text{(20.4)}$, we will analyze $$T(u) \\le T(u^{1 - 1 / k}) + T(u^{1 / k}) + O(1).$$ This is a good choice for analysis because for many operations we first check the summary vEB tree, which will have size $u^{1 / k}$ (the second term). And then possible have to check a vEB tree somewhere in cluster, which will have size $u^{1 - 1/k}$ (the first term). We let $T(2^m) = S(m)$, so the equation becomes $$S(m) \\le S(m(1 - 1/k)) + S(m/k) + O(1).$$ If $k > 2$ the first term dominates, so by master theorem, we'll have that $S(m)$ is $O(\\lg m)$, this means that T will be $O(\\lg(\\lg u))$ just as in the original case where we took squareroots.","title":"20.3-5"},{"location":"Chap20/20.3/#203-6","text":"Creating a vEB tree with universe size $u$ requires $O(u)$ time. Suppose we wish to explicitly account for that time. What is the smallest number of operations $n$ for which the amortized time of each operation in a vEB tree is $O(\\lg\\lg u)$? Set $n = u / \\lg\\lg u$. Then performing $n$ operations takes $c(u + n\\lg\\lg u)$ time for some constant $c$. Using the aggregate amortized analysis, we divide by $n$ to see that the amortized cost of each operations is $c(\\lg\\lg u + \\lg\\lg u) = O(\\lg\\lg u)$ per operation. Thus we need $n \\ge u/ \\lg \\lg u$.","title":"20.3-6"},{"location":"Chap20/Problems/20-1/","text":"This problem explores the space requirements for van Emde Boas trees and suggests a way to modify the data structure to make its space requirement depend on the number $n$ of elements actually stored in the tree, rather than on the universe size $u$. For simplicity, assume that $\\sqrt u$ is always an integer. a. Explain why the following recurrence characterizes the space requirement $P(u)$ of a van Emde Boas tree with universe size $u$: $$P(u) = (\\sqrt u + 1) P(\\sqrt u) + \\Theta(\\sqrt u). \\tag{20.5}$$ b. Prove that recurrence $\\text{(20.5)}$ has the solution $P(u) = O(u)$. In order to reduce the space requirements, let us define a reduced-space van Emde Boas tree , or RS-vEB tree , as a vEB tree $V$ but with the following changes: The attribute $V.cluster$, rather than being stored as a simple array of pointers to vEB trees with universe size $\\sqrt u$, is a hash table (see Chapter 11) stored as a dynamic table (see Section 17.4). Corresponding to the array version of $V.cluster$, the hash table stores pointers to RS-vEB trees with universe size $\\sqrt u$. To find the $i$th cluster, we look up the key $i$ in the hash table, so that we can find the $i$th cluster by a single search in the hash table. The hash table stores only pointers to nonempty clusters. A search in the hash table for an empty cluster returns $\\text{NIL}$, indicating that the cluster is empty. The attribute $V.summary$ is $\\text{NIL}$ if all clusters are empty. Otherwise, $V.summary$ points to an RS-vEB tree with universe size $\\sqrt u$. Because the hash table is implemented with a dynamic table, the space it requires is proportional to the number of nonempty clusters. When we need to insert an element into an empty RS-vEB tree, we create the RS-vEB tree by calling the following procedure, where the parameter u is the universe size of the RS-vEB tree: CREATE - NEW - RS - vEB - TREE ( u ) allocate a new vEB tree V V . u = u V . min = NIL V . max = NIL V . summary = NIL create V . cluster as an empty dynamic hash table return V c. Modify the $\\text{VEB-TREE-INSERT}$ procedure to produce pseudocode for the procedure $\\text{RS-VEB-TREE-INSERT}(V, x)$, which inserts $x$ into the RS-vEB tree $V$, calling $\\text{CREATE-NEW-RS-VEB-TREE}$ as appropriate. d. Modify the $\\text{VEB-TREE-SUCCESSOR}$ procedure to produce pseudocode for the procedure $\\text{RS-VEB-TREE-SUCCESSOR}(V, x)$, which returns the successor of $x$ in RS-vEB tree $V$, or $\\text{NIL}$ if $x$ has no successor in $V$. e. Prove that, under the assumption of simple uniform hashing, your $\\text{RS-VEBTREE-INSERT}$ and $\\text{RS-VEB-TREE-SUCCESSOR}$ procedures run in $O(\\lg\\lg u)$ expected time. f. Assuming that elements are never deleted from a vEB tree, prove that the space requirement for the RS-vEB tree structure is $O(n)$, where $n$ is the number of elements actually stored in the RS-vEB tree. g. RS-vEB trees have another advantage over vEB trees: they require less time to create. How long does it take to create an empty RS-vEB tree? a. Lets look at what has to be stored for a vEB tree. Each vEB tree contains one vEB tree of size $\\sqrt[+]u$ and $\\sqrt[+]u$ vEB trees of size $\\sqrt[1]u$. It also is storing three numbers each of order $O(u)$, so they need $\\Theta(\\lg(u))$ space each. Lastly, it needs to store $\\sqrt u$ many pointers to the cluster vEB trees. We'll combine these last two contributions which are $\\Theta(\\lg(u))$ and $\\Theta(\\sqrt u)$ respectively into a single term that is $\\Theta(\\sqrt u)$. This gets us the recurrence $$P(u) = P(\\sqrt[+]u) + \\sqrt[+]u P(\\sqrt[-]u) + \\Theta(\\sqrt u).$$ Then, we have that $u = 2^{2m}$ (which follows from the assumption that $\\sqrt u$ was an integer), this equation becomes $$ \\begin{aligned} P(u) & = (1 + 2^m)P(2^m) + \\Theta(\\sqrt u) \\\\ & = (1 + \\sqrt u)P(\\sqrt u) + \\Theta(\\sqrt u) \\end{aligned} $$ as desired. b. We recall from our solution to problem 3-6.e (it seems like so long ago now) that given a number $n$, a bound on the number of times that we need to take the squareroot of a number before it falls below $2$ is $\\lg\\lg n$. So, if we just unroll out recurrence, we get that $$P(u) \\le \\Big(\\prod_{i = 1}^{\\lg\\lg u}(u^{1 / 2^i} + 1) \\Big) P(2) + \\sum_{i = 1}^{\\lg\\lg u} \\Theta(u^{1 / 2^i})(u^{1 / 2i} + 1).$$ The first product has a highest power of $u$ corresponding to always multiplying the first terms of each binomial. The power in this term is equal to $\\sum_{i = 1}^{\\lg\\lg u}$ which is a partial sum of a geometric series whose sum is $1$. This means that the first term is $o(u)$. The order of the ith term in the summation appearing in the formula is $u^{2 / 2^i}$. In particular, for $i = 1$ is it $O(u)$, and for any $i > 1$, we have that $2 / 2^i < 1$, so those terms will be $o(u)$. Putting it all together, the largest term appearing is $O(u)$, and so, $P(u)$ is $O(u)$. c. For this problem we just use the version written for normal vEB trees, with minor modifications. That is, since there are entries in cluster that may not exist, and summary may of not yet been initialized, just before we try to access either, we check to see if it's initialized. If it isn't, we do so then. d. As in the previous problem, we just wait until just before either of the two things that may of not been allocated try to get used then allocate them if need be. e. Since the initialization performed only take constant time, those modifications don't ruin the the desired runtime bound for the original algorithms already had. So, our responses to parts (c) and (d) are $O(\\lg\\lg n)$. f. As mentioned in the errata, this part should instead be changed to $O(n\\lg n)$ space. When we are adding an element, we may have to add an entry to a dynamic hash table, which means that a constant amount of extra space would be needed. If we are adding an element to that table, we also have to add an element to the RS-vEB tree in the summary, but the entry that we add in the cluster will be a constant size RS-vEB tree. We can charge the cost of that addition to the summary table to the making the minimum element entry that we added in the cluster table. Since we are always making at least one element be added as a new min entry somewhere, this amortization will mean that it is only a constant amount of time in order to store the new entry. g. It only takes a constant amount of time to create an empty RS-vEB tree. This is immediate since the only dependence on $u$ in $\\text{CREATE-NEW-RSvEB-TREE}(u)$ is on line 2 when $V.u$ is initialized, but this only takes a constant amount of time. Since nothing else in the procedure depends on $u$, it must take a constant amount of time.","title":"20-1 Space requirements for van Emde Boas trees"},{"location":"Chap20/Problems/20-2/","text":"This problem investigates D. Willard's \"$y$-fast tries\" which, like van Emde Boas trees, perform each of the operations $\\text{MEMBER}$, $\\text{MINIMUM}$, $\\text{MAXIMUM}$, $\\text{PREDECESSOR}$, and $\\text{SUCCESSOR}$ on elements drawn from a universe with size $u$ in $O(\\lg\\lg u)$ worst-case time. The $\\text{INSERT}$ and $\\text{DELETE}$ operations take $O(\\lg\\lg u)$ amortized time. Like reduced-space van Emde Boas trees (see Problem 20-1), $y$-fast tries use only $O(n)$ space to store $n$ elements. The design of $y$-fast tries relies on perfect hashing (see Section 11.5). As a preliminary structure, suppose that we create a perfect hash table containing not only every element in the dynamic set, but every prefix of the binary representation of every element in the set. For example, if $u = 16$, so that $\\lg u = 4$, and $x = 13$ is in the set, then because the binary representation of $13$ is $1101$, the perfect hash table would contain the strings $1$, $11$, $110$, and $1101$. In addition to the hash table, we create a doubly linked list of the elements currently in the set, in increasing order. a. How much space does this structure require? b. Show how to perform the $\\text{MINIMUM}$ and $\\text{MAXIMUM}$ operations in $O(1)$ time; the $\\text{MEMBER}$, $\\text{PREDECESSOR}$, and $\\text{SUCCESSOR}$ operations in $O(\\lg\\lg u)$ time; and the $\\text{INSERT}$ and $\\text{DELETE}$ operations in $O(\\lg u)$ time. To reduce the space requirement to $O(n)$, we make the following changes to the data structure: We cluster the $n$ elements into $n / \\lg u$ groups of size $\\lg u$. (Assume for now that $\\lg u$ divides $n$.) The first group consists of the $\\lg u$ smallest elements in the set, the second group consists of the next $\\lg u$ smallest elements, and so on. We designate a \"representative\" value for each group. The representative of the $i$th group is at least as large as the largest element in the $i$th group, and it is smaller than every element of the $(i + 1)$st group. (The representative of the last group can be the maximum possible element $u - 1$.) Note that a representative might be a value not currently in the set. We store the $\\lg u$ elements of each group in a balanced binary search tree, such as a red-black tree. Each representative points to the balanced binary search tree for its group, and each balanced binary search tree points to its group's representative. The perfect hash table stores only the representatives, which are also stored in a doubly linked list in increasing order. We call this structure a $y$-fast trie . c. Show that a $y$-fast trie requires only $O(n)$ space to store $n$ elements. d. Show how to perform the $\\text{MINIMUM}$ and $\\text{MAXIMUM}$ operations in $O(\\lg\\lg u)$ time with a $y$-fast trie. e. Show how to perform the $\\text{MEMBER}$ operation in $O(\\lg\\lg u)$ time. f. Show how to perform the $\\text{PREDECESSOR}$ and $\\text{SUCCESSOR}$ operations in $O(\\lg\\lg u)$ time. g. Explain why the $\\text{INSERT}$ and $\\text{DELETE}$ operations take $\\Omega(\\lg\\lg u)$ time. h. Show how to relax the requirement that each group in a $y$-fast trie has exactly $\\lg u$ elements to allow $\\text{INSERT}$ and $\\text{DELETE}$ to run in $O(\\lg\\lg u)$ amortized time without affecting the asymptotic running times of the other operations. a. By 11.5, the perfect hash table uses $O(m)$ space to store m elements. In a universe of size $u$, each element contributes $\\lg u$ entries to the hash table, so the requirement is $O(n\\lg u)$. Since the linked list requires $O(n)$, the total space requirement is $O(n\\lg u)$. b. $\\text{MINIMUM}$ and $\\text{MAXIMUM}$ are easy. We just examine the first and last elements of the associated doubly linked list. $\\text{MEMBER}$ can actually be performed in $O(1)$, since we are simply checking membership in a perfect hash table. $\\text{PREDECESSOR}$ and $\\text{SUCCESSOR}$ are a bit more complicated. Assume that we have a binary tree in which we store all the elements and their prefixes. When we query the hash table for an element, we get a pointer to that element's location in the binary search tree, if the element is in the tree, and $\\text{NIL}$ otherwise. Moreover, assume that every leaf node comes with a pointer to its position in the doubly linked list. Let $x$ be the number whose successor we seek. Begin by performing a binary search of the prefixes in the hash table to find the longest hashed prefix $y$ which matches a prefix of $x$. This takes $O(\\lg\\lg u)$ since we can check if any prefix is in the hash table in $O(1)$. Observe that $y$ can have at most one child in the BST, because if it had both children then one of these would share a longer prefix with $x$. If the left child is missing, have the left child pointer point to the largest labeled leaf node in the BST which is less than $y$. If the right child is missing, use its pointer to point to the successor of $y$. If $y$ is a leaf node then $y = x$, so we simply follow the pointer to $x$ in the doubly linked list, in $O(1)$, and its successor is the next element on the list. If $y$ is not a leaf node, we follow its predecessor or successor node, depending on which we need. This gives us $O(1)$ access to the proper element, so the total runtime is $O(\\lg\\lg u)$. $\\text{INSERT}$ and $\\text{DELETE}$ must take $O(\\lg u)$ since we need to insert one entry into the hash table for each of their bits and update the pointers. c. The doubly linked list has less than $n$ elements, while the binary search trees contains $n$ nodes, thus a $y$-fast trie requires $O(n)$ space. d. $\\text{MINIMUM}$: Find the minimum representative in the doubly linked list in $\\Theta(1)$, then find the minimum element in the binary search tree in $O(\\lg\\lg u)$. e. Find the smallest representative greater than $k$ with binary searching in $\\Theta(\\lg\\lg u)$, find the element in the binary search tree in $O(\\lg\\lg u)$. f. If we can find the largest representative greater than or equal to $x$, we can determine which binary tree contains the predecessor or successor of $x$. To do this, just call $\\text{PREDECESSOR}$ or $\\text{SUCCESSOR}$ on $x$ to locate the appropriate tree in $O(\\lg\\lg u)$. Since the tree has height $\\lg u$, we can find the predecessor or successor in $O(\\lg\\lg u)$. g. Same as e , we need to find the cluster in $\\Theta(\\lg\\lg u)$, then the operations in the binary search tree takes $O(\\lg\\lg u)$. h. We can relax the requirements and only impose the condition that each group has at least $\\frac{1}{2}\\lg u$ elements and at most $2\\lg u$ elements. If a red-black tree is too big, we split it in half at the median. If a red-black tree is too small, we merge it with a neighboring tree. If this causes the merged tree to become too large, we split it at the median. If a tree splits, we create a new representative. If two trees merge, we delete the lost representative. Any split or merge takes $O(\\lg u)$ since we have to insert or delete an element in the data structure storing our representatives, which by part (b) takes $O(\\lg u)$. However, we only split a tree after at least $\\lg u$ insertions, since the size of one of the red-black trees needs to increase from $\\lg u$ to $2\\lg u$ and we only merge two trees after at least $(1 / 2)\\lg u$ deletions, because the size of the merging tree needs to have decreased from $\\lg u$ to $(1 / 2)\\lg u$. Thus, the amortized cost of the merges, splits, and updates to representatives is $O(1)$ per insertion or deletion, so the amortized cost is $O(\\lg\\lg u)$ as desired.","title":"20-2 $y$-fast tries"},{"location":"Chap21/21.1/","text":"21.1-1 Suppose that $\\text{CONNECTED-COMPONENTS}$ is run on the undirected graph $G = (V, E)$, where $V = \\{a, b, c, d, e, f, g, h, i, j, k\\}$ and the edges of $E$ are processed in the order $(d, i)$, $(f, k)$, $(g, i)$, $(b, g)$, $(a, h)$, $(i, j)$, $(d, k)$, $(b, j)$, $(d, f)$, $(g, j)$, $(a, e)$. List the vertices in each connected component after each iteration of lines 3\u20135. $$ \\begin{array}{c|lllllllllll} \\text{Edge processed} & \\\\ \\hline initial & \\{a\\} & \\{b\\} & \\{c\\} & \\{d\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & \\{i\\} & \\{j\\} & \\{k\\} \\\\ (d, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\{k\\} \\\\ (f, k) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f, k\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\\\ (g, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i, g\\} & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (b, g) & \\{a\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (a, h) & \\{a, h\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & \\{j\\} & \\\\ (i, j) & \\{a, h\\} & \\{b, d, i, g, j\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & & \\\\ (d, k) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (b, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (d, f) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (g, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (a, e) & \\{a, h, e\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & & & & & & & \\end{array} $$ So, the connected components that we are left with are $\\{a, h, e\\}$, $\\{b, d, i, g, j, f, k\\}$, and $\\{c\\}$. 21.1-2 Show that after all edges are processed by $\\text{CONNECTED-COMPONENTS}$, two vertices are in the same connected component if and only if they are in the same set. First suppose that two vertices are in the same connected component. Then there exists a path of edges connecting them. If two vertices are connected by a single edge, then they are put into the same set when that edge is processed. At some point during the algorithm every edge of the path will be processed, so all vertices on the path will be in the same set, including the endpoints. Now suppose two vertices $u$ and $v$ wind up in the same set. Since every vertex starts off in its own set, some sequence of edges in $G$ must have resulted in eventually combining the sets containing $u$ and $v$. From among these, there must be a path of edges from $u$ to $v$, implying that $u$ and $v$ are in the same connected component. 21.1-3 During the execution of $\\text{CONNECTED-COMPONENTS}$ on an undirected graph $G = (V, E)$ with $k$ connected components, how many times is $\\text{FIND-SET}$ called? How many times is $\\text{UNION}$ called? Express your answers in terms of $|V|$, $|E|$, and $k$. Find set is called twice on line 4, this is run once per edge in the graph, so, we have that find set is run $2|E|$ times. Since we start with $|V|$ sets, at the end only have $k$, and each call to $\\text{UNION}$ reduces the number of sets by one, we have that we have to made $|V| - k$ calls to $\\text{UNION}$.","title":"21.1 Disjoint-set operations"},{"location":"Chap21/21.1/#211-1","text":"Suppose that $\\text{CONNECTED-COMPONENTS}$ is run on the undirected graph $G = (V, E)$, where $V = \\{a, b, c, d, e, f, g, h, i, j, k\\}$ and the edges of $E$ are processed in the order $(d, i)$, $(f, k)$, $(g, i)$, $(b, g)$, $(a, h)$, $(i, j)$, $(d, k)$, $(b, j)$, $(d, f)$, $(g, j)$, $(a, e)$. List the vertices in each connected component after each iteration of lines 3\u20135. $$ \\begin{array}{c|lllllllllll} \\text{Edge processed} & \\\\ \\hline initial & \\{a\\} & \\{b\\} & \\{c\\} & \\{d\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & \\{i\\} & \\{j\\} & \\{k\\} \\\\ (d, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\{k\\} \\\\ (f, k) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i\\} & \\{e\\} & \\{f, k\\} & \\{g\\} & \\{h\\} & & \\{j\\} & \\\\ (g, i) & \\{a\\} & \\{b\\} & \\{c\\} & \\{d, i, g\\} & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (b, g) & \\{a\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & \\{h\\} & & \\{j\\} & \\\\ (a, h) & \\{a, h\\} & \\{b, d, i, g\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & \\{j\\} & \\\\ (i, j) & \\{a, h\\} & \\{b, d, i, g, j\\} & \\{c\\} & & \\{e\\} & \\{f, k\\} & & & & & \\\\ (d, k) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (b, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (d, f) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (g, j) & \\{a, h\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & \\{e\\} & & & & & & \\\\ (a, e) & \\{a, h, e\\} & \\{b, d, i, g, j, f, k\\} & \\{c\\} & & & & & & & & \\end{array} $$ So, the connected components that we are left with are $\\{a, h, e\\}$, $\\{b, d, i, g, j, f, k\\}$, and $\\{c\\}$.","title":"21.1-1"},{"location":"Chap21/21.1/#211-2","text":"Show that after all edges are processed by $\\text{CONNECTED-COMPONENTS}$, two vertices are in the same connected component if and only if they are in the same set. First suppose that two vertices are in the same connected component. Then there exists a path of edges connecting them. If two vertices are connected by a single edge, then they are put into the same set when that edge is processed. At some point during the algorithm every edge of the path will be processed, so all vertices on the path will be in the same set, including the endpoints. Now suppose two vertices $u$ and $v$ wind up in the same set. Since every vertex starts off in its own set, some sequence of edges in $G$ must have resulted in eventually combining the sets containing $u$ and $v$. From among these, there must be a path of edges from $u$ to $v$, implying that $u$ and $v$ are in the same connected component.","title":"21.1-2"},{"location":"Chap21/21.1/#211-3","text":"During the execution of $\\text{CONNECTED-COMPONENTS}$ on an undirected graph $G = (V, E)$ with $k$ connected components, how many times is $\\text{FIND-SET}$ called? How many times is $\\text{UNION}$ called? Express your answers in terms of $|V|$, $|E|$, and $k$. Find set is called twice on line 4, this is run once per edge in the graph, so, we have that find set is run $2|E|$ times. Since we start with $|V|$ sets, at the end only have $k$, and each call to $\\text{UNION}$ reduces the number of sets by one, we have that we have to made $|V| - k$ calls to $\\text{UNION}$.","title":"21.1-3"},{"location":"Chap21/21.2/","text":"21.2-1 Write pseudocode for $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. Make sure to specify the attributes that you assume for set objects and list objects. The three algorithms follow the english description and are provided here. There are alternate versions using the weighted union heuristic, suffixed with $\\text{WU}$. MAKE - SET ( x ) let o be an object with three fields , next , value , and set let L be a linked list object with head = tail = o o . next = NIL o . set = L o . value = x return L FIND - SET ( x ) return o . set . head . value UNION ( x , y ) L1 = x . set L2 = y . set L1 . tail . next = L2 . head z = L2 . head while z . next != NIL z . set = L1 z = z . next L1 . tail = L2 . tail return L1 21.2-2 Show the data structure that results and the answers returned by the $\\text{FIND-SET}$ operations in the following program. Use the linked-list representation with the weighted-union heuristic. for i = 1 to 16 MAKE - SET ( x [ i ]) for i = 1 to 15 by 2 UNION ( x [ i ], x [ i + 1 ]) for i = 1 to 13 by 4 UNION ( x [ i ], x [ i + 2 ]) UNION ( x [ 1 ], x [ 5 ]) UNION ( x [ 11 ], x [ 13 ]) UNION ( x [ 1 ], x [ 10 ]) FIND - SET ( x [ 2 ]) FIND - SET ( x [ 9 ]) Assume that if the sets containing $x_i$ and $x_j$ have the same size, then the operation $\\text{UNION}(x_i, x_j)$ appends $x_j$'s list onto $x_i$'s list. Originally we have $16$ sets, each containing $x_i$. In the following, we'll replace $x_i$ by $i$. After the for loop in line 3 we have: $$\\{1,2\\}, \\{3, 4\\}, \\{5, 6\\}, \\{7, 8\\}, \\{9, 10\\}, \\{11, 12\\}, \\{13, 14\\}, \\{15, 16\\}.$$ After the for loop on line 5 we have $$\\{1, 2, 3, 4\\}, \\{5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 7 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 8 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12, 13, 14, 15, 16\\}.$$ Line 9 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\\}.$$ $\\text{FIND-SET}(x_2)$ and $\\text{FIND-SET}(x_9)$ each return pointers to $x_1$. MAKE - SET - WU ( x ) L = MAKE - SET ( x ) L . size = 1 return L UNION - WU ( x , y ) L1 = x . set L2 = y . set if L1 . size \u2265 L2 . size L = UNION ( x , y ) else L = UNION ( y , x ) L . size = L1 . size + L2 . size return L 21.2-3 Adapt the aggregate proof of Theorem 21.1 to obtain amortized time bounds of $O(1)$ for $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ and $O(\\lg n)$ for $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. During the proof of theorem 21.1, we concluded that the time for the $n$ $\\text{UNION}$ operations to run was at most $O(n \\lg n)$. This means that each of them took an amortized time of at most $O(\\lg n)$. Also, since there is only a constant actual amount of work in performing $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ operations, and none of that ease is used to offset costs of $\\text{UNION}$ operations, they both have $O(1)$ runtime. 21.2-4 Give a tight asymptotic bound on the running time of the sequence of operations in Figure 21.3 assuming the linked-list representation and the weighted-union heuristic. We call $\\text{MAKE-SET}$ $n$ times, which contributes $\\Theta(n)$. In each union, the smaller set is of size $1$, so each of these takes $\\Theta(1)$ time. Since we union $n - 1$ times, the runtime is $\\Theta(n)$. 21.2-5 Professor Gompers suspects that it might be possible to keep just one pointer in each set object, rather than two ($head$ and $tail$), while keeping the number of pointers in each list element at two. Show that the professor's suspicion is well founded by describing how to represent each set by a linked list such that each operation has the same running time as the operations described in this section. Describe also how the operations work. Your scheme should allow for the weighted-union heuristic, with the same effect as described in this section. ($\\textit{Hint:}$ Use the tail of a linked list as its set's representative.) For each member of the set, we will make its first field which used to point back to the set object point instead to the last element of the linked list. Then, given any set, we can find its last element by going ot the head and following the pointer that that object maintains to the last element of the linked list. This only requires following exactly two pointers, so it takes a constant amount of time. Some care must be taken when unioning these modified sets. Since the set representative is the last element in the set, when we combine two linked lists, we place the smaller of the two sets before the larger, since we need to update their set representative pointers, unlike the original situation, where we update the representative of the objects that are placed on to the end of the linked list. 21.2-6 Suggest a simple change to the $\\text{UNION}$ procedure for the linked-list representation that removes the need to keep the $tail$ pointer to the last object in each list. Whether or not the weighted-union heuristic is used, your change should not change the asymptotic running time of the $\\text{UNION}$ procedure. ($\\textit{Hint:}$ Rather than appending one list to another, splice them together.) Instead of appending the second list to the end of the first, we can imagine splicing it into the first list, in between the head and the elements. Store a pointer to the first element in $S_1$. Then for each element $x$ in $S_2$, set $x.head = S_1.head$. When the last element of $S_2$ is reached, set its next pointer to the first element of $S_1$. If we always let $S_2$ play the role of the smaller set, this works well with the weighted-union heuristic and don't affect the asymptotic running time of $\\text{UNION}$.","title":"21.2 Linked-list representation of disjoint sets"},{"location":"Chap21/21.2/#212-1","text":"Write pseudocode for $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. Make sure to specify the attributes that you assume for set objects and list objects. The three algorithms follow the english description and are provided here. There are alternate versions using the weighted union heuristic, suffixed with $\\text{WU}$. MAKE - SET ( x ) let o be an object with three fields , next , value , and set let L be a linked list object with head = tail = o o . next = NIL o . set = L o . value = x return L FIND - SET ( x ) return o . set . head . value UNION ( x , y ) L1 = x . set L2 = y . set L1 . tail . next = L2 . head z = L2 . head while z . next != NIL z . set = L1 z = z . next L1 . tail = L2 . tail return L1","title":"21.2-1"},{"location":"Chap21/21.2/#212-2","text":"Show the data structure that results and the answers returned by the $\\text{FIND-SET}$ operations in the following program. Use the linked-list representation with the weighted-union heuristic. for i = 1 to 16 MAKE - SET ( x [ i ]) for i = 1 to 15 by 2 UNION ( x [ i ], x [ i + 1 ]) for i = 1 to 13 by 4 UNION ( x [ i ], x [ i + 2 ]) UNION ( x [ 1 ], x [ 5 ]) UNION ( x [ 11 ], x [ 13 ]) UNION ( x [ 1 ], x [ 10 ]) FIND - SET ( x [ 2 ]) FIND - SET ( x [ 9 ]) Assume that if the sets containing $x_i$ and $x_j$ have the same size, then the operation $\\text{UNION}(x_i, x_j)$ appends $x_j$'s list onto $x_i$'s list. Originally we have $16$ sets, each containing $x_i$. In the following, we'll replace $x_i$ by $i$. After the for loop in line 3 we have: $$\\{1,2\\}, \\{3, 4\\}, \\{5, 6\\}, \\{7, 8\\}, \\{9, 10\\}, \\{11, 12\\}, \\{13, 14\\}, \\{15, 16\\}.$$ After the for loop on line 5 we have $$\\{1, 2, 3, 4\\}, \\{5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 7 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12\\}, \\{13, 14, 15, 16\\}.$$ Line 8 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8\\}, \\{9, 10, 11, 12, 13, 14, 15, 16\\}.$$ Line 9 results in: $$\\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\\}.$$ $\\text{FIND-SET}(x_2)$ and $\\text{FIND-SET}(x_9)$ each return pointers to $x_1$. MAKE - SET - WU ( x ) L = MAKE - SET ( x ) L . size = 1 return L UNION - WU ( x , y ) L1 = x . set L2 = y . set if L1 . size \u2265 L2 . size L = UNION ( x , y ) else L = UNION ( y , x ) L . size = L1 . size + L2 . size return L","title":"21.2-2"},{"location":"Chap21/21.2/#212-3","text":"Adapt the aggregate proof of Theorem 21.1 to obtain amortized time bounds of $O(1)$ for $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ and $O(\\lg n)$ for $\\text{UNION}$ using the linked-list representation and the weighted-union heuristic. During the proof of theorem 21.1, we concluded that the time for the $n$ $\\text{UNION}$ operations to run was at most $O(n \\lg n)$. This means that each of them took an amortized time of at most $O(\\lg n)$. Also, since there is only a constant actual amount of work in performing $\\text{MAKE-SET}$ and $\\text{FIND-SET}$ operations, and none of that ease is used to offset costs of $\\text{UNION}$ operations, they both have $O(1)$ runtime.","title":"21.2-3"},{"location":"Chap21/21.2/#212-4","text":"Give a tight asymptotic bound on the running time of the sequence of operations in Figure 21.3 assuming the linked-list representation and the weighted-union heuristic. We call $\\text{MAKE-SET}$ $n$ times, which contributes $\\Theta(n)$. In each union, the smaller set is of size $1$, so each of these takes $\\Theta(1)$ time. Since we union $n - 1$ times, the runtime is $\\Theta(n)$.","title":"21.2-4"},{"location":"Chap21/21.2/#212-5","text":"Professor Gompers suspects that it might be possible to keep just one pointer in each set object, rather than two ($head$ and $tail$), while keeping the number of pointers in each list element at two. Show that the professor's suspicion is well founded by describing how to represent each set by a linked list such that each operation has the same running time as the operations described in this section. Describe also how the operations work. Your scheme should allow for the weighted-union heuristic, with the same effect as described in this section. ($\\textit{Hint:}$ Use the tail of a linked list as its set's representative.) For each member of the set, we will make its first field which used to point back to the set object point instead to the last element of the linked list. Then, given any set, we can find its last element by going ot the head and following the pointer that that object maintains to the last element of the linked list. This only requires following exactly two pointers, so it takes a constant amount of time. Some care must be taken when unioning these modified sets. Since the set representative is the last element in the set, when we combine two linked lists, we place the smaller of the two sets before the larger, since we need to update their set representative pointers, unlike the original situation, where we update the representative of the objects that are placed on to the end of the linked list.","title":"21.2-5"},{"location":"Chap21/21.2/#212-6","text":"Suggest a simple change to the $\\text{UNION}$ procedure for the linked-list representation that removes the need to keep the $tail$ pointer to the last object in each list. Whether or not the weighted-union heuristic is used, your change should not change the asymptotic running time of the $\\text{UNION}$ procedure. ($\\textit{Hint:}$ Rather than appending one list to another, splice them together.) Instead of appending the second list to the end of the first, we can imagine splicing it into the first list, in between the head and the elements. Store a pointer to the first element in $S_1$. Then for each element $x$ in $S_2$, set $x.head = S_1.head$. When the last element of $S_2$ is reached, set its next pointer to the first element of $S_1$. If we always let $S_2$ play the role of the smaller set, this works well with the weighted-union heuristic and don't affect the asymptotic running time of $\\text{UNION}$.","title":"21.2-6"},{"location":"Chap21/21.3/","text":"21.3-1 Redo Exercise 21.2-2 using a disjoint-set forest with union by rank and path compression. 1 / / \\ \\ 2 3 5 9 | / \\ / \\ \\ 4 6 7 10 11 13 | | / \\ 8 12 14 15 | 16 21.3-2 Write a nonrecursive version of $\\text{FIND-SET}$ with path compression. To implement $\\text{FIND-SET}$ nonrecursively, let $x$ be the element we call the function on. Create a linked list $A$ which contains a pointer to $x$. Each time we most one element up the tree, insert a pointer to that element into $A$. Once the root $r$ has been found, use the linked list to find each node on the path from the root to $x$ and update its parent to $r$. 21.3-3 Give a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, that takes $\\Omega(m\\lg n)$ time when we use union by rank only. Suppose that $n' = 2k$ is the smallest power of two less than $n$. To see that this sequences of operations does take the required amount of time, we'll first note that after each iteration of the for loop indexed by $j$, we have that the elements $x_1, \\dots, x_{n'}$ are in trees of depth $i$. So, after we finish the outer for loop, we have that $x_1, \\dots, x_{n'}$ all lie in the same set, but are represented by a tree of depth $k \\in \\Omega(\\lg n)$. Then, since we repeatedly call $\\text{FIND-SET}$ on an item that is $\\lg n$ away from its set representative, we have that each one takes time $\\lg n$. So, the last for loop alltogther takes time $\\Omega(m \\lg n)$. for i = 1 to n MAKE - SET ( x [ i ]) for i = 1 to k for j = 1. . n ' - 2 ^ { i = 1 } by 2 ^ i UNION ( x [ i ], x [ i + 2 ^ { j - 1 }]) for i = 1 to m FIND - SET ( x [ 1 ]) 21.3-4 Suppose that we wish to add the operation $\\text{PRINT-SET}(x)$, which is given a node $x$ and prints all the members of $x$'s set, in any order. Show how we can add just a single attribute to each node in a disjoint-set forest so that $\\text{PRINT-SET}(x)$ takes time linear in the number of members of $x$'s set and the asymptotic running times of the other operations are unchanged. Assume that we can print each member of the set in $O(1)$ time. In addition to each tree, we'll store a linked list (whose set object contains a single tail pointer) with which keeps track of all the names of elements in the tree. The only additional information we'll store in each node is a pointer $x.l$ to that element's position in the list. When we call $\\text{MAKE-SET}(x)$, we'll also create a new linked list, insert the label of $x$ into the list, and set $x.l$ to a pointer to that label. This is all done in $O(1)$. $\\text{FIND-SET}$ will remain unchanged. $\\text{UNION}(x, y)$ will work as usual, with the additional requirement that we union the linked lists of $x$ and $y$, since we don't need to update pointers to the head, we can link up the lists in constant time, thus preserving the runtime of $\\text{UNION}$. Finally, $\\text{PRINT-SET}(x)$ works as follows: first, set $s = \\text{FIND-SET}(x)$. Then print the elements in the linked list, starting with the element pointed to by $x$. (This will be the first element in the list). Since the list contains the same number of elements as the set and printing takes $O(1)$, this operation takes linear time in the number of set members. 21.3-5 $\\star$ Show that any sequence of $m$ $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{LINK}$ operations, where all the $\\text{LINK}$ operations appear before any of the $\\text{FIND-SET}$ operations, takes only $O(m)$ time if we use both path compression and union by rank. What happens in the same situation if we use only the path-compression heuristic? Clearly each $\\text{MAKE-SET}$ and $\\text{LINK}$ operation only takes time $O(1)$, so, supposing that $n$ is the number of $\\text{FIND-SET}$ operations occuring after the making and linking, we need to show that all the $\\text{FIND-SET}$ operations only take time $O(n)$. To do this, we will ammortize some of the cost of the $\\text{FIND-SET}$ operations into the cost of the $\\text{MAKE-SET}$ operations. Imagine paying some constant amount extra for each $\\text{MAKE-SET}$ operation. Then, when doing a $\\text{FIND-SET}(x)$ operation, we have three possibilities: First, we could have that $x$ is the representative of its own set. In this case, it clearly only takes constant time to run. Second, we could have that the path from $x$ to its set's representative is already compressed, so it only takes a single step to find the set representative. In this case also, the time required is constant. Third, we could have that $x$ is not the representative and it's path has not been compressed. Then, suppose that there are k nodes between $x$ and its representative. The time of this $\\text{FIND-SET}$ operation is $O(k)$, but it also ends up compressing the paths of $k$ nodes, so we use that extra amount that we paid during the $\\text{MAKE-SET}$ operations for these $k$ nodes whose paths were compressed. Any subsequent call to find set for these nodes will take only a constant amount of time, so we would never try to use the work that amortization amount twice for a given node.","title":"21.3 Disjoint-set forests"},{"location":"Chap21/21.3/#213-1","text":"Redo Exercise 21.2-2 using a disjoint-set forest with union by rank and path compression. 1 / / \\ \\ 2 3 5 9 | / \\ / \\ \\ 4 6 7 10 11 13 | | / \\ 8 12 14 15 | 16","title":"21.3-1"},{"location":"Chap21/21.3/#213-2","text":"Write a nonrecursive version of $\\text{FIND-SET}$ with path compression. To implement $\\text{FIND-SET}$ nonrecursively, let $x$ be the element we call the function on. Create a linked list $A$ which contains a pointer to $x$. Each time we most one element up the tree, insert a pointer to that element into $A$. Once the root $r$ has been found, use the linked list to find each node on the path from the root to $x$ and update its parent to $r$.","title":"21.3-2"},{"location":"Chap21/21.3/#213-3","text":"Give a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, that takes $\\Omega(m\\lg n)$ time when we use union by rank only. Suppose that $n' = 2k$ is the smallest power of two less than $n$. To see that this sequences of operations does take the required amount of time, we'll first note that after each iteration of the for loop indexed by $j$, we have that the elements $x_1, \\dots, x_{n'}$ are in trees of depth $i$. So, after we finish the outer for loop, we have that $x_1, \\dots, x_{n'}$ all lie in the same set, but are represented by a tree of depth $k \\in \\Omega(\\lg n)$. Then, since we repeatedly call $\\text{FIND-SET}$ on an item that is $\\lg n$ away from its set representative, we have that each one takes time $\\lg n$. So, the last for loop alltogther takes time $\\Omega(m \\lg n)$. for i = 1 to n MAKE - SET ( x [ i ]) for i = 1 to k for j = 1. . n ' - 2 ^ { i = 1 } by 2 ^ i UNION ( x [ i ], x [ i + 2 ^ { j - 1 }]) for i = 1 to m FIND - SET ( x [ 1 ])","title":"21.3-3"},{"location":"Chap21/21.3/#213-4","text":"Suppose that we wish to add the operation $\\text{PRINT-SET}(x)$, which is given a node $x$ and prints all the members of $x$'s set, in any order. Show how we can add just a single attribute to each node in a disjoint-set forest so that $\\text{PRINT-SET}(x)$ takes time linear in the number of members of $x$'s set and the asymptotic running times of the other operations are unchanged. Assume that we can print each member of the set in $O(1)$ time. In addition to each tree, we'll store a linked list (whose set object contains a single tail pointer) with which keeps track of all the names of elements in the tree. The only additional information we'll store in each node is a pointer $x.l$ to that element's position in the list. When we call $\\text{MAKE-SET}(x)$, we'll also create a new linked list, insert the label of $x$ into the list, and set $x.l$ to a pointer to that label. This is all done in $O(1)$. $\\text{FIND-SET}$ will remain unchanged. $\\text{UNION}(x, y)$ will work as usual, with the additional requirement that we union the linked lists of $x$ and $y$, since we don't need to update pointers to the head, we can link up the lists in constant time, thus preserving the runtime of $\\text{UNION}$. Finally, $\\text{PRINT-SET}(x)$ works as follows: first, set $s = \\text{FIND-SET}(x)$. Then print the elements in the linked list, starting with the element pointed to by $x$. (This will be the first element in the list). Since the list contains the same number of elements as the set and printing takes $O(1)$, this operation takes linear time in the number of set members.","title":"21.3-4"},{"location":"Chap21/21.3/#213-5-star","text":"Show that any sequence of $m$ $\\text{MAKE-SET}$, $\\text{FIND-SET}$, and $\\text{LINK}$ operations, where all the $\\text{LINK}$ operations appear before any of the $\\text{FIND-SET}$ operations, takes only $O(m)$ time if we use both path compression and union by rank. What happens in the same situation if we use only the path-compression heuristic? Clearly each $\\text{MAKE-SET}$ and $\\text{LINK}$ operation only takes time $O(1)$, so, supposing that $n$ is the number of $\\text{FIND-SET}$ operations occuring after the making and linking, we need to show that all the $\\text{FIND-SET}$ operations only take time $O(n)$. To do this, we will ammortize some of the cost of the $\\text{FIND-SET}$ operations into the cost of the $\\text{MAKE-SET}$ operations. Imagine paying some constant amount extra for each $\\text{MAKE-SET}$ operation. Then, when doing a $\\text{FIND-SET}(x)$ operation, we have three possibilities: First, we could have that $x$ is the representative of its own set. In this case, it clearly only takes constant time to run. Second, we could have that the path from $x$ to its set's representative is already compressed, so it only takes a single step to find the set representative. In this case also, the time required is constant. Third, we could have that $x$ is not the representative and it's path has not been compressed. Then, suppose that there are k nodes between $x$ and its representative. The time of this $\\text{FIND-SET}$ operation is $O(k)$, but it also ends up compressing the paths of $k$ nodes, so we use that extra amount that we paid during the $\\text{MAKE-SET}$ operations for these $k$ nodes whose paths were compressed. Any subsequent call to find set for these nodes will take only a constant amount of time, so we would never try to use the work that amortization amount twice for a given node.","title":"21.3-5 $\\star$"},{"location":"Chap21/21.4/","text":"21.4-1 Prove Lemma 21.4. The lemma states: For all nodes $x$, we have $x.rank \\le x.p.rank$, with strict inequality if $x \\ne x.p$. The value of $x.rank$ is initially $0$ and increases through time until $x \\ne x.p$; from then on, $x.rank$ does not change. The value of $x.p.rank$ monotonically increases over time. The initial value of $x.rank$ is $0$, as it is initialized in line 2 of the $\\text{MAKE-SET}(x)$ procedure. When we run $\\text{LINK}(x, y)$, whichever one has the larger rank is placed as the parent of the other, and if there is a tie, the parent's rank is incremented. This means that after any $\\text{LINK}(y, x)$, the two nodes being linked satisfy this strict inequality of ranks. Also, if we have that $x \\ne x.p$, then, we have that $x$ is not its own set representative, so, any linking together of sets that would occur would not involve $x$, but that's the only way for ranks to increase, so, we have that $x.rank$ must remain constant after that point. 21.4-2 Prove that every node has rank at most $\\lfloor \\lg n \\rfloor$. We'll prove the claim by strong induction on the number of nodes. If $n = 1$, then that node has rank equal to $0 = \\lfloor \\lg 1 \\rfloor$. Now suppose that the claim holds for $1, 2, \\ldots, n$ nodes. Given $n + 1$ nodes, suppose we perform a $\\text{UNION}$ operation on two disjoint sets with $a$ and $b$ nodes respectively, where $a, b \\le n$. Then the root of the first set has rank at most $\\lfloor \\lg a \\rfloor$ and the root of the second set has rank at most $\\lfloor \\lg b\\rfloor$. If the ranks are unequal, then the $\\text{UNION}$ operation preserves rank and we are done, so suppose the ranks are equal. Then the rank of the union increases by $1$, and the resulting set has rank $\\lfloor\\lg a\\rfloor + 1 \\le\\lfloor\\lg(n + 1) / 2\\rfloor + 1 = \\lfloor\\lg(n + 1)\\rfloor$. 21.4-3 In light of Exercise 21.4-2, how many bits are necessary to store $x.rank$ for each node $x$? Since their value is at most $\\lfloor \\lg n \\rfloor$, we can represent them using $\\Theta(\\lg(\\lg(n)))$ bits, and may need to use that many bits to represent a number that can take that many values. 21.4-4 Using Exercise 21.4-2, give a simple proof that operations on a disjoint-set forest with union by rank but without path compression run in $O(m\\lg n)$ time. $\\text{MAKE-SET}$ takes constant time and both $\\text{FIND-SET}$ and $\\text{UNION}$ are bounded by the largest rank among all the sets. Exercise 21.4-2 bounds this from about by $\\lceil \\lg n \\rceil$, so the actual cost of each operation is $O(\\lg n)$. Therefore the actual cost of $m$ operations is $O(m\\lg n)$. 21.4-5 Professor Dante reasons that because node ranks increase strictly along a simple path to the root, node levels must monotonically increase along the path. In other words, if $x.rank > 0$ and $x.p$ is not a root, then $\\text{level}(x) \\le \\text{level}(x.p)$. Is the professor correct? Professor Dante is not correct. Suppose that we had that $x.p.rank > A_2(x.rank)$ but that $x.p.p.rank = 1 + x.p.rank$, then we would have that $\\text{level}(x.p) = 0$, but $\\text{level}(x) \\ge 2$. So, we don't have that $\\text{level}(x) \\le \\text{level}(x.p)$ even though we have that the ranks are monotonically increasing as we go up in the tree. Put another way, even though the ranks are monotonically increasing, the rate at which they are increasing (roughly captured by the level values) doesn't have to be increasing. 21.4-6 $\\star$ Consider the function $\\alpha'(n) = \\min \\{k: A_k(1) \\ge \\lg(n + 1)\\}$. Show that $\\alpha'(n) \\le 3$ for all practical values of $n$ and, using Exercise 21.4-2, show how to modify the potential-function argument to prove that we can perform a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, on a disjoint-set forest with union by rank and path compression in worst-case time $O(m \\alpha'(n))$. First, observe that by a change of variables, $\\alpha'(2^{n \u2212 1}) = \\alpha(n)$. Earlier in the section we saw that $\\alpha(n) \\le 3$ for $0 \\le n \\le 2047$. This means that $\\alpha'(n) \\le 2$ for $0 \\le n \\le 2^{2046}$, which is larger than the estimated number of atoms in the observable universe. To prove the improved bound $O(m\\alpha'(n))$ on the operations, the general structure will be essentially the same as that given in the section. First, modify bound 21.2 by observing that $A_{\\alpha'(n)}(x.rank) \\ge A_{\\alpha'(n)}(1) \\ge \\lg(n + 1) > x.p.rank$ which implies $\\text{level}(x) \\le \\alpha'(n)$. Next, redefine the potential replacing $\\alpha(n)$ by $\\alpha'(n)$. Lemma 21.8 now goes through just as before. All subsequent lemmas rely on these previous observations, and their proofs go through exactly as in the section, yielding the bound.","title":"21.4 Analysis of union by rank with path compression"},{"location":"Chap21/21.4/#214-1","text":"Prove Lemma 21.4. The lemma states: For all nodes $x$, we have $x.rank \\le x.p.rank$, with strict inequality if $x \\ne x.p$. The value of $x.rank$ is initially $0$ and increases through time until $x \\ne x.p$; from then on, $x.rank$ does not change. The value of $x.p.rank$ monotonically increases over time. The initial value of $x.rank$ is $0$, as it is initialized in line 2 of the $\\text{MAKE-SET}(x)$ procedure. When we run $\\text{LINK}(x, y)$, whichever one has the larger rank is placed as the parent of the other, and if there is a tie, the parent's rank is incremented. This means that after any $\\text{LINK}(y, x)$, the two nodes being linked satisfy this strict inequality of ranks. Also, if we have that $x \\ne x.p$, then, we have that $x$ is not its own set representative, so, any linking together of sets that would occur would not involve $x$, but that's the only way for ranks to increase, so, we have that $x.rank$ must remain constant after that point.","title":"21.4-1"},{"location":"Chap21/21.4/#214-2","text":"Prove that every node has rank at most $\\lfloor \\lg n \\rfloor$. We'll prove the claim by strong induction on the number of nodes. If $n = 1$, then that node has rank equal to $0 = \\lfloor \\lg 1 \\rfloor$. Now suppose that the claim holds for $1, 2, \\ldots, n$ nodes. Given $n + 1$ nodes, suppose we perform a $\\text{UNION}$ operation on two disjoint sets with $a$ and $b$ nodes respectively, where $a, b \\le n$. Then the root of the first set has rank at most $\\lfloor \\lg a \\rfloor$ and the root of the second set has rank at most $\\lfloor \\lg b\\rfloor$. If the ranks are unequal, then the $\\text{UNION}$ operation preserves rank and we are done, so suppose the ranks are equal. Then the rank of the union increases by $1$, and the resulting set has rank $\\lfloor\\lg a\\rfloor + 1 \\le\\lfloor\\lg(n + 1) / 2\\rfloor + 1 = \\lfloor\\lg(n + 1)\\rfloor$.","title":"21.4-2"},{"location":"Chap21/21.4/#214-3","text":"In light of Exercise 21.4-2, how many bits are necessary to store $x.rank$ for each node $x$? Since their value is at most $\\lfloor \\lg n \\rfloor$, we can represent them using $\\Theta(\\lg(\\lg(n)))$ bits, and may need to use that many bits to represent a number that can take that many values.","title":"21.4-3"},{"location":"Chap21/21.4/#214-4","text":"Using Exercise 21.4-2, give a simple proof that operations on a disjoint-set forest with union by rank but without path compression run in $O(m\\lg n)$ time. $\\text{MAKE-SET}$ takes constant time and both $\\text{FIND-SET}$ and $\\text{UNION}$ are bounded by the largest rank among all the sets. Exercise 21.4-2 bounds this from about by $\\lceil \\lg n \\rceil$, so the actual cost of each operation is $O(\\lg n)$. Therefore the actual cost of $m$ operations is $O(m\\lg n)$.","title":"21.4-4"},{"location":"Chap21/21.4/#214-5","text":"Professor Dante reasons that because node ranks increase strictly along a simple path to the root, node levels must monotonically increase along the path. In other words, if $x.rank > 0$ and $x.p$ is not a root, then $\\text{level}(x) \\le \\text{level}(x.p)$. Is the professor correct? Professor Dante is not correct. Suppose that we had that $x.p.rank > A_2(x.rank)$ but that $x.p.p.rank = 1 + x.p.rank$, then we would have that $\\text{level}(x.p) = 0$, but $\\text{level}(x) \\ge 2$. So, we don't have that $\\text{level}(x) \\le \\text{level}(x.p)$ even though we have that the ranks are monotonically increasing as we go up in the tree. Put another way, even though the ranks are monotonically increasing, the rate at which they are increasing (roughly captured by the level values) doesn't have to be increasing.","title":"21.4-5"},{"location":"Chap21/21.4/#214-6-star","text":"Consider the function $\\alpha'(n) = \\min \\{k: A_k(1) \\ge \\lg(n + 1)\\}$. Show that $\\alpha'(n) \\le 3$ for all practical values of $n$ and, using Exercise 21.4-2, show how to modify the potential-function argument to prove that we can perform a sequence of $m$ $\\text{MAKE-SET}$, $\\text{UNION}$, and $\\text{FIND-SET}$ operations, $n$ of which are $\\text{MAKE-SET}$ operations, on a disjoint-set forest with union by rank and path compression in worst-case time $O(m \\alpha'(n))$. First, observe that by a change of variables, $\\alpha'(2^{n \u2212 1}) = \\alpha(n)$. Earlier in the section we saw that $\\alpha(n) \\le 3$ for $0 \\le n \\le 2047$. This means that $\\alpha'(n) \\le 2$ for $0 \\le n \\le 2^{2046}$, which is larger than the estimated number of atoms in the observable universe. To prove the improved bound $O(m\\alpha'(n))$ on the operations, the general structure will be essentially the same as that given in the section. First, modify bound 21.2 by observing that $A_{\\alpha'(n)}(x.rank) \\ge A_{\\alpha'(n)}(1) \\ge \\lg(n + 1) > x.p.rank$ which implies $\\text{level}(x) \\le \\alpha'(n)$. Next, redefine the potential replacing $\\alpha(n)$ by $\\alpha'(n)$. Lemma 21.8 now goes through just as before. All subsequent lemmas rely on these previous observations, and their proofs go through exactly as in the section, yielding the bound.","title":"21.4-6 $\\star$"},{"location":"Chap21/Problems/21-1/","text":"The off-line minimum problem asks us to maintain a dynamic set $T$ of elements from the domain $\\{1, 2, \\ldots, n\\}$ under the operations $\\text{INSERT}$ and $\\text{EXTRACT-MIN}$. We are given a sequence $S$ of $n$ $\\text{INSERT}$ and $m$ $\\text{EXTRACT-MIN}$ calls, where each key in $\\{1, 2, \\ldots, n\\}$ is inserted exactly once. We wish to determine which key is returned by each $\\text{EXTRACT-MIN}$ call. Specifically, we wish to fill in an array $extracted[1..m]$, where for $i = 1, 2, \\ldots, m$, $extracted[i]$ is the key returned by the $i$th $\\text{EXTRACT-MIN}$ call. The problem is \"off-line\" in the sense that we are allowed to process the entire sequence $S$ before determining any of the returned keys. a. In the following instance of the off-line minimum problem, each operation $\\text{INSERT}(i)$ is represented by the value of $i$ and each $\\text{EXTRACT-MIN}$ is represented by the letter $\\text E$: $$4, 8, \\text E, 3, \\text E, 9, 2, 6, \\text E, \\text E, \\text E, 1, 7, \\text E, 5.$$ Fill in the correct values in the extracted array. To develop an algorithm for this problem, we break the sequence $S$ into homogeneous subsequences. That is, we represent $S$ by $$\\text I_1, \\text E, \\text I_2, \\text E, \\text I_3, \\ldots, \\text I_m,\\text E, \\text I_{m + 1},$$ where each $\\text E$ represents a single $\\text{EXTRACT-MIN}$ call and each $\\text{I}_j$ represents a (possibly empty) sequence of $\\text{INSERT}$ calls. For each subsequence $\\text{I}_j$ , we initially place the keys inserted by these operations into a set $K_j$, which is empty if $\\text{I}_j$ is empty. We then do the following: OFF - LINE - MINIMUM ( m , n ) for i = 1 to n determine j such that i \u2208 K [ j ] if j != m + 1 extracted [ j ] = i let l be the smallest value greater than j for which set K [ l ] exists K [ l ] = K [ j ] \u222a K [ l ], destroying K [ j ] return extracted b. Argue that the array extracted returned by $\\text{OFF-LINE-MINIMUM}$ is correct. c. Describe how to implement $\\text{OFF-LINE-MINIMUM}$ efficiently with a disjoint-set data structure. Give a tight bound on the worst-case running time of your implementation. a. $$ \\begin{array}{|c|c|} \\hline index & value \\\\ \\hline 1 & 4 \\\\ 2 & 3 \\\\ 3 & 2 \\\\ 4 & 6 \\\\ 5 & 8 \\\\ 6 & 1 \\\\ \\hline \\end{array} $$ b. As we run the for loop, we are picking off the smallest of the possible elements to be removed, knowing for sure that it will be removed by the next unused $\\text{EXTRACT-MIN}$ operation. Then, since that $\\text{EXTRACT-MIN}$ operation is used up, we can pretend that it no longer exists, and combine the set of things that were inserted by that segment with those inserted by the next, since we know that the $\\text{EXTRACT-MIN}$ operation that had separated the two is now used up. Since we proceed to figure out what the various extract operations do one at a time, by the time we are done, we have figured them all out. c. We let each of the sets be represented by a disjoint set structure. To union them (as on line 6) just call $\\text{UNION}$. Checking that they exist is just a matter of keeping track of a linked list of which ones exist(needed for line 5), initially containing all of them, but then, when deleting the set on line 6, we delete it from the linked list that we were maintaining. The only other interaction with the sets that we have to worry about is on line 2, which just amounts to a call of $\\text{FIND-SET}(j)$. Since line 2 takes amortized time $\\alpha(n)$ and we call it exactly $n$ times, then, since the rest of the for loop only takes constant time, the total runtime is $O(n\\alpha(n))$.","title":"21-1 Off-line minimum"},{"location":"Chap21/Problems/21-2/","text":"In the depth-determination problem , we maintain a forest $\\mathcal F = \\{T_i\\}$ of rooted trees under three operations: $\\text{MAKE-TREE}(v)$ creates a tree whose only node is $v$. $\\text{FIND-DEPTH}(v)$ returns the depth of node $v$ within its tree. $\\text{GRAFT}(r, v)$ makes node $r$, which is assumed to be the root of a tree, become the child of node $v$, which is assumed to be in a different tree than $r$ but may or may not itself be a root. a. Suppose that we use a tree representation similar to a disjoint-set forest: $v.p$ is the parent of node $v$, except that $v.p = v$ if $v$ is a root. Suppose further that we implement $\\text{GRAFT}(r, v)$ by setting $r.p = v$ and $\\text{FIND-DEPTH}(v)$ by following the find path up to the root, returning a count of all nodes other than $v$ encountered. Show that the worst-case running time of a sequence of $m$ $\\text{MAKE-TREE}$, $\\text{FIND-DEPTH}$, and $\\text{GRAFT}$ operations is $\\Theta(m^2)$. By using the union-by-rank and path-compression heuristics, we can reduce the worst-case running time. We use the disjoint-set forest $\\mathcal S = \\{S_i\\}$, where each set $S_i$ (which is itself a tree) corresponds to a tree $T_i$ in the forest $\\mathcal F$. The tree structure within a set $S_i$, however, does not necessarily correspond to that of $T_i$. In fact, the implementation of $S_i$ does not record the exact parent-child relationships but nevertheless allows us to determine any node's depth in $T_i$. The key idea is to maintain in each node $v$ a \"pseudodistance\" $v.d$, which is defined so that the sum of the pseudodistances along the simple path from $v$ to the root of its set $S_i$ equals the depth of $v$ in $T_i$. That is, if the simple path from $v$ to its root in $S_i$ is $v_0, v_1, \\ldots, v_k$, where $v_0 = v$ and $v_k$ is $S_i$'s root, then the depth of $v$ in $T_i$ is $\\sum_{j = 0}^k v_j.d$. b. Give an implementation of $\\text{MAKE-TREE}$. c. Show how to modify $\\text{FIND-SET}$ to implement $\\text{FIND-DEPTH}$. Your implementation should perform path compression, and its running time should be linear in the length of the find path. Make sure that your implementation updates pseudodistances correctly. d. Show how to implement $\\text{GRAFT}(r, v)$, which combines the sets containing $r$ and $v$, by modifying the $\\text{UNION}$ and $\\text{LINK}$ procedures. Make sure that your implementation updates pseudodistances correctly. Note that the root of a set $S_i$ is not necessarily the root of the corresponding tree $T_i$. e. Give a tight bound on the worst-case running time of a sequence of $m$ $\\text{MAKE-TREE}$, $\\text{FIND-DEPTH}$, and $\\text{GRAFT}$ operations, $n$ of which are $\\text{MAKE-TREE}$ operations. a. $\\text{MAKE-TREE}$ and $\\text{GRAFT}$ are both constant time operations. $\\text{FINDDEPTH}$ is linear in the depth of the node. In a sequence of $m$ operations the maximal depth which can be achieved is $m/2$, so $\\text{FIND-DEPTH}$ takes at most $O(m)$. Thus, $m$ operations take at most $O(m^2)$. This is achieved as follows: Create $m / 3$ new trees. Graft them together into a chain using $m / 3$ calls to $\\text{GRAFT}$. Now call $\\text{FIND-DEPTH}$ on the deepest node $m / 3$ times. Each call takes time at least $m / 3$, so the total runtime is $\\Omega((m / 3)^2) = \\Omega(m^2)$. Thus the worst-case runtime of the $m$ operations is $\\Theta(m^2)$. b. Since the new set will contain only a single node, its depth must be zero and its parent is itself. In this case, the set and its corresponding tree are indistinguishable. MAKE - TREE ( v ) v = ALLOCATE - NODE () v . d = 0 v . p = v return v c. In addition to returning the set object, modify $\\text{FIND-SET}$ to also return the depth of the parent node. Update the pseudodistance of the current node $v$ to be $v.d$ plus the returned pseudodistance. Since this is done recursively, the running time is unchanged. It is still linear in the length of the find path. To implement $\\text{FIND-DEPTH}$, simply recurse up the tree containing $v$, keeping a running total of pseudodistances. FIND - SET ( v ) if v != v . p ( v . p , d ) = FIND - SET ( v . p ) v . d = v . d + d return ( v . p , v . d ) return ( v , 0 ) d. To implement $\\text{GRAFT}$ we need to find $v$'s actual depth and add it to the pseudodistance of the root of the tree $S_i$ which contains $r$. GRAFT ( r , v ) ( x , d_1 ) = FIND - SET ( r ) ( y , d_2 ) = FIND - SET ( v ) if x . rank > y . rank y . p = x x . d = x . d + d_2 + y . d else x . p = y x . d = x . d + d_2 if x . rank == y . rank y . rank = y . rank + 1 e. The three implemented operations have the same asymptotic running time as $\\text{MAKE}$, $\\text{FIND}$, and $\\text{UNION}$ for disjoint sets, so the worst-case runtime of $m$ such operations, $n$ of which are $\\text{MAKE-TREE}$ operations, is $O(m\\alpha(n))$.","title":"21-2 Depth determination"},{"location":"Chap21/Problems/21-3/","text":"The least common ancestor of two nodes $u$ and $v$ in a rooted tree $T$ is the node $w$ that is an ancestor of both $u$ and $v$ and that has the greatest depth in $T$. In the off-line least-common-ancestors problem , we are given a rooted tree $T$ and an arbitrary set $P = \\{\\{u, v\\}\\}$ of unordered pairs of nodes in $T$, and we wish to determine the least common ancestor of each pair in $P$. To solve the off-line least-common-ancestors problem, the following procedure performs a tree walk of $T$ with the initial call $\\text{LCA}(T.root)$. We assume that each node is colored $\\text{WHITE}$ prior to the walk. LCA ( u ) MAKE - SET ( u ) FIND - SET ( u ). ancestor = u for each child v of u in T LCA ( v ) UNION ( u , v ) FIND - SET ( u ). ancestor = u u . color = BLACK for each node v such that { u , v } \u2208 P if v . color == BLACK print \"The least common ancestor of\" u \"and\" v \"is\" FIND - SET ( v ). ancestor a. Argue that line 10 executes exactly once for each pair $\\{u, v\\} \\in P$. b. Argue that at the time of the call $\\text{LCA}(u)$, the number of sets in the disjoint-set data structure equals the depth of $u$ in $T$. c. Prove that $\\text{LCA}$ correctly prints the least common ancestor of $u$ and $v$ for each pair $\\{u, v\\} \\in P$. d. Analyze the running time of $\\text{LCA}$, assuming that we use the implementation of the disjoint-set data structure in Section 21.3. a. Suppose that we let $\\le_{LCA}$ to be an ordering on the vertices so that $u \\le_{LCA} v$ if we run line 7 of $\\text{LCA}(u)$ before line 7 of $\\text{LCA}(v)$. Then, when we are running line 7 of $\\text{LCA}(u)$, we immediately go on to the for loop on line 8. So, while we are doing this for loop, we still haven't called line 7 of $\\text{LCA}(v)$. This means that $v.color$ is white, and so, the pair $\\{u, v\\}$ is not considered during the run of $\\text{LCA}(u)$. However, during the for loop of $\\text{LCA}(v)$, since line 7 of $\\text{LCA}(u)$ has already run, $u.color = black$. This means that we will consider the pair $\\{u, v\\}$ during the running of $\\text{LCA}(v)$. It is not obvious what the ordering $\\le_{LCA}$ is, as it will be implementation dependent. It depends on the order in which child vertices are iterated in the for loop on line 3. That is, it doesn't just depend on the graph structure. b. We suppose that it is true prior to a given call of $\\text{LCA}$, and show that this property is preserved throughout a run of the procedure, increasing the number of disjoint sets by one by the end of the procedure. So, supposing that $u$ has depth $d$ and there are $d$ items in the disjoint set data structure before it runs, it increases to $d + 1$ disjoint sets on line 1. So, by the time we get to line 4, and call $\\text{LCA}$ of a child of $u$, there are $d + 1$ disjoint sets, this is exactly the depth of the child. After line 4, there are now $d + 2$ disjoint sets, so, line 5 brings it back down to $d + 1$ disjoint sets for the subsequent times through the loop. After the loop, there are no more changes to the number of disjoint sets, so, the algorithm terminates with $\\text{d + 1}$ disjoint sets, as desired. Since this holds for any arbitrary run of $\\text{LCA}$, it holds for all runs of $\\text{LCA}$. c. Suppose that the pair $u$ and $v$ have the least common ancestor $w$. Then, when running $\\text{LCA}(w)$, $u$ will be in the subtree rooted at one of $w$'s children, and $v$ will be in another. WLOG, suppose that the subtree containing $u$ runs first. So, when we are done with running that subtree, all of their ancestor values will point to $w$ and their colors will be black, and their ancestor values will not change until $\\text{LCA}(w)$ returns. However, we run $\\text{LCA}(v)$ before $\\text{LCA}(w)$ returns, so in the for loop on line 8 of $\\text{LCA}(v)$, we will be considering the pair $\\{u, v\\}$, since $u.color = black$. Since $u.ancestor$ is still $w$, that is what will be output, which is the correct answer for their $\\text{LCA}$. d. The time complexity of lines 1 and 2 are just constant. Then, for each child, we have a call to the same procedure, a $\\text{UNION}$ operation which only takes constant time, and a $\\text{FIND-SET}$ operation which can take at most amortized inverse Ackerman's time. Since we check each and every thing that is adjacent to $u$ for being black, we are only checking each pair in $P$ at most twice in lines 8-10, among all the runs of $\\text{LCA}$. This means that the total runtime is $O(|T|\\alpha(|T|) + |P|)$.","title":"21-3 Tarjan's off-line least-common-ancestors algorithm"},{"location":"Chap22/22.1/","text":"22.1-1 Given an adjacency-list representation of a directed graph, how long does it take to compute the $\\text{out-degree}$ of every vertex? How long does it take to compute the $\\text{in-degree}$s? The time to compute the $\\text{out-degree}$ of every vertex is $$\\sum_{v \\in V}O(\\text{out-degree}(v)) = O(|E| + |V|),$$ which is straightforward. As for the $\\text{in-degree}$, we have to scan through all adjacency lists and keep counters for how many times each vertex has been pointed to. Thus, the time complexity is also $O(|E| + |V|)$ because we'll visit all nodes and edges. 22.1-2 Give an adjacency-list representation for a complete binary tree on $7$ vertices. Give an equivalent adjacency-matrix representation. Assume that vertices are numbered from $1$ to $7$ as in a binary heap. Adjacency-list representation $$ \\begin{aligned} 1 & \\to 2 \\to 3 \\\\ 2 & \\to 1 \\to 4 \\to 5 \\\\ 3 & \\to 1 \\to 6 \\to 7 \\\\ 4 & \\to 2 \\\\ 5 & \\to 2 \\\\ 6 & \\to 3 \\\\ 7 & \\to 3 \\end{aligned} $$ Adjacency-matrix representation $$ \\begin{array}{c|ccccccc|} & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\ 2 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\\\ 3 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\ 4 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 5 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 6 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 7 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ \\hline \\end{array} $$ 22.1-3 The transpose of a directed graph $G = (V, E)$ is the graph $G^\\text T = (V, E^\\text T)$, where $E^\\text T = \\{(v, u) \\in V \\times V: (u, v) \\in E \\}$. Thus, $G^\\text T$ is $G$ with all its edges reversed. Describe efficient algorithms for computing $G^\\text T$ from $G$, for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation Assume the original adjacency list is $Adj$. let Adj ' [ 1. . | V | ] be a new adjacency list of the transposed G ^ T for each vertex u \u2208 G . V for each vertex v \u2208 Adj [ u ] INSERT ( Adj ' [ v ], u ) Time complexity: $O(|E| + |V|)$. Adjacency-matrix representation Transpose the original matrix by looking along every entry above the diagonal, and swapping it with the entry that occurs below the diagonal. Time complexity: $O(|V|^2)$. 22.1-4 Given an adjacency-list representation of a multigraph $G = (V, E)$, describe an $O(V + E)$-time algorithm to compute the adjacency-list representation of the \"equivalent\" undirected graph $G' = (V, E')$, where $E'$ consists of the edges in $E$ with all multiple edges between two vertices replaced by a single edge and with all self-loops removed. EQUIVALENT - UNDIRECTED - GRAPH let Adj ' [ 1. . | V | ] be a new adjacency list let A be a 0 - initialized array of size | V | for each vertex u \u2208 G . V for each v \u2208 Adj [ u ] if v != u && A [ v ] != u A [ v ] = u INSERT ( Adj ' [ u ], v ) Note that $A$ does not contain any element with value $u$ before each iteration of the inner for-loop. That's why we use $A[v] = u$ to mark the existence of an edge $(u, v)$ in the inner for-loop. Since we lookup in the adjacency-list $Adj$ for $|V| + |E|$ times, the time complexity is $O(|V| + |E|)$. 22.1-5 The square of a directed graph $G = (V, E)$ is the graph $G^2 = (V, E^2)$ such that $(u, v) \\in E^2$ if and only if $G$ contains a path with at most two edges between $u$ and $v$. Describe efficient algorithms for computing $G^2$ from $G$ for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation To compute $G^2$ from the adjacency-list representation $Adj$ of $G$, we perform the following for each $Adj[u]$: for each v \u2208 Adj [ u ] INSERT ( Adj2 [ u ], v ) for each w \u2208 Adj [ v ] // edge(u, w) \u2208 E^2 INSERT ( Adj2 [ u ], w ) where $Adj2$ is the adjacency-list representation of $G^2$. For every edge in $Adj$ we scan at most $|V|$ vertices, we compute $Adj2$ in time $O(|V||E|)$. After we have computed $Adj2$, we have to remove duplicate edges from the lists. Removing duplicate edges is done in $O(V + E')$ where $E' = O(VE)$ is the number of edges in $Adj2$ as shown in exercise 22.1-4. Thus the total running time is $$O(VE) + O(V + VE) = O(VE).$$ However, if the original graph $G$ contains self-loops, we should modify the algorithm so that self-loops are not removed. Adjacency-matrix representation Let $A$ denote the adjacency-matrix representation of $G$. The adjacency-matrix representation of $G^2$ is the square of $A$. Computing $A^2$ can be done in time $O(V^3)$ (and even faster, theoretically; Strassen's algorithm for example will compute $A^2$ in $O(V^{\\lg 7})$). 22.1-6 Most graph algorithms that take an adjacency-matrix representation as input require time $\\Omega(V^2)$, but there are some exceptions. Show how to determine whether a directed graph $G$ contains a universal sink $-$ a vertex with $\\text{in-degree}$ $|V| - 1$ and $\\text{out-degree}$ $0$ $-$ in time $O(V)$, given an adjacency matrix for $G$. Start by examining position $(1, 1)$ in the adjacency matrix. When examining position $(i, j)$, if a $1$ is encountered, examine position $(i + 1, j)$, and if a $0$ is encountered, examine position $(i, j + 1)$. Once either $i$ or $j$ is equal to $|V|$, terminate. IS - CONTAIN - UNIVERSAL - SINK ( M ) i = j = 1 while i < | V | and j < | V | // There's an out-going edge, so examine the next row if M [ i , j ] == 1 i = i + 1 // There's no out-going edge, so see if we could reach the last column of current row else if M [ i , j ] == 0 j = j + 1 check if vertex i is a universal sink If a graph contains a universal sink, then it must be at vertex $i$. To see this, suppose that vertex $k$ is a universal sink. Since $k$ is a universal sink, row $k$ will be filled with $0$'s, and column $k$ will be filled with $1$'s except for $M[k, k]$, which is filled with a $0$. Eventually, once row $k$ is hit, the algorithm will continue to increment column $j$ until $j = |V|$. To be sure that row $k$ is eventually hit, note that once column $k$ is reached, the algorithm will continue to increment $i$ until it reaches $k$. This algorithm runs in $O(V)$ and checking if vertex $i$ is a universal sink is done in $O(V)$. Therefore, the total running time is $O(V) + O(V) = O(V)$. 22.1-7 The incidence matrix of a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $B = (b_{ij})$ such that $$ b_{ij} = \\begin{cases} -1 & \\text{if edge $j$ leaves vertex $i$}, \\\\ 1 & \\text{if edge $j$ enters vertex $i$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ Describe what the entries of the matrix product $BB^\\text T$ represent, where $B^\\text T$ is the transpose of $B$. $$BB^\\text T(i, j) = \\sum\\limits_{e \\in E}b_{ie} b_{ej}^\\text T = \\sum\\limits_{e \\in E} b_{ie}b_{je}.$$ If $i = j$, then $b_{ie} b_{je} = 1$ (it is $1 \\cdot 1$ or $(-1) \\cdot (-1)$) whenever $e$ enters or leaves vertex $i$, and $0$ otherwise. If $i \\ne j$, then $b_{ie} b_{je} = -1$ when $e = (i, j)$ or $e = (j, i)$, and $0$ otherwise. Thus, $$ BB^\\text T(i, j) = \\begin{cases} \\text{degree of $i$ = in-degree + out-degree} & \\text{if $i = j$}, \\\\ \\text{$-$(\\# of edges connecting $i$ and $j$)} & \\text{if $i \\ne j$}. \\end{cases} $$ 22.1-8 Suppose that instead of a linked list, each array entry $Adj[u]$ is a hash table containing the vertices $v$ for which $(u, v) \\in E$. If all edge lookups are equally likely, what is the expected time to determine whether an edge is in the graph? What disadvantages does this scheme have? Suggest an alternate data structure for each edge list that solves these problems. Does your alternative have disadvantages compared to the hash table? The expected lookup time is $O(1)$, but in the worst case it could take $O(|V|)$. If we first sorted vertices in each adjacency list then we could perform a binary search so that the worst case lookup time is $O(\\lg |V|)$, but this has the disadvantage of having a much worse expected lookup time.","title":"22.1 Representations of graphs"},{"location":"Chap22/22.1/#221-1","text":"Given an adjacency-list representation of a directed graph, how long does it take to compute the $\\text{out-degree}$ of every vertex? How long does it take to compute the $\\text{in-degree}$s? The time to compute the $\\text{out-degree}$ of every vertex is $$\\sum_{v \\in V}O(\\text{out-degree}(v)) = O(|E| + |V|),$$ which is straightforward. As for the $\\text{in-degree}$, we have to scan through all adjacency lists and keep counters for how many times each vertex has been pointed to. Thus, the time complexity is also $O(|E| + |V|)$ because we'll visit all nodes and edges.","title":"22.1-1"},{"location":"Chap22/22.1/#221-2","text":"Give an adjacency-list representation for a complete binary tree on $7$ vertices. Give an equivalent adjacency-matrix representation. Assume that vertices are numbered from $1$ to $7$ as in a binary heap. Adjacency-list representation $$ \\begin{aligned} 1 & \\to 2 \\to 3 \\\\ 2 & \\to 1 \\to 4 \\to 5 \\\\ 3 & \\to 1 \\to 6 \\to 7 \\\\ 4 & \\to 2 \\\\ 5 & \\to 2 \\\\ 6 & \\to 3 \\\\ 7 & \\to 3 \\end{aligned} $$ Adjacency-matrix representation $$ \\begin{array}{c|ccccccc|} & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\ \\hline 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\ 2 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\\\ 3 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\ 4 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 5 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 6 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 7 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ \\hline \\end{array} $$","title":"22.1-2"},{"location":"Chap22/22.1/#221-3","text":"The transpose of a directed graph $G = (V, E)$ is the graph $G^\\text T = (V, E^\\text T)$, where $E^\\text T = \\{(v, u) \\in V \\times V: (u, v) \\in E \\}$. Thus, $G^\\text T$ is $G$ with all its edges reversed. Describe efficient algorithms for computing $G^\\text T$ from $G$, for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation Assume the original adjacency list is $Adj$. let Adj ' [ 1. . | V | ] be a new adjacency list of the transposed G ^ T for each vertex u \u2208 G . V for each vertex v \u2208 Adj [ u ] INSERT ( Adj ' [ v ], u ) Time complexity: $O(|E| + |V|)$. Adjacency-matrix representation Transpose the original matrix by looking along every entry above the diagonal, and swapping it with the entry that occurs below the diagonal. Time complexity: $O(|V|^2)$.","title":"22.1-3"},{"location":"Chap22/22.1/#221-4","text":"Given an adjacency-list representation of a multigraph $G = (V, E)$, describe an $O(V + E)$-time algorithm to compute the adjacency-list representation of the \"equivalent\" undirected graph $G' = (V, E')$, where $E'$ consists of the edges in $E$ with all multiple edges between two vertices replaced by a single edge and with all self-loops removed. EQUIVALENT - UNDIRECTED - GRAPH let Adj ' [ 1. . | V | ] be a new adjacency list let A be a 0 - initialized array of size | V | for each vertex u \u2208 G . V for each v \u2208 Adj [ u ] if v != u && A [ v ] != u A [ v ] = u INSERT ( Adj ' [ u ], v ) Note that $A$ does not contain any element with value $u$ before each iteration of the inner for-loop. That's why we use $A[v] = u$ to mark the existence of an edge $(u, v)$ in the inner for-loop. Since we lookup in the adjacency-list $Adj$ for $|V| + |E|$ times, the time complexity is $O(|V| + |E|)$.","title":"22.1-4"},{"location":"Chap22/22.1/#221-5","text":"The square of a directed graph $G = (V, E)$ is the graph $G^2 = (V, E^2)$ such that $(u, v) \\in E^2$ if and only if $G$ contains a path with at most two edges between $u$ and $v$. Describe efficient algorithms for computing $G^2$ from $G$ for both the adjacency-list and adjacency-matrix representations of $G$. Analyze the running times of your algorithms. Adjacency-list representation To compute $G^2$ from the adjacency-list representation $Adj$ of $G$, we perform the following for each $Adj[u]$: for each v \u2208 Adj [ u ] INSERT ( Adj2 [ u ], v ) for each w \u2208 Adj [ v ] // edge(u, w) \u2208 E^2 INSERT ( Adj2 [ u ], w ) where $Adj2$ is the adjacency-list representation of $G^2$. For every edge in $Adj$ we scan at most $|V|$ vertices, we compute $Adj2$ in time $O(|V||E|)$. After we have computed $Adj2$, we have to remove duplicate edges from the lists. Removing duplicate edges is done in $O(V + E')$ where $E' = O(VE)$ is the number of edges in $Adj2$ as shown in exercise 22.1-4. Thus the total running time is $$O(VE) + O(V + VE) = O(VE).$$ However, if the original graph $G$ contains self-loops, we should modify the algorithm so that self-loops are not removed. Adjacency-matrix representation Let $A$ denote the adjacency-matrix representation of $G$. The adjacency-matrix representation of $G^2$ is the square of $A$. Computing $A^2$ can be done in time $O(V^3)$ (and even faster, theoretically; Strassen's algorithm for example will compute $A^2$ in $O(V^{\\lg 7})$).","title":"22.1-5"},{"location":"Chap22/22.1/#221-6","text":"Most graph algorithms that take an adjacency-matrix representation as input require time $\\Omega(V^2)$, but there are some exceptions. Show how to determine whether a directed graph $G$ contains a universal sink $-$ a vertex with $\\text{in-degree}$ $|V| - 1$ and $\\text{out-degree}$ $0$ $-$ in time $O(V)$, given an adjacency matrix for $G$. Start by examining position $(1, 1)$ in the adjacency matrix. When examining position $(i, j)$, if a $1$ is encountered, examine position $(i + 1, j)$, and if a $0$ is encountered, examine position $(i, j + 1)$. Once either $i$ or $j$ is equal to $|V|$, terminate. IS - CONTAIN - UNIVERSAL - SINK ( M ) i = j = 1 while i < | V | and j < | V | // There's an out-going edge, so examine the next row if M [ i , j ] == 1 i = i + 1 // There's no out-going edge, so see if we could reach the last column of current row else if M [ i , j ] == 0 j = j + 1 check if vertex i is a universal sink If a graph contains a universal sink, then it must be at vertex $i$. To see this, suppose that vertex $k$ is a universal sink. Since $k$ is a universal sink, row $k$ will be filled with $0$'s, and column $k$ will be filled with $1$'s except for $M[k, k]$, which is filled with a $0$. Eventually, once row $k$ is hit, the algorithm will continue to increment column $j$ until $j = |V|$. To be sure that row $k$ is eventually hit, note that once column $k$ is reached, the algorithm will continue to increment $i$ until it reaches $k$. This algorithm runs in $O(V)$ and checking if vertex $i$ is a universal sink is done in $O(V)$. Therefore, the total running time is $O(V) + O(V) = O(V)$.","title":"22.1-6"},{"location":"Chap22/22.1/#221-7","text":"The incidence matrix of a directed graph $G = (V, E)$ with no self-loops is a $|V| \\times |E|$ matrix $B = (b_{ij})$ such that $$ b_{ij} = \\begin{cases} -1 & \\text{if edge $j$ leaves vertex $i$}, \\\\ 1 & \\text{if edge $j$ enters vertex $i$}, \\\\ 0 & \\text{otherwise}. \\end{cases} $$ Describe what the entries of the matrix product $BB^\\text T$ represent, where $B^\\text T$ is the transpose of $B$. $$BB^\\text T(i, j) = \\sum\\limits_{e \\in E}b_{ie} b_{ej}^\\text T = \\sum\\limits_{e \\in E} b_{ie}b_{je}.$$ If $i = j$, then $b_{ie} b_{je} = 1$ (it is $1 \\cdot 1$ or $(-1) \\cdot (-1)$) whenever $e$ enters or leaves vertex $i$, and $0$ otherwise. If $i \\ne j$, then $b_{ie} b_{je} = -1$ when $e = (i, j)$ or $e = (j, i)$, and $0$ otherwise. Thus, $$ BB^\\text T(i, j) = \\begin{cases} \\text{degree of $i$ = in-degree + out-degree} & \\text{if $i = j$}, \\\\ \\text{$-$(\\# of edges connecting $i$ and $j$)} & \\text{if $i \\ne j$}. \\end{cases} $$","title":"22.1-7"},{"location":"Chap22/22.1/#221-8","text":"Suppose that instead of a linked list, each array entry $Adj[u]$ is a hash table containing the vertices $v$ for which $(u, v) \\in E$. If all edge lookups are equally likely, what is the expected time to determine whether an edge is in the graph? What disadvantages does this scheme have? Suggest an alternate data structure for each edge list that solves these problems. Does your alternative have disadvantages compared to the hash table? The expected lookup time is $O(1)$, but in the worst case it could take $O(|V|)$. If we first sorted vertices in each adjacency list then we could perform a binary search so that the worst case lookup time is $O(\\lg |V|)$, but this has the disadvantage of having a much worse expected lookup time.","title":"22.1-8"},{"location":"Chap22/22.2/","text":"22.2-1 Show the $d$ and $\\pi$ values that result from running breadth-first search on the directed graph of Figure 22.2(a), using vertex $3$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline d & \\infty & 3 & 0 & 2 & 1 & 1 \\\\ \\pi & \\text{NIL} & 4 & \\text{NIL} & 5 & 3 & 3 \\end{array} $$ 22.2-2 Show the $d$ and $\\pi$ values that result from running breadth-first search on the undirected graph of Figure 22.3, using vertex $u$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & r & s & t & u & v & w & x & y \\\\ \\hline d & 4 & 3 & 1 & 0 & 5 & 2 & 1 & 1 \\\\ \\pi & s & w & u & \\text{NIL} & r & t & u & u \\end{array} $$ 22.2-3 Show that using a single bit to store each vertex color suffices by arguing that the $\\text{BFS}$ procedure would produce the same result if lines 5 and 14 were removed. The textbook introduces the $\\text{GRAY}$ color for the pedagogical purpose to distinguish between the $\\text{GRAY}$ nodes (which are enqueued) and the $\\text{BLACK}$ nodes (which are dequeued). Therefore, it suffices to use a single bit to store each vertex color. 22.2-4 What is the running time of $\\text{BFS}$ if we represent its input graph by an adjacency matrix and modify the algorithm to handle this form of input? The time of iterating all edges becomes $O(V^2)$ from $O(E)$. Therefore, the running time is $O(V + V^2)$. 22.2-5 Argue that in a breadth-first search, the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list. Using Figure 22.3 as an example, show that the breadth-first tree computed by $\\text{BFS}$ can depend on the ordering within adjacency lists. First, we will show that the value $d$ assigned to a vertex is independent of the order that entries appear in adjacency lists. To show this, we rely on theorem 22.5, which proves correctness of $\\text{BFS}$. In particular, the theorem states that $v.d = \\delta(s, v)$ at the termination of $\\text{BFS}$. Since $\\delta(s, v)$ is a property of the underlying graph, for any adjacency list representation of the graph (including any reordering of the adjacency lists), $\\delta(s, v)$ will not change. Since the $d$ values are equal to $\\delta(s, v)$ and $\\delta(s, v)$ is invariant for any ordering of the adjacency list, $d$ is also not dependent of the ordering of the adjacency list. Now, to show that $\\pi$ does depend on the ordering of the adjacency lists, we will be using Figure 22.3 as a guide. First, we note that in the given worked out procedure, we have that in the adjacency list for $w$, $t$ precedes $x$. Also, in the worked out procedure, we have that $u.\\pi = t$. Now, suppose instead that we had $x$ preceding $t$ in the adjacency list of $w$. Then, it would get added to the queue before $t$, which means that it would $u$ as it's child before we have a chance to process the children of $t$. This will mean that $u.\\pi = x$ in this different ordering of the adjacency list for $w$. 22.2-6 Give an example of a directed graph $G = (V, E)$, a source vertex $s \\in V$, and a set of tree edges $E_\\pi \\subseteq E$ such that for each vertex $v \\in V$, the unique simple path in the graph $(V, E_\\pi)$ from $s$ to $v$ is a shortest path in $G$, yet the set of edges $E_\\pi$ cannot be produced by running $\\text{BFS}$ on $G$, no matter how the vertices are ordered in each adjacency list. Let $G$ be the graph shown in the first picture, $G_\\pi = (V, E_\\pi)$ be the graph shown in the second picture, and $s$ be the source vertex. We could see that $E_\\pi$ will never be produced by running BFS on $G$. If $y$ precedes $v$ in the $Adj[s]$. We'll dequeue $y$ before $v$, so $u.\\pi$ and $x.\\pi$ are both $y$. However, this is not the case. If $v$ preceded $y$ in the $Adj[s]$. We'll dequeue $v$ before $y$, so $u.\\pi$ and $x.\\pi$ are both $v$, which again isn't true. Nonetheless, the unique simple path in $G_\\pi$ from $s$ to any vertex is a shortest path in $G$. 22.2-7 There are two types of professional wrestlers: \"babyfaces\" (\"good guys\") and \"heels\" (\"bad guys\"). Between any pair of professional wrestlers, there may or may not be a rivalry. Suppose we have $n$ professional wrestlers and we have a list of $r$ pairs of wrestlers for which there are rivalries. Give an $O(n + r)$-time algorithm that determines whether it is possible to designate some of the wrestlers as babyfaces and the remainder as heels such that each rivalry is between a babyface and a heel. If it is possible to perform such a designation, your algorithm should produce it. This problem is basically just a obfuscated version of two coloring. We will try to color the vertices of this graph of rivalries by two colors, \"babyface\" and \"heel\". To have that no two babyfaces and no two heels have a rivalry is the same as saying that the coloring is proper. To two color, we perform a breadth first search of each connected component to get the $d$ values for each vertex. Then, we give all the odd ones one color say \"heel\", and all the even d values a different color. We know that no other coloring will succeed where this one fails since if we gave any other coloring, we would have that a vertex $v$ has the same color as $v.\\pi$ since $v$ and $v.\\pi$ must have different parities for their $d$ values. Since we know that there is no better coloring, we just need to check each edge to see if this coloring is valid. If each edge works, it is possible to find a designation, if a single edge fails, then it is not possible. Since the BFS took time $O(n + r)$ and the checking took time $O(r)$, the total runtime is $O(n + r)$. 22.2-8 $\\star$ The diameter of a tree $T = (V, E)$ is defined as $\\max_{u,v \\in V} \\delta(u, v)$, that is, the largest of all shortest-path distances in the tree. Give an efficient algorithm to compute the diameter of a tree, and analyze the running time of your algorithm. Suppose that a and b are the endpoints of the path in the tree which achieve the diameter, and without loss of generality assume that $a$ and $b$ are the unique pair which do so. Let $s$ be any vertex in $T$. We claim that the result of a single $\\text{BFS}$ will return either $a$ or $b$ (or both) as the vertex whose distance from $s$ is greatest. To see this, suppose to the contrary that some other vertex $x$ is shown to be furthest from $s$. (Note that $x$ cannot be on the path from $a$ to $b$, otherwise we could extend). Then we have $$d(s, a) < d(s, x)$$ and $$d(s, b) < d(s, x).$$ Let $c$ denote the vertex on the path from $a$ to $b$ which minimizes $d(s, c)$. Since the graph is in fact a tree, we must have $$d(s, a) = d(s, c) + d(c, a)$$ and $$d(s, b) = d(s, c) + d(c, b).$$ (If there were another path, we could form a cycle). Using the triangle inequality and inequalities and equalities mentioned above we must have $$ \\begin{aligned} d(a, b) + 2d(s, c) & = d(s, c) + d(c, b) + d(s, c) + d(c, a) \\\\ & < d(s, x) + d(s, c) + d(c, b). \\end{aligned} $$ I claim that $d(x, b) = d(s, x) + d(s, b)$. If not, then by the triangle inequality we must have a strict less-than. In other words, there is some path from $x$ to $b$ which does not go through $c$. This gives the contradiction, because it implies there is a cycle formed by concatenating these paths. Then we have $$d(a, b) < d(a, b) + 2d(s, c) < d(x, b).$$ Since it is assumed that $d(a, b)$ is maximal among all pairs, we have a contradiction. Therefore, since trees have $|V| - 1$ edges, we can run $\\text{BFS}$ a single time in $O(V)$ to obtain one of the vertices which is the endpoint of the longest simple path contained in the graph. Running $\\text{BFS}$ again will show us where the other one is, so we can solve the diameter problem for trees in $O(V)$. 22.2-9 Let $G = (V, E)$ be a connected, undirected graph. Give an $O(V + E)$-time algorithm to compute a path in $G$ that traverses each edge in $E$ exactly once in each direction. Describe how you can find your way out of a maze if you are given a large supply of pennies. First, the algorithm computes a minimum spanning tree of the graph. Note that this can be done using the procedures of Chapter 23. It can also be done by performing a breadth first search, and restricting to the edges between $v$ and $v.\\pi$ for every $v$. To aide in not double counting edges, fix any ordering $\\le$ on the vertices before hand. Then, we will construct the sequence of steps by calling $\\text{MAKE-PATH}(s)$, where $s$ was the root used for the $\\text{BFS}$. MAKE - PATH ( u ) for each v \u2208 Adj [ u ] but not in the tree such that u \u2264 v go to v and back to u for each v \u2208 Adj [ u ] but not equal to u . \u03c0 go to v perform the path proscribed by MAKE - PATH ( v ) go to u . \u03c0","title":"22.2 Breadth-first search"},{"location":"Chap22/22.2/#222-1","text":"Show the $d$ and $\\pi$ values that result from running breadth-first search on the directed graph of Figure 22.2(a), using vertex $3$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline d & \\infty & 3 & 0 & 2 & 1 & 1 \\\\ \\pi & \\text{NIL} & 4 & \\text{NIL} & 5 & 3 & 3 \\end{array} $$","title":"22.2-1"},{"location":"Chap22/22.2/#222-2","text":"Show the $d$ and $\\pi$ values that result from running breadth-first search on the undirected graph of Figure 22.3, using vertex $u$ as the source. $$ \\begin{array}{c|cccccc} \\text{vertex} & r & s & t & u & v & w & x & y \\\\ \\hline d & 4 & 3 & 1 & 0 & 5 & 2 & 1 & 1 \\\\ \\pi & s & w & u & \\text{NIL} & r & t & u & u \\end{array} $$","title":"22.2-2"},{"location":"Chap22/22.2/#222-3","text":"Show that using a single bit to store each vertex color suffices by arguing that the $\\text{BFS}$ procedure would produce the same result if lines 5 and 14 were removed. The textbook introduces the $\\text{GRAY}$ color for the pedagogical purpose to distinguish between the $\\text{GRAY}$ nodes (which are enqueued) and the $\\text{BLACK}$ nodes (which are dequeued). Therefore, it suffices to use a single bit to store each vertex color.","title":"22.2-3"},{"location":"Chap22/22.2/#222-4","text":"What is the running time of $\\text{BFS}$ if we represent its input graph by an adjacency matrix and modify the algorithm to handle this form of input? The time of iterating all edges becomes $O(V^2)$ from $O(E)$. Therefore, the running time is $O(V + V^2)$.","title":"22.2-4"},{"location":"Chap22/22.2/#222-5","text":"Argue that in a breadth-first search, the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list. Using Figure 22.3 as an example, show that the breadth-first tree computed by $\\text{BFS}$ can depend on the ordering within adjacency lists. First, we will show that the value $d$ assigned to a vertex is independent of the order that entries appear in adjacency lists. To show this, we rely on theorem 22.5, which proves correctness of $\\text{BFS}$. In particular, the theorem states that $v.d = \\delta(s, v)$ at the termination of $\\text{BFS}$. Since $\\delta(s, v)$ is a property of the underlying graph, for any adjacency list representation of the graph (including any reordering of the adjacency lists), $\\delta(s, v)$ will not change. Since the $d$ values are equal to $\\delta(s, v)$ and $\\delta(s, v)$ is invariant for any ordering of the adjacency list, $d$ is also not dependent of the ordering of the adjacency list. Now, to show that $\\pi$ does depend on the ordering of the adjacency lists, we will be using Figure 22.3 as a guide. First, we note that in the given worked out procedure, we have that in the adjacency list for $w$, $t$ precedes $x$. Also, in the worked out procedure, we have that $u.\\pi = t$. Now, suppose instead that we had $x$ preceding $t$ in the adjacency list of $w$. Then, it would get added to the queue before $t$, which means that it would $u$ as it's child before we have a chance to process the children of $t$. This will mean that $u.\\pi = x$ in this different ordering of the adjacency list for $w$.","title":"22.2-5"},{"location":"Chap22/22.2/#222-6","text":"Give an example of a directed graph $G = (V, E)$, a source vertex $s \\in V$, and a set of tree edges $E_\\pi \\subseteq E$ such that for each vertex $v \\in V$, the unique simple path in the graph $(V, E_\\pi)$ from $s$ to $v$ is a shortest path in $G$, yet the set of edges $E_\\pi$ cannot be produced by running $\\text{BFS}$ on $G$, no matter how the vertices are ordered in each adjacency list. Let $G$ be the graph shown in the first picture, $G_\\pi = (V, E_\\pi)$ be the graph shown in the second picture, and $s$ be the source vertex. We could see that $E_\\pi$ will never be produced by running BFS on $G$. If $y$ precedes $v$ in the $Adj[s]$. We'll dequeue $y$ before $v$, so $u.\\pi$ and $x.\\pi$ are both $y$. However, this is not the case. If $v$ preceded $y$ in the $Adj[s]$. We'll dequeue $v$ before $y$, so $u.\\pi$ and $x.\\pi$ are both $v$, which again isn't true. Nonetheless, the unique simple path in $G_\\pi$ from $s$ to any vertex is a shortest path in $G$.","title":"22.2-6"},{"location":"Chap22/22.2/#222-7","text":"There are two types of professional wrestlers: \"babyfaces\" (\"good guys\") and \"heels\" (\"bad guys\"). Between any pair of professional wrestlers, there may or may not be a rivalry. Suppose we have $n$ professional wrestlers and we have a list of $r$ pairs of wrestlers for which there are rivalries. Give an $O(n + r)$-time algorithm that determines whether it is possible to designate some of the wrestlers as babyfaces and the remainder as heels such that each rivalry is between a babyface and a heel. If it is possible to perform such a designation, your algorithm should produce it. This problem is basically just a obfuscated version of two coloring. We will try to color the vertices of this graph of rivalries by two colors, \"babyface\" and \"heel\". To have that no two babyfaces and no two heels have a rivalry is the same as saying that the coloring is proper. To two color, we perform a breadth first search of each connected component to get the $d$ values for each vertex. Then, we give all the odd ones one color say \"heel\", and all the even d values a different color. We know that no other coloring will succeed where this one fails since if we gave any other coloring, we would have that a vertex $v$ has the same color as $v.\\pi$ since $v$ and $v.\\pi$ must have different parities for their $d$ values. Since we know that there is no better coloring, we just need to check each edge to see if this coloring is valid. If each edge works, it is possible to find a designation, if a single edge fails, then it is not possible. Since the BFS took time $O(n + r)$ and the checking took time $O(r)$, the total runtime is $O(n + r)$.","title":"22.2-7"},{"location":"Chap22/22.2/#222-8-star","text":"The diameter of a tree $T = (V, E)$ is defined as $\\max_{u,v \\in V} \\delta(u, v)$, that is, the largest of all shortest-path distances in the tree. Give an efficient algorithm to compute the diameter of a tree, and analyze the running time of your algorithm. Suppose that a and b are the endpoints of the path in the tree which achieve the diameter, and without loss of generality assume that $a$ and $b$ are the unique pair which do so. Let $s$ be any vertex in $T$. We claim that the result of a single $\\text{BFS}$ will return either $a$ or $b$ (or both) as the vertex whose distance from $s$ is greatest. To see this, suppose to the contrary that some other vertex $x$ is shown to be furthest from $s$. (Note that $x$ cannot be on the path from $a$ to $b$, otherwise we could extend). Then we have $$d(s, a) < d(s, x)$$ and $$d(s, b) < d(s, x).$$ Let $c$ denote the vertex on the path from $a$ to $b$ which minimizes $d(s, c)$. Since the graph is in fact a tree, we must have $$d(s, a) = d(s, c) + d(c, a)$$ and $$d(s, b) = d(s, c) + d(c, b).$$ (If there were another path, we could form a cycle). Using the triangle inequality and inequalities and equalities mentioned above we must have $$ \\begin{aligned} d(a, b) + 2d(s, c) & = d(s, c) + d(c, b) + d(s, c) + d(c, a) \\\\ & < d(s, x) + d(s, c) + d(c, b). \\end{aligned} $$ I claim that $d(x, b) = d(s, x) + d(s, b)$. If not, then by the triangle inequality we must have a strict less-than. In other words, there is some path from $x$ to $b$ which does not go through $c$. This gives the contradiction, because it implies there is a cycle formed by concatenating these paths. Then we have $$d(a, b) < d(a, b) + 2d(s, c) < d(x, b).$$ Since it is assumed that $d(a, b)$ is maximal among all pairs, we have a contradiction. Therefore, since trees have $|V| - 1$ edges, we can run $\\text{BFS}$ a single time in $O(V)$ to obtain one of the vertices which is the endpoint of the longest simple path contained in the graph. Running $\\text{BFS}$ again will show us where the other one is, so we can solve the diameter problem for trees in $O(V)$.","title":"22.2-8 $\\star$"},{"location":"Chap22/22.2/#222-9","text":"Let $G = (V, E)$ be a connected, undirected graph. Give an $O(V + E)$-time algorithm to compute a path in $G$ that traverses each edge in $E$ exactly once in each direction. Describe how you can find your way out of a maze if you are given a large supply of pennies. First, the algorithm computes a minimum spanning tree of the graph. Note that this can be done using the procedures of Chapter 23. It can also be done by performing a breadth first search, and restricting to the edges between $v$ and $v.\\pi$ for every $v$. To aide in not double counting edges, fix any ordering $\\le$ on the vertices before hand. Then, we will construct the sequence of steps by calling $\\text{MAKE-PATH}(s)$, where $s$ was the root used for the $\\text{BFS}$. MAKE - PATH ( u ) for each v \u2208 Adj [ u ] but not in the tree such that u \u2264 v go to v and back to u for each v \u2208 Adj [ u ] but not equal to u . \u03c0 go to v perform the path proscribed by MAKE - PATH ( v ) go to u . \u03c0","title":"22.2-9"},{"location":"Chap22/22.3/","text":"22.3-1 Make a $3$-by-$3$ chart with row and column labels $\\text{WHITE}$, $\\text{GRAY}$, and $\\text{BLACK}$. In each cell $(i, j)$, indicate whether, at any point during a depth-first search of a directed graph, there can be an edge from a vertex of color $i$ to a vertex of color $j$. For each possible edge, indicate what edge types it can be. Make a second such chart for depth-first search of an undirected graph. According to Theorem 22.7 (Parenthesis theorem), there are 3 cases of relationship between intervals of vertex $u$ and $v$: $[u.d, u.f]$ and $[v.d, v.f]$ are entirely disjointed, $[u.d, u.f] \\subset [v.d, v.f]$, and $[v.d, v.f] \\subset [u.d, u.f]$. We judge the possibility according to this Theorem. For directed graph , we can use the edge classification given by exercise 22.3-5 to simplify the problem. $$ \\begin{array}{c|ccc} from \\diagdown to & \\text{WHITE} & \\text{GRAY} & \\text{BLACK} \\\\ \\hline \\text{WHITE} & \\text{All kinds} & \\text{Cross, Back} & \\text{Cross} \\\\ \\text{GRAY} & \\text{Tree, Forward} & \\text{Tree, Forward, Back} & \\text{Tree, Forward, Cross} \\\\ \\text{BLACK} & - & \\text{Back} & \\text{All kinds} \\end{array} $$ For undirected graph , starting from directed chart, we remove the forward edge and the cross edge, and when a back edge exist, we add a tree edge; when a tree edge exist, we add a back edge. This is correct for the following reasons: Theorem 22.10: In a depth-first search of an undirected graph $G$, every edge of $G$ is either a tree or back edge. So tree and back edge only. If $(u, v)$ is a tree edge from $u$'s perspective, $(u, v)$ is also a back edge from $v$'s perspective. $$ \\begin{array}{c|ccc} from \\diagdown to & \\text{WHITE} & \\text{GRAY} & \\text{BLACK} \\\\ \\hline \\text{WHITE} & - & \\text{Tree, Back} & \\text{Tree, Back} \\\\ \\text{GRAY} & \\text{Tree, Back} & \\text{Tree, Back} & \\text{Tree, Back} \\\\ \\text{BLACK} & \\text{Tree, Back} & \\text{Tree, Back} & - \\end{array} $$ 22.3-2 Show how depth-first search works on the graph of Figure 22.6. Assume that the for loop of lines 5\u20137 of the $\\text{DFS}$ procedure considers the vertices in alphabetical order, and assume that each adjacency list is ordered alphabetically. Show the discovery and finishing times for each vertex, and show the classification of each edge. The following table gives the discovery time and finish time for each vetex in the graph. See the C++ demo . $$ \\begin{array}{ccc} \\text{Vertex} & \\text{Discovered} & \\text{Finished} \\\\ \\hline q & 1 & 16 \\\\ r & 17 & 20 \\\\ s & 2 & 7 \\\\ t & 8 & 15 \\\\ u & 18 & 19 \\\\ v & 3 & 6 \\\\ w & 4 & 5 \\\\ x & 9 & 12 \\\\ y & 13 & 14 \\\\ z & 10 & 11 \\end{array} $$ Tree edges: $(q, s)$, $(s, v)$, $(v, w)$, $(q, t)$, $(t, x)$, $(x, z)$, $(t, y)$, $(r, u)$. Back edges: $(w, s)$, $(z, x)$, $(y, q)$. Forward edges: $(q, w)$. Cross edges: $(r, y)$, $(u, y)$. 22.3-3 Show the parenthesis structure of the depth-first search of Figure 22.4. The parentheses structure of the depth-first search of Figure 22.4 is $(u(v(y(xx)y)v)u)(w(zz)w)$. 22.3-4 Show that using a single bit to store each vertex color suffices by arguing that the $\\text{DFS}$ procedure would produce the same result if line 3 of $\\text{DFS-VISIT}$ was removed. Change line 3 to color = BLACK and remove line 8. Then, the algorithm would produce the same result. 22.3-5 Show that edge $(u, v)$ is a. a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$, b. a back edge if and only if $v.d \\le u.d < u.f \\le v.f$, and c. a cross edge if and only if $v.d < v.f < u.d < u.f$. a. $u$ is an ancestor of $v$. b. $u$ is a descendant of $v$. c. $v$ is visited before $u$. 22.3-6 Show that in an undirected graph, classifying an edge $(u, v)$ as a tree edge or a back edge according to whether $(u, v)$ or $(v, u)$ is encountered first during the depth-first search is equivalent to classifying it according to the ordering of the four types in the classification scheme. By Theorem 22.10, every edge of an undirected graph is either a tree edge or a back edge. First suppose that $v$ is first discovered by exploring edge $(u, v)$. Then by definition, $(u, v)$ is a tree edge. Moreover, $(u, v)$ must have been discovered before $(v, u)$ because once $(v, u)$ is explored, $v$ is necessarily discovered. Now suppose that $v$ isn't first discovered by $(u, v)$. Then it must be discovered by $(r, v)$ for some $r\\ne u$. If $u$ hasn't yet been discovered then if $(u, v)$ is explored first, it must be a back edge since $v$ is an ancestor of $u$. If $u$ has been discovered then $u$ is an ancestor of $v$, so $(v, u)$ is a back edge. 22.3-7 Rewrite the procedure $\\text{DFS}$, using a stack to eliminate recursion. See the C++ demo . Also, see this issue for @i-to 's discussion. DFS - STACK ( G ) for each vertex u \u2208 G . V u . color = WHITE u . \u03c0 = NIL time = 0 for each vertex u \u2208 G . V if u . color == WHITE DFS - VISIT - STACK ( G , u ) DFS - VISIT - STACK ( G , u ) S = \u00d8 PUSH ( S , u ) time = time + 1 // white vertex u has just been discovered u . d = time u . color = GRAY while ! STACK - EMPTY ( S ) u = TOP ( S ) v = FIRST - WHITE - NEIGHBOR ( G , u ) if v == NIL // u's adjacency list has been fully explored POP ( S ) time = time + 1 u . f = time u . color = BLACK // blackend u; it is finished else // u's adjacency list hasn't been fully explored v . \u03c0 = u time = time + 1 v . d = time v . color = GRAY PUSH ( S , v ) FIRST - WHITE - NEIGHBOR ( G , u ) for each vertex v \u2208 G . Adj [ u ] if v . color == WHITE return v return NIL 22.3-8 Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, and if $u.d < v.d$ in a depth-first search of $G$, then $v$ is a descendant of $u$ in the depth-first forest produced. Consider a graph with $3$ vertices $u$, $v$, and $w$, and with edges $(w, u)$, $(u, w)$, and $(w, v)$. Suppose that $\\text{DFS}$ first explores $w$, and that $w$'s adjacency list has $u$ before $v$. We next discover $u$. The only adjacent vertex is $w$, but $w$ is already grey, so $u$ finishes. Since $v$ is not yet a descendant of $u$ and $u$ is finished, $v$ can never be a descendant of $u$. 22.3-9 Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, then any depth-first search must result in $v.d \\le u.f$. Consider the directed graph on the vertices $\\{1, 2, 3\\}$, and having the edges $(1, 2)$, $(1, 3)$, $(2, 1)$ then there is a path from $2$ to $3$. However, if we start a $\\text{DFS}$ at $1$ and process $2$ before $3$, we will have $2.f = 3 < 4 = 3.d$ which provides a counterexample to the given conjecture. 22.3-10 Modify the pseudocode for depth-first search so that it prints out every edge in the directed graph $G$, together with its type. Show what modifications, if any, you need to make if $G$ is undirected. If $G$ is undirected we don't need to make any modifications. See the C++ demo . DFS - VISIT - PRINT ( G , u ) time = time + 1 u . d = time u . color = GRAY for each vertex v \u2208 G . Adj [ u ] if v . color == WHITE print \"(u, v) is a tree edge.\" v . \u03c0 = u DFS - VISIT - PRINT ( G , v ) else if v . color == GRAY print \"(u, v) is a back edge.\" else if v . d > u . d print \"(u, v) is a forward edge.\" else print \"(u, v) is a cross edge.\" u . color = BLACK time = time + 1 u . f = time 22.3-11 Explain how a vertex $u$ of a directed graph can end up in a depth-first tree containing only $u$, even though $u$ has both incoming and outgoing edges in $G$. Suppose that we have a directed graph on the vertices $\\{1, 2, 3\\}$ and having edges $(1, 2)$ and $(2, 3)$. Then, $2$ has both incoming and outgoing edges. If we pick our first root to be $3$, that will be in its own $\\text{DFS}$ tree. Then, we pick our second root to be $2$, since the only thing it points to has already been marked $\\text{BLACK}$, we won't be exploring it. Then, picking the last root to be $1$, we don't screw up the fact that $2$ is along in a $\\text{DFS}$ tree even though it has both an incoming and outgoing edge in $G$. 22.3-12 Show that we can use a depth-first search of an undirected graph $G$ to identify the connected components of $G$, and that the depth-first forest contains as many trees as $G$ has connected components. More precisely, show how to modify depth-first search so that it assigns to each vertex $v$ an integer label $v.cc$ between $1$ and $k$, where $k$ is the number of connected components of $G$, such that $u.cc = v.cc$ if and only if $u$ and $v$ are in the same connected component. The modifications work as follows: each time the if -condition of line 8 is satisfied in $\\text{DFS-CC}$, we have a new root of a tree in the forest, so we update its $cc$ label to be a new value of $k$. In the recursive calls to $\\text{DFS-VISIT-CC}$, we always update a descendant's connected component to agree with its ancestor's. See the C++ demo . DFS - CC ( G ) for each vertex u \u2208 G . V u . color = WHITE u . \u03c0 = NIL time = 0 cc = 1 for each vertex u \u2208 G . V if u . color == WHITE u . cc = cc cc = cc + 1 DFS - VISIT - CC ( G , u ) DFS - VISIT - CC ( G , u ) time = time + 1 u . d = time u . color = GRAY for each vertex v \u2208 G . Adj [ u ] if v . color == WHITE v . cc = u . cc v . \u03c0 = u DFS - VISIT - CC ( G , v ) u . color = BLACK time = time + 1 u . f = time 22.3-13 $\\star$ A directed graph $G = (V, E)$ is singly connected if $u \\leadsto v$ implies that $G$ contains at most one simple path from $u$ to $v$ for all vertices $u, v \\in V$. Give an efficient algorithm to determine whether or not a directed graph is singly connected. This can be done in time $O(|V||E|)$. To do this, first perform a topological sort of the vertices. Then, we will contain for each vertex a list of it's ancestors with $in\\text-degree$ $0$. We compute these lists for each vertex in the order starting from the earlier ones topologically. Then, if we ever have a vertex that has the same degree $0$ vertex appearing in the lists of two of its immediate parents, we know that the graph is not singly connected. however, if at each step we have that at each step all of the parents have disjoint sets of degree $0$ vertices as ancestors, the graph is singly connected. Since, for each vertex, the amount of time required is bounded by the number of vertices times the $in\\text-degree$ of the particular vertex, the total runtime is bounded by $O(|V||E|)$.","title":"22.3 Depth-first search"},{"location":"Chap22/22.3/#223-1","text":"Make a $3$-by-$3$ chart with row and column labels $\\text{WHITE}$, $\\text{GRAY}$, and $\\text{BLACK}$. In each cell $(i, j)$, indicate whether, at any point during a depth-first search of a directed graph, there can be an edge from a vertex of color $i$ to a vertex of color $j$. For each possible edge, indicate what edge types it can be. Make a second such chart for depth-first search of an undirected graph. According to Theorem 22.7 (Parenthesis theorem), there are 3 cases of relationship between intervals of vertex $u$ and $v$: $[u.d, u.f]$ and $[v.d, v.f]$ are entirely disjointed, $[u.d, u.f] \\subset [v.d, v.f]$, and $[v.d, v.f] \\subset [u.d, u.f]$. We judge the possibility according to this Theorem. For directed graph , we can use the edge classification given by exercise 22.3-5 to simplify the problem. $$ \\begin{array}{c|ccc} from \\diagdown to & \\text{WHITE} & \\text{GRAY} & \\text{BLACK} \\\\ \\hline \\text{WHITE} & \\text{All kinds} & \\text{Cross, Back} & \\text{Cross} \\\\ \\text{GRAY} & \\text{Tree, Forward} & \\text{Tree, Forward, Back} & \\text{Tree, Forward, Cross} \\\\ \\text{BLACK} & - & \\text{Back} & \\text{All kinds} \\end{array} $$ For undirected graph , starting from directed chart, we remove the forward edge and the cross edge, and when a back edge exist, we add a tree edge; when a tree edge exist, we add a back edge. This is correct for the following reasons: Theorem 22.10: In a depth-first search of an undirected graph $G$, every edge of $G$ is either a tree or back edge. So tree and back edge only. If $(u, v)$ is a tree edge from $u$'s perspective, $(u, v)$ is also a back edge from $v$'s perspective. $$ \\begin{array}{c|ccc} from \\diagdown to & \\text{WHITE} & \\text{GRAY} & \\text{BLACK} \\\\ \\hline \\text{WHITE} & - & \\text{Tree, Back} & \\text{Tree, Back} \\\\ \\text{GRAY} & \\text{Tree, Back} & \\text{Tree, Back} & \\text{Tree, Back} \\\\ \\text{BLACK} & \\text{Tree, Back} & \\text{Tree, Back} & - \\end{array} $$","title":"22.3-1"},{"location":"Chap22/22.3/#223-2","text":"Show how depth-first search works on the graph of Figure 22.6. Assume that the for loop of lines 5\u20137 of the $\\text{DFS}$ procedure considers the vertices in alphabetical order, and assume that each adjacency list is ordered alphabetically. Show the discovery and finishing times for each vertex, and show the classification of each edge. The following table gives the discovery time and finish time for each vetex in the graph. See the C++ demo . $$ \\begin{array}{ccc} \\text{Vertex} & \\text{Discovered} & \\text{Finished} \\\\ \\hline q & 1 & 16 \\\\ r & 17 & 20 \\\\ s & 2 & 7 \\\\ t & 8 & 15 \\\\ u & 18 & 19 \\\\ v & 3 & 6 \\\\ w & 4 & 5 \\\\ x & 9 & 12 \\\\ y & 13 & 14 \\\\ z & 10 & 11 \\end{array} $$ Tree edges: $(q, s)$, $(s, v)$, $(v, w)$, $(q, t)$, $(t, x)$, $(x, z)$, $(t, y)$, $(r, u)$. Back edges: $(w, s)$, $(z, x)$, $(y, q)$. Forward edges: $(q, w)$. Cross edges: $(r, y)$, $(u, y)$.","title":"22.3-2"},{"location":"Chap22/22.3/#223-3","text":"Show the parenthesis structure of the depth-first search of Figure 22.4. The parentheses structure of the depth-first search of Figure 22.4 is $(u(v(y(xx)y)v)u)(w(zz)w)$.","title":"22.3-3"},{"location":"Chap22/22.3/#223-4","text":"Show that using a single bit to store each vertex color suffices by arguing that the $\\text{DFS}$ procedure would produce the same result if line 3 of $\\text{DFS-VISIT}$ was removed. Change line 3 to color = BLACK and remove line 8. Then, the algorithm would produce the same result.","title":"22.3-4"},{"location":"Chap22/22.3/#223-5","text":"Show that edge $(u, v)$ is a. a tree edge or forward edge if and only if $u.d < v.d < v.f < u.f$, b. a back edge if and only if $v.d \\le u.d < u.f \\le v.f$, and c. a cross edge if and only if $v.d < v.f < u.d < u.f$. a. $u$ is an ancestor of $v$. b. $u$ is a descendant of $v$. c. $v$ is visited before $u$.","title":"22.3-5"},{"location":"Chap22/22.3/#223-6","text":"Show that in an undirected graph, classifying an edge $(u, v)$ as a tree edge or a back edge according to whether $(u, v)$ or $(v, u)$ is encountered first during the depth-first search is equivalent to classifying it according to the ordering of the four types in the classification scheme. By Theorem 22.10, every edge of an undirected graph is either a tree edge or a back edge. First suppose that $v$ is first discovered by exploring edge $(u, v)$. Then by definition, $(u, v)$ is a tree edge. Moreover, $(u, v)$ must have been discovered before $(v, u)$ because once $(v, u)$ is explored, $v$ is necessarily discovered. Now suppose that $v$ isn't first discovered by $(u, v)$. Then it must be discovered by $(r, v)$ for some $r\\ne u$. If $u$ hasn't yet been discovered then if $(u, v)$ is explored first, it must be a back edge since $v$ is an ancestor of $u$. If $u$ has been discovered then $u$ is an ancestor of $v$, so $(v, u)$ is a back edge.","title":"22.3-6"},{"location":"Chap22/22.3/#223-7","text":"Rewrite the procedure $\\text{DFS}$, using a stack to eliminate recursion. See the C++ demo . Also, see this issue for @i-to 's discussion. DFS - STACK ( G ) for each vertex u \u2208 G . V u . color = WHITE u . \u03c0 = NIL time = 0 for each vertex u \u2208 G . V if u . color == WHITE DFS - VISIT - STACK ( G , u ) DFS - VISIT - STACK ( G , u ) S = \u00d8 PUSH ( S , u ) time = time + 1 // white vertex u has just been discovered u . d = time u . color = GRAY while ! STACK - EMPTY ( S ) u = TOP ( S ) v = FIRST - WHITE - NEIGHBOR ( G , u ) if v == NIL // u's adjacency list has been fully explored POP ( S ) time = time + 1 u . f = time u . color = BLACK // blackend u; it is finished else // u's adjacency list hasn't been fully explored v . \u03c0 = u time = time + 1 v . d = time v . color = GRAY PUSH ( S , v ) FIRST - WHITE - NEIGHBOR ( G , u ) for each vertex v \u2208 G . Adj [ u ] if v . color == WHITE return v return NIL","title":"22.3-7"},{"location":"Chap22/22.3/#223-8","text":"Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, and if $u.d < v.d$ in a depth-first search of $G$, then $v$ is a descendant of $u$ in the depth-first forest produced. Consider a graph with $3$ vertices $u$, $v$, and $w$, and with edges $(w, u)$, $(u, w)$, and $(w, v)$. Suppose that $\\text{DFS}$ first explores $w$, and that $w$'s adjacency list has $u$ before $v$. We next discover $u$. The only adjacent vertex is $w$, but $w$ is already grey, so $u$ finishes. Since $v$ is not yet a descendant of $u$ and $u$ is finished, $v$ can never be a descendant of $u$.","title":"22.3-8"},{"location":"Chap22/22.3/#223-9","text":"Give a counterexample to the conjecture that if a directed graph $G$ contains a path from $u$ to $v$, then any depth-first search must result in $v.d \\le u.f$. Consider the directed graph on the vertices $\\{1, 2, 3\\}$, and having the edges $(1, 2)$, $(1, 3)$, $(2, 1)$ then there is a path from $2$ to $3$. However, if we start a $\\text{DFS}$ at $1$ and process $2$ before $3$, we will have $2.f = 3 < 4 = 3.d$ which provides a counterexample to the given conjecture.","title":"22.3-9"},{"location":"Chap22/22.3/#223-10","text":"Modify the pseudocode for depth-first search so that it prints out every edge in the directed graph $G$, together with its type. Show what modifications, if any, you need to make if $G$ is undirected. If $G$ is undirected we don't need to make any modifications. See the C++ demo . DFS - VISIT - PRINT ( G , u ) time = time + 1 u . d = time u . color = GRAY for each vertex v \u2208 G . Adj [ u ] if v . color == WHITE print \"(u, v) is a tree edge.\" v . \u03c0 = u DFS - VISIT - PRINT ( G , v ) else if v . color == GRAY print \"(u, v) is a back edge.\" else if v . d > u . d print \"(u, v) is a forward edge.\" else print \"(u, v) is a cross edge.\" u . color = BLACK time = time + 1 u . f = time","title":"22.3-10"},{"location":"Chap22/22.3/#223-11","text":"Explain how a vertex $u$ of a directed graph can end up in a depth-first tree containing only $u$, even though $u$ has both incoming and outgoing edges in $G$. Suppose that we have a directed graph on the vertices $\\{1, 2, 3\\}$ and having edges $(1, 2)$ and $(2, 3)$. Then, $2$ has both incoming and outgoing edges. If we pick our first root to be $3$, that will be in its own $\\text{DFS}$ tree. Then, we pick our second root to be $2$, since the only thing it points to has already been marked $\\text{BLACK}$, we won't be exploring it. Then, picking the last root to be $1$, we don't screw up the fact that $2$ is along in a $\\text{DFS}$ tree even though it has both an incoming and outgoing edge in $G$.","title":"22.3-11"},{"location":"Chap22/22.3/#223-12","text":"Show that we can use a depth-first search of an undirected graph $G$ to identify the connected components of $G$, and that the depth-first forest contains as many trees as $G$ has connected components. More precisely, show how to modify depth-first search so that it assigns to each vertex $v$ an integer label $v.cc$ between $1$ and $k$, where $k$ is the number of connected components of $G$, such that $u.cc = v.cc$ if and only if $u$ and $v$ are in the same connected component. The modifications work as follows: each time the if -condition of line 8 is satisfied in $\\text{DFS-CC}$, we have a new root of a tree in the forest, so we update its $cc$ label to be a new value of $k$. In the recursive calls to $\\text{DFS-VISIT-CC}$, we always update a descendant's connected component to agree with its ancestor's. See the C++ demo . DFS - CC ( G ) for each vertex u \u2208 G . V u . color = WHITE u . \u03c0 = NIL time = 0 cc = 1 for each vertex u \u2208 G . V if u . color == WHITE u . cc = cc cc = cc + 1 DFS - VISIT - CC ( G , u ) DFS - VISIT - CC ( G , u ) time = time + 1 u . d = time u . color = GRAY for each vertex v \u2208 G . Adj [ u ] if v . color == WHITE v . cc = u . cc v . \u03c0 = u DFS - VISIT - CC ( G , v ) u . color = BLACK time = time + 1 u . f = time","title":"22.3-12"},{"location":"Chap22/22.3/#223-13-star","text":"A directed graph $G = (V, E)$ is singly connected if $u \\leadsto v$ implies that $G$ contains at most one simple path from $u$ to $v$ for all vertices $u, v \\in V$. Give an efficient algorithm to determine whether or not a directed graph is singly connected. This can be done in time $O(|V||E|)$. To do this, first perform a topological sort of the vertices. Then, we will contain for each vertex a list of it's ancestors with $in\\text-degree$ $0$. We compute these lists for each vertex in the order starting from the earlier ones topologically. Then, if we ever have a vertex that has the same degree $0$ vertex appearing in the lists of two of its immediate parents, we know that the graph is not singly connected. however, if at each step we have that at each step all of the parents have disjoint sets of degree $0$ vertices as ancestors, the graph is singly connected. Since, for each vertex, the amount of time required is bounded by the number of vertices times the $in\\text-degree$ of the particular vertex, the total runtime is bounded by $O(|V||E|)$.","title":"22.3-13 $\\star$"},{"location":"Chap22/22.4/","text":"22.4-1 Show the ordering of vertices produced by $\\text{TOPOLOGICAL-SORT}$ when it is run on the dag of Figure 22.8, under the assumption of Exercise 22.3-2. Our start and finish times from performing the $\\text{DFS}$ are $$ \\begin{array}{ccc} \\text{label} & d & f \\\\ \\hline m & 1 & 20 \\\\ q & 2 & 5 \\\\ t & 3 & 4 \\\\ r & 6 & 19 \\\\ u & 7 & 8 \\\\ y & 9 & 18 \\\\ v & 10 & 17 \\\\ w & 11 & 14 \\\\ z & 12 & 13 \\\\ x & 15 & 16 \\\\ n & 21 & 26 \\\\ o & 22 & 25 \\\\ s & 23 & 24 \\\\ p & 27 & 28 \\end{array} $$ And so, by reading off the entries in decreasing order of finish time, we have the sequence $p, n, o, s, m, r, y, v, x, w, z, u, q, t$. 22.4-2 Give a linear-time algorithm that takes as input a directed acyclic graph $G = (V, E)$ and two vertices $s$ and $t$, and returns the number of simple paths from $s$ to $t$ in $G$. For example, the directed acyclic graph of Figure 22.8 contains exactly four simple paths from vertex $p$ to vertex $v: pov$, $poryv$, $posryv$, and $psryv$. (Your algorithm needs only to count the simple paths, not list them.) The algorithm works as follows. The attribute $u.paths$ of node $u$ tells the number of simple paths from $u$ to $v$, where we assume that $v$ is fixed throughout the entire process. First of all, a topo sort should be conducted and list the vertex between $u$, $v$ as $\\{v[1], v[2], \\dots, v[k - 1]\\}$. To count the number of paths, we should construct a solution from $v$ to $u$. Let's call $u$ as $v[0]$ and $v$ as $v[k]$, to avoid overlapping subproblem, the number of paths between $v_k$ and $u$ should be remembered and used as $k$ decrease to $0$. Only in this way can we solve the problem in $\\Theta(V + E)$. An bottom-up iterative version is possible only if the graph uses adjacency matrix so whether $v$ is adjacency to $u$ can be determined in $O(1)$ time. But building a adjacency matrix would cost $\\Theta(|V|^2)$, so never mind. SIMPLE - PATHS ( G , u , v ) TOPO - SORT ( G ) let { v [ 1 ], v [ 2 ].. v [ k - 1 ]} be the vertex between u and v v [ 0 ] = u v [ k ] = v for j = 0 to k - 1 DP [ j ] = \u221e DP [ k ] = 1 return SIMPLE - PATHS - AID ( G , DP , 0 ) SIMPLE - PATHS - AID ( G , DP , i ) if i > k return 0 else if DP [ i ] != \u221e return DP [ i ] else DP [ i ] = 0 for v [ m ] in G . adj [ v [ i ]] and 0 < m \u2264 k DP [ i ] += SIMPLE - PATHS - AID ( G , DP , m ) return DP [ i ] 22.4-3 Give an algorithm that determines whether or not a given undirected graph $G = (V, E)$ contains a cycle. Your algorithm should run in $O(V)$ time, independent of $|E|$. (Removed) 22.4-4 Prove or disprove: If a directed graph $G$ contains cycles, then $\\text{TOPOLOGICAL-SORT}(G)$ produces a vertex ordering that minimizes the number of \"bad\" edges that are inconsistent with the ordering produced. This is not true. Consider the graph $G$ consisting of vertices $a, b, c$, and $d$. Let the edges be $(a, b)$, $(b, c)$, $(a, d)$, $(d, c)$, and $(c, a)$. Suppose that we start the $\\text{DFS}$ of $\\text{TOPOLOGICAL-SORT}$ at vertex $c$. Assuming that $b$ appears before $d$ in the adjacency list of $a$, the order, from latest to earliest, of finish times is $c, a, d, b$. The \"bad\" edges in this case are $(b, c)$ and $(d, c)$. However, if we had instead ordered them by $a, b, d, c$ then the only bad edges would be $(c, a)$. Thus $\\text{TOPOLOGICAL-SORT}$ doesn't always minimizes the number of \"bad\" edges 22.4-5 Another way to perform topological sorting on a directed acyclic graph $G = (V, E)$ is to repeatedly find a vertex of $\\text{in-degree}$ $0$, output it, and remove it and all of its outgoing edges from the graph. Explain how to implement this idea so that it runs in time $O(V + E)$. What happens to this algorithm if $G$ has cycles? (Removed)","title":"22.4 Topological sort"},{"location":"Chap22/22.4/#224-1","text":"Show the ordering of vertices produced by $\\text{TOPOLOGICAL-SORT}$ when it is run on the dag of Figure 22.8, under the assumption of Exercise 22.3-2. Our start and finish times from performing the $\\text{DFS}$ are $$ \\begin{array}{ccc} \\text{label} & d & f \\\\ \\hline m & 1 & 20 \\\\ q & 2 & 5 \\\\ t & 3 & 4 \\\\ r & 6 & 19 \\\\ u & 7 & 8 \\\\ y & 9 & 18 \\\\ v & 10 & 17 \\\\ w & 11 & 14 \\\\ z & 12 & 13 \\\\ x & 15 & 16 \\\\ n & 21 & 26 \\\\ o & 22 & 25 \\\\ s & 23 & 24 \\\\ p & 27 & 28 \\end{array} $$ And so, by reading off the entries in decreasing order of finish time, we have the sequence $p, n, o, s, m, r, y, v, x, w, z, u, q, t$.","title":"22.4-1"},{"location":"Chap22/22.4/#224-2","text":"Give a linear-time algorithm that takes as input a directed acyclic graph $G = (V, E)$ and two vertices $s$ and $t$, and returns the number of simple paths from $s$ to $t$ in $G$. For example, the directed acyclic graph of Figure 22.8 contains exactly four simple paths from vertex $p$ to vertex $v: pov$, $poryv$, $posryv$, and $psryv$. (Your algorithm needs only to count the simple paths, not list them.) The algorithm works as follows. The attribute $u.paths$ of node $u$ tells the number of simple paths from $u$ to $v$, where we assume that $v$ is fixed throughout the entire process. First of all, a topo sort should be conducted and list the vertex between $u$, $v$ as $\\{v[1], v[2], \\dots, v[k - 1]\\}$. To count the number of paths, we should construct a solution from $v$ to $u$. Let's call $u$ as $v[0]$ and $v$ as $v[k]$, to avoid overlapping subproblem, the number of paths between $v_k$ and $u$ should be remembered and used as $k$ decrease to $0$. Only in this way can we solve the problem in $\\Theta(V + E)$. An bottom-up iterative version is possible only if the graph uses adjacency matrix so whether $v$ is adjacency to $u$ can be determined in $O(1)$ time. But building a adjacency matrix would cost $\\Theta(|V|^2)$, so never mind. SIMPLE - PATHS ( G , u , v ) TOPO - SORT ( G ) let { v [ 1 ], v [ 2 ].. v [ k - 1 ]} be the vertex between u and v v [ 0 ] = u v [ k ] = v for j = 0 to k - 1 DP [ j ] = \u221e DP [ k ] = 1 return SIMPLE - PATHS - AID ( G , DP , 0 ) SIMPLE - PATHS - AID ( G , DP , i ) if i > k return 0 else if DP [ i ] != \u221e return DP [ i ] else DP [ i ] = 0 for v [ m ] in G . adj [ v [ i ]] and 0 < m \u2264 k DP [ i ] += SIMPLE - PATHS - AID ( G , DP , m ) return DP [ i ]","title":"22.4-2"},{"location":"Chap22/22.4/#224-3","text":"Give an algorithm that determines whether or not a given undirected graph $G = (V, E)$ contains a cycle. Your algorithm should run in $O(V)$ time, independent of $|E|$. (Removed)","title":"22.4-3"},{"location":"Chap22/22.4/#224-4","text":"Prove or disprove: If a directed graph $G$ contains cycles, then $\\text{TOPOLOGICAL-SORT}(G)$ produces a vertex ordering that minimizes the number of \"bad\" edges that are inconsistent with the ordering produced. This is not true. Consider the graph $G$ consisting of vertices $a, b, c$, and $d$. Let the edges be $(a, b)$, $(b, c)$, $(a, d)$, $(d, c)$, and $(c, a)$. Suppose that we start the $\\text{DFS}$ of $\\text{TOPOLOGICAL-SORT}$ at vertex $c$. Assuming that $b$ appears before $d$ in the adjacency list of $a$, the order, from latest to earliest, of finish times is $c, a, d, b$. The \"bad\" edges in this case are $(b, c)$ and $(d, c)$. However, if we had instead ordered them by $a, b, d, c$ then the only bad edges would be $(c, a)$. Thus $\\text{TOPOLOGICAL-SORT}$ doesn't always minimizes the number of \"bad\" edges","title":"22.4-4"},{"location":"Chap22/22.4/#224-5","text":"Another way to perform topological sorting on a directed acyclic graph $G = (V, E)$ is to repeatedly find a vertex of $\\text{in-degree}$ $0$, output it, and remove it and all of its outgoing edges from the graph. Explain how to implement this idea so that it runs in time $O(V + E)$. What happens to this algorithm if $G$ has cycles? (Removed)","title":"22.4-5"},{"location":"Chap22/22.5/","text":"22.5-1 How can the number of strongly connected components of a graph change if a new edge is added? It can either stay the same or decrease. To see that it is possible to stay the same, just suppose you add some edge to a cycle. To see that it is possible to decrease, suppose that your original graph is on three vertices, and is just a path passing through all of them, and the edge added completes this path to a cycle. To see that it cannot increase, notice that adding an edge cannot remove any path that existed before. So, if $u$ and $v$ are in the same connected component in the original graph, then there are a path from one to the other, in both directions. Adding an edge wont disturb these two paths, so we know that $u$ and $v$ will still be in the same $\\text{SCC}$ in the graph after adding the edge. Since no components can be split apart, this means that the number of them cannot increase since they form a partition of the set of vertices. 22.5-2 Show how the procedure $\\text{STRONGLY-CONNECTED-COMPONENTS}$ works on the graph of Figure 22.6. Specifically, show the finishing times computed in line 1 and the forest produced in line 3. Assume that the loop of lines 5\u20137 of $\\text{DFS}$ considers vertices in alphabetical order and that the adjacency lists are in alphabetical order. The finishing times of each vertex were computed in exercise 22.3-2. The forest consists of 5 trees, each of which is a chain. We'll list the vertices of each tree in order from root to leaf: $r$, $u$, $q - y - t$, $x - z$, and $s - w - v$. 22.5-3 Professor Bacon claims that the algorithm for strongly connected components would be simpler if it used the original (instead of the transpose) graph in the second depth-first search and scanned the vertices in order of increasing finishing times. Does this simpler algorithm always produce correct results? Professor Bacon's suggestion doesn't work out. As an example, suppose that our graph is on the three vertices $\\{1, 2, 3\\}$ and consists of the edges $(2, 1), (2, 3), (3, 2)$. Then, we should end up with $\\{2, 3\\}$ and $\\{1\\}$ as our $\\text{SCC}$'s. However, a possible $\\text{DFS}$ starting at $2$ could explore $3$ before $1$, this would mean that the finish time of $3$ is lower than of $1$ and $2$. This means that when we first perform the $\\text{DFS}$ starting at $3$. However, a $\\text{DFS}$ starting at $3$ will be able to reach all other vertices. This means that the algorithm would return that the entire graph is a single $\\text{SCC}$, even though this is clearly not the case since there is neither a path from $1$ to $2$ of from $1$ to $3$. 22.5-4 Prove that for any directed graph $G$, we have $((G^\\text T)^{\\text{SCC}})^\\text T = G^{\\text{SCC}}$. That is, the transpose of the component graph of $G^\\text T$ is the same as the component graph of $G$. First observe that $C$ is a strongly connected component of $G$ if and only if it is a strongly connected component of $G^\\text T$. Thus the vertex sets of $G^{\\text{SCC}}$ and $(G^\\text T)^{\\text{SCC}}$ are the same, which implies the vertex sets of $((G^\\text T)^\\text{SCC})^\\text T$ and $G^{\\text{SCC}}$ are the same. It suffices to show that their edge sets are the same. Suppose $(v_i, v_j)$ is an edge in $((G^\\text T)^{\\text{SCC}})^\\text T$. Then $(v_j, v_i)$ is an edge in $(G^\\text T)^{\\text{SCC}}$. Thus there exist $x \\in C_j$ and $y \\in C_i$ such that $(x, y)$ is an edge of $G^\\text T$, which implies $(y, x)$ is an edge of $G$. Since components are preserved, this means that $(v_i, v_j)$ is an edge in $G^{\\text{SCC}}$. For the opposite implication we simply note that for any graph $G$ we have $(G^\\text T)^{\\text T} = G$. 22.5-5 Give an $O(V + E)$-time algorithm to compute the component graph of a directed graph $G = (V, E)$. Make sure that there is at most one edge between two vertices in the component graph your algorithm produces. (Removed) 22.5-6 Given a directed graph $G = (V, E)$, explain how to create another graph $G' = (V, E')$ such that (a) $G'$ has the same strongly connected components as $G$, (b) $G'$ has the same component graph as $G$, and (c) $E'$ is as small as possible. Describe a fast algorithm to compute $G'$. (Removed) 22.5-7 A directed graph $G = (V, E)$ is semiconnected if, for all pairs of vertices $u, v \\in V$, we have $u \\leadsto v$ or $v \\leadsto u$. Give an efficient algorithm to determine whether or not $G$ is semiconnected. Prove that your algorithm is correct, and analyze its running time. (Removed)","title":"22.5 Strongly connected components"},{"location":"Chap22/22.5/#225-1","text":"How can the number of strongly connected components of a graph change if a new edge is added? It can either stay the same or decrease. To see that it is possible to stay the same, just suppose you add some edge to a cycle. To see that it is possible to decrease, suppose that your original graph is on three vertices, and is just a path passing through all of them, and the edge added completes this path to a cycle. To see that it cannot increase, notice that adding an edge cannot remove any path that existed before. So, if $u$ and $v$ are in the same connected component in the original graph, then there are a path from one to the other, in both directions. Adding an edge wont disturb these two paths, so we know that $u$ and $v$ will still be in the same $\\text{SCC}$ in the graph after adding the edge. Since no components can be split apart, this means that the number of them cannot increase since they form a partition of the set of vertices.","title":"22.5-1"},{"location":"Chap22/22.5/#225-2","text":"Show how the procedure $\\text{STRONGLY-CONNECTED-COMPONENTS}$ works on the graph of Figure 22.6. Specifically, show the finishing times computed in line 1 and the forest produced in line 3. Assume that the loop of lines 5\u20137 of $\\text{DFS}$ considers vertices in alphabetical order and that the adjacency lists are in alphabetical order. The finishing times of each vertex were computed in exercise 22.3-2. The forest consists of 5 trees, each of which is a chain. We'll list the vertices of each tree in order from root to leaf: $r$, $u$, $q - y - t$, $x - z$, and $s - w - v$.","title":"22.5-2"},{"location":"Chap22/22.5/#225-3","text":"Professor Bacon claims that the algorithm for strongly connected components would be simpler if it used the original (instead of the transpose) graph in the second depth-first search and scanned the vertices in order of increasing finishing times. Does this simpler algorithm always produce correct results? Professor Bacon's suggestion doesn't work out. As an example, suppose that our graph is on the three vertices $\\{1, 2, 3\\}$ and consists of the edges $(2, 1), (2, 3), (3, 2)$. Then, we should end up with $\\{2, 3\\}$ and $\\{1\\}$ as our $\\text{SCC}$'s. However, a possible $\\text{DFS}$ starting at $2$ could explore $3$ before $1$, this would mean that the finish time of $3$ is lower than of $1$ and $2$. This means that when we first perform the $\\text{DFS}$ starting at $3$. However, a $\\text{DFS}$ starting at $3$ will be able to reach all other vertices. This means that the algorithm would return that the entire graph is a single $\\text{SCC}$, even though this is clearly not the case since there is neither a path from $1$ to $2$ of from $1$ to $3$.","title":"22.5-3"},{"location":"Chap22/22.5/#225-4","text":"Prove that for any directed graph $G$, we have $((G^\\text T)^{\\text{SCC}})^\\text T = G^{\\text{SCC}}$. That is, the transpose of the component graph of $G^\\text T$ is the same as the component graph of $G$. First observe that $C$ is a strongly connected component of $G$ if and only if it is a strongly connected component of $G^\\text T$. Thus the vertex sets of $G^{\\text{SCC}}$ and $(G^\\text T)^{\\text{SCC}}$ are the same, which implies the vertex sets of $((G^\\text T)^\\text{SCC})^\\text T$ and $G^{\\text{SCC}}$ are the same. It suffices to show that their edge sets are the same. Suppose $(v_i, v_j)$ is an edge in $((G^\\text T)^{\\text{SCC}})^\\text T$. Then $(v_j, v_i)$ is an edge in $(G^\\text T)^{\\text{SCC}}$. Thus there exist $x \\in C_j$ and $y \\in C_i$ such that $(x, y)$ is an edge of $G^\\text T$, which implies $(y, x)$ is an edge of $G$. Since components are preserved, this means that $(v_i, v_j)$ is an edge in $G^{\\text{SCC}}$. For the opposite implication we simply note that for any graph $G$ we have $(G^\\text T)^{\\text T} = G$.","title":"22.5-4"},{"location":"Chap22/22.5/#225-5","text":"Give an $O(V + E)$-time algorithm to compute the component graph of a directed graph $G = (V, E)$. Make sure that there is at most one edge between two vertices in the component graph your algorithm produces. (Removed)","title":"22.5-5"},{"location":"Chap22/22.5/#225-6","text":"Given a directed graph $G = (V, E)$, explain how to create another graph $G' = (V, E')$ such that (a) $G'$ has the same strongly connected components as $G$, (b) $G'$ has the same component graph as $G$, and (c) $E'$ is as small as possible. Describe a fast algorithm to compute $G'$. (Removed)","title":"22.5-6"},{"location":"Chap22/22.5/#225-7","text":"A directed graph $G = (V, E)$ is semiconnected if, for all pairs of vertices $u, v \\in V$, we have $u \\leadsto v$ or $v \\leadsto u$. Give an efficient algorithm to determine whether or not $G$ is semiconnected. Prove that your algorithm is correct, and analyze its running time. (Removed)","title":"22.5-7"},{"location":"Chap22/Problems/22-1/","text":"A depth-first forest classifies the edges of a graph into tree, back, forward, and cross edges. A breadth-first tree can also be used to classify the edges reachable from the source of the search into the same four categories. a. Prove that in a breadth-first search of an undirected graph, the following properties hold: There are no back edges and no forward edges. For each tree edge $(u, v)$, we have $v.d = u.d + 1$. For each cross edge $(u, v)$, we have $v.d = u.d$ or $v.d = u.d + 1$. b. Prove that in a breadth-first search of a directed graph, the following properties hold: There are no forward edges. For each tree edge $(u, v)$, we have $v.d = u.d + 1$. For each cross edge $(u, v)$, we have $v.d \\le u.d + 1$. For each back edge $(u, v)$, we have $0 \\le v.d \\le u.d$. a. If we found a back edge, this means that there are two vertices, one a descendant of the other, but there is already a path from the ancestor to the child that doesn\u2019t involve moving up the tree. This is a contradiction since the only children in the bfs tree are those that are a single edge away, which means there cannot be any other paths to that child because that would make it more than a single edge away. To see that there are no forward edges, We do a similar procedure. A forward edge would mean that from a given vertex we notice it has a child that has already been processed, but this cannot happen because all children are only one edge away, and for it to of already been processed, it would need to have gone through some other vertex first. An edge is placed on the list to be processed if it goes to a vertex that has not yet been considered. This means that the path from that vertex to the root must be at least the distance from the current vertex plus $1$. It is also at most that since we can just take the path that consists of going to the current vertex and taking its path to the root. We know that a cross edge cannot be going to a depth more than one less, otherwise it would be used as a tree edge when we were processing that earlier element. It also cannot be going to a vertex of depth more than one more, because we wouldn\u2019t of already processed a vertex that was that much further away from the root. Since the depths of the vertices in the cross edge cannot be more than one apart, the conclusion follows by possibly interchanging the roles of $u$ and $v$, which we can do because the edges are unordered. b. To have a forward edge, we would need to have already processed a vertex using more than one edge, even though there is a path to it using a single edge. Since breadth first search always considers shorter paths first, this is not possible. Suppose that $(u, v)$ is a tree edge. Then, this means that there is a path from the root to $v$ of length $u.d + 1$ by just appending $(u, v)$ on to the path from the root to $u$. To see that there is no shorter path, we just note that we would of processed $v$ sooner, and so wouldn\u2019t currently have a tree edge if there were. To see this, all we need to do is note that there is some path from the root to $v$ of length $u.d + 1$ obtained by appending $(u, v)$ to $v.d$. Since there is a path of that length, it serves as an upper bound on the minimum length of all such paths from the root to $v$. It is trivial that $0 \\le v.d$, since it is impossible to have a path from the root to $v$ of negative length. The more interesting inequality is $v.d \\le u.d$. We know that there is some path from $v$ to $u$, consisting of tree edges, this is the defining property of $(u, v)$ being a back edge. This means that is $v, v_1, v_2, \\dots, v_k, u$ is this path (it is unique because the tree edges form a tree). Then, we have that $u.d = v_k.d + 1 = v_{k \u2212 1}.d + 2 = \\cdots = v_1.d + k = v.d + k + 1$. So, we have that $u.d > v.d$. In fact, we just showed that we have the stronger conclusion, that $0 \\le v.d < u.d$.","title":"22-1 Classifying edges by breadth-first search"},{"location":"Chap22/Problems/22-2/","text":"Let $G = (V, E)$ be a connected, undirected graph. An articulation point of $G$ is a vertex whose removal disconnects $G$. A bridge of $G$ is an edge whose removal disconnects $G$. A biconnected component of $G$ is a maximal set of edges such that any two edges in the set lie on a common simple cycle. Figure 22.10 illustrates these definitions. We can determine articulation points, bridges, and biconnected components using depth-first search. Let $G_\\pi = (V, E_\\pi)$ be a depth-first tree of $G$. a. Prove that the root of $G_\\pi$ is an articulation point of $G$ if and only if it has at least two children in $G_\\pi$. b. Let $v$ be a nonroot vertex of $G_\\pi$. Prove that $v$ is an articulation point of $G$ if and only if $v$ has a child $s$ such that there is no back edge from $s$ or any descendant of $s$ to a proper ancestor of $v$. c. Let $$ v.low = \\min \\begin{cases} v.d, \\\\ w.d:(u,w) \\text{ is a back edge for some descendant } u \\text{ of } v. \\end{cases} $$ Show how to computer $v.low$ for all vertices $v \\in V$ in $O(E)$ time. d. Show how to compute all articulation points in $O(E)$ time. e. Prove that an edge of $G$ is a bridge if and only if it does not lie on any simple cycle of $G$. f. Show how to compute all the bridges of $G$ in $O(E)$ time. g. Prove that the biconnected components of $G$ partition the nonbridge edges of $G$. h. Give an $O(E)$-time algorithm to label each edge $e$ of $G$ with a positive integer $e.bcc$ such that $e.bcc = e'.bcc$ if and only if $e$ and $e'$ are in the same biconnected component. a. First suppose the root $r$ of $G_\\pi$ is an articulation point. Then the removal of $r$ from $G$ would cause the graph to disconnect, so $r$ has at least $2$ children in $G$. If $r$ has only one child $v$ in $G_\\pi$ then it must be the case that there is a path from $v$ to each of $r$'s other children. Since removing $r$ disconnects the graph, there must exist vertices $u$ and $w$ such that the only paths from $u$ to $w$ contain $r$. To reach $r$ from $u$, the path must first reach one of $r$'s children. This child is connect to $v$ via a path which doesn't contain $r$. To reach $w$, the path must also leave $r$ through one of its children, which is also reachable by $v$. This implies that there is a path from $u$ to $w$ which doesn't contain $r$, a contradiction. Now suppose $r$ has at least two children $u$ and $v$ in $G_\\pi$. Then there is no path from $u$ to $v$ in $G$ which doesn't go through $r$, since otherwise $u$ would be an ancestor of $v$. Thus, removing $r$ disconnects the component containing $u$ and the component containing $v$, so $r$ is an articulation point. b. Suppose that $v$ is a nonroot vertex of $G_\\pi$ and that $v$ has a child $s$ such that neither $s$ nor any of $s$'s descendants have back edges to a proper ancestor of $v$. Let $r$ be an ancestor of $v$, and remove $v$ from $G$. Since we are in the undirected case, the only edges in the graph are tree edges or back edges, which means that every edge incident with $s$ takes us to a descendant of $s$, and no descendants have back edges, so at no point can we move up the tree by taking edges. Therefore $r$ is unreachable from $s$, so the graph is disconnected and $v$ is an articulation point. Now suppose that for every child of $v$ there exists a descendant of that child which has a back edge to a proper ancestor of $v$. Remove $v$ from $G$. Every subtree of $v$ is a connected component. Within a given subtree, find the vertex which has a back edge to a proper ancestor of $v$. Since the set $T$ of vertices which aren't descendants of $v$ form a connected component, we have that every subtree of $v$ is connected to $T$. Thus, the graph remains connected after the deletion of $v$ so $v$ is not an articulation point. c. Since $v$ is discovered before all of its descendants, the only back edges which could affect $v.low$ are ones which go from a descendant of $v$ to a proper ancestor of $v$. If we know $u.low$ for every child $u$ of $v$, then we can compute $v.low$ easily since all the information is coded in its descendants. Thus, we can write the algorithm recursively: If $v$ is a leaf in $G_\\pi$ then $v.low$ is the minimum of $v.d$ and $w.d$ where $(v, w)$ is a back edge. If $v$ is not a leaf, $v$ is the minimum of $v.d$, $w.d$ where $(v, w)$ is a back edge, and $u.low$, where $u$ is a child of $v$. Computing $v.low$ for a vertex is linear in its degree. The sum of the vertices' degrees gives twice the number of edges, so the total runtime is $O(E)$. d. First apply the algorithm of part (c) in $O(E)$ to compute $v.low$ for all $v \\in V$. If $v.low$ = $v.d$ if and only if no descendant of $v$ has a back edge to a proper ancestor of $v$, if and only if $v$ is not an articulation point. Thus, we need only check $v.low$ versus $v.d$ to decide in constant time whether or not $v$ is an articulation point, so the runtime is $O(E)$. e. An edge $(u, v)$ lies on a simple cycle if and only if there exists at least one path from $u$ to $v$ which doesn't contain the edge $(u, v)$, if and only if removing $(u, v)$ doesn't disconnect the graph, if and only if $(u, v)$ is not a bridge. f. A edge $(u, v)$ lies on a simple cycle in an undirected graph if and only if either both of its endpoints are articulation points, or one of its endpoints is an articulation point and the other is a vertex of degree $1$. There's also a special case where there's only one edge whose incident vertices are both degree $1$. We can check this case in constant time. Since we can compute all articulation points in $O(E)$ and we can decide whether or not a vertex has degree $1$ in constant time, we can run the algorithm in part (d) and then decide whether each edge is a bridge in constant time, so we can find all bridges in $O(E)$ time. g. It is clear that every nonbridge edge is in some biconnected component, so we need to show that if $C_1$ and $C_2$ are distinct biconnected components, then they contain no common edges. Suppose to the contrary that $(u, v)$ is in both $C_1$ and $C_2$. Let $(a, b)$ be any edge in $C_1$ and $(c, d)$ be any edge in $C_2$. Then $(a, b)$ lies on a simple cycle with $(u, v)$, consisting of the path $$a, b, p_1, \\ldots, p_k, u, v, p_{k + 1}, \\ldots, p_n, a.$$ Similarly, $(c, d)$ lies on a simple cycle with $(u, v)$ consisting of the path $$c, d, q_1, \\ldots, q_m, u, v, q_{m + 1}, \\ldots, q_l, c.$$ This means $$a, b, p_1, \\ldots, p_k, u, q_m, \\ldots, q_1, d, c, q_l , \\ldots, q_{m + 1}, v, p_{k + 1}, \\ldots, p_n,$$ is a simple cycle containing $(a, b)$ and $(c, d)$, a contradiction. Thus, the biconnected components form a partition. h. Locate all bridge edges in $O(E)$ time using the algorithm described in part (f). Remove each bridge from $E$. The biconnected components are now simply the edges in the connected components. Assuming this has been done, run the following algorithm, which clearly runs in $O(|E|)$ where $|E|$ is the number of edges originally in $G$. VISIT - BCC ( G , u , k ) u . color = GRAY for each v \u2208 G . Adj [ u ] ( u , v ). bcc = k if v . color == WHITE VISIT - BCC ( G , v , k )","title":"22-2 Articulation points, bridges, and biconnected components"},{"location":"Chap22/Problems/22-3/","text":"An Euler tour of a strongly connected, directed graph $G = (V, E)$ is a cycle that traverses each edge of $G$ exactly once, although it may visit a vertex more than once. a. Show that $G$ has an Euler tour if and only if $\\text{in-degree}(v) = \\text{out-degree}(v)$ for each vertex $v \\in V$. b. Describe an $O(E)$-time algorithm to find an Euler tour of $G$ if one exists. ($\\textit{Hint:}$ Merge edge-disjoint cycles.) a. First, we'll show that it is necessary to have in degree equal out degree for each vertex. Suppose that there was some vertex v for which the two were not equal, suppose that $\\text{in-degree}(v) - \\text{out-degree}(v)$. Note that we may assume that in degree is greater because otherwise we would just look at the transpose graph in which we traverse the cycle backwards. If $v$ is the start of the cycle as it is listed, just shift the starting and ending vertex to any other one on the cycle. Then, in whatever cycle we take going though $v$, we must pass through $v$ some number of times, in particular, after we pass through it a times, the number of unused edges coming out of $v$ is zero, however, there are still unused edges goin in that we need to use. This means that there is no hope of using those while still being a tour, becase we would never be able to escape $v$ and get back to the vertex where the tour started. Now, we show that it is sufficient to have the in degree and out degree equal for every vertex. To do this, we will generalize the problem slightly so that it is more amenable to an inductive approach. That is, we will show that for every graph $G$ that has two vertices $v$ and $u$ so that all the vertices have the same in and out degree except that the indegree is one greater for $u$ and the out degree is one greater for $v$, then there is an Euler path from $v$ to $u$. This clearly lines up with the original statement if we pick $u = v$ to be any vertex in the graph. We now perform induction on the number of edges. If there is only a single edge, then taking just that edge is an Euler tour. Then, suppose that we start at $v$ and take any edge coming out of it. Consider the graph that is obtained from removing that edge, it inductively contains an Euler tour that we can just post-pend to the edge that we took to get out of $v$. b. To actually get the Euler circuit, we can just arbitrarily walk any way that we want so long as we don't repeat an edge, we will necessarily end up with a valid Euler tour. This is implemented in the following algorithm, $\\text{EULER-TOUR}(G)$ which takes time $O(|E|)$. It has this runtime because the for loop will get run for every edge, and takes a constant amount of time. Also, the process of initializing each edge's color will take time proportional to the number of edges. EULER - TOUR ( G ) color all edges WHITE let ( v , u ) be any edge let L be a list containing v while there is some WHITE edge ( v , w ) coming out of v color ( v , w ) BLACK v = w append v to L","title":"22-3 Euler tour"},{"location":"Chap22/Problems/22-4/","text":"Let $G = (V, E)$ be a directed graph in which each vertex $u \\in V$ is labeled with a unique integer $L(U)$ from the set $\\{1, 2, \\ldots, |V|\\}$. For each vertex $u \\in V$, let $R(u) = \\{v \\in V: u \\leadsto v \\}$ be the set of vertices that are reachable from $u$. Define $\\min(u)$ to be the vertex in $R(u)$ whose label is minimum, i.e., $\\min(u)$ is the vertex $v$ such that $L(v) = \\min \\{L(w): w \\in R(u) \\}$. Give an $O(V + E)$-time algorithm that computes $\\min(u)$ for all vertices $u \\in V$. 1. Compute the component graph $G^{\\text{SCC}}$ (in order to remove simple cycles from graph $G$), and label each vertex in $G^{\\text{SCC}}$ with the smallest label of vertex in that $G^{\\text{SCC}}$. Following chapter 22.5 the time complexity of this procedure is $O(V + E)$. 2. On $G^{\\text{SCC}}$, execute the below algorithm. Notice that if we memorize this function it will be invoked at most $V + E$ times. Its time complexity is also $O(V + E)$. REACHABILITY ( u ) u . min = u . label for each v \u2208 Adj [ u ] u . min = min ( u . min , REACHABILITY ( v )) return u . min 3. Back to graph $G$, the value of $\\min(u)$ on Graph $G$ is the value of $\\min(u.scc)$ on Graph $G^{\\text{SCC}}$. Alternate solution: Transpose the graph. Call $\\text{DFS}$, but in the main loop of $\\text{DFS}$, consider the vertices in order of their labels. In the $\\text{DFS-VISIT}$ subroutine, upon discovering a new node, we set its $\\text{min}$ to be the label of its root.","title":"22-4 Reachability"},{"location":"Chap23/23.1/","text":"23.1-1 Let $(u, v)$ be a minimum-weight edge in a connected graph $G$. Show that $(u, v)$ belongs to some minimum spanning tree of $G$. (Removed) 23.1-2 Professor Sabatier conjectures the following converse of Theorem 23.1. Let $G = (V, E)$ be a connected, undirected graph with a real-valued weight function $w$ defined on $E$. Let $A$ be a subset of $E$ that is included in some minimum spanning tree for $G$, let $(S, V - S)$ be any cut of $G$ that respects $A$, and let $(u, v)$ be a safe edge for $A$ crossing $(S, V - S)$. Then, $(u, v)$ is a light edge for the cut. Show that the professor's conjecture is incorrect by giving a counterexample. Let $G$ be the graph with $4$ vertices: $u, v, w, z$. Let the edges of the graph be $(u, v), (u, w), (w, z)$ with weights $3$, $1$, and $2$ respectively. Suppose $A$ is the set $\\{(u, w)\\}$. Let $S = A$. Then $S$ clearly respects $A$. Since $G$ is a tree, its minimum spanning tree is itself, so $A$ is trivially a subset of a minimum spanning tree. Moreover, every edge is safe. In particular, $(u, v)$ is safe but not a light edge for the cut. Therefore Professor Sabatier's conjecture is false. 23.1-3 Show that if an edge $(u, v)$ is contained in some minimum spanning tree, then it is a light edge crossing some cut of the graph. Let $T_0$ and $T_1$ be the two trees that are obtained by removing edge $(u, v)$ from a $\\text{MST}$. Suppose that $V_0$ and $V_1$ are the vertices of $T_0$ and $T_1$ respectively. Consider the cut which separates $V_0$ from $V_1$. Suppose to a contradiction that there is some edge that has weight less than that of $(u, v)$ in this cut. Then, we could construct a minimum spanning tree of the whole graph by adding that edge to $T_1 \\cup T_0$. This would result in a minimum spanning tree that has weight less than the original minimum spanning tree that contained $(u, v)$. 23.1-4 Give a simple example of a connected graph such that the set of edges $\\{(u, v):$ there exists a cut $(S, V - S)$ such that $(u, v)$ is a light edge crossing $(S, V - S)\\}$ does not form a minimum spanning tree. (Removed) 23.1-5 Let $e$ be a maximum-weight edge on some cycle of connected graph $G = (V, E)$. Prove that there is a minimum spanning tree of $G' = (V, E - \\{e\\})$ that is also a minimum spanning tree of $G$. That is, there is a minimum spanning tree of $G$ that does not include $e$. Let $A$ be any cut that causes some vertices in the cycle on once side of the cut, and some vertices in the cycle on the other. For any of these cuts, we know that the edge $e$ is not a light edge for this cut. Since all the other cuts won't have the edge $e$ crossing it, we won't have that the edge is light for any of those cuts either. This means that we have that e is not safe. 23.1-6 Show that a graph has a unique minimum spanning tree if, for every cut of the graph, there is a unique light edge crossing the cut. Show that the converse is not true by giving a counterexample. (Removed) 23.1-7 Argue that if all edge weights of a graph are positive, then any subset of edges that connects all vertices and has minimum total weight must be a tree. Give an example to show that the same conclusion does not follow if we allow some weights to be nonpositive. First, we show that the subset of edges of minimum total weight that connects all the vertices is a tree. To see this, suppose not, that it had a cycle. This would mean that removing any of the edges in this cycle would mean that the remaining edges would still connect all the vertices, but would have a total weight that's less by the weight of the edge that was removed. This would contradict the minimality of the total weight of the subset of vertices. Since the subset of edges forms a tree, and has minimal total weight, it must also be a minimum spanning tree. To see that this conclusion is not true if we allow negative edge weights, we provide a construction. Consider the graph $K_3$ with all edge weights equal to $-1$. The only minimum weight set of edges that connects the graph has total weight $-3$, and consists of all the edges. This is clearly not a $\\text{MST}$ because it is not a tree, which can be easily seen because it has one more edge than a tree on three vertices should have. Any $\\text{MST}$ of this weighted graph must have weight that is at least $-2$. 23.1-8 Let $T$ be a minimum spanning tree of a graph $G$, and let $L$ be the sorted list of the edge weights of $T$. Show that for any other minimum spanning tree $T'$ of $G$, the list $L$ is also the sorted list of edge weights of $T'$. Suppose that $L'$ is another sorted list of edge weights of a minimum spanning tree. If $L' \\ne L$, there must be a first edge $(u, v)$ in $T$ or $T'$ which is of smaller weight than the corresponding edge $(x, y)$ in the other set. Without loss of generality, assume $(u, v)$ is in $T$. Let $C$ be the graph obtained by adding $(u, v)$ to $L'$. Then we must have introduced a cycle. If there exists an edge on that cycle which is of larger weight than $(u, v)$, we can remove it to obtain a tree $C'$ of weight strictly smaller than the weight of $T'$, contradicting the fact that $T'$ is a minimum spanning tree. Thus, every edge on the cycle must be of lesser or equal weight than $(u, v)$. Suppose that every edge is of strictly smaller weight. Remove $(u, v)$ from $T$ to disconnect it into two components. There must exist some edge besides $(u, v)$ on the cycle which would connect these, and since it has smaller weight we can use that edge instead to create a spanning tree with less weight than $T$, a contradiction. Thus, some edge on the cycle has the same weight as $(u, v)$. Replace that edge by $(u, v)$. The corresponding lists $L$ and $L'$ remain unchanged since we have swapped out an edge of equal weight, but the number of edges which $T$ and $T'$ have in common has increased by $1$. If we continue in this way, eventually they must have every edge in common, contradicting the fact that their edge weights differ somewhere. Therefore all minimum spanning trees have the same sorted list of edge weights. 23.1-9 Let $T$ be a minimum spanning tree of a graph $G = (V, E)$, and let $V'$ be a subset of $V$. Let $T'$ be the subgraph of $T$ induced by $V'$, and let $G'$ be the subgraph of $G$ induced by $V'$. Show that if $T'$ is connected, then $T'$ is a minimum spanning tree of $G'$. Suppose that there was some cheaper spanning tree than $T'$. That is, we have that there is some $T''$ so that $w(T'') < w(T')$. Then, let $S$ be the edges in $T$ but not in $T'$. We can then construct a minimum spanning tree of $G$ by considering $S \\cup T''$. This is a spanning tree since $S \\cup T'$ is, and $T''$ makes all the vertices in $V'$ connected just like $T'$ does. However, we have that $$w(S \\cup T'') = w(S) + w(T'') < w(S) + w(T') = w(S \\cup T') = w(T).$$ This means that we just found a spanning tree that has a lower total weight than a minimum spanning tree. This is a contradiction, and so our assumption that there was a spanning tree of $V'$ cheaper than $T'$ must be false. 23.1-10 Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges in $T$. Show that $T$ is still a minimum spanning tree for $G$. More formally, let $T$ be a minimum spanning tree for $G$ with edge weights given by weight function $w$. Choose one edge $(x, y) \\in T$ and a positive number $k$, and define the weight function $w'$ by $$ w'(u, v) = \\begin{cases} w(u, v) & \\text{ if }(u, v) \\ne (x, y), \\\\ w(x, y) - k & \\text{ if }(u, v) = (x, y). \\end{cases} $$ Show that $T$ is a minimum spanning tree for $G$ with edge weights given by $w'$. (Removed) 23.1-11 $\\star$ Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges not in $T$. Give an algorithm for finding the minimum spanning tree in the modified graph. If we were to add in this newly decreased edge to the given tree, we would be creating a cycle. Then, if we were to remove any one of the edges along this cycle, we would still have a spanning tree. This means that we look at all the weights along this cycle formed by adding in the decreased edge, and remove the edge in the cycle of maximum weight. This does exactly what we want since we could only possibly want to add in the single decreased edge, and then, from there we change the graph back to a tree in the way that makes its total weight minimized.","title":"23.1 Growing a minimum spanning tree"},{"location":"Chap23/23.1/#231-1","text":"Let $(u, v)$ be a minimum-weight edge in a connected graph $G$. Show that $(u, v)$ belongs to some minimum spanning tree of $G$. (Removed)","title":"23.1-1"},{"location":"Chap23/23.1/#231-2","text":"Professor Sabatier conjectures the following converse of Theorem 23.1. Let $G = (V, E)$ be a connected, undirected graph with a real-valued weight function $w$ defined on $E$. Let $A$ be a subset of $E$ that is included in some minimum spanning tree for $G$, let $(S, V - S)$ be any cut of $G$ that respects $A$, and let $(u, v)$ be a safe edge for $A$ crossing $(S, V - S)$. Then, $(u, v)$ is a light edge for the cut. Show that the professor's conjecture is incorrect by giving a counterexample. Let $G$ be the graph with $4$ vertices: $u, v, w, z$. Let the edges of the graph be $(u, v), (u, w), (w, z)$ with weights $3$, $1$, and $2$ respectively. Suppose $A$ is the set $\\{(u, w)\\}$. Let $S = A$. Then $S$ clearly respects $A$. Since $G$ is a tree, its minimum spanning tree is itself, so $A$ is trivially a subset of a minimum spanning tree. Moreover, every edge is safe. In particular, $(u, v)$ is safe but not a light edge for the cut. Therefore Professor Sabatier's conjecture is false.","title":"23.1-2"},{"location":"Chap23/23.1/#231-3","text":"Show that if an edge $(u, v)$ is contained in some minimum spanning tree, then it is a light edge crossing some cut of the graph. Let $T_0$ and $T_1$ be the two trees that are obtained by removing edge $(u, v)$ from a $\\text{MST}$. Suppose that $V_0$ and $V_1$ are the vertices of $T_0$ and $T_1$ respectively. Consider the cut which separates $V_0$ from $V_1$. Suppose to a contradiction that there is some edge that has weight less than that of $(u, v)$ in this cut. Then, we could construct a minimum spanning tree of the whole graph by adding that edge to $T_1 \\cup T_0$. This would result in a minimum spanning tree that has weight less than the original minimum spanning tree that contained $(u, v)$.","title":"23.1-3"},{"location":"Chap23/23.1/#231-4","text":"Give a simple example of a connected graph such that the set of edges $\\{(u, v):$ there exists a cut $(S, V - S)$ such that $(u, v)$ is a light edge crossing $(S, V - S)\\}$ does not form a minimum spanning tree. (Removed)","title":"23.1-4"},{"location":"Chap23/23.1/#231-5","text":"Let $e$ be a maximum-weight edge on some cycle of connected graph $G = (V, E)$. Prove that there is a minimum spanning tree of $G' = (V, E - \\{e\\})$ that is also a minimum spanning tree of $G$. That is, there is a minimum spanning tree of $G$ that does not include $e$. Let $A$ be any cut that causes some vertices in the cycle on once side of the cut, and some vertices in the cycle on the other. For any of these cuts, we know that the edge $e$ is not a light edge for this cut. Since all the other cuts won't have the edge $e$ crossing it, we won't have that the edge is light for any of those cuts either. This means that we have that e is not safe.","title":"23.1-5"},{"location":"Chap23/23.1/#231-6","text":"Show that a graph has a unique minimum spanning tree if, for every cut of the graph, there is a unique light edge crossing the cut. Show that the converse is not true by giving a counterexample. (Removed)","title":"23.1-6"},{"location":"Chap23/23.1/#231-7","text":"Argue that if all edge weights of a graph are positive, then any subset of edges that connects all vertices and has minimum total weight must be a tree. Give an example to show that the same conclusion does not follow if we allow some weights to be nonpositive. First, we show that the subset of edges of minimum total weight that connects all the vertices is a tree. To see this, suppose not, that it had a cycle. This would mean that removing any of the edges in this cycle would mean that the remaining edges would still connect all the vertices, but would have a total weight that's less by the weight of the edge that was removed. This would contradict the minimality of the total weight of the subset of vertices. Since the subset of edges forms a tree, and has minimal total weight, it must also be a minimum spanning tree. To see that this conclusion is not true if we allow negative edge weights, we provide a construction. Consider the graph $K_3$ with all edge weights equal to $-1$. The only minimum weight set of edges that connects the graph has total weight $-3$, and consists of all the edges. This is clearly not a $\\text{MST}$ because it is not a tree, which can be easily seen because it has one more edge than a tree on three vertices should have. Any $\\text{MST}$ of this weighted graph must have weight that is at least $-2$.","title":"23.1-7"},{"location":"Chap23/23.1/#231-8","text":"Let $T$ be a minimum spanning tree of a graph $G$, and let $L$ be the sorted list of the edge weights of $T$. Show that for any other minimum spanning tree $T'$ of $G$, the list $L$ is also the sorted list of edge weights of $T'$. Suppose that $L'$ is another sorted list of edge weights of a minimum spanning tree. If $L' \\ne L$, there must be a first edge $(u, v)$ in $T$ or $T'$ which is of smaller weight than the corresponding edge $(x, y)$ in the other set. Without loss of generality, assume $(u, v)$ is in $T$. Let $C$ be the graph obtained by adding $(u, v)$ to $L'$. Then we must have introduced a cycle. If there exists an edge on that cycle which is of larger weight than $(u, v)$, we can remove it to obtain a tree $C'$ of weight strictly smaller than the weight of $T'$, contradicting the fact that $T'$ is a minimum spanning tree. Thus, every edge on the cycle must be of lesser or equal weight than $(u, v)$. Suppose that every edge is of strictly smaller weight. Remove $(u, v)$ from $T$ to disconnect it into two components. There must exist some edge besides $(u, v)$ on the cycle which would connect these, and since it has smaller weight we can use that edge instead to create a spanning tree with less weight than $T$, a contradiction. Thus, some edge on the cycle has the same weight as $(u, v)$. Replace that edge by $(u, v)$. The corresponding lists $L$ and $L'$ remain unchanged since we have swapped out an edge of equal weight, but the number of edges which $T$ and $T'$ have in common has increased by $1$. If we continue in this way, eventually they must have every edge in common, contradicting the fact that their edge weights differ somewhere. Therefore all minimum spanning trees have the same sorted list of edge weights.","title":"23.1-8"},{"location":"Chap23/23.1/#231-9","text":"Let $T$ be a minimum spanning tree of a graph $G = (V, E)$, and let $V'$ be a subset of $V$. Let $T'$ be the subgraph of $T$ induced by $V'$, and let $G'$ be the subgraph of $G$ induced by $V'$. Show that if $T'$ is connected, then $T'$ is a minimum spanning tree of $G'$. Suppose that there was some cheaper spanning tree than $T'$. That is, we have that there is some $T''$ so that $w(T'') < w(T')$. Then, let $S$ be the edges in $T$ but not in $T'$. We can then construct a minimum spanning tree of $G$ by considering $S \\cup T''$. This is a spanning tree since $S \\cup T'$ is, and $T''$ makes all the vertices in $V'$ connected just like $T'$ does. However, we have that $$w(S \\cup T'') = w(S) + w(T'') < w(S) + w(T') = w(S \\cup T') = w(T).$$ This means that we just found a spanning tree that has a lower total weight than a minimum spanning tree. This is a contradiction, and so our assumption that there was a spanning tree of $V'$ cheaper than $T'$ must be false.","title":"23.1-9"},{"location":"Chap23/23.1/#231-10","text":"Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges in $T$. Show that $T$ is still a minimum spanning tree for $G$. More formally, let $T$ be a minimum spanning tree for $G$ with edge weights given by weight function $w$. Choose one edge $(x, y) \\in T$ and a positive number $k$, and define the weight function $w'$ by $$ w'(u, v) = \\begin{cases} w(u, v) & \\text{ if }(u, v) \\ne (x, y), \\\\ w(x, y) - k & \\text{ if }(u, v) = (x, y). \\end{cases} $$ Show that $T$ is a minimum spanning tree for $G$ with edge weights given by $w'$. (Removed)","title":"23.1-10"},{"location":"Chap23/23.1/#231-11-star","text":"Given a graph $G$ and a minimum spanning tree $T$, suppose that we decrease the weight of one of the edges not in $T$. Give an algorithm for finding the minimum spanning tree in the modified graph. If we were to add in this newly decreased edge to the given tree, we would be creating a cycle. Then, if we were to remove any one of the edges along this cycle, we would still have a spanning tree. This means that we look at all the weights along this cycle formed by adding in the decreased edge, and remove the edge in the cycle of maximum weight. This does exactly what we want since we could only possibly want to add in the single decreased edge, and then, from there we change the graph back to a tree in the way that makes its total weight minimized.","title":"23.1-11 $\\star$"},{"location":"Chap23/23.2/","text":"23.2-1 Kruskal's algorithm can return different spanning trees for the same input graph $G$, depending on how it breaks ties when the edges are sorted into order. Show that for each minimum spanning tree $T$ of $G$, there is a way to sort the edges of $G$ in Kruskal's algorithm so that the algorithm returns $T$. Suppose that we wanted to pick $T$ as our minimum spanning tree. Then, to obtain this tree with Kruskal's algorithm, we will order the edges first by their weight, but then will resolve ties in edge weights by picking an edge first if it is contained in the minimum spanning tree, and treating all the edges that aren't in $T$ as being slightly larger, even though they have the same actual weight. With this ordering, we will still be finding a tree of the same weight as all the minimum spanning trees $w(T)$. However, since we prioritize the edges in $T$, we have that we will pick them over any other edges that may be in other minimum spanning trees. 23.2-2 Suppose that we represent the graph $G = (V, E)$ as an adjacency matrix. Give a simple implementation of Prim's algorithm for this case that runs in $O(V^2)$ time. At each step of the algorithm we will add an edge from a vertex in the tree created so far to a vertex not in the tree, such that this edge has minimum weight. Thus, it will be useful to know, for each vertex not in the tree, the edge from that vertex to some vertex in the tree of minimal weight. We will store this information in an array $A$, where $A[u] = (v, w)$ if $w$ is the weight of $(u, v)$ and is minimal among the weights of edges from $u$ to some vertex $v$ in the tree built so far. We'll use $A[u].1$ to access $v$ and $A[u].2$ to access $w$. PRIM - ADJ ( G , w , r ) initialize A with every entry = ( NIL , \u221e ) T = { r } for i = 1 to V if Adj [ r , i ] != 0 A [ i ] = ( r , w ( r , i )) for each u in V - T k = min ( A [ i ] .2 ) T = T \u222a { k } k . \u03c0 = A [ k ] .1 for i = 1 to V if Adf [ k , i ] != 0 and Adj [ k , i ] < A [ i ] .2 A [ i ] = ( k , Adj [ k , i ]) 23.2-3 For a sparse graph $G = (V, E)$, where $|E| = \\Theta(V)$, is the implementation of Prim's algorithm with a Fibonacci heap asymptotically faster than the binary-heap implementation? What about for a dense graph, where $|E| = \\Theta(V^2)$? How must the sizes $|E|$ and $|V|$ be related for the Fibonacci-heap implementation to be asymptotically faster than the binary-heap implementation? Prim's algorithm implemented with a Binary heap has runtime $O((V + E)\\lg V)$, which in the sparse case, is just $O(V\\lg V)$. The implementation with Fibonacci heaps is $$O(E + V\\lg V) = O(V + V\\lg V) = O(V \\lg V).$$ In the sparse case, the two algorithms have the same asymptotic runtimes. In the dense case. The binary heap implementation has a runtime of $$O((V + E)\\lg V) = O((V + V^2)\\lg V) = O(V^2\\lg V).$$ The Fibonacci heap implementation has a runtime of $$O(E + V\\lg V) = O(V^2 + V\\lg V) = O(V^2).$$ So, in the dense case, we have that the Fibonacci heap implementation is asymptotically faster. The Fibonacci heap implementation will be asymptotically faster so long as $E = \\omega(V)$. Suppose that we have some function that grows more quickly than linear, say $f$, and $E = f(V)$. The binary heap implementation will have runtime of $$O((V + E)\\lg V) = O((V + f(V))\\lg V) = O(f(V)\\lg V).$$ However, we have that the runtime of the Fibonacci heap implementation will have runtime of $$O(E + V\\lg V) = O(f(V) + V\\lg V).$$ This runtime is either $O(f(V))$ or $O(V\\lg V)$ depending on if $f(V)$ grows more or less quickly than $V\\lg V$ respectively. In either case, we have that the runtime is faster than $O(f(V)\\lg V)$. 23.2-4 Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Kruskal's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? (Removed) 23.2-5 Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Prim's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? For the first case, we can use a van Emde Boas tree to improve the time bound to $O(E \\lg \\lg V)$. Comparing to the Fibonacci heap implementation, this improves the asymptotic running time only for sparse graphs, and it cannot improve the running time polynomially. An advantage of this implementation is that it may have a lower overhead. For the second case, we can use a collection of doubly linked lists, each corresponding to an edge weight. This improves the bound to $O(E)$. 23.2-6 $\\star$ Suppose that the edge weights in a graph are uniformly distributed over the halfopen interval $[0, 1)$. Which algorithm, Kruskal's or Prim's, can you make run faster? For input drawn from a uniform distribution I would use bucket sort with Kruskal's algorithm, for expected linear time sorting of edges by weight. This would achieve expected runtime $O(E\\alpha(V))$. 23.2-7 $\\star$ Suppose that a graph $G$ has a minimum spanning tree already computed. How quickly can we update the minimum spanning tree if we add a new vertex and incident edges to $G$? (Removed) 23.2-8 Professor Borden proposes a new divide-and-conquer algorithm for computing minimum spanning trees, which goes as follows. Given a graph $G = (V, E)$, partition the set $V$ of vertices into two sets $V_1$ and $V_2$ such that $|V_1|$ and $|V_2|$ differ by at most $1$. Let $E_1$ be the set of edges that are incident only on vertices in $V_1$, and let $E_2$ be the set of edges that are incident only on vertices in $V_2$. Recursively solve a minimum-spanning-tree problem on each of the two subgraphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$. Finally, select the minimum-weight edge in $E$ that crosses the cut $(V_1, V_2)$, and use this edge to unite the resulting two minimum spanning trees into a single spanning tree. Either argue that the algorithm correctly computes a minimum spanning tree of $G$, or provide an example for which the algorithm fails. The algorithm fails. Suppose $E = \\{(u, v), (u, w), (v, w)\\}$, the weight of $(u, v)$ and $(u, w)$ is $1$, and the weight of $(v, w)$ is $1000$, partition the set into two sets $V_1 = \\{u\\}$ and $V_2 = \\{v, w\\}$.","title":"23.2 The algorithms of Kruskal and Prim"},{"location":"Chap23/23.2/#232-1","text":"Kruskal's algorithm can return different spanning trees for the same input graph $G$, depending on how it breaks ties when the edges are sorted into order. Show that for each minimum spanning tree $T$ of $G$, there is a way to sort the edges of $G$ in Kruskal's algorithm so that the algorithm returns $T$. Suppose that we wanted to pick $T$ as our minimum spanning tree. Then, to obtain this tree with Kruskal's algorithm, we will order the edges first by their weight, but then will resolve ties in edge weights by picking an edge first if it is contained in the minimum spanning tree, and treating all the edges that aren't in $T$ as being slightly larger, even though they have the same actual weight. With this ordering, we will still be finding a tree of the same weight as all the minimum spanning trees $w(T)$. However, since we prioritize the edges in $T$, we have that we will pick them over any other edges that may be in other minimum spanning trees.","title":"23.2-1"},{"location":"Chap23/23.2/#232-2","text":"Suppose that we represent the graph $G = (V, E)$ as an adjacency matrix. Give a simple implementation of Prim's algorithm for this case that runs in $O(V^2)$ time. At each step of the algorithm we will add an edge from a vertex in the tree created so far to a vertex not in the tree, such that this edge has minimum weight. Thus, it will be useful to know, for each vertex not in the tree, the edge from that vertex to some vertex in the tree of minimal weight. We will store this information in an array $A$, where $A[u] = (v, w)$ if $w$ is the weight of $(u, v)$ and is minimal among the weights of edges from $u$ to some vertex $v$ in the tree built so far. We'll use $A[u].1$ to access $v$ and $A[u].2$ to access $w$. PRIM - ADJ ( G , w , r ) initialize A with every entry = ( NIL , \u221e ) T = { r } for i = 1 to V if Adj [ r , i ] != 0 A [ i ] = ( r , w ( r , i )) for each u in V - T k = min ( A [ i ] .2 ) T = T \u222a { k } k . \u03c0 = A [ k ] .1 for i = 1 to V if Adf [ k , i ] != 0 and Adj [ k , i ] < A [ i ] .2 A [ i ] = ( k , Adj [ k , i ])","title":"23.2-2"},{"location":"Chap23/23.2/#232-3","text":"For a sparse graph $G = (V, E)$, where $|E| = \\Theta(V)$, is the implementation of Prim's algorithm with a Fibonacci heap asymptotically faster than the binary-heap implementation? What about for a dense graph, where $|E| = \\Theta(V^2)$? How must the sizes $|E|$ and $|V|$ be related for the Fibonacci-heap implementation to be asymptotically faster than the binary-heap implementation? Prim's algorithm implemented with a Binary heap has runtime $O((V + E)\\lg V)$, which in the sparse case, is just $O(V\\lg V)$. The implementation with Fibonacci heaps is $$O(E + V\\lg V) = O(V + V\\lg V) = O(V \\lg V).$$ In the sparse case, the two algorithms have the same asymptotic runtimes. In the dense case. The binary heap implementation has a runtime of $$O((V + E)\\lg V) = O((V + V^2)\\lg V) = O(V^2\\lg V).$$ The Fibonacci heap implementation has a runtime of $$O(E + V\\lg V) = O(V^2 + V\\lg V) = O(V^2).$$ So, in the dense case, we have that the Fibonacci heap implementation is asymptotically faster. The Fibonacci heap implementation will be asymptotically faster so long as $E = \\omega(V)$. Suppose that we have some function that grows more quickly than linear, say $f$, and $E = f(V)$. The binary heap implementation will have runtime of $$O((V + E)\\lg V) = O((V + f(V))\\lg V) = O(f(V)\\lg V).$$ However, we have that the runtime of the Fibonacci heap implementation will have runtime of $$O(E + V\\lg V) = O(f(V) + V\\lg V).$$ This runtime is either $O(f(V))$ or $O(V\\lg V)$ depending on if $f(V)$ grows more or less quickly than $V\\lg V$ respectively. In either case, we have that the runtime is faster than $O(f(V)\\lg V)$.","title":"23.2-3"},{"location":"Chap23/23.2/#232-4","text":"Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Kruskal's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? (Removed)","title":"23.2-4"},{"location":"Chap23/23.2/#232-5","text":"Suppose that all edge weights in a graph are integers in the range from $1$ to $|V|$. How fast can you make Prim's algorithm run? What if the edge weights are integers in the range from $1$ to $W$ for some constant $W$? For the first case, we can use a van Emde Boas tree to improve the time bound to $O(E \\lg \\lg V)$. Comparing to the Fibonacci heap implementation, this improves the asymptotic running time only for sparse graphs, and it cannot improve the running time polynomially. An advantage of this implementation is that it may have a lower overhead. For the second case, we can use a collection of doubly linked lists, each corresponding to an edge weight. This improves the bound to $O(E)$.","title":"23.2-5"},{"location":"Chap23/23.2/#232-6-star","text":"Suppose that the edge weights in a graph are uniformly distributed over the halfopen interval $[0, 1)$. Which algorithm, Kruskal's or Prim's, can you make run faster? For input drawn from a uniform distribution I would use bucket sort with Kruskal's algorithm, for expected linear time sorting of edges by weight. This would achieve expected runtime $O(E\\alpha(V))$.","title":"23.2-6 $\\star$"},{"location":"Chap23/23.2/#232-7-star","text":"Suppose that a graph $G$ has a minimum spanning tree already computed. How quickly can we update the minimum spanning tree if we add a new vertex and incident edges to $G$? (Removed)","title":"23.2-7 $\\star$"},{"location":"Chap23/23.2/#232-8","text":"Professor Borden proposes a new divide-and-conquer algorithm for computing minimum spanning trees, which goes as follows. Given a graph $G = (V, E)$, partition the set $V$ of vertices into two sets $V_1$ and $V_2$ such that $|V_1|$ and $|V_2|$ differ by at most $1$. Let $E_1$ be the set of edges that are incident only on vertices in $V_1$, and let $E_2$ be the set of edges that are incident only on vertices in $V_2$. Recursively solve a minimum-spanning-tree problem on each of the two subgraphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$. Finally, select the minimum-weight edge in $E$ that crosses the cut $(V_1, V_2)$, and use this edge to unite the resulting two minimum spanning trees into a single spanning tree. Either argue that the algorithm correctly computes a minimum spanning tree of $G$, or provide an example for which the algorithm fails. The algorithm fails. Suppose $E = \\{(u, v), (u, w), (v, w)\\}$, the weight of $(u, v)$ and $(u, w)$ is $1$, and the weight of $(v, w)$ is $1000$, partition the set into two sets $V_1 = \\{u\\}$ and $V_2 = \\{v, w\\}$.","title":"23.2-8"},{"location":"Chap23/Problems/23-1/","text":"Let $G = (V, E)$ be an undirected, connected graph whose weight function is $w: E \\rightarrow \\mathbb R$, and suppose that $|E| \\ge |V|$ and all edge weights are distinct. We define a second-best minimum spanning tree as follows. Let $\\mathcal T$ be the set of all spanning trees of $G$, and let $T'$ be a minimum spanning tree of $G$. Then a second-best minimum spanning tree is a spanning tree $T$ such that $W(T) = \\min_{T'' \\in \\mathcal T - \\{T'\\}} \\{w(T'')\\}$. a. Show that the minimum spanning tree is unique, but that the second-best minimum spanning tree need not be unique. b. Let $T$ be the minimum spanning tree of $G$. Prove that $G$ contains edges $(u, v) \\in T$ and $(x, y) \\notin T$ such that $T - \\{(u, v)\\} \\cup \\{(x, y)\\}$ is a second-best minimum spanning tree of $G$. c. Let $T$ be a spanning tree of $G$ and, for any two vertices $u, v \\in V$, let $max[u, v]$ denote an edge of maximum weight on the unique simple path between $u$ and $v$ in $T$. Describe an $O(V^2)$-time algorithm that, given $T$, computes $max[u, v]$ for all $u, v \\in V$. d. Give an efficient algorithm to compute the second-best minimum spanning tree of $G$. a. To see that the second best minimum spanning tree need not be unique, we consider the following example graph on four vertices. Suppose the vertices are $\\{a, b, c, d\\}$, and the edge weights are as follows: $$ \\begin{array}{c|c|c|c|c|} & a & b & c & d \\\\ \\hline a & - & 1 & 4 & 3 \\\\ \\hline b & 1 & - & 5 & 2 \\\\ \\hline c & 4 & 5 & - & 6 \\\\ \\hline d & 3 & 2 & 6 & - \\\\ \\hline \\end{array} $$ Then, the minimum spanning tree has weight $7$, but there are two spanning trees of the second best weight, $8$. b. We are trying to show that there is a single edge swap that can demote our minimum spanning tree to a second best minimum spanning tree. In obtaining the second best minimum spanning tree, there must be some cut of a single vertex away from the rest for which the edge that is added is not light, otherwise, we would find the minimum spanning tree, not the second best minimum spanning tree. Call the edge that is selected for that cut for the second best minimum spanning tree $(x, y)$. Now, consider the same cut, except look at the edge that was selected when obtaining $T$, call it $(u, v)$. Then, we have that if consider $T - \\{(u, v)\\} \\cup \\{(x, y)\\}$, it will be a second best minimum spanning tree. This is because if the second best minimum spanning tree also selected a non-light edge for another cut, it would end up more expensive than all the minimum spanning trees. This means that we need for every cut other than the one that the selected edge was light. This means that the choices all align with what the minimum spanning tree was. c. We give here a dynamic programming solution. Suppose that we want to find it for $(u, v)$. First, we will identify the vertex $x$ that occurs immediately after $u$ on the simple path from $u$ to $v$. We will then make $\\max[u, v]$ equal to the max of $w((u, x))$ and $\\max[w, v]$. Lastly, we just consider the case that $u$ and $v$ are adjacent, in which case the maximum weight edge is just the single edge between the two. If we can find $x$ in constant time, then we will have the whole dynamic program running in time $O(V^2)$, since that's the size of the table that's being built up. To find $x$ in constant time, we preprocess the tree. We first pick an arbitrary root. Then, we do the preprocessing for Tarjan's off-line least common ancestors algorithm (See problem 21-3). This takes time just a little more than linear, $O(|V|\\alpha(|V|))$. Once we've computed all the least common ancestors, we can just look up that result at some point later in constant time. Then, to find the $w$ that we should pick, we first see if $u = \\text{LCA}(u, v)$ if it does not, then we just pick the parent of $u$ in the tree. If it does, then we flip the question on its head and try to compute $\\max[v, u]$, we are guaranteed to not have this situation of $v = \\text{LCA}(v, u)$ because we know that $u$ is an ancestor of $v$. d. We provide here an algorithm that takes time $O(V^2)$ and leave open if there exists a linear time solution, that is a $O(E + V)$ time solution. First, we find a minimum spanning tree in time $O(E + V \\lg(V))$, which is in $O(V^2)$. Then, using the algorithm from part c, we find the double array max. Then, we take a running minimum over all pairs of vertices $u$, $v$, of the value of $w(u, v) - \\max[u, v]$. If there is no edge between $u$ and $v$, we think of the weight being infinite. Then, for the pair that resulted in the minimum value of this difference, we add in that edge and remove from the minimum spanning tree, an edge that is in the path from $u$ to $v$ that has weight $\\max[u, v]$.","title":"23-1 Second-best minimum spanning tree"},{"location":"Chap23/Problems/23-2/","text":"For a very sparse connected graph $G = (V, E)$, we can further improve upon the $O(E + V\\lg V)$ running time of Prim's algorithm with Fibonacci heaps by preprocessing $G$ to decrease the number of vertices before running Prim's algorithm. In particular, we choose, for each vertex $u$, the minimum-weight edge $(u, v)$ incident on $u$, and we put $(u, v)$ into the minimum spanning tree under construction. We then contract all chosen edges (see Section B.4). Rather than contracting these edges one at a time, we first identify sets of vertices that are united into the same new vertex. Then we create the graph that would have resulted from contracting these edges one at a time, but we do so by \"renaming\" edges according to the sets into which their endpoints were placed. Several edges from the original graph may be renamed the same as each other. In such a case, only one edge results, and its weight is the minimum of the weights of the corresponding original edges. Initially, we set the minimum spanning tree $T$ being constructed to be empty, and for each edge $(u, v) \\in E$, we initialize the attributes $(u, v).orig = (u, v)$ and $(u, v).c = w(u, v)$. We use the $orig$ attribute to reference the edge from the initial graph that is associated with an edge in the contracted graph. The $c$ attribute holds the weight of an edge, and as edges are contracted, we update it according to the above scheme for choosing edge weights. The procedure $\\text{MST-REDUCE}$ takes inputs $G$ and $T$, and it returns a contracted graph $G'$ with updated attributes $orig'$ and $c'$. The procedure also accumulates edges of $G$ into the minimum spanning tree $T$. MST - REDUCE ( G , T ) for each v \u2208 G . V v . mark = false MAKE - SET ( v ) for each u \u2208 G . V if u . mark == false choose v \u2208 G . Adj [ u ] such that ( u , v ). c is minimized UNION ( u , v ) T = T \u222a {( u , v ). orig } u . mark = v . mark = true G ' . V = { FIND - SET ( v ) : v \u2208 G . V } G ' . E = \u00d8 for each ( x , y ) \u2208 G . E u = FIND - SET ( x ) v = FIND - SET ( y ) if ( u , v ) \u2209 G ' . E G ' . E = G ' . E \u222a {( u , v )} ( u , v ). orig ' = ( x , y ). orig ( u , v ). c ' = ( x , y ). c else if ( x , y ). c < ( u , v ). c ' ( u , v ). orig ' = ( x , y ). orig ( u , v ). c ' = ( x , y ). c construct adjacency lists G ' . Adj for G ' return G ' and T a. Let $T$ be the set of edges returned by $\\text{MST-REDUCE}$, and let $A$ be the minimum spanning tree of the graph $G'$ formed by the call $\\text{MST-PRIM}(G', c', r)$, where $c'$ is the weight attribute on the edges of $G'.E$ and $r$ is any vertex in $G'.V$. Prove that $T \\cup \\{(x,y).orig': (x, y) \\in A\\}$ is a minimum spanning tree of $G$. b. Argue that $|G'.V| \\le |V| / 2$. c. Show how to implement $\\text{MST-REDUCE}$ so that it runs in $O(E)$ time. ($\\textit{Hint:}$ Use simple data structures.) d. Suppose that we run $k$ phases of $\\text{MST-REDUCE}$, using the output $G'$ produced by one phase as the input $G$ to the next phase and accumulating edges in $T$. Argue that the overall running time of the $k$ phases is $O(kE)$. e. Suppose that after running $k$ phases of $\\text{MST-REDUCE}$, as in part (d), we run Prim's algorithm by calling $\\text{MST-PRIM}(G', c', r)$, where $G'$, with weight attribute $c'$, is returned by the last phase and $r$ is any vertex in $G'.V$. Show how to pick $k$ so that the overall running time is $O(E\\lg\\lg V)$. Argue that your choice of $k$ minimizes the overall asymptotic running time. f. For what values of $|E|$ (in terms of $|V|$) does Prim's algorithm with preprocessing asymptotically beat Prim's algorithm without preprocessing? a. We'll show that the edges added at each step are safe. Consider an unmarked vertex $u$. Set $S = \\{u\\}$ and let $A$ be the set of edges in the tree so far. Then the cut respects $A$, and the next edge we add is a light edge, so it is safe for $A$. Thus, every edge in $T$ before we run Prim's algorithm is safe for $T$. Any edge that Prim's would normally add at this point would have to connect two of the trees already created, and it would be chosen as minimal. Moreover, we choose exactly one between any two trees. Thus, the fact that we only have the smallest edges available to us is not a problem. The resulting tree must be minimal. b. We argue by induction on the number of vertices in $G$. We'll assume that $|V| > 1$, since otherwise $\\text{MST-REDUCE}$ will encounter an error on line 6 because there is no way to choose $v$. Let $|V| = 2$. Since $G$ is connected, there must be an edge between $u$ and $v$, and it is trivially of minimum weight. They are joined, and $|G'.V| = 1 = |V| / 2$. Suppose the claim holds for $|V| = n$. Let $G$ be a connected graph on $n + 1$ vertices. Then $G'.V \\le n / 2$ prior to the final vertex $v$ being examined in the for-loop of line 4. If $v$ is marked then we're done, and if $v$ isn't marked then we'll connect it to some other vertex, which must be marked since $v$ is the last to be processed. Either way, $v$ can't contribute an additional vertex to $G'.V$. so $$|G'.V| \\le n / 2 \\le (n + 1) / 2.$$ c. Rather than using the disjoint set structures of chapter 21, we can simply use an array to keep track of which component a vertex is in. Let $A$ be an array of length $|V|$ such that $A[u] = v$ if $v = \\text{FIND-SET}(u)$. Then $\\text{FIND-SET}(u)$ can now be replaced with $A[u]$ and $\\text{UNION}(u, v)$ can be replaced by $A[v] = A[u]$. Since these operations run in constant time, the runtime is $O(E)$. d. The number of edges in the output is monotonically decreasing, so each call is $O(E)$. Thus, $k$ calls take $O(kE)$ time. e. The runtime of Prim's algorithm is $O(E + V\\lg V)$. Each time we run $\\text{MST-REDUCE}$, we cut the number of vertices at least in half. Thus, after $k$ calls, the number of vertices is at most $|V| / 2^k$. We need to minimize $$E + V / 2^k\\lg(V / 2^k) + kE = E + \\frac{V\\lg V}{2^k} - \\frac{Vk}{2^k} + kE$$ with respect to $k$. If we choose $k = \\lg\\lg V$ then we achieve the overall running time of $O(E\\lg\\lg V)$ as desired. To see that this value of $k$ minimizes, note that the $\\frac{Vk}{2^k}$ term is always less than the $kE$ term since $E \\ge V$. As $k$ decreases, the contribution of $kE$ decreases, and the contribution of $\\frac{V\\lg V}{2^k}$ increases. Thus, we need to find the value of $k$ which makes them approximately equal in the worst case, when $E = V$. To do this, we set $\\frac{\\lg V}{2^k} = k$. Solving this exactly would involve the Lambert W function, but the nicest elementary function which gets close is $k = \\lg\\lg V$. f. We simply set up the inequality $$E\\lg\\lg V < E + V\\lg V$$ to find that we need $$E < \\frac{V\\lg V}{\\lg\\lg V-1} = O(\\frac{V\\lg V}{\\lg\\lg V}).$$","title":"23-2 Minimum spanning tree in sparse graphs"},{"location":"Chap23/Problems/23-3/","text":"A bottleneck spanning tree $T$ of an undirected graph $G$ is a spanning tree of $G$ whose largest edge weight is minimum over all spanning trees of $G$. We say that the value of the bottleneck spanning tree is the weight of the maximum-weight edge in $T$. a. Argue that a minimum spanning tree is a bottleneck spanning tree. Part (a) shows that finding a bottleneck spanning tree is no harder than finding a minimum spanning tree. In the remaining parts, we will show how to find a bottleneck spanning tree in linear time. b. Give a linear-time algorithm that given a graph $G$ and an integer $b$, determines whether the value of the bottleneck spanning tree is at most $b$. c. Use your algorithm for part (b) as a subroutine in a linear-time algorithm for the bottleneck-spanning-tree problem. ($\\textit{Hint:}$ You may want to use a subroutine that contracts sets of edges, as in the $\\text{MST-REDUCE}$ procedure described in Problem 23-2.) a. To see that every minimum spanning tree is also a bottleneck spanning tree. Suppose that $T$ is a minimum spanning tree. Suppose there is some edge in it $(u, v)$ that has a weight that's greater than the weight of the bottleneck spanning tree. Then, let $V_1$ be the subset of vertices of $V$ that are reachable from $u$ in $T$, without going though $v$. Define $V_2$ symmetrically. Then, consider the cut that separates $V_1$ from $V_2$. The only edge that we could add across this cut is the one of minimum weight, so we know that there are no edge across this cut of weight less than $w(u, v)$. However, we have that there is a bottleneck spanning tree with less than that weight. This is a contradiction because a bottleneck spanning tree, since it is a spanning tree, must have an edge across this cut. b. To do this, we first process the entire graph, and remove any edges that have weight greater than $b$. If the remaining graph is connected, we can just arbitrarily select any tree in it, and it will be a bottleneck spanning tree of weight at most $b$. Testing connectivity of a graph can be done in linear time by running a breadth first search and then making sure that no vertices remain white at the end. c. Write down all of the edge weights of vertices. Use the algorithm from section 9.3 to find the median of this list of numbers in time $O(E)$. Then, run the procedure from part b with this median value as input. Then there are two cases: First, we could have that there is a bottleneck spanning tree with weight at most this median. Then just throw away the edges with weight more than the median, and repeat the procedure on this new graph with half the edges. Second, we could have that there is no bottleneck spanning tree with at most that weight. Then, we should run a procedure similar to problem 23-2 to contract all of the edges that have weight at most the weight of the median. This takes time $O(E)$ and then we are left solving the problem on a graph that now has half the edges. Observe that both cases are $O(E)$ and each recursion reduces the problem size into half. The solution to this recurrence is therefore linear.","title":"23-3 Bottleneck spanning tree"},{"location":"Chap23/Problems/23-4/","text":"In this problem, we give pseudocode for three different algorithms. Each one takes a connected graph and a weight function as input and returns a set of edges $T$. For each algorithm, either prove that $T$ is a minimum spanning tree or prove that $T$ is not a minimum spanning tree. Also describe the most efficient implementation of each algorithm, whether or not it computes a minimum spanning tree. a. MAYBE - MST - A ( G , w ) sort the edges into nonincreasing order of edge weights w T = E for each edge e , taken in nonincreasing order by weight if T - { e } is a connected graph T = T - { e } return T b. MAYBE - MST - B ( G , w ) T = \u00d8 for each edge e , taken in arbitrary order if T \u222a { e } has no cycles T = T \u222a { e } return T c. MAYBE - MST - C ( G , w ) T = \u00d8 for each edge e , taken in arbitrary order T = T \u222a { e } if T has a cycle c let e ' be a maximum - weight edge on c T = T - { e } return T a. This does return an $\\text{MST}$. To see this, we'll show that we never remove an edge which must be part of a minimum spanning tree. If we remove $e$, then $e$ cannot be a bridge, which means that e lies on a simple cycle of the graph. Since we remove edges in nonincreasing order, the weight of every edge on the cycle must be less than or equal to that of $e$. By exercise 23.1-5, there is a minimum spanning tree on $G$ with edge $e$ removed. To implement this, we begin by sorting the edges in $O(E \\lg E)$ time. For each edge we need to check whether or not $T - {e}$ is connected, so we'll need to run a $\\text{DFS}$. Each one takes $O(V + E)$, so doing this for all edges takes $O(E(V + E))$. This dominates the running time, so the total time is $O(E^2)$. b. This doesn't return an $\\text{MST}$. To see this, let $G$ be the graph on 3 vertices $a$, $b$, and $c$. Let the eges be $(a, b)$, $(b, c)$, and $(c, a)$ with weights $3, 2$, and $1$ respectively. If the algorithm examines the edges in their order listed, it will take the two heaviest edges instead of the two lightest. An efficient implementation will use disjoint sets to keep track of connected components, as in $\\text{MST-REDUCE}$ in problem 23-2. Trying to union within the same component will create a cycle. Since we make $|V|$ calls to $\\text{MAKESET}$ and at most $3|E|$ calls to $\\text{FIND-SET}$ and $\\text{UNION}$, the runtime is $O(E\\alpha(V))$. c. This does return an $\\text{MST}$. To see this, we simply quote the result from exercise 23.1-5. The only edges we remove are the edges of maximum weight on some cycle, and there always exists a minimum spanning tree which doesn't include these edges. Moreover, if we remove an edge from every cycle then the resulting graph cannot have any cycles, so it must be a tree. To implement this, we use the approach taken in part (b), except now we also need to find the maximum weight edge on a cycle. For each edge which introduces a cycle we can perform a $\\text{DFS}$ to find the cycle and max weight edge. Since the tree at that time has at most one cycle, it has at most $|V|$ edges, so we can run $\\text{DFS}$ in $O(V)$. The runtime is thus $O(EV)$.","title":"23-4 Alternative minimum-spanning-tree algorithms"},{"location":"Chap24/24.1/","text":"24.1-1 Run the Bellman-Ford algorithm on the directed graph of Figure 24.4, using vertex $z$ as the source. In each pass, relax edges in the same order as in the figure, and show the $d$ and $\\pi$ values after each pass. Now, change the weight of edge $(z, x)$ to $4$ and run the algorithm again, using $s$ as the source. Using vertex $z$ as the source: $d$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline \\infty & \\infty & \\infty & \\infty & 0 \\\\ 2 & \\infty & 7 & \\infty & 0 \\\\ 2 & 5 & 7 & 9 & 0 \\\\ 2 & 5 & 6 & 9 & 0 \\\\ 2 & 4 & 6 & 9 & 0 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ z & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\ z & x & z & s & \\text{NIL} \\\\ z & x & y & s & \\text{NIL} \\\\ z & x & y & s & \\text{NIL} \\end{array} $$ Changing the weight of edge $(z, x)$ to $4$: $d$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline 0 & \\infty & \\infty & \\infty & \\infty \\\\ 0 & 6 & \\infty & 7 & \\infty \\\\ 0 & 6 & 4 & 7 & 2 \\\\ 0 & 2 & 4 & 7 & 2 \\\\ 0 & 2 & 4 & 7 & -2 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & s & \\text{NIL} & s & \\text{NIL} \\\\ \\text{NIL} & s & y & s & t \\\\ \\text{NIL} & x & y & s & t \\\\ \\text{NIL} & x & y & s & t \\end{array} $$ Consider edge $(z, x)$, it'll return $\\text{FALSE}$ since $x.d = 4 > z.d + w(z, x) = -2 + 4$. 24.1-2 Prove Corollary 24.3. Suppose there is a path from $s$ to $v$. Then there must be a shortest such path of length $\\delta(s, v)$. It must have finite length since it contains at most $|V| - 1$ edges and each edge has finite length. By Lemma 24.2, $v.d = \\delta(s, v) < \\infty$ upon termination. On the other hand, suppose $v.d < \\infty$ when $\\text{BELLMAN-FORD}$ terminates. Recall that $v.d$ is monotonically decreasing throughout the algorithm, and $\\text{RELAX}$ will update $v.d$ only if $u.d + w(u, v) < v.d$ for some $u$ adjacent to $v$. Moreover, we update $v.\\pi = u$ at this point, so $v$ has an ancestor in the predecessor subgraph. Since this is a tree rooted at $s$, there must be a path from $s$ to $v$ in this tree. Every edge in the tree is also an edge in $G$, so there is also a path in $G$ from $s$ to $v$. 24.1-3 Given a weighted, directed graph $G = (V, E)$ with no negative-weight cycles, let $m$ be the maximum over all vertices $v \\in V$ of the minimum number of edges in a shortest path from the source $s$ to $v$. (Here, the shortest path is by weight, not the number of edges.) Suggest a simple change to the Bellman-Ford algorithm that allows it to terminate in $m + 1$ passes, even if $m$ is not known in advance. By the upper bound theory, we know that after $m$ iterations, no $d$ values will ever change. Therefore, no $d$ values will change in the $(m + 1)$-th iteration. However, we do not know the exact $m$ value in advance, we cannot make the algorithm iterate exactly $m$ times and then terminate. If we try to make the algorithm stop when every $d$ values do not change anymore, then it will stop after $m + 1$ iterations. 24.1-4 Modify the Bellman-Ford algorithm so that it sets $v.d$ to $-\\infty$ for all vertices $v$ for which there is a negative-weight cycle on some path from the source to $v$. BELLMAN - FORD ' ( G , w , s ) INITIALIZE - SINGLE - SOURCE ( G , s ) for i = 1 to | G . V | - 1 for each edge ( u , v ) \u2208 G . E RELAX ( u , v , w ) for each edge ( u , v ) \u2208 G . E if v . d > u . d + w ( u , v ) mark v for each vertex v \u2208 marked vertices FOLLOW - AND - MARK - PRED ( v ) FOLLOW - AND - MARK - PRED ( v ) if v != NIL and v . d != - \u221e v . d = - \u221e FOLLOW - AND - MARK - PRED ( v . \u03c0 ) else return After running $\\text{BELLMAN-FORD}'$, run $\\text{DFS}$ with all vertices on negative-weight cycles as source vertices. All the vertices that can be reached from these vertices should have their $d$ attributes set to $-\\infty$. 24.1-5 $\\star$ Let $G = (V, E)$ be a weighted, directed graph with weight function $w : E \\rightarrow \\mathbb R$. Give an $O(VE)$-time algorithm to find, for each vertex $v \\in V$, the value $\\delta^*(v) = \\min_{u \\in V} \\{\\delta(u, v)\\}$. RELAX ( u , v , w ) if v . d > min ( w ( u , v ), w ( u , v ) + u . d ) v . d = min ( w ( u , v ), w ( u , v ) + u . d ) v . \u03c0 = u . \u03c0 24.1-6 $\\star$ Suppose that a weighted, directed graph $G = (V, E)$ has a negative-weight cycle. Give an efficient algorithm to list the vertices of one such cycle. Prove that your algorithm is correct. Based on exercise 24.1-4, $\\text{DFS}$ from a vertex $u$ that $u.d = -\\infty$, if the weight sum on the search path is negative and the next vertex is $\\text{BLACK}$, then the search path forms a negative-weight cycle.","title":"24.1 The Bellman-Ford algorithm"},{"location":"Chap24/24.1/#241-1","text":"Run the Bellman-Ford algorithm on the directed graph of Figure 24.4, using vertex $z$ as the source. In each pass, relax edges in the same order as in the figure, and show the $d$ and $\\pi$ values after each pass. Now, change the weight of edge $(z, x)$ to $4$ and run the algorithm again, using $s$ as the source. Using vertex $z$ as the source: $d$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline \\infty & \\infty & \\infty & \\infty & 0 \\\\ 2 & \\infty & 7 & \\infty & 0 \\\\ 2 & 5 & 7 & 9 & 0 \\\\ 2 & 5 & 6 & 9 & 0 \\\\ 2 & 4 & 6 & 9 & 0 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ z & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\ z & x & z & s & \\text{NIL} \\\\ z & x & y & s & \\text{NIL} \\\\ z & x & y & s & \\text{NIL} \\end{array} $$ Changing the weight of edge $(z, x)$ to $4$: $d$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline 0 & \\infty & \\infty & \\infty & \\infty \\\\ 0 & 6 & \\infty & 7 & \\infty \\\\ 0 & 6 & 4 & 7 & 2 \\\\ 0 & 2 & 4 & 7 & 2 \\\\ 0 & 2 & 4 & 7 & -2 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & s & \\text{NIL} & s & \\text{NIL} \\\\ \\text{NIL} & s & y & s & t \\\\ \\text{NIL} & x & y & s & t \\\\ \\text{NIL} & x & y & s & t \\end{array} $$ Consider edge $(z, x)$, it'll return $\\text{FALSE}$ since $x.d = 4 > z.d + w(z, x) = -2 + 4$.","title":"24.1-1"},{"location":"Chap24/24.1/#241-2","text":"Prove Corollary 24.3. Suppose there is a path from $s$ to $v$. Then there must be a shortest such path of length $\\delta(s, v)$. It must have finite length since it contains at most $|V| - 1$ edges and each edge has finite length. By Lemma 24.2, $v.d = \\delta(s, v) < \\infty$ upon termination. On the other hand, suppose $v.d < \\infty$ when $\\text{BELLMAN-FORD}$ terminates. Recall that $v.d$ is monotonically decreasing throughout the algorithm, and $\\text{RELAX}$ will update $v.d$ only if $u.d + w(u, v) < v.d$ for some $u$ adjacent to $v$. Moreover, we update $v.\\pi = u$ at this point, so $v$ has an ancestor in the predecessor subgraph. Since this is a tree rooted at $s$, there must be a path from $s$ to $v$ in this tree. Every edge in the tree is also an edge in $G$, so there is also a path in $G$ from $s$ to $v$.","title":"24.1-2"},{"location":"Chap24/24.1/#241-3","text":"Given a weighted, directed graph $G = (V, E)$ with no negative-weight cycles, let $m$ be the maximum over all vertices $v \\in V$ of the minimum number of edges in a shortest path from the source $s$ to $v$. (Here, the shortest path is by weight, not the number of edges.) Suggest a simple change to the Bellman-Ford algorithm that allows it to terminate in $m + 1$ passes, even if $m$ is not known in advance. By the upper bound theory, we know that after $m$ iterations, no $d$ values will ever change. Therefore, no $d$ values will change in the $(m + 1)$-th iteration. However, we do not know the exact $m$ value in advance, we cannot make the algorithm iterate exactly $m$ times and then terminate. If we try to make the algorithm stop when every $d$ values do not change anymore, then it will stop after $m + 1$ iterations.","title":"24.1-3"},{"location":"Chap24/24.1/#241-4","text":"Modify the Bellman-Ford algorithm so that it sets $v.d$ to $-\\infty$ for all vertices $v$ for which there is a negative-weight cycle on some path from the source to $v$. BELLMAN - FORD ' ( G , w , s ) INITIALIZE - SINGLE - SOURCE ( G , s ) for i = 1 to | G . V | - 1 for each edge ( u , v ) \u2208 G . E RELAX ( u , v , w ) for each edge ( u , v ) \u2208 G . E if v . d > u . d + w ( u , v ) mark v for each vertex v \u2208 marked vertices FOLLOW - AND - MARK - PRED ( v ) FOLLOW - AND - MARK - PRED ( v ) if v != NIL and v . d != - \u221e v . d = - \u221e FOLLOW - AND - MARK - PRED ( v . \u03c0 ) else return After running $\\text{BELLMAN-FORD}'$, run $\\text{DFS}$ with all vertices on negative-weight cycles as source vertices. All the vertices that can be reached from these vertices should have their $d$ attributes set to $-\\infty$.","title":"24.1-4"},{"location":"Chap24/24.1/#241-5-star","text":"Let $G = (V, E)$ be a weighted, directed graph with weight function $w : E \\rightarrow \\mathbb R$. Give an $O(VE)$-time algorithm to find, for each vertex $v \\in V$, the value $\\delta^*(v) = \\min_{u \\in V} \\{\\delta(u, v)\\}$. RELAX ( u , v , w ) if v . d > min ( w ( u , v ), w ( u , v ) + u . d ) v . d = min ( w ( u , v ), w ( u , v ) + u . d ) v . \u03c0 = u . \u03c0","title":"24.1-5 $\\star$"},{"location":"Chap24/24.1/#241-6-star","text":"Suppose that a weighted, directed graph $G = (V, E)$ has a negative-weight cycle. Give an efficient algorithm to list the vertices of one such cycle. Prove that your algorithm is correct. Based on exercise 24.1-4, $\\text{DFS}$ from a vertex $u$ that $u.d = -\\infty$, if the weight sum on the search path is negative and the next vertex is $\\text{BLACK}$, then the search path forms a negative-weight cycle.","title":"24.1-6 $\\star$"},{"location":"Chap24/24.2/","text":"24.2-1 Run $\\text{DAG-SHORTEST-PATHS}$ on the directed graph of Figure 24.5, using vertex $r$ as the source. $d$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline 0 & \\infty & \\infty & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & 11 & \\infty & \\infty \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & s & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\end{array} $$ 24.2-2 Suppose we change line 3 of $\\text{DAG-SHORTEST-PATHS}$ to read 3 for the first | V | - 1 vertices , taken in topologically sorted order Show that the procedure would remain correct. When we reach vertex $v$, the last vertex in the topological sort, it must have $out\\text-degree$ $0$. Otherwise there would be an edge pointing from a later vertex to an earlier vertex in the ordering, a contradiction. Thus, the body of the for-loop of line 4 is never entered for this final vertex, so we may as well not consider it. 24.2-3 The PERT chart formulation given above is somewhat unnatural. In a more natural structure, vertices would represent jobs and edges would represent sequencing constraints; that is, edge $(u, v)$ would indicate that job $u$ must be performed before job $v$. We would then assign weights to vertices, not edges. Modify the $\\text{DAG-SHORTEST-PATHS}$ procedure so that it finds a longest path in a directed acyclic graph with weighted vertices in linear time. There are two ways to transform a PERT chart $G = (V, E)$ with weights on the vertices to a PERT chart $G' = (V', E')$ with weights on edges. Both ways satisfy $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$, so we can scan $G'$ using the same algorithm to find the longest path through a directed acyclic graph. In the first way, we transform each vertex $v \\in V$ into two vertices $v'$ and $v''$ in $V'$. All edges in $E$ that enters $V$ will also enter $V'$ in $E'$, and all edges in $E$ that leaves $V$ will leave $V''$ in $E'$. Thus, if $(u, v) \\in E$, then $(u'', v') \\in E'$. All such edges have weight 0, so we can put edges $(v', v'')$ into $E'$ for all vertices $v \\in V$, and these edges are given the weight of the corresponding vertex $v$ in $G$. Finally, we get $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$, and the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$. In the second way, we leave vertices in $V$, but try to add one new source vertex $s$ to $V'$, given that $V' = V \\cup \\{s\\}$. All edges of $E$ are in $E'$, and $E'$ also includes an edge $(s, v)$ for every vertex $v \\in V$ that has in-degree 0 in $G$. Thus, the only vertex with in-degree 0 in $G'$ is the new source $s$. The weight of edge $(u, v) \\in E'$ is the weight of vertex $v$ in $G$. We have the weight of each entering edge in $G'$ is the weight of the vertex it enters in $G$. 24.2-4 Give an efficient algorithm to count the total number of paths in a directed acyclic graph. Analyze your algorithm. We will compute the total number of paths by counting the number of paths whose start point is at each vertex $v$, which will be stored in an attribute $v.paths$. Assume that initial we have $v.paths = 0$ for all $v \\in V$. Since all vertices adjacent to $u$ occur later in the topological sort and the final vertex has no neighbors, line 4 is well-defined. Topological sort takes $O(V + E)$ and the nested for-loops take $O(V + E)$ so the total runtime is $O(V + E)$. PATHS ( G ) topologically sort the vertices of G for each vertex u , taken in topologically sorted order for each v \u2208 G . Adj [ u ] v . paths = u . paths + 1 + v . paths return the sum of all paths attributes","title":"24.2 Single-source shortest paths in directed acyclic graphs"},{"location":"Chap24/24.2/#242-1","text":"Run $\\text{DAG-SHORTEST-PATHS}$ on the directed graph of Figure 24.5, using vertex $r$ as the source. $d$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline 0 & \\infty & \\infty & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & \\infty & \\infty & \\infty \\\\ 0 & 5 & 3 & 11 & \\infty & \\infty \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\\\ 0 & 5 & 3 & 10 & 7 & 5 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{cccccc} r & s & t & x & y & z \\\\ \\hline \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & s & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\\\ \\text{NIL} & r & r & t & t & t \\end{array} $$","title":"24.2-1"},{"location":"Chap24/24.2/#242-2","text":"Suppose we change line 3 of $\\text{DAG-SHORTEST-PATHS}$ to read 3 for the first | V | - 1 vertices , taken in topologically sorted order Show that the procedure would remain correct. When we reach vertex $v$, the last vertex in the topological sort, it must have $out\\text-degree$ $0$. Otherwise there would be an edge pointing from a later vertex to an earlier vertex in the ordering, a contradiction. Thus, the body of the for-loop of line 4 is never entered for this final vertex, so we may as well not consider it.","title":"24.2-2"},{"location":"Chap24/24.2/#242-3","text":"The PERT chart formulation given above is somewhat unnatural. In a more natural structure, vertices would represent jobs and edges would represent sequencing constraints; that is, edge $(u, v)$ would indicate that job $u$ must be performed before job $v$. We would then assign weights to vertices, not edges. Modify the $\\text{DAG-SHORTEST-PATHS}$ procedure so that it finds a longest path in a directed acyclic graph with weighted vertices in linear time. There are two ways to transform a PERT chart $G = (V, E)$ with weights on the vertices to a PERT chart $G' = (V', E')$ with weights on edges. Both ways satisfy $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$, so we can scan $G'$ using the same algorithm to find the longest path through a directed acyclic graph. In the first way, we transform each vertex $v \\in V$ into two vertices $v'$ and $v''$ in $V'$. All edges in $E$ that enters $V$ will also enter $V'$ in $E'$, and all edges in $E$ that leaves $V$ will leave $V''$ in $E'$. Thus, if $(u, v) \\in E$, then $(u'', v') \\in E'$. All such edges have weight 0, so we can put edges $(v', v'')$ into $E'$ for all vertices $v \\in V$, and these edges are given the weight of the corresponding vertex $v$ in $G$. Finally, we get $|V'| \\le 2|V|$ and $|E'| \\le |V| + |E|$, and the edge weight of each path in $G'$ equals the vertex weight of the corresponding path in $G$. In the second way, we leave vertices in $V$, but try to add one new source vertex $s$ to $V'$, given that $V' = V \\cup \\{s\\}$. All edges of $E$ are in $E'$, and $E'$ also includes an edge $(s, v)$ for every vertex $v \\in V$ that has in-degree 0 in $G$. Thus, the only vertex with in-degree 0 in $G'$ is the new source $s$. The weight of edge $(u, v) \\in E'$ is the weight of vertex $v$ in $G$. We have the weight of each entering edge in $G'$ is the weight of the vertex it enters in $G$.","title":"24.2-3"},{"location":"Chap24/24.2/#242-4","text":"Give an efficient algorithm to count the total number of paths in a directed acyclic graph. Analyze your algorithm. We will compute the total number of paths by counting the number of paths whose start point is at each vertex $v$, which will be stored in an attribute $v.paths$. Assume that initial we have $v.paths = 0$ for all $v \\in V$. Since all vertices adjacent to $u$ occur later in the topological sort and the final vertex has no neighbors, line 4 is well-defined. Topological sort takes $O(V + E)$ and the nested for-loops take $O(V + E)$ so the total runtime is $O(V + E)$. PATHS ( G ) topologically sort the vertices of G for each vertex u , taken in topologically sorted order for each v \u2208 G . Adj [ u ] v . paths = u . paths + 1 + v . paths return the sum of all paths attributes","title":"24.2-4"},{"location":"Chap24/24.3/","text":"24.3-1 Run Dijkstra's algorithm on the directed graph of Figure 24.2, first using vertex $s$ as the source and then using vertex $z$ as the source. In the style of Figure 24.6, show the $d$ and $\\pi$ values and the vertices in set $S$ after each iteration of the while loop. $s$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 0 & 3 & \\infty & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & s & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & s & t & s & \\text{NIL} \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\end{array} $$ $z$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 3 & \\infty & 7 & \\infty & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline z & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\end{array} $$ 24.3-2 Give a simple example of a directed graph with negative-weight edges for which Dijkstra's algorithm produces incorrect answers. Why doesn't the proof of Theorem 24.6 go through when negative-weight edges are allowed? Consider any graph with a negative cycle. $\\text{RELAX}$ is called a finite number of times but the distance to any vertex on the cycle is $-\\infty$, so Dijkstra's algorithm cannot possibly be correct here. The proof of theorem 24.6 doesn't go through because we can no longer guarantee that $$\\delta(s, y) \\le \\delta(s, u).$$ 24.3-3 Suppose we change line 4 of Dijkstra's algorithm to the following. 4 while | Q | > 1 This change causes the while loop to execute $|V| - 1$ times instead of $|V|$ times. Is this proposed algorithm correct? Yes, the algorithm is correct. Let $u$ be the leftover vertex that does not get extracted from the priority queue $Q$. If $u$ is not reachable from $s$, then $$u.d = \\delta(s, u) = \\infty.$$ If $u$ is reachable from $s$, then there is a shortest path $$p = s \\rightarrow x \\rightarrow u.$$ When the node $x$ was extracted, $$x.d = \\delta(s, x)$$ and then the edge $(x, u)$ was relaxed; thus, $$u.d = \\delta(s, u).$$ 24.3-4 Professor Gaedel has written a program that he claims implements Dijkstra's algorithm. The program produces $v.d$ and $v.\\pi$ for each vertex $v \\in V$. Give an $O(V + E)$-time algorithm to check the output of the professor's program. It should determine whether the $d$ and $\\pi$ attributes match those of some shortest-paths tree. You may assume that all edge weights are nonnegative. (Removed) 24.3-5 Professor Newman thinks that he has worked out a simpler proof of correctness for Dijkstra's algorithm. He claims that Dijkstra's algorithm relaxes the edges of every shortest path in the graph in the order in which they appear on the path, and therefore the path-relaxation property applies to every vertex reachable from the source. Show that the professor is mistaken by constructing a directed graph for which Dijkstra's algorithm could relax the edges of a shortest path out of order. (Removed) 24.3-6 We are given a directed graph $G = (V, E)$ on which each edge $(u, v) \\in E$ has an associated value $r(u, v)$, which is a real number in the range $0 \\le r(u, v) \\le 1$ that represents the reliability of a communication channel from vertex $u$ to vertex $v$. We interpret $r(u, v)$ as the probability that the channel from $u$ to $v$ will not fail, and we assume that these probabilities are independent. Give an efficient algorithm to find the most reliable path between two given vertices. (Removed) 24.3-7 Let $G = (V, E)$ be a weighted, directed graph with positive weight function $w: E \\rightarrow \\{1, 2, \\ldots, W\\}$ for some positive integer $W$, and assume that no two vertices have the same shortest-path weights from source vertex $s$. Now suppose that we define an unweighted, directed graph $G' = (V \\cup V', E')$ by replacing each edge $(u, v) \\in E$ with $w(u, v)$ unit-weight edges in series. How many vertices does $G'$ have? Now suppose that we run a breadth-first search on $G'$. Show that the order in which the breadth-first search of $G'$ colors vertices in $V$ black is the same as the order in which Dijkstra's algorithm extracts the vertices of $V$ from the priority queue when it runs on $G$. $V + \\sum_{(u, v) \\in E} w(u, v) - E$. 24.3-8 Let $G = (V, E)$ be a weighted, directed graph with nonnegative weight function $w: E \\rightarrow \\{0, 1, \\ldots, W\\}$ for some nonnegative integer $W$. Modify Dijkstra's algorithm to compute the shortest paths from a given source vertex s in $O(WV + E)$ time. (Removed) 24.3-9 Modify your algorithm from Exercise 24.3-8 to run in $O((V + E) \\lg W)$ time. ($\\textit{Hint:}$ How many distinct shortest-path estimates can there be in $V - S$ at any point in time?) (Removed) 24.3-10 Suppose that we are given a weighted, directed graph $G = (V, E)$ in which edges that leave the source vertex $s$ may have negative weights, all other edge weights are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra's algorithm correctly finds shortest paths from $s$ in this graph. The proof of correctness, Theorem 24.6, goes through exactly as stated in the text. The key fact was that $\\delta(s, y) \\le \\delta(s, u)$. It is claimed that this holds because there are no negative edge weights, but in fact that is stronger than is needed. This always holds if $y$ occurs on a shortest path from $s$ to $u$ and $y \\ne s$ because all edges on the path from $y$ to $u$ have nonnegative weight. If any had negative weight, this would imply that we had \"gone back\" to an edge incident with $s$, which implies that a cycle is involved in the path, which would only be the case if it were a negative-weight cycle. However, these are still forbidden.","title":"24.3 Dijkstra's algorithm"},{"location":"Chap24/24.3/#243-1","text":"Run Dijkstra's algorithm on the directed graph of Figure 24.2, first using vertex $s$ as the source and then using vertex $z$ as the source. In the style of Figure 24.6, show the $d$ and $\\pi$ values and the vertices in set $S$ after each iteration of the while loop. $s$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 0 & 3 & \\infty & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & \\infty \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\\\ 0 & 3 & 9 & 5 & 11 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline \\text{NIL} & s & \\text{NIL} & \\text{NIL} & \\text{NIL} \\\\ \\text{NIL} & s & t & s & \\text{NIL} \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\\\ \\text{NIL} & s & t & s & y \\end{array} $$ $z$ as the source: $d$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline 3 & \\infty & 7 & \\infty & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\\\ 3 & 6 & 7 & 8 & 0 \\end{array} $$ $\\pi$ values: $$ \\begin{array}{ccccc} s & t & x & y & z \\\\ \\hline z & \\text{NIL} & z & \\text{NIL} & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\\\ z & s & z & s & \\text{NIL} \\end{array} $$","title":"24.3-1"},{"location":"Chap24/24.3/#243-2","text":"Give a simple example of a directed graph with negative-weight edges for which Dijkstra's algorithm produces incorrect answers. Why doesn't the proof of Theorem 24.6 go through when negative-weight edges are allowed? Consider any graph with a negative cycle. $\\text{RELAX}$ is called a finite number of times but the distance to any vertex on the cycle is $-\\infty$, so Dijkstra's algorithm cannot possibly be correct here. The proof of theorem 24.6 doesn't go through because we can no longer guarantee that $$\\delta(s, y) \\le \\delta(s, u).$$","title":"24.3-2"},{"location":"Chap24/24.3/#243-3","text":"Suppose we change line 4 of Dijkstra's algorithm to the following. 4 while | Q | > 1 This change causes the while loop to execute $|V| - 1$ times instead of $|V|$ times. Is this proposed algorithm correct? Yes, the algorithm is correct. Let $u$ be the leftover vertex that does not get extracted from the priority queue $Q$. If $u$ is not reachable from $s$, then $$u.d = \\delta(s, u) = \\infty.$$ If $u$ is reachable from $s$, then there is a shortest path $$p = s \\rightarrow x \\rightarrow u.$$ When the node $x$ was extracted, $$x.d = \\delta(s, x)$$ and then the edge $(x, u)$ was relaxed; thus, $$u.d = \\delta(s, u).$$","title":"24.3-3"},{"location":"Chap24/24.3/#243-4","text":"Professor Gaedel has written a program that he claims implements Dijkstra's algorithm. The program produces $v.d$ and $v.\\pi$ for each vertex $v \\in V$. Give an $O(V + E)$-time algorithm to check the output of the professor's program. It should determine whether the $d$ and $\\pi$ attributes match those of some shortest-paths tree. You may assume that all edge weights are nonnegative. (Removed)","title":"24.3-4"},{"location":"Chap24/24.3/#243-5","text":"Professor Newman thinks that he has worked out a simpler proof of correctness for Dijkstra's algorithm. He claims that Dijkstra's algorithm relaxes the edges of every shortest path in the graph in the order in which they appear on the path, and therefore the path-relaxation property applies to every vertex reachable from the source. Show that the professor is mistaken by constructing a directed graph for which Dijkstra's algorithm could relax the edges of a shortest path out of order. (Removed)","title":"24.3-5"},{"location":"Chap24/24.3/#243-6","text":"We are given a directed graph $G = (V, E)$ on which each edge $(u, v) \\in E$ has an associated value $r(u, v)$, which is a real number in the range $0 \\le r(u, v) \\le 1$ that represents the reliability of a communication channel from vertex $u$ to vertex $v$. We interpret $r(u, v)$ as the probability that the channel from $u$ to $v$ will not fail, and we assume that these probabilities are independent. Give an efficient algorithm to find the most reliable path between two given vertices. (Removed)","title":"24.3-6"},{"location":"Chap24/24.3/#243-7","text":"Let $G = (V, E)$ be a weighted, directed graph with positive weight function $w: E \\rightarrow \\{1, 2, \\ldots, W\\}$ for some positive integer $W$, and assume that no two vertices have the same shortest-path weights from source vertex $s$. Now suppose that we define an unweighted, directed graph $G' = (V \\cup V', E')$ by replacing each edge $(u, v) \\in E$ with $w(u, v)$ unit-weight edges in series. How many vertices does $G'$ have? Now suppose that we run a breadth-first search on $G'$. Show that the order in which the breadth-first search of $G'$ colors vertices in $V$ black is the same as the order in which Dijkstra's algorithm extracts the vertices of $V$ from the priority queue when it runs on $G$. $V + \\sum_{(u, v) \\in E} w(u, v) - E$.","title":"24.3-7"},{"location":"Chap24/24.3/#243-8","text":"Let $G = (V, E)$ be a weighted, directed graph with nonnegative weight function $w: E \\rightarrow \\{0, 1, \\ldots, W\\}$ for some nonnegative integer $W$. Modify Dijkstra's algorithm to compute the shortest paths from a given source vertex s in $O(WV + E)$ time. (Removed)","title":"24.3-8"},{"location":"Chap24/24.3/#243-9","text":"Modify your algorithm from Exercise 24.3-8 to run in $O((V + E) \\lg W)$ time. ($\\textit{Hint:}$ How many distinct shortest-path estimates can there be in $V - S$ at any point in time?) (Removed)","title":"24.3-9"},{"location":"Chap24/24.3/#243-10","text":"Suppose that we are given a weighted, directed graph $G = (V, E)$ in which edges that leave the source vertex $s$ may have negative weights, all other edge weights are nonnegative, and there are no negative-weight cycles. Argue that Dijkstra's algorithm correctly finds shortest paths from $s$ in this graph. The proof of correctness, Theorem 24.6, goes through exactly as stated in the text. The key fact was that $\\delta(s, y) \\le \\delta(s, u)$. It is claimed that this holds because there are no negative edge weights, but in fact that is stronger than is needed. This always holds if $y$ occurs on a shortest path from $s$ to $u$ and $y \\ne s$ because all edges on the path from $y$ to $u$ have nonnegative weight. If any had negative weight, this would imply that we had \"gone back\" to an edge incident with $s$, which implies that a cycle is involved in the path, which would only be the case if it were a negative-weight cycle. However, these are still forbidden.","title":"24.3-10"},{"location":"Chap24/24.4/","text":"24.4-1 Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le & 1, \\\\ x_1 - x_4 & \\le & -4, \\\\ x_2 - x_3 & \\le & 2, \\\\ x_2 - x_5 & \\le & 7, \\\\ x_2 - x_6 & \\le & 5, \\\\ x_3 - x_6 & \\le & 10, \\\\ x_4 - x_2 & \\le & 2, \\\\ x_5 - x_1 & \\le & -1, \\\\ x_5 - x_4 & \\le & 3, \\\\ x_6 - x_3 & \\le & 8 \\end{aligned} $$ Our vertices of the constraint graph will be $$\\{v_0, v_1, v_2, v_3, v_4, v_5, v_6\\}.$$ The edges will be $$(v_0, v_1), (v_0, v_2), (v_0, v_3), (v_0, v_4), (v_0, v_5), (v_0, v_6), (v_2, v_1), (v_4, v_1), (v_3, v_2), (v_5, v_2), (v_6, v_2), (v_6, v_3),$$ with edge weights $$0, 0, 0, 0, 0, 0, 1, -4, 2, 7, 5, 10, 2, -1, 3, -8$$ respectively. Then, computing $$(\\delta(v_0, v_1), \\delta(v_0, v_2), \\delta(v_0, v_3), \\delta(v_0, v_4), \\delta(v_0, v_5), \\delta(v_0, v_6)),$$ we get $$(-5, -3, 0, -1, -6, -8),$$ which is a feasible solution by Theorem 24.9. 24.4-2 Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le &4, \\\\ x_1 - x_5 & \\le &5, \\\\ x_2 - x_4 & \\le &-6, \\\\ x_3 - x_2 & \\le &1, \\\\ x_4 - x_1 & \\le &3, \\\\ x_4 - x_3 & \\le &5, \\\\ x_4 - x_5 & \\le &10, \\\\ x_5 - x_3 & \\le &-4, \\\\ x_5 - x_4 & \\le &-8. \\end{aligned} $$ There is no feasible solution because the constraint graph contains a negative-weight cycle: $(v_1, v_4, v_2, v_3, v_5, v_1)$ has weight $-1$. 24.4-3 Can any shortest-path weight from the new vertex $v_0$ in a constraint graph be positive? Explain. No, it cannot be positive. This is because for every vertex $v \\ne v_0$, there is an edge $(v_0, v)$ with weight zero. So, there is some path from the new vertex to every other of weight zero. Since $\\delta(v_0, v)$ is a minimum weight of all paths, it cannot be greater than the weight of this weight zero path that consists of a single edge. 24.4-4 Express the single-pair shortest-path problem as a linear program. (Removed) 24.4-5 Show how to modify the Bellman-Ford algorithm slightly so that when we use it to solve a system of difference constraints with $m$ inequalities on $n$ unknowns, the running time is $O(nm)$. We can follow the advice of problem 14.4-7 and solve the system of constraints on a modified constraint graph in which there is no new vertex $v_0$. This is simply done by initializing all of the vertices to have a $d$ value of $0$ before running the iterated relaxations of Bellman Ford. Since we don't add a new vertex and the $n$ edges going from it to to vertex corresponding to each variable, we are just running Bellman Ford on a graph with $n$ vertices and $m$ edges, and so it will have a runtime of $O(mn)$. 24.4-6 Suppose that in addition to a system of difference constraints, we want to handle equality constraints of the form $x_i = x_j + b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. To obtain the equality constraint $x_i = x_j + b_k$ we simply use the inequalities $x_i - x_j \\le b_k$ and $x_j - x_i \\le -bk$, then solve the problem as usual. 24.4-7 Show how to solve a system of difference constraints by a Bellman-Ford-like algorithm that runs on a constraint graph without the extra vertex $v_0$. (Removed) 24.4-8 $\\star$ Let $Ax \\le b$ be a system of $m$ difference constraints in $n$ unknowns. Show that the Bellman-Ford algorithm, when run on the corresponding constraint graph, maximizes $\\sum_{i = 1}^n x_i$ subject to $Ax \\le b$ and $x_i \\le 0$ for all $x_i$. Bellman-Ford correctly solves the system of difference constraints so $Ax \\le b$ is always satisfied. We also have that $x_i = \\delta(v_0, v_i) \\le w(v_0, v_i) = 0$ so $x_i \\le 0$ for all $i$. To show that $\\sum x_i$ is maximized, we'll show that for any feasible solution $(y_1, y_2, \\ldots, y_n)$ which satisfies the constraints we have $yi \\le \\delta(v_0, v_i) = x_i$. Let $v_0, v_{i_1}, \\ldots, v_{i_k}$ be a shortest path from $v_0$ to $v_i$ in the constraint graph. Then we must have the constraints $y_{i_2} - y_{i_1} \\le w(v_{i_1}, v_{i_2}), \\ldots, y_{i_k} - y_{i_{k - 1}} \\le w(v_{i_{k - 1}},v_{i_k})$. Summing these up we have $$y_i \\le y_i - y_1 \\le \\sum_{m = 2}^k w(v_{i_m}, v_{i_{m - 1}}) = \\delta(v_0, v_i) = x_i.$$ 24.4-9 $\\star$ Show that the Bellman-Ford algorithm, when run on the constraint graph for a system $Ax \\le b$ of difference constraints, minimizes the quantity $(\\max\\{x_i\\} - \\min\\{x_i\\})$ subject to $Ax \\le b$. Explain how this fact might come in handy if the algorithm is used to schedule construction jobs. We can see that the Bellman-Ford algorithm run on the graph whose construction is described in this section causes the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ to be minimized. We know that the largest value assigned to any of the vertices in the constraint graph is a $0$. It is clear that it won't be greater than zero, since just the single edge path to each of the vertices has cost zero. We also know that we cannot have every vertex having a shortest path with negative weight. To see this, notice that this would mean that the pointer for each vertex has it's $p$ value going to some other vertex that is not the source. This means that if we follow the procedure for reconstructing the shortest path for any of the vertices, we have that it can never get back to the source, a contradiction to the fact that it is a shortest path from the source to that vertex. Next, we note that when we run Bellman-Ford, we are maximizing $\\min\\{x_i\\}$. The shortest distance in the constraint graphs is the bare minimum of what is required in order to have all the constraints satisfied, if we were to increase any of the values we would be violating a constraint. This could be in handy when scheduling construction jobs because the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ is equal to the difference in time between the last task and the first task. Therefore, it means that minimizing it would mean that the total time that all the jobs takes is also minimized. And, most people want the entire process of construction to take as short of a time as possible. 24.4-10 Suppose that every row in the matrix $A$ of a linear program $Ax \\le b$ corresponds to a difference constraint, a single-variable constraint of the form $x_i \\le b_k$, or a singlevariable constraint of the form $-x_i \\le b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. (Removed) 24.4-11 Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and all of the unknowns $x_i$ must be integers. To do this, just take the floor of (largest integer that is less than or equal to) each of the $b$ values and solve the resulting integer difference problem. These modified constraints will be admitting exactly the same set of assignments since we required that the solution have integer values assigned to the variables. This is because since the variables are integers, all of their differences will also be integers. For an integer to be less than or equal to a real number, it is necessary and sufficient for it to be less than or equal to the floor of that real number. 24.4-12 $\\star$ Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and a specified subset of some, but not necessarily all, of the unknowns $x_i$ must be integers. To solve the problem of $Ax \\le b$ where the elements of $b$ are real-valued we carry out the same procedure as before, running Bellman-Ford, but allowing our edge weights to be real-valued. To impose the integer condition on the $x_i$'s, we modify the $\\text{RELAX}$ procedure. Suppose we call $\\text{RELAX}(v_i, v_j, w)$ where $v_j$ is required to be integral valued. If $v_j.d > \\lfloor v_i.d + w(v_i, v_j) \\rfloor$, set $v_j.d = \\lfloor v_i.d + w(v_i, v_j) \\rfloor$. This guarantees that the condition that $v_j.d - v_i.d \\le w(v_i, v_j)$ as desired. It also ensures that $v_j$ is integer valued. Since the triangle inequality still holds, $x = (v_1.d, v_2.d, \\ldots, v_n.d)$ is a feasible solution for the system, provided that $G$ contains no negative weight cycles.","title":"24.4 Difference constraints and shortest paths"},{"location":"Chap24/24.4/#244-1","text":"Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le & 1, \\\\ x_1 - x_4 & \\le & -4, \\\\ x_2 - x_3 & \\le & 2, \\\\ x_2 - x_5 & \\le & 7, \\\\ x_2 - x_6 & \\le & 5, \\\\ x_3 - x_6 & \\le & 10, \\\\ x_4 - x_2 & \\le & 2, \\\\ x_5 - x_1 & \\le & -1, \\\\ x_5 - x_4 & \\le & 3, \\\\ x_6 - x_3 & \\le & 8 \\end{aligned} $$ Our vertices of the constraint graph will be $$\\{v_0, v_1, v_2, v_3, v_4, v_5, v_6\\}.$$ The edges will be $$(v_0, v_1), (v_0, v_2), (v_0, v_3), (v_0, v_4), (v_0, v_5), (v_0, v_6), (v_2, v_1), (v_4, v_1), (v_3, v_2), (v_5, v_2), (v_6, v_2), (v_6, v_3),$$ with edge weights $$0, 0, 0, 0, 0, 0, 1, -4, 2, 7, 5, 10, 2, -1, 3, -8$$ respectively. Then, computing $$(\\delta(v_0, v_1), \\delta(v_0, v_2), \\delta(v_0, v_3), \\delta(v_0, v_4), \\delta(v_0, v_5), \\delta(v_0, v_6)),$$ we get $$(-5, -3, 0, -1, -6, -8),$$ which is a feasible solution by Theorem 24.9.","title":"24.4-1"},{"location":"Chap24/24.4/#244-2","text":"Find a feasible solution or determine that no feasible solution exists for the following system of difference constraints: $$ \\begin{aligned} x_1 - x_2 & \\le &4, \\\\ x_1 - x_5 & \\le &5, \\\\ x_2 - x_4 & \\le &-6, \\\\ x_3 - x_2 & \\le &1, \\\\ x_4 - x_1 & \\le &3, \\\\ x_4 - x_3 & \\le &5, \\\\ x_4 - x_5 & \\le &10, \\\\ x_5 - x_3 & \\le &-4, \\\\ x_5 - x_4 & \\le &-8. \\end{aligned} $$ There is no feasible solution because the constraint graph contains a negative-weight cycle: $(v_1, v_4, v_2, v_3, v_5, v_1)$ has weight $-1$.","title":"24.4-2"},{"location":"Chap24/24.4/#244-3","text":"Can any shortest-path weight from the new vertex $v_0$ in a constraint graph be positive? Explain. No, it cannot be positive. This is because for every vertex $v \\ne v_0$, there is an edge $(v_0, v)$ with weight zero. So, there is some path from the new vertex to every other of weight zero. Since $\\delta(v_0, v)$ is a minimum weight of all paths, it cannot be greater than the weight of this weight zero path that consists of a single edge.","title":"24.4-3"},{"location":"Chap24/24.4/#244-4","text":"Express the single-pair shortest-path problem as a linear program. (Removed)","title":"24.4-4"},{"location":"Chap24/24.4/#244-5","text":"Show how to modify the Bellman-Ford algorithm slightly so that when we use it to solve a system of difference constraints with $m$ inequalities on $n$ unknowns, the running time is $O(nm)$. We can follow the advice of problem 14.4-7 and solve the system of constraints on a modified constraint graph in which there is no new vertex $v_0$. This is simply done by initializing all of the vertices to have a $d$ value of $0$ before running the iterated relaxations of Bellman Ford. Since we don't add a new vertex and the $n$ edges going from it to to vertex corresponding to each variable, we are just running Bellman Ford on a graph with $n$ vertices and $m$ edges, and so it will have a runtime of $O(mn)$.","title":"24.4-5"},{"location":"Chap24/24.4/#244-6","text":"Suppose that in addition to a system of difference constraints, we want to handle equality constraints of the form $x_i = x_j + b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. To obtain the equality constraint $x_i = x_j + b_k$ we simply use the inequalities $x_i - x_j \\le b_k$ and $x_j - x_i \\le -bk$, then solve the problem as usual.","title":"24.4-6"},{"location":"Chap24/24.4/#244-7","text":"Show how to solve a system of difference constraints by a Bellman-Ford-like algorithm that runs on a constraint graph without the extra vertex $v_0$. (Removed)","title":"24.4-7"},{"location":"Chap24/24.4/#244-8-star","text":"Let $Ax \\le b$ be a system of $m$ difference constraints in $n$ unknowns. Show that the Bellman-Ford algorithm, when run on the corresponding constraint graph, maximizes $\\sum_{i = 1}^n x_i$ subject to $Ax \\le b$ and $x_i \\le 0$ for all $x_i$. Bellman-Ford correctly solves the system of difference constraints so $Ax \\le b$ is always satisfied. We also have that $x_i = \\delta(v_0, v_i) \\le w(v_0, v_i) = 0$ so $x_i \\le 0$ for all $i$. To show that $\\sum x_i$ is maximized, we'll show that for any feasible solution $(y_1, y_2, \\ldots, y_n)$ which satisfies the constraints we have $yi \\le \\delta(v_0, v_i) = x_i$. Let $v_0, v_{i_1}, \\ldots, v_{i_k}$ be a shortest path from $v_0$ to $v_i$ in the constraint graph. Then we must have the constraints $y_{i_2} - y_{i_1} \\le w(v_{i_1}, v_{i_2}), \\ldots, y_{i_k} - y_{i_{k - 1}} \\le w(v_{i_{k - 1}},v_{i_k})$. Summing these up we have $$y_i \\le y_i - y_1 \\le \\sum_{m = 2}^k w(v_{i_m}, v_{i_{m - 1}}) = \\delta(v_0, v_i) = x_i.$$","title":"24.4-8 $\\star$"},{"location":"Chap24/24.4/#244-9-star","text":"Show that the Bellman-Ford algorithm, when run on the constraint graph for a system $Ax \\le b$ of difference constraints, minimizes the quantity $(\\max\\{x_i\\} - \\min\\{x_i\\})$ subject to $Ax \\le b$. Explain how this fact might come in handy if the algorithm is used to schedule construction jobs. We can see that the Bellman-Ford algorithm run on the graph whose construction is described in this section causes the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ to be minimized. We know that the largest value assigned to any of the vertices in the constraint graph is a $0$. It is clear that it won't be greater than zero, since just the single edge path to each of the vertices has cost zero. We also know that we cannot have every vertex having a shortest path with negative weight. To see this, notice that this would mean that the pointer for each vertex has it's $p$ value going to some other vertex that is not the source. This means that if we follow the procedure for reconstructing the shortest path for any of the vertices, we have that it can never get back to the source, a contradiction to the fact that it is a shortest path from the source to that vertex. Next, we note that when we run Bellman-Ford, we are maximizing $\\min\\{x_i\\}$. The shortest distance in the constraint graphs is the bare minimum of what is required in order to have all the constraints satisfied, if we were to increase any of the values we would be violating a constraint. This could be in handy when scheduling construction jobs because the quantity $\\max\\{x_i\\} - \\min\\{x_i\\}$ is equal to the difference in time between the last task and the first task. Therefore, it means that minimizing it would mean that the total time that all the jobs takes is also minimized. And, most people want the entire process of construction to take as short of a time as possible.","title":"24.4-9 $\\star$"},{"location":"Chap24/24.4/#244-10","text":"Suppose that every row in the matrix $A$ of a linear program $Ax \\le b$ corresponds to a difference constraint, a single-variable constraint of the form $x_i \\le b_k$, or a singlevariable constraint of the form $-x_i \\le b_k$. Show how to adapt the Bellman-Ford algorithm to solve this variety of constraint system. (Removed)","title":"24.4-10"},{"location":"Chap24/24.4/#244-11","text":"Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and all of the unknowns $x_i$ must be integers. To do this, just take the floor of (largest integer that is less than or equal to) each of the $b$ values and solve the resulting integer difference problem. These modified constraints will be admitting exactly the same set of assignments since we required that the solution have integer values assigned to the variables. This is because since the variables are integers, all of their differences will also be integers. For an integer to be less than or equal to a real number, it is necessary and sufficient for it to be less than or equal to the floor of that real number.","title":"24.4-11"},{"location":"Chap24/24.4/#244-12-star","text":"Give an efficient algorithm to solve a system $Ax \\le b$ of difference constraints when all of the elements of $b$ are real-valued and a specified subset of some, but not necessarily all, of the unknowns $x_i$ must be integers. To solve the problem of $Ax \\le b$ where the elements of $b$ are real-valued we carry out the same procedure as before, running Bellman-Ford, but allowing our edge weights to be real-valued. To impose the integer condition on the $x_i$'s, we modify the $\\text{RELAX}$ procedure. Suppose we call $\\text{RELAX}(v_i, v_j, w)$ where $v_j$ is required to be integral valued. If $v_j.d > \\lfloor v_i.d + w(v_i, v_j) \\rfloor$, set $v_j.d = \\lfloor v_i.d + w(v_i, v_j) \\rfloor$. This guarantees that the condition that $v_j.d - v_i.d \\le w(v_i, v_j)$ as desired. It also ensures that $v_j$ is integer valued. Since the triangle inequality still holds, $x = (v_1.d, v_2.d, \\ldots, v_n.d)$ is a feasible solution for the system, provided that $G$ contains no negative weight cycles.","title":"24.4-12 $\\star$"},{"location":"Chap24/24.5/","text":"24.5-1 Give two shortest-paths trees for the directed graph of Figure 24.2 (on page 648) other than the two shown. Since the induced shortest path trees on $\\{s, t, y\\}$ and on $\\{t, x, y, z\\}$ are independent and have to possible configurations each, there are four total arising from that. So, we have the two not shown in the figure are the one consisting of the edges $\\{(s, t), (s, y), (y, x), (x, z)\\}$ and the one consisting of the edges $\\{(s, t), (t, y), (t, x), (y, z)\\}$. 24.5-2 Give an example of a weighted, directed graph $G = (V, E)$ with weight function $w: E \\rightarrow \\mathbb R$ and source vertex $s$ such that $G$ satisfies the following property: For every edge $(u, v) \\in E$, there is a shortest-paths tree rooted at $s$ that contains $(u, v)$ and another shortest-paths tree rooted at $s$ that does not contain $(u, v)$. Let $G$ have $3$ vertices $s$, $x$, and $y$. Let the edges be $(s, x)$, $(s, y)$, and $(x, y)$ with weights $1$, $1$, and $0$ respectively. There are $3$ possible trees on these vertices rooted at $s$, and each is a shortest paths tree which gives $\\delta(s, x) = \\delta(s, y) = 1$. 24.5-3 Embellish the proof of Lemma 24.10 to handle cases in which shortest-path weights are $\\infty$ or $-\\infty$. To modify Lemma 24.10 to allow for possible shortest path weights of $\\infty$ and $-\\infty$, we need to define our addition as $\\infty + c = \\infty$, and $-\\infty + c = -\\infty$. This will make the statement behave correctly, that is, we can take the shortest path from $s$ to $u$ and tack on the edge $(u, v)$ to the end. That is, if there is a negative weight cycle on your way to $u$ and there is an edge from $u$ to $v$, there is a negative weight cycle on our way to $v$. Similarly, if we cannot reach $v$ and there is an edge from $u$ to $v$, we cannot reach $u$. 24.5-4 Let $G = (V, E)$ be a weighted, directed graph with source vertex $s$, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that if a sequence of relaxation steps sets $s.\\pi$ to a non-$\\text{NIL}$ value, then $G$ contains a negative-weight cycle. (Removed) 24.5-5 Let $G = (V, E)$ be a weighted, directed graph with no negative-weight edges. Let $s \\in V$ be the source vertex, and suppose that we allow $v.\\pi$ to be the predecessor of $v$ on any shortest path to $v$ from source $s$ if $v \\in V - \\{s\\}$ is reachable from $s$, and $\\text{NIL}$ otherwise. Give an example of such a graph $G$ and an assignment of $\\pi$ values that produces a cycle in $G_\\pi$. (By Lemma 24.16, such an assignment cannot be produced by a sequence of relaxation steps.) Suppose that we have a grap hon three vertices $\\{s, u, v\\}$ and containing edges $(s, u), (s, v), (u, v), (v, u)$ all with weight $0$. Then, there is a shortest path from $s$ to $v$ of $s$, $u$, $v$ and a shortest path from $s$ to $u$ of $s$ $v$, $u$. Based off of these, we could set $v.\\pi = u$ and $u.\\pi = v$. This then means that there is a cycle consisting of $u, v$ in $G_\\pi$. 24.5-6 Let $G = (V, E)$ be a weighted, directed graph with weight function $w: E \\rightarrow \\mathbb R$ and no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that for every vertex $v \\in V_\\pi$, there exists a path from $s$ to $v$ in $G_\\pi$ and that this property is maintained as an invariant over any sequence of relaxations. We will prove this by induction on the number of relaxations performed. For the base-case, we have just called $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. The only vertex in $V_\\pi$ is $s$, and there is trivially a path from $s$ to itself. Now suppose that after any sequence of $n$ relaxations, for every vertex $v \\in V_\\pi$ there exists a path from $s$ to $v$ in $G_\\pi$. Consider the $(n + 1)$th relaxation. Suppose it is such that $v.d > u.d + w(u, v)$. When we relax $v$, we update $v.\\pi = u.\\pi$. By the induction hypothesis, there was a path from $s$ to $u$ in $G_\\pi$. Now $v$ is in $V_\\pi$, and the path from $s$ to $u$, followed by the edge $(u,v) = (v.\\pi, v)$ is a path from s to $v$ in $G_\\pi$, so the claim holds. 24.5-7 Let $G = (V, E)$ be a weighted, directed graph that contains no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that there exists a sequence of $|V| - 1$ relaxation steps that produces $v.d = \\delta(s, v)$ for all $v \\in V$. (Removed) 24.5-8 Let $G$ be an arbitrary weighted, directed graph with a negative-weight cycle reachable from the source vertex $s$. Show how to construct an infinite sequence of relaxations of the edges of $G$ such that every relaxation causes a shortest-path estimate to change. (Removed)","title":"24.5 Proofs of shortest-paths properties"},{"location":"Chap24/24.5/#245-1","text":"Give two shortest-paths trees for the directed graph of Figure 24.2 (on page 648) other than the two shown. Since the induced shortest path trees on $\\{s, t, y\\}$ and on $\\{t, x, y, z\\}$ are independent and have to possible configurations each, there are four total arising from that. So, we have the two not shown in the figure are the one consisting of the edges $\\{(s, t), (s, y), (y, x), (x, z)\\}$ and the one consisting of the edges $\\{(s, t), (t, y), (t, x), (y, z)\\}$.","title":"24.5-1"},{"location":"Chap24/24.5/#245-2","text":"Give an example of a weighted, directed graph $G = (V, E)$ with weight function $w: E \\rightarrow \\mathbb R$ and source vertex $s$ such that $G$ satisfies the following property: For every edge $(u, v) \\in E$, there is a shortest-paths tree rooted at $s$ that contains $(u, v)$ and another shortest-paths tree rooted at $s$ that does not contain $(u, v)$. Let $G$ have $3$ vertices $s$, $x$, and $y$. Let the edges be $(s, x)$, $(s, y)$, and $(x, y)$ with weights $1$, $1$, and $0$ respectively. There are $3$ possible trees on these vertices rooted at $s$, and each is a shortest paths tree which gives $\\delta(s, x) = \\delta(s, y) = 1$.","title":"24.5-2"},{"location":"Chap24/24.5/#245-3","text":"Embellish the proof of Lemma 24.10 to handle cases in which shortest-path weights are $\\infty$ or $-\\infty$. To modify Lemma 24.10 to allow for possible shortest path weights of $\\infty$ and $-\\infty$, we need to define our addition as $\\infty + c = \\infty$, and $-\\infty + c = -\\infty$. This will make the statement behave correctly, that is, we can take the shortest path from $s$ to $u$ and tack on the edge $(u, v)$ to the end. That is, if there is a negative weight cycle on your way to $u$ and there is an edge from $u$ to $v$, there is a negative weight cycle on our way to $v$. Similarly, if we cannot reach $v$ and there is an edge from $u$ to $v$, we cannot reach $u$.","title":"24.5-3"},{"location":"Chap24/24.5/#245-4","text":"Let $G = (V, E)$ be a weighted, directed graph with source vertex $s$, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that if a sequence of relaxation steps sets $s.\\pi$ to a non-$\\text{NIL}$ value, then $G$ contains a negative-weight cycle. (Removed)","title":"24.5-4"},{"location":"Chap24/24.5/#245-5","text":"Let $G = (V, E)$ be a weighted, directed graph with no negative-weight edges. Let $s \\in V$ be the source vertex, and suppose that we allow $v.\\pi$ to be the predecessor of $v$ on any shortest path to $v$ from source $s$ if $v \\in V - \\{s\\}$ is reachable from $s$, and $\\text{NIL}$ otherwise. Give an example of such a graph $G$ and an assignment of $\\pi$ values that produces a cycle in $G_\\pi$. (By Lemma 24.16, such an assignment cannot be produced by a sequence of relaxation steps.) Suppose that we have a grap hon three vertices $\\{s, u, v\\}$ and containing edges $(s, u), (s, v), (u, v), (v, u)$ all with weight $0$. Then, there is a shortest path from $s$ to $v$ of $s$, $u$, $v$ and a shortest path from $s$ to $u$ of $s$ $v$, $u$. Based off of these, we could set $v.\\pi = u$ and $u.\\pi = v$. This then means that there is a cycle consisting of $u, v$ in $G_\\pi$.","title":"24.5-5"},{"location":"Chap24/24.5/#245-6","text":"Let $G = (V, E)$ be a weighted, directed graph with weight function $w: E \\rightarrow \\mathbb R$ and no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that for every vertex $v \\in V_\\pi$, there exists a path from $s$ to $v$ in $G_\\pi$ and that this property is maintained as an invariant over any sequence of relaxations. We will prove this by induction on the number of relaxations performed. For the base-case, we have just called $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. The only vertex in $V_\\pi$ is $s$, and there is trivially a path from $s$ to itself. Now suppose that after any sequence of $n$ relaxations, for every vertex $v \\in V_\\pi$ there exists a path from $s$ to $v$ in $G_\\pi$. Consider the $(n + 1)$th relaxation. Suppose it is such that $v.d > u.d + w(u, v)$. When we relax $v$, we update $v.\\pi = u.\\pi$. By the induction hypothesis, there was a path from $s$ to $u$ in $G_\\pi$. Now $v$ is in $V_\\pi$, and the path from $s$ to $u$, followed by the edge $(u,v) = (v.\\pi, v)$ is a path from s to $v$ in $G_\\pi$, so the claim holds.","title":"24.5-6"},{"location":"Chap24/24.5/#245-7","text":"Let $G = (V, E)$ be a weighted, directed graph that contains no negative-weight cycles. Let $s \\in V$ be the source vertex, and let $G$ be initialized by $\\text{INITIALIZE-SINGLE-SOURCE}(G, s)$. Prove that there exists a sequence of $|V| - 1$ relaxation steps that produces $v.d = \\delta(s, v)$ for all $v \\in V$. (Removed)","title":"24.5-7"},{"location":"Chap24/24.5/#245-8","text":"Let $G$ be an arbitrary weighted, directed graph with a negative-weight cycle reachable from the source vertex $s$. Show how to construct an infinite sequence of relaxations of the edges of $G$ such that every relaxation causes a shortest-path estimate to change. (Removed)","title":"24.5-8"},{"location":"Chap24/Problems/24-1/","text":"Suppose that we order the edge relaxations in each pass of the Bellman-Ford algorithm as follows. Before the first pass, we assign an arbitrary linear order $v_1, v_2, \\ldots, v_{|V|}$ to the vertices of the input graph $G = (V, E)$. Then, we partition the edge set $E$ into $E_f \\cup E_b$, where $E_f = \\{(v_i, v_j) \\in E: i < j\\}$ and $E_b = \\{(v_i, v_j) \\in E: i > j\\}$. (Assume that $G$ contains no self-loops, so that every edge is in either $E_f$ or $E_b$.) Define $G_f = (V, E_f)$ and $G_b = (V, E_b)$. a. Prove that $G_f$ is acyclic with topological sort $\\langle v_1, v_2, \\ldots, v_{|V|} \\rangle$ and that $G_b$ is acyclic with topological sort $\\langle v_{|V|}, v_{|V| - 1}, \\ldots, v_1 \\rangle$. Suppose that we implement each pass of the Bellman-Ford algorithm in the following way. We visit each vertex in the order $v_1, v_2, \\ldots, v_{|V|}$, relaxing edges of $E_f$ that leave the vertex. We then visit each vertex in the order $v_{|V|}, v_{|V| - 1}, \\ldots, v_1$, relaxing edges of $E_b$ that leave the vertex. b. Prove that with this scheme, if $G$ contains no negative-weight cycles that are reachable from the source vertex $s$, then after only $\\lceil |V| / 2 \\rceil$ passes over the edges, $v.d = \\delta(s, v)$ for all vertices $v \\in V$. c. Does this scheme improve the asymptotic running time of the Bellman-Ford algorithm? a. Since in $G_f$ edges only go from vertices with smaller index to vertices with greater index, there is no way that we could pick a vertex, and keep increasing it's index, and get back to having the index equal to what we started with. This means that $G_f$ is acyclic. Similarly, there is no way to pick an index, keep decreasing it, and get back to the same vertex index. By these definitions, since $G_f$ only has vertices going from lower indices to higher indices, $(v_1, \\dots, v_{|V|})$ is a topological ordering of the vertices. Similarly, for $G_b$, $(v_{|V|}, \\dots, v_1)$ is a topological ordering of the vertices. b. Suppose that we are trying to find the shortest path from $s$ to $v$. Then, list out the vertices of this shortest path $v_{k_1}, v_{k_2}, \\dots, v_{k_m}$. Then, we have that the number of times that the sequence $\\{k_i\\}_i$ goes from increasing to decreasing or from decreasing to increasing is the number of passes over the edges that are necessary to notice this path. This is because any increasing sequence of vertices will be captured in a pass through $E_f$ and any decreasing sequence will be captured in a pass through $E_b$. Any sequence of integers of length $|V|$ can only change direction at most $\\lfloor |V| / 2 \\rfloor$ times. However, we need to add one more in to account for the case that the source appears later in the ordering of the vertices than $v_{k_2}$, as it is in a sense initially expecting increasing vertex indices, as it runs through $E_f$ before $E_b$. c. It does not improve the asymptotic runtime of Bellman ford, it just drops the runtime from having a leading coefficient of $1$ to a leading coefficient of $\\frac{1}{2}$. Both in the original and in the modified version, the runtime is $O(EV)$.","title":"24-1 Yen's improvement to Bellman-Ford"},{"location":"Chap24/Problems/24-2/","text":"A $d$-dimensional box with dimensions $(x_1, x_2, \\ldots, x_d)$ nests within another box with dimensions $(y_1, y_2, \\ldots, y_d)$ if there exists a permutation $\\pi$ on $\\{1, 2, \\ldots, d\\}$ such that $x_{\\pi(1)} < y_1$, $x_{\\pi(2)} < y_2$, $\\ldots$, $x_{\\pi(d)} < y_d$. a. Argue that the nesting relation is transitive. b. Describe an efficient method to determine whether or not one $d$-dimensional box nests inside another. c. Suppose that you are given a set of $n$ $d$-dimensional boxes $\\{B_1, B_2, \\ldots, B_n\\}$. Give an efficient algorithm to find the longest sequence $\\langle B_{i_1}, B_{i_2}, \\ldots, B_{i_k} \\rangle$ of boxes such that $B_{i_j}$ nests within $B_{i_{j + 1}}$ for $j = 1, 2, \\ldots, k - 1$. Express the running time of your algorithm in terms of $n$ and $d$. a. Suppose that box $x = (x_1, \\dots, x_d)$ nests with box $y = (y_1, \\dots, y_d)$ and box $y$ nests with box $z = (z_1, \\dots, z_d)$. Then there exist permutations $\\pi$ and $\\sigma$ such that $x_{\\pi(1)} < y_1, \\dots, x_{\\pi(d)} < y_d$ and $y_{\\sigma(1)} < z_1, \\dots, y_{\\sigma(d)} < z_d$. This implies $x_{\\pi(\\sigma(1))} < z_1, \\dots, x_{\\pi(\\sigma(d))} < z_d$, so $x$ nests with $z$ and the nesting relation is transitive. b. Box $x$ nests inside box $y$ if and only if the increasing sequence of dimensions of $x$ is component-wise strictly less than the increasing sequence of dimensions of $y$. Thus, it will suffice to sort both sequences of dimensions and compare them. Sorting both length $d$ sequences is done in $O(d\\lg d)$, and comparing their elements is done in $O(d)$, so the total time is $O(d\\lg d)$. c. We will create a nesting-graph $G$ with vertices $B_1, \\dots, B_n$ as follows. For each pair of boxes $B_i$ , $B_j$, we decide if one nests inside the other. If $B_i$ nests in $B_j$, draw an arrow from $B_i$ to $B_j$. If $B_j$ nests in $B_i$, draw an arrow from $B_j$ to $B_i$. If neither nests, draw no arrow. To determine the arrows efficiently, after sorting each list of dimensions in $O(nd\\lg d)$ we compair all pairs of boxes using the algorithm from part (b) in $O(n^2 d)$. By part (a), the resulted graph is acyclic, which allows us to easily find the longest chain in it in $O(n^2)$ in a bottom-up manner. This chain is our answer. Thus, the total time is $O(nd\\max(\\lg d, n))$.","title":"24-2 Nesting boxes"},{"location":"Chap24/Problems/24-3/","text":"Arbitrage is the use of discrepancies in currency exchange rates to transform one unit of a currency into more than one unit of the same currency. For example, suppose that $1$ U.S. dollar buys $49$ Indian rupees, $1$ Indian rupee buys $2$ Japanese yen, and $1$ Japanese yen buys $0.0107$ U.S. dollars. Then, by converting currencies, a trader can start with $1$ U.S. dollar and buy $49 \\times 2 \\times 0.0107 = 1.0486$ U.S. dollars, thus turning a profit of $4.86$ percent. Suppose that we are given $n$ currencies $c_1, c_2, \\ldots, c_n$ and an $n \\times n$ table $R$ of exchange rates, such that one unit of currency $c_i$ buys $R[i, j]$ units of currency $c_j$. a. Give an efficient algorithm to determine whether or not there exists a sequence of currencies $\\langle c_{i_1}, c_{i_2}, \\ldots, c_{i_k} \\rangle$ such that $$R[i_1, i_2] \\cdot R[i_2, i_3] \\cdots R[i_{k - 1}, i_k] \\cdot R[i_k, i_1] > 1.$$ Analyze the running time of your algorithm. b. Give an efficient algorithm to print out such a sequence if one exists. Analyze the running time of your algorithm. a. To do this we take the negative of the natural log (or any other base will also work) of all the values $c_i$ that are on the edges between the currencies. Then, we detect the presence or absence of a negative weight cycle by applying Bellman Ford. To see that the existence of an arbitrage situation is equivalent to there being a negative weight cycle in the original graph, consider the following sequence of steps: $$ \\begin{aligned} R[i_1, i_2] \u00b7 R[i_2, i_3] \\cdot \\cdots \\cdot R[i_k, i_1] & > 1 \\\\ \\ln(R[i_1, i_2]) + \\ln(R[i_2, i_3]) + \\cdots + \\ln(R[i_k, i_1]) & > 0 \\\\ \u2212\\ln(R[i_1, i_2]) \u2212 \\ln(R[i_2, i_3]) \u2212 \\cdots \u2212 \\ln(R[i_k, i_1]) & < 0. \\end{aligned} $$ b. To do this, we first perform the same modification of all the edge weights as done in part (a) of this problem. Then, we wish to detect the negative weight cycle. To do this, we relax all the edges $|V| \u2212 1$ many times, as in BellmanFord algorithm. Then, we record all of the $d$ values of the vertices. Then, we relax all the edges $|V|$ more times. Then, we check to see which vertices had their $d$ value decrease since we recorded them. All of these vertices must lie on some (possibly disjoint) set of negative weight cycles. Call $S$ this set of vertices. To find one of these cycles in particular, we can pick any vertex in $S$ and greedily keep picking any vertex that it has an edge to that is also in $S$. Then, we just keep an eye out for a repeat. This finds us our cycle. We know that we will never get to a dead end in this process because the set $S$ consists of vertices that are in some union of cycles, and so every vertex has out degree at least $1$.","title":"24-3 Arbitrage"},{"location":"Chap24/Problems/24-4/","text":"A scaling algorithm solves a problem by initially considering only the highestorder bit of each relevant input value (such as an edge weight). It then refines the initial solution by looking at the two highest-order bits. It progressively looks at more and more high-order bits, refining the solution each time, until it has examined all bits and computed the correct solution. In this problem, we examine an algorithm for computing the shortest paths from a single source by scaling edge weights. We are given a directed graph $G = (V, E)$ with nonnegative integer edge weights $w$. Let $W = \\max_{(u, v) \\in E} \\{w(u, v)\\}$. Our goal is to develop an algorithm that runs in $O(E\\lg W)$ time. We assume that all vertices are reachable from the source. The algorithm uncovers the bits in the binary representation of the edge weights one at a time, from the most significant bit to the least significant bit. Specifically, let $k = \\lceil \\lg(W + 1) \\rceil$ be the number of bits in the binary representation of $W$, and for $i = 1, 2, \\ldots, k$, let $w_i(u, v) = \\lfloor w(u, v) / 2^{k - i} \\rfloor$. That is, $w_i(u, v)$ is the \"scaled-down\" version of $w(u, v)$ given by the $i$ most significant bits of $w(u, v)$. (Thus, $w_k(u, v) = w(u, v)$ for all $(u, v) \\in E$.) For example, if $k = 5$ and $w(u, v) = 25$, which has the binary representation $\\langle 11001 \\rangle$, then $w_3(u, v) = \\langle 110 \\rangle = 6$. As another example with $k = 5$, if $w(u, v) = \\langle 00100 \\rangle = 4$, then $w_3(u, v) = \\langle 001 \\rangle = 1$. Let us define $\\delta_i(u, v)$ as the shortest-path weight from vertex $u$ to vertex $v$ using weight function $w_i$. Thus, $\\delta_k(u, v) = \\delta(u, v)$ for all $u, v \\in V$. For a given source vertex $s$, the scaling algorithm first computes the shortest-path weights $\\delta_1(s, v)$ for all $v \\in V$, then computes $\\delta_2(s, v)$ for all $v \\in V$, and so on, until it computes $\\delta_k(s, v)$ for all $v \\in V$. We assume throughout that $|E| \\ge |V| - 1$, and we shall see that computing $\\delta_i$ from $\\delta_{i - 1}$ takes $O(E)$ time, so that the entire algorithm takes $O(kE) = O(E\\lg W)$ time. a. Suppose that for all vertices $v \\in V$, we have $\\delta(s, v) \\le |E|$. Show that we can compute $\\delta(s, v)$ for all $v \\in V$ in $O(E)$ time. b. Show that we can compute $\\delta_1(s, v)$ for all $v \\in V$ in $O(E)$ time. Let us now focus on computing $\\delta_i$ from $\\delta_{i - 1}$. c. Prove that for $i = 2, 3, \\ldots, k$, we have either $w_i(u, v) = 2w_{i - 1}(u, v)$ or $w_i(u, v) = 2w_{i - 1}(u, v) + 1$. Then, prove that $$2\\delta_{i - 1}(s, v) \\le \\delta_i(s, v) \\le 2\\delta_{i - 1}(s, v) + |V| - 1$$ for all $v \\in V$. d. Define for $i = 2, 3, \\ldots, k$ and all $(u, v) \\in E$, $$\\hat w_i = w_i(u, v) + 2\\delta_{i - 1}(s, u) - 2\\delta_{i - 1}(s, v).$$ Prove that for $i = 2, 3, \\ldots, k$ and all $u, v \\in V$, the \"reweighted\" value $\\hat w_i(u, v)$ of edge $(u, v)$ is a nonnegative integer. e. Now, define $\\hat\\delta_i(s, v)$ as the shortest-path weight from $s$ to $v$ using the weight function $\\hat w_i$. Prove that for $i = 2, 3, \\ldots, k$ and all $v \\in V$, $$\\delta_i(s, v) = \\hat\\delta_i(s, v) + 2\\delta_{i - 1}(s, v)$$ and that $\\hat\\delta_i(s, v) \\le |E|$. f. Show how to compute $\\delta_i(s, v)$ from $\\delta_{i - 1}(s, v)$ for all $v \\in V$ in $O(E)$ time, and conclude that we can compute $\\delta(s, v)$ for all $v \\in V$ in $O(E\\lg W)$ time. a. We can do this in $O(E)$ by the algorithm described in exercise 24.3-8 since our \"priority queue\" takes on only integer values and is bounded in size by $E$. b. We can do this in $O(E)$ by the algorithm described in exercise 24.3-8 since $w$ takes values in $\\{0, 1\\}$ and $V = O(E)$. c. If the $i$th digit, read from left to right, of $w(u, v)$ is $0$, then $w_i(u, v) = 2w_{i \u2212 1}(u, v)$. If it is a $1$, then $w_i(u, v) = 2w_{i \u2212 1}(u, v) + 1$. Now let $s = v_0, v_1, \\dots, v_n = v$ be a shortest path from $s$ to $v$ under $w_i$. Note that any shortest path under $w_i$ is necessarily also a shortest path under $w_{i \u2212 1}$. Then we have $$ \\begin{aligned} \\delta_i(s, v) & = \\sum_{m = 1}^n w_i(v_{m \u2212 1}, v_m) \\\\ & \\le \\sum_{m = 1}^n [2w_{i \u2212 1}(u, v) + 1] \\\\ & \\le \\sum_{m = 1}^n w_{i \u2212 1}(u, v) + n \\\\ & \\le 2\\delta_{i \u2212 1}(s, v) + |V| \u2212 1. \\end{aligned} $$ On the other hand, we also have $$ \\begin{aligned} \\delta_i(s, v) & = \\sum_{m = 1}^n w_i(v_{m - 1}, v_m) \\\\ & \\ge \\sum_{m = 1}^n 2w_{i - 1}(v_{m - 1}, v_m) \\\\ & \\ge 2\\delta_{i - 1}(s, v). \\end{aligned} $$ d. Note that every quantity in the definition of $\\hat w_i$ is an integer, so $\\hat w_i$ is clearly an integer. Since $w_i(u, v) \\ge 2w_{i - 1}(u, v)$, it will suffice to show that $w_{i - 1}(u, v) + \\delta_{i - 1}(s, u) \\ge \\delta_{i - 1}(s, v)$ to prove nonnegativity. This follows immediately from the triangle inequality. e. First note that $s = v_0, v_1, \\dots, v_n = v$ is a shortest path from $s$ to $v$ with respect to $\\hatw$ if and only if it is a shortest path with respect to $w$. Then we have $$ \\begin{aligned} \\hat\\delta_i(s, v) & = \\sum_{m = 1}^n w_i(v_{m - 1}, v_m) + 2\\delta_{i - 1}(s, v_{m - 1}) \u2212 2\\delta_{i - 1}(s, v_m) \\\\ & = \\sum_{m = 1}^n w_i(v_{m - 1}, v_m) \u2212 2\\delta_{i - 1}(s, v_n) \\\\ & = \\delta_i(s, v) \u2212 2\\delta_{i - 1}(s, v). \\end{aligned} $$ f. By part (a) we can compute $\\hat\\delta_i(s, v)$ for all $v \\in V$ in $O(E)$ time. If we have already computed $\\delta_i - 1$ then we can compute $\\delta_i$ in $O(E)$ time. Since we can compute $\\delta_1$ in $O(E)$ by part b, we can compute $\\delta_i$ from scratch in $O(iE)$ time. Thus, we can compute $\\delta = \\delta_k$ in $O(Ek) = O(E\\lg W)$ time.","title":"24-4 Gabow's scaling algorithm for single-source shortest paths"},{"location":"Chap24/Problems/24-5/","text":"Let $G = (V, E)$ be a directed graph with weight function $w: E \\to \\mathbb R$, and let $n = |V|$. We define the mean weight of a cycle $c = \\langle e_1, e_2, \\ldots, e_k \\rangle$ of edges in $E$ to be $$\\mu(c) = \\frac{1}{k} \\sum_{i = 1}^k w(e_i).$$ Let $\\mu^* = \\min_c \\mu(c)$, where $c$ ranges over all directed cycles in $G$. We call a cycle $c$ for which $\\mu(c) = \\mu^*$ a minimum mean-weight cycle . This problem investigates an efficient algorithm for computing $\\mu^*$. Assume without loss of generality that every vertex $v \\in V$ is reachable from a source vertex $s \\in V$. Let $\\delta(s, v)$ be the weight of a shortest path from $s$ to $v$, and let $\\delta_k(s, v)$ be the weight of a shortest path from $s$ to $v$ consisting of exactly $k$ edges. If there is no path from $s$ to $v$ with exactly $k$ edges, then $\\delta_k(s, v) = \\infty$. a. Show that if $\\mu^* = 0$, then $G$ contains no negative-weight cycles and $\\delta(s, v) = \\min_{0 \\le k \\le n - 1} \\delta_k(s, v)$ for all vertices $v \\in V$. b. Show that if $\\mu^* = 0$, then $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\ge 0$$ for all vertices $v \\in V$. ($\\textit{Hint:}$ Use both properties from part (a).) c. Let $c$ be a $0$-weight cycle, and let $u$ and $v$ be any two vertices on $c$. Suppose that $\\mu^* = 0$ and that the weight of the simple path from $u$ to $v$ along the cycle is $x$. Prove that $\\delta(s, v) = \\delta(s, u) + x$. ($\\textit{Hint:}$ The weight of the simple path from $v$ to $u$ along the cycle is $-x$.) d. Show that if $\\mu^* = 0$, then on each minimum mean-weight cycle there exists a vertex $v$ such that $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$ ($\\textit{Hint:}$ Show how to extend a shortest path to any vertex on a minimum meanweight cycle along the cycle to make a shortest path to the next vertex on the cycle.) e. Show that if $\\mu^* = 0$, then $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$ f. Show that if we add a constant $t$ to the weight of each edge of $G$, then $\\mu^*$ increases by $t$. Use this fact to show that $$\\mu^* = \\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k}.$$ g. Give an $O(VE)$-time algorithm to compute $\\mu^*$. a. If $\\mu^* = 0$, then we have that the lowest that $\\frac{1}{k}_{i = 1}^k w(e_i)$ can be zero. This means that the lowest $\\sum_{i = 1}^k w(e_i)$ can be $0$. This means that no cycle can have negative weight. Also, we know that for any path from $s$ to $v$, we can make it simple by removing any cycles that occur. This means that it had a weight equal to some path that has at most $n - 1$ edges in it. Since we take the minimum over all possible number of edges, we have the minimum over all paths. b. To show that $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\ge 0,$$ we need to show that $$\\max_{0 \\le k \\le n - 1} \\delta_n(s, v) - \\delta_k(s, v) \\ge 0.$$ Since we have that $\\mu^* = 0$, there aren't any negative weight cycles. This means that we can't have the minimum cost of a path decrease as we increase the possible length of the path past $n - 1$. This means that there will be a path that at least ties for cheapest when we restrict to the path being less than length $n$. Note that there may also be cheapest path of longer length since we necessarily do have zero cost cycles. However, this isn't guaranteed since the zero cost cycle may not lie along a cheapest path from $s$ to $v$. c. Since the total cost of the cycle is $0$, and one part of it has cost $x$, in order to balance that out, the weight of the rest of the cycle has to be $-x$. So, suppose we have some shortest length path from $s$ to $u$, then, we could traverse the path from $u$ to $v$ along the cycle to get a path from $s$ to $u$ that has length $\\delta(s, u) + x$. This gets us that $\\delta(s, v) \\le \\delta(s, u) + x$. To see the converse inequality, suppose that we have some shortest length path from $s$ to $v$. Then, we can traverse the cycle going from $v$ to $u$. We already said that this part of the cycle had total cost $-x$. This gets us that $\\delta(s, u) \\le \\delta(s, v) - x$. Or, rearranging, we have $\\delta(s, u) + x \\le \\delta(s, v)$. Since we have inequalities both ways, we must have equality. d. To see this, we find a vertex $v$ and natural number $k \\le n - 1$ so that $\\delta_n(s, v) - \\delta_k(s, v) = 0$. To do this, we will first take any shortest length, smallest number of edges path from $s$ to any vertex on the cycle. Then, we will just keep on walking around the cycle until we've walked along $n$ edges. Whatever vertex we end up on at that point will be our $v$. Since we did not change the $d$ value of $v$ after looking at length $n$ paths, by part (a), we know that there was some length of this path, say $k$, which had the same cost. That is, we have $\\delta_n(s, v) = \\delta_k(s,v)$. e. This is an immediate result of the previous problem and part (b). Part (a) says that the inequality holds for all $v$, so, we have $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta(s, v)}{n - k} \\ge 0.$$ The previous part says that there is some $v$ on each minimum weight cycle so that $$\\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta(s, v)}{n - k} = 0,$$ which means that $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} \\le 0.$$ Putting the two inequalities together, we have the desired equality. f. If we add $t$ to the weight of each edge, the mean weight of any cycle becomes $$\\mu(c) = \\frac{1}{k} \\sum_{i = 1}^k (w(e_i) + t) = \\frac{1}{k} \\Big(\\sum_i^k w(e_i) \\Big) + \\frac{kt}{k} = \\frac{1}{k} \\Big(\\sum_i^k w(e_i) \\Big) + t.$$ This is the original, unmodified mean weight cycle, plus $t$. Since this is how the mean weight of every cycle is changed, the lowest mean weight cycle stays the lowest mean weight cycle. This means that $\\mu^*$ will increase by $t$. Suppose that we first compute $\\mu^*$. Then, we subtract from every edge weight the value $\\mu^*$. This will make the new $\\mu^*$ equal zero, which by part (e) means that $$\\min_{v \\in V} \\max_{0 \\le k \\le n - 1} \\frac{\\delta_n(s, v) - \\delta_k(s, v)}{n - k} = 0.$$ Since they are both equal to zero, they are both equal to each other. g. By the previous part, it suffices to compute the expression on the previous line. We will start by creating a table that lists $\\delta_k(s, v)$ for every $k \\in \\{1, \\ldots, n\\}$ and $v \\in V$. This can be done in time $O(V(E + V))$ by creating a $|V|$ by $|V|$ table, where the $k$th row and vth column represent $\\delta_k(s, v)$ when wanting to compute a particular entry, we need look at a number of entries in the previous row equal to the in degree of the vertex we want to compute. So, summing over the computation required for each row, we need $O(E + V)$. Note that this total runtime can be bumped down to $O(VE)$ by not including in the table any isolated vertices, this will ensure that $E \\in \\Omega(V)$. So, $O(V(E + V))$ becomes $O(VE)$. Once we have this table of values computed, it is simple to just replace each row with the last row minus what it was, and divide each entry by $n - k$, then, find the min column in each row, and take the max of those numbers.","title":"24-5 Karp's minimum mean-weight cycle algorithm"},{"location":"Chap24/Problems/24-6/","text":"A sequence is bitonic if it monotonically increases and then monotonically decreases, or if by a circular shift it monotonically increases and then monotonically decreases. For example the sequences $\\langle 1, 4, 6, 8, 3, -2 \\rangle$, $\\langle 9, 2, -4, -10, -5 \\rangle$, and $\\langle 1, 2, 3, 4 \\rangle$ are bitonic, but $\\langle 1, 3, 12, 4, 2, 10 \\rangle$ is not bitonic. (See Problem 15-3 for the bitonic euclidean traveling-salesman problem.) Suppose that we are given a directed graph $G = (V, E)$ with weight function $w: E \\to \\mathbb R$, where all edge weights are unique, and we wish to find single-source shortest paths from a source vertex $s$. We are given one additional piece of information: for each vertex $v \\in V$, the weights of the edges along any shortest path from $s$ to $v$ form a bitonic sequence. Give the most efficient algorithm you can to solve this problem, and analyze its running time. We'll use the Bellman-Ford algorithm, but with a careful choice of the order in which we relax the edges in order to perform a smaller number of $\\text{RELAX}$ operations. In any bitonic path there can be at most two distinct increasing sequences of edge weights, and similarly at most two distinct decreasing sequences of edge weights. Thus, by the path-relaxation property, if we relax the edges in order of increasing weight then decreasing weight twice (for a total of four times relaxing every edge) the we are guaranteed that $v.d$ will equal $\\delta(s, v)$ for all $v \\in V$ . Sorting the edges takes $O(E\\lg E)$. We relax every edge $4$ times, taking $O(E)$. Thus the total runtime is $O(E\\lg E) + O(E) = O(E\\lg E)$, which is asymptotically faster than the usual $O(VE)$ runtime of Bellman-Ford.","title":"24-6 Bitonic shortest paths"},{"location":"Chap25/25.1/","text":"25.1-1 Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the weighted, directed graph of Figure 25.2, showing the matrices that result for each iteration of the loop. Then do the same for $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$. Initial: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & \\infty & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & 3 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ Slow: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 3$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -2 & -3 & 0 & -1 & 2 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ Fast: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 8$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ 25.1-2 Why do we require that $w_{ii} = 0$ for all $1 \\le i \\le n$? This is consistent with the fact that the shortest path from a vertex to itself is the empty path of weight $0$. If there were another path of weight less than $0$ then it must be a negative-weight cycle, since it starts and ends at $v_i$. If $w_{ii} \\ne 0$, then $L^{(1)}$ produced after the first run of $\\text{EXTEND-SHORTEST-PATHS}$ would not contain the minimum weight of any path from $i$ to its neighbours. If $w_{ii} = 0$, then in line 7 of $\\text{EXTEND-SHORTEST-PATHS}$, the second argument to $min$ would not equal the weight of the edge going from $i$ to its neighbours. 25.1-3 What does the matrix $$ L^{(0)} = \\begin{pmatrix} 0 & \\infty & \\infty & \\cdots & \\infty \\\\ \\infty & 0 & \\infty & \\cdots & \\infty \\\\ \\infty & \\infty & 0 & \\cdots & \\infty \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\infty & \\infty & \\infty & \\cdots & 0 \\end{pmatrix} $$ used in the shortest-paths algorithms correspond to in regular matrix multiplication? The identity matrix. 25.1-4 Show that matrix multiplication defined by $\\text{EXTEND-SHORTEST-PATHS}$ is associative. To verify associativity, we need to check that $(W^iW^j)W^p = W^i(W^jW^p)$ for all $i$, $j$ and $p$, where we use the matrix multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Consider entry $(a, b)$ of the left hand side. This is: $$ \\begin{aligned} \\min_{1 \\le k \\le n} [W^iW^j]_{a, k} + W_{k, b}^p & = \\min_{1 \\le k \\le n} \\min_{1 \\le q \\le n} W_{a, q}^i + W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + \\min_{1 \\le k \\le n} W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + [W^jW^p]_{q, b}, \\end{aligned} $$ which is precisely entry $(a, b)$ of the right hand side. 25.1-5 Show how to express the single-source shortest-paths problem as a product of matrices and a vector. Describe how evaluating this product corresponds to a Bellman-Ford-like algorithm (see Section 24.1). (Removed) 25.1-6 Suppose we also wish to compute the vertices on shortest paths in the algorithms of this section. Show how to compute the predecessor matrix $\\prod$ from the completed matrix $L$ of shortest-path weights in $O(n^3)$ time. For each source vertex $v_i$ we need to compute the shortest-paths tree for $v_i$. To do this, we need to compute the predecessor for each $j \\ne i$. For fixed $i$ and $j$, this is the value of $k$ such that $L_{i, k} + w(k, j) = L[i, j]$. Since there are $n$ vertices whose trees need computing, $n$ vertices for each such tree whose predecessors need computing, and it takes $O(n)$ to compute this for each one (checking each possible $k$), the total time is $O(n^3)$. 25.1-7 We can also compute the vertices on shortest paths as we compute the shortestpath weights. Define $\\pi_{ij}^{(m)}$ as the predecessor of vertex $j$ on any minimum-weight path from $i$ to $j$ that contains at most $m$ edges. Modify the $\\text{EXTEND-SHORTESTPATHS}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ procedures to compute the matrices$\\prod^{(1)}, \\prod^{(2)}, \\ldots, \\prod^{(n - 1)}$ as the matrices $L^{(1)}, L^{(2)}, \\ldots, L^{(n - 1)}$ are computed. To have the procedure compute the predecessor along the shortest path, see the modified procedures, $\\text{EXTEND-SHORTEST-PATH-MOD}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD}$ EXTEND - SHORTEST - PATH - MOD ( \u220f , L , W ) n = L . row let L ' = l ' [ i , j ] be a new n \u00d7 n matirx \u220f' = \u03c0' [ i , j ] is a new n \u00d7 n matrix for i = 1 to n for j = 1 to n l ' [ i , j ] = \u221e \u03c0' [ i , j ] = NIL for k = 1 to n if l [ i , k ] + l [ k , j ] < l [ i , j ] l [ i , j ] = l [ i , k ] + l [ k , j ] if k != j \u03c0' [ i , j ] = k else \u03c0' [ i , j ] = \u03c0 [ i , j ] return ( \u220f' , L ' ) SLOW - ALL - PAIRS - SHORTEST - PATHS - MOD ( W ) n = W . rows L ( 1 ) = W \u220f ( 1 ) = \u03c0 [ i , j ]( 1 ) where \u03c0 [ i , j ]( 1 ) = i if there is an edge from i to j , and NIL otherwise for m = 2 to n - 1 \u220f ( m ), L ( m ) = EXTEND - SHORTEST - PATH - MOD ( \u220f ( m - 1 ), L ( m - 1 ), W ) return ( \u220f ( n - 1 ), L ( n - 1 )) 25.1-8 The $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ procedure, as written, requires us to store $\\lceil \\lg(n - 1) \\rceil$ matrices, each with $n^2$ elements, for a total space requirement of $\\Theta(n^2\\lg n)$. Modify the procedure to require only $\\Theta(n^2)$ space by using only two $n \\times n$ matrices. We can overwrite matrices as we go. Let $A \\star B$ denote multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Then we modify $\\text{FASTER-ALL-EXTEND-SHORTEST-PATHS}(W)$. We initially create an $n$ by $n$ matrix $L$. Delete line 5 of the algorithm, and change line 6 to $L = W \\star W$, followed by $W = L$. 25.1-9 Modify $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ so that it can determine whether the graph contains a negative-weight cycle. For the modification, keep computing for one step more than the original, that is, we compute all the way up to $L^{(2k + 1)}$ where $2^k > n - 1$. Then, if there aren't any negative weight cycles, then, we will have that the two matrices should be equal since having no negative weight cycles means that between any two vertices, there is a path that is tied for shortest and contains at most $n - 1$ edges. However, if there is a cycle of negative total weight, we know that it's length is at most $n$, so, since we are allowing paths to be larger by $2k \\ge n$ between these two matrices, we have that we would need to have all of the vertices on the cycle have their distance reduce by at least the negative weight of the cycle. Since we can detect exactly when there is a negative cycle, based on when these two matrices are different. This algorithm works. It also only takes time equal to a single matrix multiplication which is littlee oh of the unmodified algorithm. 25.1-10 Give an efficient algorithm to find the length (number of edges) of a minimum-length negative-weight cycle in a graph. (Removed)","title":"25.1 Shortest paths and matrix multiplication"},{"location":"Chap25/25.1/#251-1","text":"Run $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ on the weighted, directed graph of Figure 25.2, showing the matrices that result for each iteration of the loop. Then do the same for $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$. Initial: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & \\infty & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & 3 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ Slow: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 3$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -2 & -3 & 0 & -1 & 2 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ Fast: $m = 2$: $$ \\begin{pmatrix} 0 & 6 & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & -3 & 0 & 4 & \\infty & -8 \\\\ -4 & 10 & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & \\infty & 0 \\end{pmatrix} $$ $m = 4$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -3 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $m = 8$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$","title":"25.1-1"},{"location":"Chap25/25.1/#251-2","text":"Why do we require that $w_{ii} = 0$ for all $1 \\le i \\le n$? This is consistent with the fact that the shortest path from a vertex to itself is the empty path of weight $0$. If there were another path of weight less than $0$ then it must be a negative-weight cycle, since it starts and ends at $v_i$. If $w_{ii} \\ne 0$, then $L^{(1)}$ produced after the first run of $\\text{EXTEND-SHORTEST-PATHS}$ would not contain the minimum weight of any path from $i$ to its neighbours. If $w_{ii} = 0$, then in line 7 of $\\text{EXTEND-SHORTEST-PATHS}$, the second argument to $min$ would not equal the weight of the edge going from $i$ to its neighbours.","title":"25.1-2"},{"location":"Chap25/25.1/#251-3","text":"What does the matrix $$ L^{(0)} = \\begin{pmatrix} 0 & \\infty & \\infty & \\cdots & \\infty \\\\ \\infty & 0 & \\infty & \\cdots & \\infty \\\\ \\infty & \\infty & 0 & \\cdots & \\infty \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\infty & \\infty & \\infty & \\cdots & 0 \\end{pmatrix} $$ used in the shortest-paths algorithms correspond to in regular matrix multiplication? The identity matrix.","title":"25.1-3"},{"location":"Chap25/25.1/#251-4","text":"Show that matrix multiplication defined by $\\text{EXTEND-SHORTEST-PATHS}$ is associative. To verify associativity, we need to check that $(W^iW^j)W^p = W^i(W^jW^p)$ for all $i$, $j$ and $p$, where we use the matrix multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Consider entry $(a, b)$ of the left hand side. This is: $$ \\begin{aligned} \\min_{1 \\le k \\le n} [W^iW^j]_{a, k} + W_{k, b}^p & = \\min_{1 \\le k \\le n} \\min_{1 \\le q \\le n} W_{a, q}^i + W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + \\min_{1 \\le k \\le n} W_{q, k}^j + W_{k, b}^p \\\\ & = \\min_{1 \\le q \\le n} W_{a, q}^i + [W^jW^p]_{q, b}, \\end{aligned} $$ which is precisely entry $(a, b)$ of the right hand side.","title":"25.1-4"},{"location":"Chap25/25.1/#251-5","text":"Show how to express the single-source shortest-paths problem as a product of matrices and a vector. Describe how evaluating this product corresponds to a Bellman-Ford-like algorithm (see Section 24.1). (Removed)","title":"25.1-5"},{"location":"Chap25/25.1/#251-6","text":"Suppose we also wish to compute the vertices on shortest paths in the algorithms of this section. Show how to compute the predecessor matrix $\\prod$ from the completed matrix $L$ of shortest-path weights in $O(n^3)$ time. For each source vertex $v_i$ we need to compute the shortest-paths tree for $v_i$. To do this, we need to compute the predecessor for each $j \\ne i$. For fixed $i$ and $j$, this is the value of $k$ such that $L_{i, k} + w(k, j) = L[i, j]$. Since there are $n$ vertices whose trees need computing, $n$ vertices for each such tree whose predecessors need computing, and it takes $O(n)$ to compute this for each one (checking each possible $k$), the total time is $O(n^3)$.","title":"25.1-6"},{"location":"Chap25/25.1/#251-7","text":"We can also compute the vertices on shortest paths as we compute the shortestpath weights. Define $\\pi_{ij}^{(m)}$ as the predecessor of vertex $j$ on any minimum-weight path from $i$ to $j$ that contains at most $m$ edges. Modify the $\\text{EXTEND-SHORTESTPATHS}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ procedures to compute the matrices$\\prod^{(1)}, \\prod^{(2)}, \\ldots, \\prod^{(n - 1)}$ as the matrices $L^{(1)}, L^{(2)}, \\ldots, L^{(n - 1)}$ are computed. To have the procedure compute the predecessor along the shortest path, see the modified procedures, $\\text{EXTEND-SHORTEST-PATH-MOD}$ and $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS-MOD}$ EXTEND - SHORTEST - PATH - MOD ( \u220f , L , W ) n = L . row let L ' = l ' [ i , j ] be a new n \u00d7 n matirx \u220f' = \u03c0' [ i , j ] is a new n \u00d7 n matrix for i = 1 to n for j = 1 to n l ' [ i , j ] = \u221e \u03c0' [ i , j ] = NIL for k = 1 to n if l [ i , k ] + l [ k , j ] < l [ i , j ] l [ i , j ] = l [ i , k ] + l [ k , j ] if k != j \u03c0' [ i , j ] = k else \u03c0' [ i , j ] = \u03c0 [ i , j ] return ( \u220f' , L ' ) SLOW - ALL - PAIRS - SHORTEST - PATHS - MOD ( W ) n = W . rows L ( 1 ) = W \u220f ( 1 ) = \u03c0 [ i , j ]( 1 ) where \u03c0 [ i , j ]( 1 ) = i if there is an edge from i to j , and NIL otherwise for m = 2 to n - 1 \u220f ( m ), L ( m ) = EXTEND - SHORTEST - PATH - MOD ( \u220f ( m - 1 ), L ( m - 1 ), W ) return ( \u220f ( n - 1 ), L ( n - 1 ))","title":"25.1-7"},{"location":"Chap25/25.1/#251-8","text":"The $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ procedure, as written, requires us to store $\\lceil \\lg(n - 1) \\rceil$ matrices, each with $n^2$ elements, for a total space requirement of $\\Theta(n^2\\lg n)$. Modify the procedure to require only $\\Theta(n^2)$ space by using only two $n \\times n$ matrices. We can overwrite matrices as we go. Let $A \\star B$ denote multiplication defined by the $\\text{EXTEND-SHORTEST-PATHS}$ procedure. Then we modify $\\text{FASTER-ALL-EXTEND-SHORTEST-PATHS}(W)$. We initially create an $n$ by $n$ matrix $L$. Delete line 5 of the algorithm, and change line 6 to $L = W \\star W$, followed by $W = L$.","title":"25.1-8"},{"location":"Chap25/25.1/#251-9","text":"Modify $\\text{FASTER-ALL-PAIRS-SHORTEST-PATHS}$ so that it can determine whether the graph contains a negative-weight cycle. For the modification, keep computing for one step more than the original, that is, we compute all the way up to $L^{(2k + 1)}$ where $2^k > n - 1$. Then, if there aren't any negative weight cycles, then, we will have that the two matrices should be equal since having no negative weight cycles means that between any two vertices, there is a path that is tied for shortest and contains at most $n - 1$ edges. However, if there is a cycle of negative total weight, we know that it's length is at most $n$, so, since we are allowing paths to be larger by $2k \\ge n$ between these two matrices, we have that we would need to have all of the vertices on the cycle have their distance reduce by at least the negative weight of the cycle. Since we can detect exactly when there is a negative cycle, based on when these two matrices are different. This algorithm works. It also only takes time equal to a single matrix multiplication which is littlee oh of the unmodified algorithm.","title":"25.1-9"},{"location":"Chap25/25.1/#251-10","text":"Give an efficient algorithm to find the length (number of edges) of a minimum-length negative-weight cycle in a graph. (Removed)","title":"25.1-10"},{"location":"Chap25/25.2/","text":"25.2-1 Run the Floyd-Warshall algorithm on the weighted, directed graph of Figure 25.2. Show the matrix $D^{(k)}$ that results for each iteration of the outer loop. $k = 1$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ $k = 2$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & - 8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 3$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 4$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 6$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ 25.2-2 Show how to compute the transitive closure using the technique of Section 25.1. We set $w_{ij} = 1$ if $(i, j)$ is an edge, and $w_{ij} = 0$ otherwise. Then we replace line 7 of $\\text{EXTEND-SHORTEST-PATHS}(L, W)$ by $l''_{ij} = l''_{ij} \\lor (l_{ik} \\land w_{kj})$. Then run the $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ algorithm. 25.2-3 Modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\prod^{(k)}$ matrices according to equations $\\text{(25.6)}$ and $\\text{(25.7)}$. Prove rigorously that for all $i \\in V$, the predecessor subgraph $G_{\\pi, i}$ is a shortest-paths tree with root $i$. ($\\textit{Hint:}$ To show that $G_{\\pi, i}$ is acyclic, first show that $\\pi_{ij}^{(k)} = l$ implies $d_{ij}^{(k)} \\ge d_{il}^{(k)} + w_{lj}$, according to the definition of $\\pi_{ij}^{(k)}$. Then, adapt the proof of Lemma 23.16.) MOD - FLOYD - WARSHALL ( W ) n = W . rows D ( 0 ) = W let \u03c0 ( 0 ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if i != j and D [ i , j ]( 0 ) < \u221e \u03c0 [ i , j ]( 0 ) = i for k = 1 to n let D ( k ) be a new n \u00d7 n matrix let \u03c0 ( k ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if d [ i , j ]( k - 1 ) \u2264 d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) d [ i , j ]( k ) = d [ i , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ i , j ]( k - 1 ) else d [ i , j ]( k ) = d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ k , j ]( k - 1 ) In order to have that $\\pi^{(k)}_{ij} = l$, we need that $d^{(k)}_{ij} \\ge d^{(k)}_{il} + w_{lj}$. To see this fact, we will note that having $\\pi^{(k)}_{ij} = l$ means that a shortest path from $i$ to $j$ last goes through $l$. A path that last goes through $l$ corresponds to taking a chepest path from $i$ to $l$ and then following the single edge from $l$ to $j$. However, This means that $d_{il} \\le d_{ij} - w_{ij}$, which we can rearrange to get the desired inequality. We can just continue following this inequality around, and if we ever get some cycle, $i_1, i_2, \\ldots, i_c$, then we would have that $d_{ii_1} \\le d_{ii_1} + w_{i_1i_2} + w_{i_2i_3} + \\cdots + w_{i_ci_1}$. So, if we subtract the common term sfrom both sides, we get that $0 \\le w_{i_ci_1} + \\sum_{q = 1}^{c - 1} w_{i_qi_{q + 1}}$. So, we have that we would only have a cycle in the precedessor graph if we ahvt that there is a zero weight cycle in the original graph. However, we would never have to go around the weight zero cycle since the constructed path of shortest weight favors ones with a fewer number of edges because of the way that we handle the equality case in equation $\\text{(25.7)}$. 25.2-4 As it appears above, the Floyd-Warshall algorithm requires $\\Theta(n^3)$ space, since we compute $d_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$. Show that the following procedure, which simply drops all the superscripts, is correct, and thus only $\\Theta(n^2)$ space is required. FLOYD - WARSHALL ' ( W ) n = W . rows D = W for k = 1 to n for i = 1 to n for j = 1 to n d [ i , j ] = min ( d [ i , j ], d [ i , k ] + d [ k , j ]) return D (Removed) 25.2-5 Suppose that we modify the way in which equation $\\text{(25.7)}$ handles equality: $$ \\pi_{ij}^{(k)} = \\begin{cases} \\pi_{ij}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} < d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}, \\\\ \\pi_{kj}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} \\ge d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}. \\end{cases} $$ Is this alternative definition of the predecessor matrix $\\prod$ correct? If we change the way that we handle the equality case, we will still be generating a the correct values for the $\\pi$ matrix. This is because updating the $\\pi$ values to make paths that are longer but still tied for the lowest weight. Making $\\pi_{ij} = \\pi_{kj}$ means that we are making the shortest path from $i$ to $j$ passes through $k$ at some point. This has the same cost as just going from $i$ to $j$, since $d_{ij} = d_{ik} + d_{kj}$. 25.2-6 How can we use the output of the Floyd-Warshall algorithm to detect the presence of a negative-weight cycle? (Removed) 25.2-7 Another way to reconstruct shortest paths in the Floyd-Warshall algorithm uses values $\\phi_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$, where $\\phi_{ij}^{(k)}$ is the highest-numbered intermediate vertex of a shortest path from $i$ to $j$ in which all intermediate vertices are in the set $\\{1, 2, \\ldots, k \\}$. Give a recursive formulation for $\\phi_{ij}^{(k)}$, modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\phi_{ij}^{(k)}$ values, and rewrite the $\\text{PRINT-ALLPAIRS-SHORTEST-PATH}$ procedure to take the matrix $\\Phi = \\big(\\phi_{ij}^{(n)}\\big)$ as an input. How is the matrix $\\Phi$ like the $s$ table in the matrix-chain multiplication problem of Section 15.2? We can recursively compute the values of $\\phi_{ij}^{(k)}$ by, letting it be $\\phi_{ij}^{(k - 1)}$ if $d_{ik}^{(k)} + d_{kj}^{(k)} \\ge d_{ij}^{(k - 1)}$, and otherwise, let it be $k$. This works correctly because it perfectly captures whether we decided to use vertex $k$ when we were repeatedly allowing ourselves use of each vertex one at a time. To modify Floyd-Warshall to compute this, we would just need to stick within the innermost for loop, something that computes $\\phi(k)$ by this recursive rule, this would only be a constant amount of work in this innermost for loop, and so would not cause the asymptotic runtime to increase. It is similar to the s table in matrix-chain multiplication because it is computed by a similar recurrence. If we already have the $n^3$ values in $\\phi_{ij}^{(k)}$ provided, then we can reconstruct the shortest path from $i$ to $j$ because we know that the largest vertex in the path from $i$ to $j$ is $\\phi_{ij}^{(n)}$, call it $a_1$. Then, we know that the largest vertex in the path before $a_1$ will be $\\phi_{ia_1}^{(a_1 - 1)}$ and the largest after $a_1$ will be $\\phi_{a_1j}^{(a_1 - 1)}$. By continuing to recurse until we get that the largest element showing up at some point is $\\text{NIL}$, we will be able to continue subdividing the path until it is entirely constructed. 25.2-8 Give an $O(VE)$-time algorithm for computing the transitive closure of a directed graph $G = (V, E)$. We can determine the vertices reachable from a particular vertex in $O(V + E)$ time using any basic graph searching algorithm. Thus we can compute the transitive closure in $O(VE + V^2)$ time by searching the graph with each vertex as the source. If $|V| = O(E)$, we're done as $VE$ is now the dominating term in the running time bound. If not, we preprocess the graph and mark all degree-$0$ vertices in $O(V + E)$ time. The rows representing these vertices in the transitive closure are all $0$s, which means that the algorithm remains correct if we ignore these vertices when searching. After preprocessing, $|V| = O(E)$ as $|E| \\geq |V|/2$. Therefore searching can be done in $O(VE)$ time. 25.2-9 Suppose that we can compute the transitive closure of a directed acyclic graph in $f(|V|, |E|)$ time, where $f$ is a monotonically increasing function of $|V|$ and $|E|$. Show that the time to compute the transitive closure $G^* = (V, E^*)$ of a general directed graph $G = (V, E)$ is then $f(|V|, |E|) + O(V + E^*)$. First, compute the strongly connected components of the directed graph, and look at it's component graph. This component graph is going to be acyclic and have at most as many vertices and at most as many edges as the original graph. Since it is acyclic, we can run our transitive closure algorithm on it. Then, for every edge $(S_1, S_2)$ that shows up in the transitive closure of the component graph, we add an edge from each vertex in $S_1$ to a vertex in $S_2$. This takes time equal to $O(V + E')$. So, the total time required is $\\le f(|V|, |E|) + O(V + E)$.","title":"25.2 The Floyd-Warshall algorithm"},{"location":"Chap25/25.2/#252-1","text":"Run the Floyd-Warshall algorithm on the weighted, directed graph of Figure 25.2. Show the matrix $D^{(k)}$ that results for each iteration of the outer loop. $k = 1$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ \\infty & 2 & 0 & \\infty & \\infty & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ \\infty & 7 & \\infty & \\infty & 0 & \\infty \\\\ \\infty & 5 & 10 & \\infty & \\infty & 0 \\end{pmatrix} $$ $k = 2$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & - 8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 3$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ 1 & 0 & \\infty & 2 & 0 & \\infty \\\\ 3 & 2 & 0 & 4 & 2 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 8 & 7 & \\infty & 9 & 0 & \\infty \\\\ 6 & 5 & 10 & 7 & 5 & 0 \\end{pmatrix} $$ $k = 4$: $$ \\begin{pmatrix} 0 & \\infty & \\infty & \\infty & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & \\infty & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 5$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ 0 & 2 & 0 & 4 & -1 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$ $k = 6$: $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} $$","title":"25.2-1"},{"location":"Chap25/25.2/#252-2","text":"Show how to compute the transitive closure using the technique of Section 25.1. We set $w_{ij} = 1$ if $(i, j)$ is an edge, and $w_{ij} = 0$ otherwise. Then we replace line 7 of $\\text{EXTEND-SHORTEST-PATHS}(L, W)$ by $l''_{ij} = l''_{ij} \\lor (l_{ik} \\land w_{kj})$. Then run the $\\text{SLOW-ALL-PAIRS-SHORTEST-PATHS}$ algorithm.","title":"25.2-2"},{"location":"Chap25/25.2/#252-3","text":"Modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\prod^{(k)}$ matrices according to equations $\\text{(25.6)}$ and $\\text{(25.7)}$. Prove rigorously that for all $i \\in V$, the predecessor subgraph $G_{\\pi, i}$ is a shortest-paths tree with root $i$. ($\\textit{Hint:}$ To show that $G_{\\pi, i}$ is acyclic, first show that $\\pi_{ij}^{(k)} = l$ implies $d_{ij}^{(k)} \\ge d_{il}^{(k)} + w_{lj}$, according to the definition of $\\pi_{ij}^{(k)}$. Then, adapt the proof of Lemma 23.16.) MOD - FLOYD - WARSHALL ( W ) n = W . rows D ( 0 ) = W let \u03c0 ( 0 ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if i != j and D [ i , j ]( 0 ) < \u221e \u03c0 [ i , j ]( 0 ) = i for k = 1 to n let D ( k ) be a new n \u00d7 n matrix let \u03c0 ( k ) be a new n \u00d7 n matrix for i = 1 to n for j = 1 to n if d [ i , j ]( k - 1 ) \u2264 d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) d [ i , j ]( k ) = d [ i , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ i , j ]( k - 1 ) else d [ i , j ]( k ) = d [ i , k ]( k - 1 ) + d [ k , j ]( k - 1 ) \u03c0 [ i , j ]( k ) = \u03c0 [ k , j ]( k - 1 ) In order to have that $\\pi^{(k)}_{ij} = l$, we need that $d^{(k)}_{ij} \\ge d^{(k)}_{il} + w_{lj}$. To see this fact, we will note that having $\\pi^{(k)}_{ij} = l$ means that a shortest path from $i$ to $j$ last goes through $l$. A path that last goes through $l$ corresponds to taking a chepest path from $i$ to $l$ and then following the single edge from $l$ to $j$. However, This means that $d_{il} \\le d_{ij} - w_{ij}$, which we can rearrange to get the desired inequality. We can just continue following this inequality around, and if we ever get some cycle, $i_1, i_2, \\ldots, i_c$, then we would have that $d_{ii_1} \\le d_{ii_1} + w_{i_1i_2} + w_{i_2i_3} + \\cdots + w_{i_ci_1}$. So, if we subtract the common term sfrom both sides, we get that $0 \\le w_{i_ci_1} + \\sum_{q = 1}^{c - 1} w_{i_qi_{q + 1}}$. So, we have that we would only have a cycle in the precedessor graph if we ahvt that there is a zero weight cycle in the original graph. However, we would never have to go around the weight zero cycle since the constructed path of shortest weight favors ones with a fewer number of edges because of the way that we handle the equality case in equation $\\text{(25.7)}$.","title":"25.2-3"},{"location":"Chap25/25.2/#252-4","text":"As it appears above, the Floyd-Warshall algorithm requires $\\Theta(n^3)$ space, since we compute $d_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$. Show that the following procedure, which simply drops all the superscripts, is correct, and thus only $\\Theta(n^2)$ space is required. FLOYD - WARSHALL ' ( W ) n = W . rows D = W for k = 1 to n for i = 1 to n for j = 1 to n d [ i , j ] = min ( d [ i , j ], d [ i , k ] + d [ k , j ]) return D (Removed)","title":"25.2-4"},{"location":"Chap25/25.2/#252-5","text":"Suppose that we modify the way in which equation $\\text{(25.7)}$ handles equality: $$ \\pi_{ij}^{(k)} = \\begin{cases} \\pi_{ij}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} < d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}, \\\\ \\pi_{kj}^{(k - 1)} & \\text{ if } d_{ij}^{(k - 1)} \\ge d_{ik}^{(k - 1)} + d_{kj}^{(k - 1)}. \\end{cases} $$ Is this alternative definition of the predecessor matrix $\\prod$ correct? If we change the way that we handle the equality case, we will still be generating a the correct values for the $\\pi$ matrix. This is because updating the $\\pi$ values to make paths that are longer but still tied for the lowest weight. Making $\\pi_{ij} = \\pi_{kj}$ means that we are making the shortest path from $i$ to $j$ passes through $k$ at some point. This has the same cost as just going from $i$ to $j$, since $d_{ij} = d_{ik} + d_{kj}$.","title":"25.2-5"},{"location":"Chap25/25.2/#252-6","text":"How can we use the output of the Floyd-Warshall algorithm to detect the presence of a negative-weight cycle? (Removed)","title":"25.2-6"},{"location":"Chap25/25.2/#252-7","text":"Another way to reconstruct shortest paths in the Floyd-Warshall algorithm uses values $\\phi_{ij}^{(k)}$ for $i, j, k = 1, 2, \\ldots, n$, where $\\phi_{ij}^{(k)}$ is the highest-numbered intermediate vertex of a shortest path from $i$ to $j$ in which all intermediate vertices are in the set $\\{1, 2, \\ldots, k \\}$. Give a recursive formulation for $\\phi_{ij}^{(k)}$, modify the $\\text{FLOYD-WARSHALL}$ procedure to compute the $\\phi_{ij}^{(k)}$ values, and rewrite the $\\text{PRINT-ALLPAIRS-SHORTEST-PATH}$ procedure to take the matrix $\\Phi = \\big(\\phi_{ij}^{(n)}\\big)$ as an input. How is the matrix $\\Phi$ like the $s$ table in the matrix-chain multiplication problem of Section 15.2? We can recursively compute the values of $\\phi_{ij}^{(k)}$ by, letting it be $\\phi_{ij}^{(k - 1)}$ if $d_{ik}^{(k)} + d_{kj}^{(k)} \\ge d_{ij}^{(k - 1)}$, and otherwise, let it be $k$. This works correctly because it perfectly captures whether we decided to use vertex $k$ when we were repeatedly allowing ourselves use of each vertex one at a time. To modify Floyd-Warshall to compute this, we would just need to stick within the innermost for loop, something that computes $\\phi(k)$ by this recursive rule, this would only be a constant amount of work in this innermost for loop, and so would not cause the asymptotic runtime to increase. It is similar to the s table in matrix-chain multiplication because it is computed by a similar recurrence. If we already have the $n^3$ values in $\\phi_{ij}^{(k)}$ provided, then we can reconstruct the shortest path from $i$ to $j$ because we know that the largest vertex in the path from $i$ to $j$ is $\\phi_{ij}^{(n)}$, call it $a_1$. Then, we know that the largest vertex in the path before $a_1$ will be $\\phi_{ia_1}^{(a_1 - 1)}$ and the largest after $a_1$ will be $\\phi_{a_1j}^{(a_1 - 1)}$. By continuing to recurse until we get that the largest element showing up at some point is $\\text{NIL}$, we will be able to continue subdividing the path until it is entirely constructed.","title":"25.2-7"},{"location":"Chap25/25.2/#252-8","text":"Give an $O(VE)$-time algorithm for computing the transitive closure of a directed graph $G = (V, E)$. We can determine the vertices reachable from a particular vertex in $O(V + E)$ time using any basic graph searching algorithm. Thus we can compute the transitive closure in $O(VE + V^2)$ time by searching the graph with each vertex as the source. If $|V| = O(E)$, we're done as $VE$ is now the dominating term in the running time bound. If not, we preprocess the graph and mark all degree-$0$ vertices in $O(V + E)$ time. The rows representing these vertices in the transitive closure are all $0$s, which means that the algorithm remains correct if we ignore these vertices when searching. After preprocessing, $|V| = O(E)$ as $|E| \\geq |V|/2$. Therefore searching can be done in $O(VE)$ time.","title":"25.2-8"},{"location":"Chap25/25.2/#252-9","text":"Suppose that we can compute the transitive closure of a directed acyclic graph in $f(|V|, |E|)$ time, where $f$ is a monotonically increasing function of $|V|$ and $|E|$. Show that the time to compute the transitive closure $G^* = (V, E^*)$ of a general directed graph $G = (V, E)$ is then $f(|V|, |E|) + O(V + E^*)$. First, compute the strongly connected components of the directed graph, and look at it's component graph. This component graph is going to be acyclic and have at most as many vertices and at most as many edges as the original graph. Since it is acyclic, we can run our transitive closure algorithm on it. Then, for every edge $(S_1, S_2)$ that shows up in the transitive closure of the component graph, we add an edge from each vertex in $S_1$ to a vertex in $S_2$. This takes time equal to $O(V + E')$. So, the total time required is $\\le f(|V|, |E|) + O(V + E)$.","title":"25.2-9"},{"location":"Chap25/25.3/","text":"25.3-1 Use Johnson's algorithm to find the shortest paths between all pairs of vertices in the graph of Figure 25.2. Show the values of $h$ and $\\hat w$ computed by the algorithm. $$ \\begin{array}{c|c} v & h(v) \\\\ \\hline 1 & -5 \\\\ 2 & -3 \\\\ 3 & 0 \\\\ 4 & -1 \\\\ 5 & -6 \\\\ 6 & -8 \\end{array} $$ $$ \\begin{array}{ccc|ccc} u & v & \\hat w(u, v) & u & v & \\hat w(u, v) \\\\ \\hline 1 & 2 & \\text{NIL} & 4 & 1 & 0 \\\\ 1 & 3 & \\text{NIL} & 4 & 2 & \\text{NIL} \\\\ 1 & 4 & \\text{NIL} & 4 & 3 & \\text{NIL} \\\\ 1 & 5 & 0 & 4 & 5 & 8 \\\\ 1 & 6 & \\text{NIL} & 4 & 6 & \\text{NIL} \\\\ 2 & 1 & 3 & 5 & 1 & \\text{NIL} \\\\ 2 & 3 & \\text{NIL} & 5 & 2 & 4 \\\\ 2 & 4 & 0 & 5 & 3 & \\text{NIL} \\\\ 2 & 5 & \\text{NIL} & 5 & 4 & \\text{NIL} \\\\ 2 & 6 & \\text{NIL} & 5 & 6 & \\text{NIL} \\\\ 3 & 1 & \\text{NIL} & 6 & 1 & \\text{NIL} \\\\ 3 & 2 & 5 & 6 & 2 & 0 \\\\ 3 & 4 & \\text{NIL} & 6 & 3 & 2 \\\\ 3 & 5 & \\text{NIL} & 6 & 4 & \\text{NIL} \\\\ 3 & 6 & 0 & 6 & 5 & \\text{NIL} \\\\ \\end{array} $$ So, the $d_{ij}$ values that we get are $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} . $$ 25.3-2 What is the purpose of adding the new vertex $s$ to $V'$, yielding $V'$? This is only important when there are negative-weight cycles in the graph. Using a dummy vertex gets us around the problem of trying to compute $-\\infty + \\infty$ to find $\\hat w$. Moreover, if we had instead used a vertex $v$ in the graph instead of the new vertex $s$, then we run into trouble if a vertex fails to be reachable from $v$. 25.3-3 Suppose that $w(u, v) \\ge 0$ for all edges $(u, v) \\in E$. What is the relationship between the weight functions $w$ and $\\hat w$? If all the edge weights are nonnegative, then the values computed as the shortest distances when running Bellman-Ford will be all zero. This is because when constructing $G'$ on the first line of Johnson's algorithm, we place an edge of weight zero from s to every other vertex. Since any path within the graph has no negative edges, its cost cannot be negative, and so, cannot beat the trivial path that goes straight from $s$ to any given vertex. Since we have that $h(u) = h(v)$ for every $u$ and $v$, the reweighting that occurs only adds and subtracts $0$, and so we have that $w(u, v) = \\hat w(u, v)$ 25.3-4 Professor Greenstreet claims that there is a simpler way to reweight edges than the method used in Johnson's algorithm. Letting $w^* = \\min_{(u, v) \\in E} \\{w(u, v)\\}$, just define $\\hat w(u, v) = w(u, v) - w^*$ for all edges $(u, v) \\in E$. What is wrong with the professor's method of reweighting? (Removed) 25.3-5 Suppose that we run Johnson's algorithm on a directed graph $G$ with weight function $w$. Show that if $G$ contains a $0$-weight cycle $c$, then $\\hat w(u, v) = 0$ for every edge $(u, v)$ in $c$. If $\\delta(s, v) - \\delta(s, u) \\le w(u, v)$, we have $$\\delta(s, u) \\le \\delta(s, v) + (0 - w(u, v)) < \\delta(s, u) + w(u, v) - w(u, v) = \\delta(s, u),$$ which is impossible, thus $\\delta(s, v) - \\delta(s, u) = w(u, v)$, $\\hat w(u, v) = w(u, v) + \\delta(s, u) - \\delta(s, v) = 0$. 25.3-6 Professor Michener claims that there is no need to create a new source vertex in line 1 of $\\text{JOHNSON}$. He claims that instead we can just use $G' = G$ and let $s$ be any vertex. Give an example of a weighted, directed graph $G$ for which incorporating the professor's idea into $\\text{JOHNSON}$ causes incorrect answers. Then show that if $G$ is strongly connected (every vertex is reachable from every other vertex), the results returned by $\\text{JOHNSON}$ with the professor's modification are correct. (Removed)","title":"25.3 Johnson's algorithm for sparse graphs"},{"location":"Chap25/25.3/#253-1","text":"Use Johnson's algorithm to find the shortest paths between all pairs of vertices in the graph of Figure 25.2. Show the values of $h$ and $\\hat w$ computed by the algorithm. $$ \\begin{array}{c|c} v & h(v) \\\\ \\hline 1 & -5 \\\\ 2 & -3 \\\\ 3 & 0 \\\\ 4 & -1 \\\\ 5 & -6 \\\\ 6 & -8 \\end{array} $$ $$ \\begin{array}{ccc|ccc} u & v & \\hat w(u, v) & u & v & \\hat w(u, v) \\\\ \\hline 1 & 2 & \\text{NIL} & 4 & 1 & 0 \\\\ 1 & 3 & \\text{NIL} & 4 & 2 & \\text{NIL} \\\\ 1 & 4 & \\text{NIL} & 4 & 3 & \\text{NIL} \\\\ 1 & 5 & 0 & 4 & 5 & 8 \\\\ 1 & 6 & \\text{NIL} & 4 & 6 & \\text{NIL} \\\\ 2 & 1 & 3 & 5 & 1 & \\text{NIL} \\\\ 2 & 3 & \\text{NIL} & 5 & 2 & 4 \\\\ 2 & 4 & 0 & 5 & 3 & \\text{NIL} \\\\ 2 & 5 & \\text{NIL} & 5 & 4 & \\text{NIL} \\\\ 2 & 6 & \\text{NIL} & 5 & 6 & \\text{NIL} \\\\ 3 & 1 & \\text{NIL} & 6 & 1 & \\text{NIL} \\\\ 3 & 2 & 5 & 6 & 2 & 0 \\\\ 3 & 4 & \\text{NIL} & 6 & 3 & 2 \\\\ 3 & 5 & \\text{NIL} & 6 & 4 & \\text{NIL} \\\\ 3 & 6 & 0 & 6 & 5 & \\text{NIL} \\\\ \\end{array} $$ So, the $d_{ij}$ values that we get are $$ \\begin{pmatrix} 0 & 6 & \\infty & 8 & -1 & \\infty \\\\ -2 & 0 & \\infty & 2 & -3 & \\infty \\\\ -5 & -3 & 0 & -1 & -6 & -8 \\\\ -4 & 2 & \\infty & 0 & -5 & \\infty \\\\ 5 & 7 & \\infty & 9 & 0 & \\infty \\\\ 3 & 5 & 10 & 7 & 2 & 0 \\end{pmatrix} . $$","title":"25.3-1"},{"location":"Chap25/25.3/#253-2","text":"What is the purpose of adding the new vertex $s$ to $V'$, yielding $V'$? This is only important when there are negative-weight cycles in the graph. Using a dummy vertex gets us around the problem of trying to compute $-\\infty + \\infty$ to find $\\hat w$. Moreover, if we had instead used a vertex $v$ in the graph instead of the new vertex $s$, then we run into trouble if a vertex fails to be reachable from $v$.","title":"25.3-2"},{"location":"Chap25/25.3/#253-3","text":"Suppose that $w(u, v) \\ge 0$ for all edges $(u, v) \\in E$. What is the relationship between the weight functions $w$ and $\\hat w$? If all the edge weights are nonnegative, then the values computed as the shortest distances when running Bellman-Ford will be all zero. This is because when constructing $G'$ on the first line of Johnson's algorithm, we place an edge of weight zero from s to every other vertex. Since any path within the graph has no negative edges, its cost cannot be negative, and so, cannot beat the trivial path that goes straight from $s$ to any given vertex. Since we have that $h(u) = h(v)$ for every $u$ and $v$, the reweighting that occurs only adds and subtracts $0$, and so we have that $w(u, v) = \\hat w(u, v)$","title":"25.3-3"},{"location":"Chap25/25.3/#253-4","text":"Professor Greenstreet claims that there is a simpler way to reweight edges than the method used in Johnson's algorithm. Letting $w^* = \\min_{(u, v) \\in E} \\{w(u, v)\\}$, just define $\\hat w(u, v) = w(u, v) - w^*$ for all edges $(u, v) \\in E$. What is wrong with the professor's method of reweighting? (Removed)","title":"25.3-4"},{"location":"Chap25/25.3/#253-5","text":"Suppose that we run Johnson's algorithm on a directed graph $G$ with weight function $w$. Show that if $G$ contains a $0$-weight cycle $c$, then $\\hat w(u, v) = 0$ for every edge $(u, v)$ in $c$. If $\\delta(s, v) - \\delta(s, u) \\le w(u, v)$, we have $$\\delta(s, u) \\le \\delta(s, v) + (0 - w(u, v)) < \\delta(s, u) + w(u, v) - w(u, v) = \\delta(s, u),$$ which is impossible, thus $\\delta(s, v) - \\delta(s, u) = w(u, v)$, $\\hat w(u, v) = w(u, v) + \\delta(s, u) - \\delta(s, v) = 0$.","title":"25.3-5"},{"location":"Chap25/25.3/#253-6","text":"Professor Michener claims that there is no need to create a new source vertex in line 1 of $\\text{JOHNSON}$. He claims that instead we can just use $G' = G$ and let $s$ be any vertex. Give an example of a weighted, directed graph $G$ for which incorporating the professor's idea into $\\text{JOHNSON}$ causes incorrect answers. Then show that if $G$ is strongly connected (every vertex is reachable from every other vertex), the results returned by $\\text{JOHNSON}$ with the professor's modification are correct. (Removed)","title":"25.3-6"},{"location":"Chap25/Problems/25-1/","text":"Suppose that we wish to maintain the transitive closure of a directed graph $G = (V, E)$ as we insert edges into $E$. That is, after each edge has been inserted, we want to update the transitive closure of the edges inserted so far. Assume that the graph $G$ has no edges initially and that we represent the transitive closure as a boolean matrix. a. Show how to update the transitive closure $G^* = (V, E^*)$ of a graph $G = (V, E)$ in $O(V^2)$ time when a new edge is added to $G$. b. Give an example of a graph $G$ and an edge $e$ such that $\\Omega(V^2)$ time is required to update the transitive closure after the insertion of $e$ into $G$, no matter what algorithm is used. c. Describe an efficient algorithm for updating the transitive closure as edges are inserted into the graph. For any sequence of $n$ insertions, your algorithm should run in total time $\\sum_{i = 1}^n t_i = O(V^3)$, where $t_i$ is the time to update the transitive closure upon inserting the $i$th edge. Prove that your algorithm attains this time bound. a. We can update the transitive closure in time $O(V^2)$ as follows. Suppose that we add the edge $(x_1, x_2)$. Then, we will consider every pair of vertices $(u, v)$. In order to of created a path between them, we would need some part of that path that goes from $u$ to $x_1$ and some second part of that path that goes from $x_2$ to $v$. This means that we add the edge $(u, v)$ to the transitive closure if and only if the transitive closure contains the edges $(u, x_1)$ and $(x_2, v)$. Since we only had to consider every pair of vertices once, the runtime of this update is only $O(V^2)$. b. Suppose that we currently have two strongly connected components, each of size $|V| / 2$ with no edges between them. Then their transitive closures computed so far will consist of two complete directed graphs on $|V| / 2$ vertices each. So, there will be a total of $|V|^2 / 2$ edges adding the number of edges in each together. Then, we add a single edge from one component to the other. This will mean that every vertex in the component the edge is coming from will have an edge going to every vertex in the component that the edge is going to. So, the total number of edges after this operation will be $|V| / 2 + |V| / 4$ So, the number of edges increased by $|V| / 4$. Since each time we add an edge, we need to use at least constant time, since there is no cheap way to add many edges at once, the total amount of time needed is $\\Omega(|V|^2)$. c. We will have each vertex maintain a tree of vertices that have a path to it and a tree of vertices that it has a path to. The second of which is the transitive closure at each step. Then, upon inserting an edge, $(u, v)$, we will look at successive ancestors of $u$, and add $v$ to their successor tree, just past $u$. If we ever don't insert an edge when doing this, we can stop exploring that branch of the ancestor tree. Similarly, we keep doing this for all of the ancestors of $v$. Since we are able to short circuit if we ever notice that we have already added an edge, we know that we will only ever reconsider the same edge at most $n$ times, and, since the number of edges is $O(n^2)$, the total runtime is $O(n^3)$.","title":"25-1 Transitive closure of a dynamic graph"},{"location":"Chap25/Problems/25-2/","text":"A graph $G = (V, E)$ is $\\epsilon$-dense if $|E| = \\Theta(V^{1 + \\epsilon})$ for some constant $\\epsilon$ in the range $0 < \\epsilon \\le 1$. By using $d$-ary min-heaps (see Problem 6-2) in shortest-paths algorithms on $\\epsilon$-dense graphs, we can match the running times of Fibonacci-heap-based algorithms without using as complicated a data structure. a. What are the asymptotic running times for $\\text{INSERT}$, $\\text{EXTRACT-MIN}$, and $\\text{DECREASE-KEY}$, as a function of $d$ and the number $n$ of elements in a $d$-ary min-heap? What are these running times if we choose $d = \\Theta(n^\\alpha)$ for some constant $0 < \\alpha \\le 1$? Compare these running times to the amortized costs of these operations for a Fibonacci heap. b. Show how to compute shortest paths from a single source on an $\\epsilon$-dense directed graph $G = (V, E)$ with no negative-weight edges in $O(E)$ time. ($\\textit{Hint:}$ Pick $d$ as a function of $\\epsilon$.) c. Show how to solve the all-pairs shortest-paths problem on an $\\epsilon$-dense directed graph $G = (V, E)$ with no negative-weight edges in $O(VE)$ time. d. Show how to solve the all-pairs shortest-paths problem in $O(VE)$ time on an $\\epsilon$-dense directed graph $G = (V, E)$ that may have negative-weight edges but has no negative-weight cycles. a. $\\text{INSERT}$: $\\Theta(\\log_d n) = \\Theta(1 / \\alpha)$. $\\text{EXTRACT-MIN}$: $\\Theta(d\\log_d n) = \\Theta(n^\\alpha / \\alpha)$. $\\text{DECREASE-KEY}$: $\\Theta(\\log_d n) = \\Theta(1 / \\alpha)$. b. Dijkstra, $O(d\\log_d V \\cdot V + \\log_d V \\cdot E)$, if $d = V^\\epsilon$, then $$ \\begin{aligned} O(d \\log_d V \\cdot V + \\log_d V \\cdot E) & = O(V^\\epsilon \\cdot V / \\epsilon + E / \\epsilon) \\\\ & = O((V^{1+\\epsilon} + E) / \\epsilon) \\\\ & = O((E + E) / \\epsilon) \\\\ & = O(E). \\end{aligned} $$ c. Run $|V|$ times Dijkstra, since the algorithm is $O(E)$ based on (b), the total time is $O(VE)$. d. Johnson's reweight is $O(VE)$.","title":"25-2 Shortest paths in epsilon-dense graphs"},{"location":"Chap26/26.1/","text":"26.1-1 Show that splitting an edge in a flow network yields an equivalent network. More formally, suppose that flow network $G$ contains edge $(u, v)$, and we create a new flow network $G'$ by creating a new vertex $x$ and replacing $(u, v)$ by new edges $(u, x)$ and $(x, v)$ with $c(u, x) = c(x, v) = c(u, v)$. Show that a maximum flow in $G'$ has the same value as a maximum flow in $G$. (Removed) 26.1-2 Extend the flow properties and definitions to the multiple-source, multiple-sink problem. Show that any flow in a multiple-source, multiple-sink flow network corresponds to a flow of identical value in the single-source, single-sink network obtained by adding a supersource and a supersink, and vice versa. Capacity constraint: for all $u, v \\in V$, we require $0 \\le f(u, v) \\le c(u, v)$. Flow conservation: for all $u \\in V - S - T$, we require $\\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v)$. 26.1-3 Suppose that a flow network $G = (V, E)$ violates the assumption that the network contains a path $s \\leadsto v \\leadsto t$ for all vertices $v \\in V$. Let $u$ be a vertex for which there is no path $s \\leadsto u \\leadsto t$. Show that there must exist a maximum flow $f$ in $G$ such that $f(u, v) = f(v, u) = 0$ for all vertices $v \\in V$. (Removed) 26.1-4 Let $f$ be a flow in a network, and let $\\alpha$ be a real number. The scalar flow product , denoted $\\alpha f$, is a function from $V \\times V$ to $\\mathbb{R}$ defined by $$(\\alpha f)(u, v) = \\alpha \\cdot f(u, v).$$ Prove that the flows in a network form a convex set . That is, show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ in the range $0 \\le \\alpha \\le 1$. (Removed) 26.1-5 State the maximum-flow problem as a linear-programming problem. $$ \\begin{array}{ll} \\max & \\sum\\limits_{v \\in V} f(s, v) - \\sum\\limits_{v \\in V} f(v, s) \\\\ s.t. & 0 \\le f(u, v) \\le c(u, v) \\\\ & \\sum\\limits_{v \\in V} f(v, u) - \\sum\\limits_{v \\in V} f(u, v) = 0 \\end{array} $$ 26.1-6 Professor Adam has two children who, unfortunately, dislike each other. The problem is so severe that not only do they refuse to walk to school together, but in fact each one refuses to walk on any block that the other child has stepped on that day. The children have no problem with their paths crossing at a corner. Fortunately both the professor's house and the school are on corners, but beyond that he is not sure if it is going to be possible to send both of his children to the same school. The professor has a map of his town. Show how to formulate the problem of determining whether both his children can go to the same school as a maximum-flow problem. (Removed) 26.1-7 Suppose that, in addition to edge capacities, a flow network has vertex capacities . That is each vertex $v$ has a limit $l(v)$ on how much flow can pass though $v$. Show how to transform a flow network $G = (V, E)$ with vertex capacities into an equivalent flow network $G' = (V', E')$ without vertex capacities, such that a maximum flow in $G'$ has the same value as a maximum flow in $G$. How many vertices and edges does $G'$ have? (Removed)","title":"26.1 Flow networks"},{"location":"Chap26/26.1/#261-1","text":"Show that splitting an edge in a flow network yields an equivalent network. More formally, suppose that flow network $G$ contains edge $(u, v)$, and we create a new flow network $G'$ by creating a new vertex $x$ and replacing $(u, v)$ by new edges $(u, x)$ and $(x, v)$ with $c(u, x) = c(x, v) = c(u, v)$. Show that a maximum flow in $G'$ has the same value as a maximum flow in $G$. (Removed)","title":"26.1-1"},{"location":"Chap26/26.1/#261-2","text":"Extend the flow properties and definitions to the multiple-source, multiple-sink problem. Show that any flow in a multiple-source, multiple-sink flow network corresponds to a flow of identical value in the single-source, single-sink network obtained by adding a supersource and a supersink, and vice versa. Capacity constraint: for all $u, v \\in V$, we require $0 \\le f(u, v) \\le c(u, v)$. Flow conservation: for all $u \\in V - S - T$, we require $\\sum_{v \\in V} f(v, u) = \\sum_{v \\in V} f(u, v)$.","title":"26.1-2"},{"location":"Chap26/26.1/#261-3","text":"Suppose that a flow network $G = (V, E)$ violates the assumption that the network contains a path $s \\leadsto v \\leadsto t$ for all vertices $v \\in V$. Let $u$ be a vertex for which there is no path $s \\leadsto u \\leadsto t$. Show that there must exist a maximum flow $f$ in $G$ such that $f(u, v) = f(v, u) = 0$ for all vertices $v \\in V$. (Removed)","title":"26.1-3"},{"location":"Chap26/26.1/#261-4","text":"Let $f$ be a flow in a network, and let $\\alpha$ be a real number. The scalar flow product , denoted $\\alpha f$, is a function from $V \\times V$ to $\\mathbb{R}$ defined by $$(\\alpha f)(u, v) = \\alpha \\cdot f(u, v).$$ Prove that the flows in a network form a convex set . That is, show that if $f_1$ and $f_2$ are flows, then so is $\\alpha f_1 + (1 - \\alpha) f_2$ for all $\\alpha$ in the range $0 \\le \\alpha \\le 1$. (Removed)","title":"26.1-4"},{"location":"Chap26/26.1/#261-5","text":"State the maximum-flow problem as a linear-programming problem. $$ \\begin{array}{ll} \\max & \\sum\\limits_{v \\in V} f(s, v) - \\sum\\limits_{v \\in V} f(v, s) \\\\ s.t. & 0 \\le f(u, v) \\le c(u, v) \\\\ & \\sum\\limits_{v \\in V} f(v, u) - \\sum\\limits_{v \\in V} f(u, v) = 0 \\end{array} $$","title":"26.1-5"},{"location":"Chap26/26.1/#261-6","text":"Professor Adam has two children who, unfortunately, dislike each other. The problem is so severe that not only do they refuse to walk to school together, but in fact each one refuses to walk on any block that the other child has stepped on that day. The children have no problem with their paths crossing at a corner. Fortunately both the professor's house and the school are on corners, but beyond that he is not sure if it is going to be possible to send both of his children to the same school. The professor has a map of his town. Show how to formulate the problem of determining whether both his children can go to the same school as a maximum-flow problem. (Removed)","title":"26.1-6"},{"location":"Chap26/26.1/#261-7","text":"Suppose that, in addition to edge capacities, a flow network has vertex capacities . That is each vertex $v$ has a limit $l(v)$ on how much flow can pass though $v$. Show how to transform a flow network $G = (V, E)$ with vertex capacities into an equivalent flow network $G' = (V', E')$ without vertex capacities, such that a maximum flow in $G'$ has the same value as a maximum flow in $G$. How many vertices and edges does $G'$ have? (Removed)","title":"26.1-7"},{"location":"Chap26/26.2/","text":"26.2-1 Prove that the summations in equation $\\text{(26.6)}$ equal the summations in equation $\\text{(26.7)}$. (Removed) 26.2-2 In Figure $\\text{26.1}$(b), what is the flow across the cut $(\\{s, v_2, v_4\\}, \\{v_1, v_3, t\\})$? What is the capacity of this cut? $$ \\begin{aligned} f(S, T) & = f(s, v_1) + f(v_2, v_1) + f(v_4, v_3) + f(v_4, t) - f(v_3, v_2) = 11 + 1 + 7 + 4 - 4 = 19, \\\\ c(S, T) & = c(s, v_1) + c(v_2, v_1) + c(v_4, v_3) + c(v_4, t) = 16 + 4 + 7 + 4 = 31. \\end{aligned} $$ 26.2-3 Show the execution of the Edmonds-Karp algorithm on the flow network of Figure 26.1(a). If we perform a breadth first search where we consider the neighbors of a vertex as they appear in the ordering $\\{s, v_1, v_2, v_3, v_4, t\\}$, the first path that we will find is $s, v_1, v_3, t$. The min capacity of this augmenting path is $12$, so we send $12$ units along it. We perform a $\\text{BFS}$ on the resulting residual network. This gets us the path $s, v_2, v_4, t$. The min capacity along this path is $4$, so we send $4$ units along it. Then, the only path remaining in the residual network is $\\{s, v_2, v_4, v_3\\}$ which has a min capacity of $7$, since that's all that's left, we find it in our $\\text{BFS}$. Putting it all together, the total flow that we have found has a value of $23$. 26.2-4 In the example of Figure 26.6, what is the minimum cut corresponding to the maximum flow shown? Of the augmenting paths appearing in the example, which one cancels flow? A minimum cut corresponding to the maximum flow is $S = \\{s, v_1, v_2, v_4\\}$ and $T = \\{v_3, t\\}$. The augmenting path in part (c) cancels flow on edge $(v_3, v_2)$. 26.2-5 Recall that the construction in Section 26.1 that converts a flow network with multiple sources and sinks into a single-source, single-sink network adds edges with infinite capacity. Prove that any flow in the resulting network has a finite value if the edges of the original network with multiple sources and sinks have finite capacity. Since the only edges that have infinite value are those going from the supersource or to the supersink, as long as we pick a cut that has the supersource and all the original sources on one side, and the other side has the supersink as well as all the original sinks, then it will only cut through edges of finite capacity. Then, by Corollary 26.5, we have that the value of the flow is bounded above by the value of any of these types of cuts, which is finite. 26.2-6 Suppose that each source $s_i$ in a flow network with multiple sources and sinks produces exactly $p_i$ units of flow, so that $\\sum_{v \\in V} f(s_i, v) = p_i$. Suppose also that each sink $t_j$ consumes exactly $q_j$ units, so that $\\sum_{v \\in V} f(v, t_j) = q_j$, where $\\sum_i p_i = \\sum_j q_j$. Show how to convert the problem of finding a flow $f$ that obeys these additional constraints into the problem of finding a maximum flow in a single-source, single-sink flow network. $c(s, s_i) = p_i$, $c(t_j, t) = q_j$. 26.2-7 Prove Lemma 26.2. To check that $f_p$ is a flow, we make sure that it satisfies both the capacity constraints and the flow constraints. First, the capacity constraints. To see this, we recall our definition of $c_f(p)$, that is, it is the smallest residual capacity of any of the edges along the path $p$. Since we have that the residual capacity is always less than or equal to the initial capacity, we have that each value of the flow is less than the capacity. Second, we check the flow constraints, Since the only edges that are given any flow are along a path, we have that at each vertex interior to the path, the flow in from one edge is immediately canceled by the flow out to the next vertex in the path. Lastly, we can check that its value is equal to $c_f(p)$ because, while $s$ may show up at spots later on in the path, it will be canceled out as it leaves to go to the next vertex. So, the only net flow from s is the initial edge along the path, since it (along with all the other edges) is given flow $c_f(p)$, that is the value of the flow $f_p$. 26.2-8 Suppose that we redefine the residual network to disallow edges into $s$. Argue that the procedure $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow. (Removed) 26.2-9 Suppose that both $f$ and $f'$ are flows in a network $G$ and we compute flow $f \\uparrow f'$. Does the augmented flow satisfy the flow conservation property? Does it satisfy the capacity constraint? (Removed) 26.2-10 Show how to find a maximum flow in a network $G = (V, E)$ by a sequence of at most $|E|$ augmenting paths. ($\\textit{Hint:}$ Determine the paths after finding the maximum flow.) Suppose we already have a maximum flow $f$. Consider a new graph $G$ where we set the capacity of edge $(u, v)$ to $f(u, v)$. Run Ford-Fulkerson, with the modification that we remove an edge if its flow reaches its capacity. In other words, if $f(u, v) = c(u, v)$ then there should be no reverse edge appearing in residual network. This will still produce correct output in our case because we never exceed the actual maximum flow through an edge, so it is never advantageous to cancel flow. The augmenting paths chosen in this modified version of Ford-Fulkerson are precisely the ones we want. There are at most $|E|$ because every augmenting path produces at least one edge whose flow is equal to its capacity, which we set to be the actual flow for the edge in a maximum flow, and our modification prevents us from ever destroying this progress. 26.2-11 The edge connectivity of an undirected graph is the minimum number $k$ of edges that must be removed to disconnect the graph. For example, the edge connectivity of a tree is $1$, and the edge connectivity of a cyclic chain of vertices is $2$. Show how to determine the edge connectivity of an undirected graph $G = (V, E)$ by running a maximum-flow algorithm on at most $|V|$ flow networks, each having $O(V)$ vertices and $O(E)$ edges. Create an directed version of the graph. Then create a flow network out of it, resolving all antiparallel edges. All edges' capacities are set to $1$. Pick any vertex that wasn't created for antiparallel workaround as the sink and run maximum-flow algorithm with all vertexes that aren't for antipararrel workaround (except the sink) as sources. Find the minimum value out of all $|V| - 1$ maximum flow values. 26.2-12 Suppose that you are given a flow network $G$, and $G$ has edges entering the source $s$. Let $f$ be a flow in $G$ in which one of the edges $(v, s)$ entering the source has $f(v, s) = 1$. Prove that there must exist another flow $f'$ with $f'(v, s) = 0$ such that $|f| = |f'|$. Give an $O(E)$-time algorithm to compute $f'$, given $f$, and assuming that all edge capacities are integers. (Removed) 26.2-13 Suppose that you wish to find, among all minimum cuts in a flow network $G$ with integral capacities, one that contains the smallest number of edges. Show how to modify the capacities of $G$ to create a new flow network $G'$ in which any minimum cut in $G'$ is a minimum cut with the smallest number of edges in $G$. (Removed)","title":"26.2 The Ford-Fulkerson method"},{"location":"Chap26/26.2/#262-1","text":"Prove that the summations in equation $\\text{(26.6)}$ equal the summations in equation $\\text{(26.7)}$. (Removed)","title":"26.2-1"},{"location":"Chap26/26.2/#262-2","text":"In Figure $\\text{26.1}$(b), what is the flow across the cut $(\\{s, v_2, v_4\\}, \\{v_1, v_3, t\\})$? What is the capacity of this cut? $$ \\begin{aligned} f(S, T) & = f(s, v_1) + f(v_2, v_1) + f(v_4, v_3) + f(v_4, t) - f(v_3, v_2) = 11 + 1 + 7 + 4 - 4 = 19, \\\\ c(S, T) & = c(s, v_1) + c(v_2, v_1) + c(v_4, v_3) + c(v_4, t) = 16 + 4 + 7 + 4 = 31. \\end{aligned} $$","title":"26.2-2"},{"location":"Chap26/26.2/#262-3","text":"Show the execution of the Edmonds-Karp algorithm on the flow network of Figure 26.1(a). If we perform a breadth first search where we consider the neighbors of a vertex as they appear in the ordering $\\{s, v_1, v_2, v_3, v_4, t\\}$, the first path that we will find is $s, v_1, v_3, t$. The min capacity of this augmenting path is $12$, so we send $12$ units along it. We perform a $\\text{BFS}$ on the resulting residual network. This gets us the path $s, v_2, v_4, t$. The min capacity along this path is $4$, so we send $4$ units along it. Then, the only path remaining in the residual network is $\\{s, v_2, v_4, v_3\\}$ which has a min capacity of $7$, since that's all that's left, we find it in our $\\text{BFS}$. Putting it all together, the total flow that we have found has a value of $23$.","title":"26.2-3"},{"location":"Chap26/26.2/#262-4","text":"In the example of Figure 26.6, what is the minimum cut corresponding to the maximum flow shown? Of the augmenting paths appearing in the example, which one cancels flow? A minimum cut corresponding to the maximum flow is $S = \\{s, v_1, v_2, v_4\\}$ and $T = \\{v_3, t\\}$. The augmenting path in part (c) cancels flow on edge $(v_3, v_2)$.","title":"26.2-4"},{"location":"Chap26/26.2/#262-5","text":"Recall that the construction in Section 26.1 that converts a flow network with multiple sources and sinks into a single-source, single-sink network adds edges with infinite capacity. Prove that any flow in the resulting network has a finite value if the edges of the original network with multiple sources and sinks have finite capacity. Since the only edges that have infinite value are those going from the supersource or to the supersink, as long as we pick a cut that has the supersource and all the original sources on one side, and the other side has the supersink as well as all the original sinks, then it will only cut through edges of finite capacity. Then, by Corollary 26.5, we have that the value of the flow is bounded above by the value of any of these types of cuts, which is finite.","title":"26.2-5"},{"location":"Chap26/26.2/#262-6","text":"Suppose that each source $s_i$ in a flow network with multiple sources and sinks produces exactly $p_i$ units of flow, so that $\\sum_{v \\in V} f(s_i, v) = p_i$. Suppose also that each sink $t_j$ consumes exactly $q_j$ units, so that $\\sum_{v \\in V} f(v, t_j) = q_j$, where $\\sum_i p_i = \\sum_j q_j$. Show how to convert the problem of finding a flow $f$ that obeys these additional constraints into the problem of finding a maximum flow in a single-source, single-sink flow network. $c(s, s_i) = p_i$, $c(t_j, t) = q_j$.","title":"26.2-6"},{"location":"Chap26/26.2/#262-7","text":"Prove Lemma 26.2. To check that $f_p$ is a flow, we make sure that it satisfies both the capacity constraints and the flow constraints. First, the capacity constraints. To see this, we recall our definition of $c_f(p)$, that is, it is the smallest residual capacity of any of the edges along the path $p$. Since we have that the residual capacity is always less than or equal to the initial capacity, we have that each value of the flow is less than the capacity. Second, we check the flow constraints, Since the only edges that are given any flow are along a path, we have that at each vertex interior to the path, the flow in from one edge is immediately canceled by the flow out to the next vertex in the path. Lastly, we can check that its value is equal to $c_f(p)$ because, while $s$ may show up at spots later on in the path, it will be canceled out as it leaves to go to the next vertex. So, the only net flow from s is the initial edge along the path, since it (along with all the other edges) is given flow $c_f(p)$, that is the value of the flow $f_p$.","title":"26.2-7"},{"location":"Chap26/26.2/#262-8","text":"Suppose that we redefine the residual network to disallow edges into $s$. Argue that the procedure $\\text{FORD-FULKERSON}$ still correctly computes a maximum flow. (Removed)","title":"26.2-8"},{"location":"Chap26/26.2/#262-9","text":"Suppose that both $f$ and $f'$ are flows in a network $G$ and we compute flow $f \\uparrow f'$. Does the augmented flow satisfy the flow conservation property? Does it satisfy the capacity constraint? (Removed)","title":"26.2-9"},{"location":"Chap26/26.2/#262-10","text":"Show how to find a maximum flow in a network $G = (V, E)$ by a sequence of at most $|E|$ augmenting paths. ($\\textit{Hint:}$ Determine the paths after finding the maximum flow.) Suppose we already have a maximum flow $f$. Consider a new graph $G$ where we set the capacity of edge $(u, v)$ to $f(u, v)$. Run Ford-Fulkerson, with the modification that we remove an edge if its flow reaches its capacity. In other words, if $f(u, v) = c(u, v)$ then there should be no reverse edge appearing in residual network. This will still produce correct output in our case because we never exceed the actual maximum flow through an edge, so it is never advantageous to cancel flow. The augmenting paths chosen in this modified version of Ford-Fulkerson are precisely the ones we want. There are at most $|E|$ because every augmenting path produces at least one edge whose flow is equal to its capacity, which we set to be the actual flow for the edge in a maximum flow, and our modification prevents us from ever destroying this progress.","title":"26.2-10"},{"location":"Chap26/26.2/#262-11","text":"The edge connectivity of an undirected graph is the minimum number $k$ of edges that must be removed to disconnect the graph. For example, the edge connectivity of a tree is $1$, and the edge connectivity of a cyclic chain of vertices is $2$. Show how to determine the edge connectivity of an undirected graph $G = (V, E)$ by running a maximum-flow algorithm on at most $|V|$ flow networks, each having $O(V)$ vertices and $O(E)$ edges. Create an directed version of the graph. Then create a flow network out of it, resolving all antiparallel edges. All edges' capacities are set to $1$. Pick any vertex that wasn't created for antiparallel workaround as the sink and run maximum-flow algorithm with all vertexes that aren't for antipararrel workaround (except the sink) as sources. Find the minimum value out of all $|V| - 1$ maximum flow values.","title":"26.2-11"},{"location":"Chap26/26.2/#262-12","text":"Suppose that you are given a flow network $G$, and $G$ has edges entering the source $s$. Let $f$ be a flow in $G$ in which one of the edges $(v, s)$ entering the source has $f(v, s) = 1$. Prove that there must exist another flow $f'$ with $f'(v, s) = 0$ such that $|f| = |f'|$. Give an $O(E)$-time algorithm to compute $f'$, given $f$, and assuming that all edge capacities are integers. (Removed)","title":"26.2-12"},{"location":"Chap26/26.2/#262-13","text":"Suppose that you wish to find, among all minimum cuts in a flow network $G$ with integral capacities, one that contains the smallest number of edges. Show how to modify the capacities of $G$ to create a new flow network $G'$ in which any minimum cut in $G'$ is a minimum cut with the smallest number of edges in $G$. (Removed)","title":"26.2-13"},{"location":"Chap26/26.3/","text":"26.3-1 Run the Ford-Fulkerson algorithm on the flow network in Figure 26.8 (c) and show the residual network after each flow augmentation. Number the vertices in $L$ top to bottom from $1$ to $5$ and in $R$ top to bottom from $6$ to $9$. For each iteration, pick the augmenting path that is lexicographically smallest. First, we pick an augmenting path that passes through vertices 1 and 6. Then, we pick the path going through 2 and 8. Then, we pick the path going through 3 and 7. Then, the resulting residual graph has no path from $s$ to $t$. So, we know that we are done, and that we are pairing up vertices $(1, 6)$, $(2, 8)$, and $(3, 7)$. This number of unit augmenting paths agrees with the value of the cut where you cut the edges $(s, 3)$, $(6, t)$, and $(7, t)$. 26.3-2 Prove Theorem 26.10. We proceed by induction on the number of iterations of the while loop of Ford-Fulkerson. After the first iteration, since $c$ only takes on integer values and $(u, v).f$ is set to $0$, $c_f$ only takes on integer values. Thus, lines 7 and 8 of Ford-Fulkerson only assign integer values to $(u, v).f$. Assume that $(u, v).f \\in \\mathbb Z$ for all $(u, v)$ after the $n$th iteration. On the $(n + 1)$th iteration $c_f(p)$ is set to the minimum of $c_f(u, v)$ which is an integer by the induction hypothesis. Lines 7 and 8 compute $(u, v).f$ or $(v, u).f$. Either way, these the the sum or difference of integers by assumption, so after the $(n + 1)$th iteration we have that $(u, v).f$ is an integer for all $(u, v) \\in E$. Since the value of the flow is a sum of flows of edges, we must have $|f| \\in \\mathbb Z$ as well. 26.3-3 Let $G = (V, E)$ be a bipartite graph with vertex partition $V = L \\cup R$, and let $G'$ be its corresponding flow network. Give a good upper bound on the length of any augmenting path found in $G'$ during the execution of $\\text{FORD-FULKERSON}$. (Removed) 26.3-4 $\\star$ A perfect matching is a matching in which every vertex is matched. Let $G = (V, E)$ be an undirected bipartite graph with vertex partition $V = L \\cup R$, where $|L| = |R|$. For any $X \\subseteq V$, define the neighborhood of $X$ as $$N(X) = \\{y \\in V: (x, y) \\in E \\text{ for some } x \\in X\\},$$ that is, the set of vertices adjacent to some member of $X$. Prove Hall's theorem : there exists a perfect matching in $G$ if and only if $|A| \\le |N(A)|$ for every subset $A \\subseteq L$. First suppose there exists a perfect matching in $G$. Then for any subset $A \\subseteq L$, each vertex of $A$ is matched with a neighbor in $R$, and since it is a matching, no two such vertices are matched with the same vertex in $R$. Thus, there are at least $|A|$ vertices in the neighborhood of $A$. Now suppose that $|A| \\le |N(A)|$ for all $A \\subseteq L$. Run Ford-Fulkerson on the corresponding flow network. The flow is increased by $1$ each time an augmenting path is found, so it will suffice to show that this happens $|L|$ times. Suppose the while loop has run fewer than $L$ times, but there is no augmenting path. Then fewer than $L$ edges from $L$ to $R$ have flow $1$. Let $v_1 \\in L$ be such that no edge from $v_1$ to a vertex in $R$ has nonzero flow. By assumption, $v_1$ has at least one neighbor $v_1' \\in R$. If any of $v_1$'s neighbors are connected to $t$ in $G_f$ then there is a path, so assume this is not the case. Thus, there must be some edge $(v_2, v_1)$ with flow $1$. By assumption, $N(\\{v_1, v_2\\}) \\ge 2$, so there must exist $v_2' \\ne v_1'$ such that $v_2'\\in N(\\{v_1, v_2 \\})$. If $(v_2', t)$ is an edge in the residual network we're done since $v_2'$ must be a neighbor of $v_2$, so $s$, $v_1$, $v_1'$, $v_2$, $v_2'$, and $t$ is a path in $G_f$. Otherwise $v_2'$ must have a neighbor $v_3 \\in L$ such that $(v_3, v_2')$ is in $G_f$. Specifically, $v_3 \\ne v_1$ since $(v_3, v_2')$ has flow $1$, and $v_3 \\ne v_2$ since $(v_2, v_1')$ has flow $1$, so no more flow can leave $v_2$ without violating conservation of flow. Again by our hypothesis, $N(\\{v_1, v_2, v_3\\}) \\ge 3$, so there is another neighbor $v_3' \\in R$. Continuing in this fashion, we keep building up the neighborhood $v_i'$, expanding each time we find that $(v_i', t)$ is not an edge in $G_f$. This cannot happen $L$ times, since we have run the Ford-Fulkerson while-loop fewer than $|L|$ times, so there still exist edges into $t$ in $G_f$. Thus, the process must stop at some vertex $v_k'$, and we obtain an augmenting path $$s, v_1, v_1', v_2, v_2', v_3, \\ldots, v_k, v_k', t,$$ contradicting our assumption that there was no such path. Therefore the while loop runs at least $|L|$ times. By Corollary 26.3 the flow strictly increases each time by $f_p$. By Theorem 26.10 $f_p$ is an integer. In particular, it is equal to $1$. This implies that $|f| \\ge |L|$. It is clear that $|f| \\le |L|$, so we must have $|f| = |L|$. By Corollary 26.11 this is the cardinality of a maximum matching. Since $|L| = |R|$, any maximum matching must be a perfect matching. 26.3-5 $\\star$ We say that a bipartite graph $G = (V, E)$, where $V = L \\cup R$, is $d$-regular if every vertex $v \\in V$ has degree exactly $d$. Every $d$-regular bipartite graph has $|L| = |R|$. Prove that every $d$-regular bipartite graph has a matching of cardinality $|L|$ by arguing that a minimum cut of the corresponding flow network has capacity $|L|$. We convert the bipartite graph into a flow problem by making a new vertex for the source which has an edge of unit capacity going to each of the vertices in $L$, and a new vertex for the sink that has an edge from each of the vertices in $R$, each with unit capacity. We want to show that the number of edge between the two parts of the cut is at least $L$, this would get us by the max-flow-min-cut theorem that there is a flow of value at least $|L|$. The, we can apply the integrality theorem that all of the flow values are integers, meaning that we are selecting $|L|$ disjoint edges between $L$ and $R$. To see that every cut must have capacity at lest $|L|$, let $S_1$ be the side of the cut containing the source and let $S_2$ be the side of the cut containing the sink. Then, look at $L \\cap S_1$. The source has an edge going to each of $L \\cap (S_1)^c$, and there is an edge from $R \\cap S_1$ to the sink that will be cut. This means that we need that there are at least $|L \\cap S_1| - |R \\cap S_1|$ many edges going from $L \\cap S_1$ to $R \\cap S_2$. If we look at the set of all neighbors of $L \\cap S_1$, we get that there must be at least the same number of neighbors in $R$, because otherwise we could sum up the degrees going from $L \\cap S_1$ to $R$ on both sides, and get that some of the vertices in $R$ would need to have a degree higher than $d$. This means that the number of neighbors of $L \\cap S_1$ is at least $L \\cap S_1$, but we have that they are in $S_1$, but there are only $|R \\cap S_1|$ of those, so, we have that the size of the set of neighbors of $L \\cap S_1$ that are in $S_2$ is at least $|L \\cap S_1| - |R \\cap S_1|$. Since each of these neighbors has an edge crossing the cut, we have that the total number of edges that the cut breaks is at least $$(|L| - |L \\cap S_1|) + (|L \\cap S_1| - |R \\cap S_1|) + |R \\cap S_1| = |L|.$$ Since each of these edges is unit valued, the value of the cut is at least $|L|$.","title":"26.3 Maximum bipartite matching"},{"location":"Chap26/26.3/#263-1","text":"Run the Ford-Fulkerson algorithm on the flow network in Figure 26.8 (c) and show the residual network after each flow augmentation. Number the vertices in $L$ top to bottom from $1$ to $5$ and in $R$ top to bottom from $6$ to $9$. For each iteration, pick the augmenting path that is lexicographically smallest. First, we pick an augmenting path that passes through vertices 1 and 6. Then, we pick the path going through 2 and 8. Then, we pick the path going through 3 and 7. Then, the resulting residual graph has no path from $s$ to $t$. So, we know that we are done, and that we are pairing up vertices $(1, 6)$, $(2, 8)$, and $(3, 7)$. This number of unit augmenting paths agrees with the value of the cut where you cut the edges $(s, 3)$, $(6, t)$, and $(7, t)$.","title":"26.3-1"},{"location":"Chap26/26.3/#263-2","text":"Prove Theorem 26.10. We proceed by induction on the number of iterations of the while loop of Ford-Fulkerson. After the first iteration, since $c$ only takes on integer values and $(u, v).f$ is set to $0$, $c_f$ only takes on integer values. Thus, lines 7 and 8 of Ford-Fulkerson only assign integer values to $(u, v).f$. Assume that $(u, v).f \\in \\mathbb Z$ for all $(u, v)$ after the $n$th iteration. On the $(n + 1)$th iteration $c_f(p)$ is set to the minimum of $c_f(u, v)$ which is an integer by the induction hypothesis. Lines 7 and 8 compute $(u, v).f$ or $(v, u).f$. Either way, these the the sum or difference of integers by assumption, so after the $(n + 1)$th iteration we have that $(u, v).f$ is an integer for all $(u, v) \\in E$. Since the value of the flow is a sum of flows of edges, we must have $|f| \\in \\mathbb Z$ as well.","title":"26.3-2"},{"location":"Chap26/26.3/#263-3","text":"Let $G = (V, E)$ be a bipartite graph with vertex partition $V = L \\cup R$, and let $G'$ be its corresponding flow network. Give a good upper bound on the length of any augmenting path found in $G'$ during the execution of $\\text{FORD-FULKERSON}$. (Removed)","title":"26.3-3"},{"location":"Chap26/26.3/#263-4-star","text":"A perfect matching is a matching in which every vertex is matched. Let $G = (V, E)$ be an undirected bipartite graph with vertex partition $V = L \\cup R$, where $|L| = |R|$. For any $X \\subseteq V$, define the neighborhood of $X$ as $$N(X) = \\{y \\in V: (x, y) \\in E \\text{ for some } x \\in X\\},$$ that is, the set of vertices adjacent to some member of $X$. Prove Hall's theorem : there exists a perfect matching in $G$ if and only if $|A| \\le |N(A)|$ for every subset $A \\subseteq L$. First suppose there exists a perfect matching in $G$. Then for any subset $A \\subseteq L$, each vertex of $A$ is matched with a neighbor in $R$, and since it is a matching, no two such vertices are matched with the same vertex in $R$. Thus, there are at least $|A|$ vertices in the neighborhood of $A$. Now suppose that $|A| \\le |N(A)|$ for all $A \\subseteq L$. Run Ford-Fulkerson on the corresponding flow network. The flow is increased by $1$ each time an augmenting path is found, so it will suffice to show that this happens $|L|$ times. Suppose the while loop has run fewer than $L$ times, but there is no augmenting path. Then fewer than $L$ edges from $L$ to $R$ have flow $1$. Let $v_1 \\in L$ be such that no edge from $v_1$ to a vertex in $R$ has nonzero flow. By assumption, $v_1$ has at least one neighbor $v_1' \\in R$. If any of $v_1$'s neighbors are connected to $t$ in $G_f$ then there is a path, so assume this is not the case. Thus, there must be some edge $(v_2, v_1)$ with flow $1$. By assumption, $N(\\{v_1, v_2\\}) \\ge 2$, so there must exist $v_2' \\ne v_1'$ such that $v_2'\\in N(\\{v_1, v_2 \\})$. If $(v_2', t)$ is an edge in the residual network we're done since $v_2'$ must be a neighbor of $v_2$, so $s$, $v_1$, $v_1'$, $v_2$, $v_2'$, and $t$ is a path in $G_f$. Otherwise $v_2'$ must have a neighbor $v_3 \\in L$ such that $(v_3, v_2')$ is in $G_f$. Specifically, $v_3 \\ne v_1$ since $(v_3, v_2')$ has flow $1$, and $v_3 \\ne v_2$ since $(v_2, v_1')$ has flow $1$, so no more flow can leave $v_2$ without violating conservation of flow. Again by our hypothesis, $N(\\{v_1, v_2, v_3\\}) \\ge 3$, so there is another neighbor $v_3' \\in R$. Continuing in this fashion, we keep building up the neighborhood $v_i'$, expanding each time we find that $(v_i', t)$ is not an edge in $G_f$. This cannot happen $L$ times, since we have run the Ford-Fulkerson while-loop fewer than $|L|$ times, so there still exist edges into $t$ in $G_f$. Thus, the process must stop at some vertex $v_k'$, and we obtain an augmenting path $$s, v_1, v_1', v_2, v_2', v_3, \\ldots, v_k, v_k', t,$$ contradicting our assumption that there was no such path. Therefore the while loop runs at least $|L|$ times. By Corollary 26.3 the flow strictly increases each time by $f_p$. By Theorem 26.10 $f_p$ is an integer. In particular, it is equal to $1$. This implies that $|f| \\ge |L|$. It is clear that $|f| \\le |L|$, so we must have $|f| = |L|$. By Corollary 26.11 this is the cardinality of a maximum matching. Since $|L| = |R|$, any maximum matching must be a perfect matching.","title":"26.3-4 $\\star$"},{"location":"Chap26/26.3/#263-5-star","text":"We say that a bipartite graph $G = (V, E)$, where $V = L \\cup R$, is $d$-regular if every vertex $v \\in V$ has degree exactly $d$. Every $d$-regular bipartite graph has $|L| = |R|$. Prove that every $d$-regular bipartite graph has a matching of cardinality $|L|$ by arguing that a minimum cut of the corresponding flow network has capacity $|L|$. We convert the bipartite graph into a flow problem by making a new vertex for the source which has an edge of unit capacity going to each of the vertices in $L$, and a new vertex for the sink that has an edge from each of the vertices in $R$, each with unit capacity. We want to show that the number of edge between the two parts of the cut is at least $L$, this would get us by the max-flow-min-cut theorem that there is a flow of value at least $|L|$. The, we can apply the integrality theorem that all of the flow values are integers, meaning that we are selecting $|L|$ disjoint edges between $L$ and $R$. To see that every cut must have capacity at lest $|L|$, let $S_1$ be the side of the cut containing the source and let $S_2$ be the side of the cut containing the sink. Then, look at $L \\cap S_1$. The source has an edge going to each of $L \\cap (S_1)^c$, and there is an edge from $R \\cap S_1$ to the sink that will be cut. This means that we need that there are at least $|L \\cap S_1| - |R \\cap S_1|$ many edges going from $L \\cap S_1$ to $R \\cap S_2$. If we look at the set of all neighbors of $L \\cap S_1$, we get that there must be at least the same number of neighbors in $R$, because otherwise we could sum up the degrees going from $L \\cap S_1$ to $R$ on both sides, and get that some of the vertices in $R$ would need to have a degree higher than $d$. This means that the number of neighbors of $L \\cap S_1$ is at least $L \\cap S_1$, but we have that they are in $S_1$, but there are only $|R \\cap S_1|$ of those, so, we have that the size of the set of neighbors of $L \\cap S_1$ that are in $S_2$ is at least $|L \\cap S_1| - |R \\cap S_1|$. Since each of these neighbors has an edge crossing the cut, we have that the total number of edges that the cut breaks is at least $$(|L| - |L \\cap S_1|) + (|L \\cap S_1| - |R \\cap S_1|) + |R \\cap S_1| = |L|.$$ Since each of these edges is unit valued, the value of the cut is at least $|L|$.","title":"26.3-5 $\\star$"},{"location":"Chap26/26.4/","text":"26.4-1 Prove that, after the procedure $\\text{INITIALIZE-PREFLOW}(G, S)$ terminates, we have $s.e \\le -|f^*|$, where $f^*$ is a maximum flow for $G$. (Removed) 26.4-2 Show how to implement the generic push-relabel algorithm using $O(V)$ time per relabel operation, $O(1)$ time per push, and $O(1)$ time to select an applicable operation, for a total time of $O(V^2E)$. We must select an appropriate data structure to store all the information which will allow us to select a valid operation in constant time. To do this, we will need to maintain a list of overflowing vertices. By Lemma 26.14, a push or a relabel operation always applies to an overflowing vertex. To determine which operation to perform, we need to determine whether $u.h = v.h + 1$ for some $v \\in N(u)$. We'll do this by maintaining a list $u.high$ of all neighbors of $u$ in $G_f$ which have height greater than or equal to $u$. We'll update these attributes in the $\\text{PUSH}$ and $\\text{RELABEL}$ functions. It is clear from the pseudocode given for $\\text{PUSH}$ that we can execute it in constant time, provided we have maintain the attributes $\\delta_f(u, v)$, $u.e$, $c_f(u, v)$, $(u, v).f$ and $u.h$. Each time we call $\\text{PUSH}(u, v)$ the result is that $u$ is no longer overflowing, so we must remove it from the list. Maintain a pointer $u.overflow$ to $u$'s position in the overflow list. If a vertex $u$ is not overflowing, set $u.overflow = \\text{NIL}$. Next, check if $v$ became overflowing. If so, set $v.overflow$ equal to the head of the overflow list. Since we can update the pointer in constant time and delete from a linked list given a pointer to the element to be deleted in constant time, we can maintain the list in $O(1)$. The $\\text{RELABEL}$ operation takes $O(V)$ because we need to compute the minimum $v.h$ from among all $(u, v) \\in E_f$, and there could be $|V| - 1$ many such $v$. We will also need to update $u.high$ during $\\text{RELABEL}$. When $\\text{RELABEL}(u)$ is called, set $u.high$ equal to the empty list and for each vertex $v$ which is adjacent to $u$, if $v.h = u.h + 1$, add $u$ to the list $v.high$. Since this takes constant time per adjacent vertex we can maintain the attribute in $O(V)$ per call to relabel. 26.4-3 Prove that the generic push-relabel algorithm spends a total of only $O(VE)$ time in performing all the $O(V^2)$ relabel operations. (Removed) 26.4-4 Suppose that we have found a maximum flow in a flow network $G = (V, E)$ using a push-relabel algorithm. Give a fast algorithm to find a minimum cut in $G$. (Removed) 26.4-5 Give an efficient push-relabel algorithm to find a maximum matching in a bipartite graph. Analyze your algorithm. First, construct the flow network for the bipartite graph as in the previous section. Then, we relabel everything in $L$. Then, we push from every vertex in $L$ to a vertex in $R$, so long as it is possible. Keeping track of those that vertices of $L$ that are still overflowing can be done by a simple bit vector. Then, we relabel everything in R and push to the last vertex. Once these operations have been done, The only possible valid operations are to relabel the vertices of $L$ that weren't able to find an edge that they could push their flow along, so could possibly have to get a push back from $R$ to $L$. This continues until there are no more operations to do. This takes time of $O(V(E + V))$. 26.4-6 Suppose that all edge capacities in a flow network $G = (V, E)$ are in the set $\\{1, 2, \\ldots, k\\}$. Analyze the running time of the generic push-relabel algorithm in terms of $|V|$, $|E|$, and $k$. ($\\textit{Hint:}$ How many times can each edge support a nonsaturating push before it becomes saturated?) The number of relabel operations and saturating pushes is the same as before. An edge can handle at most $k$ nonsaturating pushes before it becomes saturated, so the number of nonsaturating pushes is at most $2k|V||E|$. Thus, the total number of basic operations is at most $2|V|^2 + 2|V||E| + 2k|V||E| = O(kVE)$. 26.4-7 Show that we could change line 6 of $\\text{INITIALIZE-PREFLOW}$ to 6 s . h = | G . V | - 2 without affecting the correctness or asymptotic performance of the generic pushrelabel algorithm. (Removed) 26.4-8 Let $\\delta_f(u, v)$ be the distance (number of edges) from $u$ to $v$ in the residual network $G_f$. Show that the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the properties that $u.h < |V|$ implies $u.h \\le \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| \\le \\delta_f(u, s)$. We'll prove the claim by induction on the number of push and relabel operations. Initially, we have $u.h = |V|$ if $u = s$ and $0$ otherwise. We have $s.h - |V| = 0 \\le \\delta_f(s, s) = 0$ and $u.h = 0 \\le \\delta_f(u, t)$ for all $u \\ne s$, so the claim holds prior to the first iteration of the while loop on line 2 of the $\\text{GENERIC-PUSH-RELABEL}$ algorithm. Suppose that the properties have been maintained thus far. If the next iteration is a nonsaturating push then the properties are maintained because the heights and existence of edges in the residual network are preserved. If it is a saturating push then edge $(u, v)$ is removed from the residual network, which increases both $\\delta_f(u, t)$ and $\\delta_f(u, s)$, so the properties are maintained regardless of the height of $u$. Now suppose that the next iteration causes a relabel of vertex $u$. For all $v$ such that $(u, v) \\in E_f$ we must have $u.h \\le v.h$. Let $v' = \\min\\{v.h \\mid (u,v) \\in E_f\\}$. There are two cases to consider. First, suppose that $v.h < |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + \\min_{(u, v)} \\in E_f \\delta_f(v, t) = \\delta_f(u, t).$$ Second, suppose that $v'.h \\ge |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + |V| + \\min_{(u, v)} \\in E_f \\delta_f(v, s) = \\delta_f(u, s) + |V|,$$ which implies that $u.h - |V| \\le \\delta_f(u, s)$. Therefore, the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the desired properties. 26.4-9 $\\star$ As in the previous exercise, let $\\delta_f(u, v)$ be the distance from $u$ to $v$ in the residual network $G_f$. Show how to modify the generic push-relabel algorithm to maintain the property that $u.h < |V|$ implies $u.h = \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| = \\delta_f(u, s)$. The total time that your implementation dedicates to maintaining this property should be $O(VE)$. What we should do is to, for successive backwards neighborhoods of $t$, relabel everything in that neighborhood. This will only take at most $O(VE)$ time (see 26.4-3). This also has the upshot of making it so that once we are done with it, every vertex's height is equal to the quantity $\\delta_f(u, t)$. Then, since we begin with equality, after doing this, the inductive step we had in the solution to the previous exercise shows that this equality is preserved. 26.4-10 Show that the number of nonsaturating pushes executed by the $\\text{GENERIC-PUSH-RELABEL}$ procedure on a flow network $G = (V, E)$ is at most $4|V|^2|E|$ for $|V| \\ge 4$. Each vertex has maximum height $2|V| - 1$. Since heights don't decrease, and there are $|V| - 2$ vertices which can be overflowing, the maximum contribution of relabels to $\\Phi$ over all vertices is $(2|V| - 1)(|V| - 2)$. A saturating push from $u$ to $v$ increases $\\Phi$ by at most $v.h \\le 2|V| - 1$, and there are at most $2|V||E|$ saturating pushes, so the total contribution over all saturating pushes to $\\Phi$ is at most $(2|V| - 1)(2|V||E|)$. Since each nonsaturating push decrements $\\Phi$ by at least on and $\\Phi$ must equal zero upon termination, we must have that the number of nonsaturating pushes is at most $$(2|V| - 1)(|V| - 2) + (2|V| - 1)(2|V||E|) = 4|V|^2|E| + 2|V|^2 - 5|V| + 3 - 2|V||E|.$$ Using the fact that $|E| \\ge |V| - 1$ and $|V| \\ge 4$ we can bound the number of saturating pushes by $4|V|^2|E|$.","title":"26.4 Push-relabel algorithms"},{"location":"Chap26/26.4/#264-1","text":"Prove that, after the procedure $\\text{INITIALIZE-PREFLOW}(G, S)$ terminates, we have $s.e \\le -|f^*|$, where $f^*$ is a maximum flow for $G$. (Removed)","title":"26.4-1"},{"location":"Chap26/26.4/#264-2","text":"Show how to implement the generic push-relabel algorithm using $O(V)$ time per relabel operation, $O(1)$ time per push, and $O(1)$ time to select an applicable operation, for a total time of $O(V^2E)$. We must select an appropriate data structure to store all the information which will allow us to select a valid operation in constant time. To do this, we will need to maintain a list of overflowing vertices. By Lemma 26.14, a push or a relabel operation always applies to an overflowing vertex. To determine which operation to perform, we need to determine whether $u.h = v.h + 1$ for some $v \\in N(u)$. We'll do this by maintaining a list $u.high$ of all neighbors of $u$ in $G_f$ which have height greater than or equal to $u$. We'll update these attributes in the $\\text{PUSH}$ and $\\text{RELABEL}$ functions. It is clear from the pseudocode given for $\\text{PUSH}$ that we can execute it in constant time, provided we have maintain the attributes $\\delta_f(u, v)$, $u.e$, $c_f(u, v)$, $(u, v).f$ and $u.h$. Each time we call $\\text{PUSH}(u, v)$ the result is that $u$ is no longer overflowing, so we must remove it from the list. Maintain a pointer $u.overflow$ to $u$'s position in the overflow list. If a vertex $u$ is not overflowing, set $u.overflow = \\text{NIL}$. Next, check if $v$ became overflowing. If so, set $v.overflow$ equal to the head of the overflow list. Since we can update the pointer in constant time and delete from a linked list given a pointer to the element to be deleted in constant time, we can maintain the list in $O(1)$. The $\\text{RELABEL}$ operation takes $O(V)$ because we need to compute the minimum $v.h$ from among all $(u, v) \\in E_f$, and there could be $|V| - 1$ many such $v$. We will also need to update $u.high$ during $\\text{RELABEL}$. When $\\text{RELABEL}(u)$ is called, set $u.high$ equal to the empty list and for each vertex $v$ which is adjacent to $u$, if $v.h = u.h + 1$, add $u$ to the list $v.high$. Since this takes constant time per adjacent vertex we can maintain the attribute in $O(V)$ per call to relabel.","title":"26.4-2"},{"location":"Chap26/26.4/#264-3","text":"Prove that the generic push-relabel algorithm spends a total of only $O(VE)$ time in performing all the $O(V^2)$ relabel operations. (Removed)","title":"26.4-3"},{"location":"Chap26/26.4/#264-4","text":"Suppose that we have found a maximum flow in a flow network $G = (V, E)$ using a push-relabel algorithm. Give a fast algorithm to find a minimum cut in $G$. (Removed)","title":"26.4-4"},{"location":"Chap26/26.4/#264-5","text":"Give an efficient push-relabel algorithm to find a maximum matching in a bipartite graph. Analyze your algorithm. First, construct the flow network for the bipartite graph as in the previous section. Then, we relabel everything in $L$. Then, we push from every vertex in $L$ to a vertex in $R$, so long as it is possible. Keeping track of those that vertices of $L$ that are still overflowing can be done by a simple bit vector. Then, we relabel everything in R and push to the last vertex. Once these operations have been done, The only possible valid operations are to relabel the vertices of $L$ that weren't able to find an edge that they could push their flow along, so could possibly have to get a push back from $R$ to $L$. This continues until there are no more operations to do. This takes time of $O(V(E + V))$.","title":"26.4-5"},{"location":"Chap26/26.4/#264-6","text":"Suppose that all edge capacities in a flow network $G = (V, E)$ are in the set $\\{1, 2, \\ldots, k\\}$. Analyze the running time of the generic push-relabel algorithm in terms of $|V|$, $|E|$, and $k$. ($\\textit{Hint:}$ How many times can each edge support a nonsaturating push before it becomes saturated?) The number of relabel operations and saturating pushes is the same as before. An edge can handle at most $k$ nonsaturating pushes before it becomes saturated, so the number of nonsaturating pushes is at most $2k|V||E|$. Thus, the total number of basic operations is at most $2|V|^2 + 2|V||E| + 2k|V||E| = O(kVE)$.","title":"26.4-6"},{"location":"Chap26/26.4/#264-7","text":"Show that we could change line 6 of $\\text{INITIALIZE-PREFLOW}$ to 6 s . h = | G . V | - 2 without affecting the correctness or asymptotic performance of the generic pushrelabel algorithm. (Removed)","title":"26.4-7"},{"location":"Chap26/26.4/#264-8","text":"Let $\\delta_f(u, v)$ be the distance (number of edges) from $u$ to $v$ in the residual network $G_f$. Show that the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the properties that $u.h < |V|$ implies $u.h \\le \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| \\le \\delta_f(u, s)$. We'll prove the claim by induction on the number of push and relabel operations. Initially, we have $u.h = |V|$ if $u = s$ and $0$ otherwise. We have $s.h - |V| = 0 \\le \\delta_f(s, s) = 0$ and $u.h = 0 \\le \\delta_f(u, t)$ for all $u \\ne s$, so the claim holds prior to the first iteration of the while loop on line 2 of the $\\text{GENERIC-PUSH-RELABEL}$ algorithm. Suppose that the properties have been maintained thus far. If the next iteration is a nonsaturating push then the properties are maintained because the heights and existence of edges in the residual network are preserved. If it is a saturating push then edge $(u, v)$ is removed from the residual network, which increases both $\\delta_f(u, t)$ and $\\delta_f(u, s)$, so the properties are maintained regardless of the height of $u$. Now suppose that the next iteration causes a relabel of vertex $u$. For all $v$ such that $(u, v) \\in E_f$ we must have $u.h \\le v.h$. Let $v' = \\min\\{v.h \\mid (u,v) \\in E_f\\}$. There are two cases to consider. First, suppose that $v.h < |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + \\min_{(u, v)} \\in E_f \\delta_f(v, t) = \\delta_f(u, t).$$ Second, suppose that $v'.h \\ge |V|$. Then after relabeling we have $$u.h = 1 + v'.h \\le 1 + |V| + \\min_{(u, v)} \\in E_f \\delta_f(v, s) = \\delta_f(u, s) + |V|,$$ which implies that $u.h - |V| \\le \\delta_f(u, s)$. Therefore, the $\\text{GENERIC-PUSH-RELABEL}$ procedure maintains the desired properties.","title":"26.4-8"},{"location":"Chap26/26.4/#264-9-star","text":"As in the previous exercise, let $\\delta_f(u, v)$ be the distance from $u$ to $v$ in the residual network $G_f$. Show how to modify the generic push-relabel algorithm to maintain the property that $u.h < |V|$ implies $u.h = \\delta_f(u, t)$ and that $u.h \\ge |V|$ implies $u.h - |V| = \\delta_f(u, s)$. The total time that your implementation dedicates to maintaining this property should be $O(VE)$. What we should do is to, for successive backwards neighborhoods of $t$, relabel everything in that neighborhood. This will only take at most $O(VE)$ time (see 26.4-3). This also has the upshot of making it so that once we are done with it, every vertex's height is equal to the quantity $\\delta_f(u, t)$. Then, since we begin with equality, after doing this, the inductive step we had in the solution to the previous exercise shows that this equality is preserved.","title":"26.4-9 $\\star$"},{"location":"Chap26/26.4/#264-10","text":"Show that the number of nonsaturating pushes executed by the $\\text{GENERIC-PUSH-RELABEL}$ procedure on a flow network $G = (V, E)$ is at most $4|V|^2|E|$ for $|V| \\ge 4$. Each vertex has maximum height $2|V| - 1$. Since heights don't decrease, and there are $|V| - 2$ vertices which can be overflowing, the maximum contribution of relabels to $\\Phi$ over all vertices is $(2|V| - 1)(|V| - 2)$. A saturating push from $u$ to $v$ increases $\\Phi$ by at most $v.h \\le 2|V| - 1$, and there are at most $2|V||E|$ saturating pushes, so the total contribution over all saturating pushes to $\\Phi$ is at most $(2|V| - 1)(2|V||E|)$. Since each nonsaturating push decrements $\\Phi$ by at least on and $\\Phi$ must equal zero upon termination, we must have that the number of nonsaturating pushes is at most $$(2|V| - 1)(|V| - 2) + (2|V| - 1)(2|V||E|) = 4|V|^2|E| + 2|V|^2 - 5|V| + 3 - 2|V||E|.$$ Using the fact that $|E| \\ge |V| - 1$ and $|V| \\ge 4$ we can bound the number of saturating pushes by $4|V|^2|E|$.","title":"26.4-10"},{"location":"Chap26/26.5/","text":"26.5-1 Illustrate the execution of $\\text{RELABEL-TO-FRONT}$ in the manner of Figure 26.10 for the flow network in Figure 26.1(a). Assume that the initial ordering of vertices in $L$ is $\\langle v_1, v_2, v_3, v_4 \\rangle$ and that the neighbor lists are $$ \\begin{aligned} v_1.N & = \\langle s, v_2, v_3 \\rangle, \\\\ v_2.N & = \\langle s, v_1, v_3, v_4 \\rangle, \\\\ v_3.N & = \\langle v_1, v_2, v_4, t \\rangle, \\\\ v_4.N & = \\langle v_2, v_3, t \\rangle. \\end{aligned} $$ When we initialize the preflow, we have $29$ units of flow leaving $s$. Then, we consider $v_1$ since it is the first element in the $L$ list. When we discharge it, we increase it's height to $1$ so that it can dump $12$ of it's excess along its edge to vertex $v_3$, to discharge the rest of it, it has to increase it's height to $|V| + 1$ to discharge it back to $s$. It was already at the front, so, we consider $v_2$. We increase its height to $1$. Then, we send all of its excess along its edge to $v_4$. We move it to the front, which means we next consider $v_1$, and do nothing because it is not overflowing. Up next is vertex $v_3$. After increasing its height to $1$, it can send all of its excess to $t$. This puts $v_3$ at the front, and we consider the non-overflowing vertices $v_2$ and $v_1$. Then, we consider $v_4$, it increases its height to $1$, then sends $4$ units to $t$. Since it still has an excess of $9$ units, it increases its height once again. Then it becomes valid for it to send flow back to $v_2$ or to $v_3$. It considers $v_2$ first because of the ordering of its neighbor list. This means that $9$ units of flow are pushed back to $v_2$. Since $v_4.h$ increased, it moves to the front of the list Then, we consider $v_2$ since it is the only still overflowing vertex. We increase its height to $3$. Then, it is overflowing by $9$ so it increases its height to $3$ to send $9$ units to $v_4$. It's height increased so it goes to the of the list. Then, we consider $v_4$, which is overflowing. It pushes $7$ units to $v_3$. Since it is still overflowing by $2$, it increases its height to $4$ and pushes the rest back to $v_2$ and goes to the front of the list. Up next is $v_2$, which increases its height by $2$ to sends its overflow to $v_4$. The excess flow keeps bobbing around the four vertices, each time requiring them to increase their height a bit to discharge to a neighbor only to have that neighbor increase to discharge it back until $v_2$ has increased in height enough to send all of it's excess back to s. Last but not least, $v_3$ pushes its overflow of $7$ units to $t$, and gives us a maximum flow of $23$. 26.5-2 $\\star$ We would like to implement a push-relabel algorithm in which we maintain a firstin, first-out queue of overflowing vertices. The algorithm repeatedly discharges the vertex at the head of the queue, and any vertices that were not overflowing before the discharge but are overflowing afterward are placed at the end of the queue. After the vertex at the head of the queue is discharged, it is removed. When the queue is empty, the algorithm terminates. Show how to implement this algorithm to compute a maximum flow in $O(V^3)$ time. Initially, the vertices adjacent to $s$ are the only ones which are overflowing. The implementation is as follows: PUSH - RELABEL - QUEUE ( G , s ) INITIALIZE - PREFLOW ( G , s ) let q be a new empty queue for v \u2208 G . Adj [ s ] PUSH ( q , v ) while q . head != NIL DISCHARGE ( q . head ) POP ( q ) Note that we need to modify the $\\text{DISCHARGE}$ algorithm to push vertices $v$ onto the queue if $v$ was not overflowing before a discharge but is overflowing after one. Between lines 7 and 8 of $\\text{DISCHARGE}(u)$, add the line \"if $v.e > 0$, $\\text{PUSH}(q, v)$.\" This is an implementation of the generic push-relabel algorithm, so we know it is correct. The analysis of runtime is almost identical to that of Theorem 26.30. We just need to verify that there are at most $|V|$ calls to $\\text{DISCHARGE}$ between two consecutive relabel operations. Observe that after calling $\\text{PUSH}(u, v)$, Corollary 26.28 tells us that no admissible edges are entering $v$. Thus, once $v$ is put into the queue because of the push, it can't be added again until it has been relabeled. Thus, at most $|V|$ vertices are added to the queue between relabel operations. 26.5-3 Show that the generic algorithm still works if $\\text{RELABEL}$ updates $u.h$ by simply computing $u.h = u.h + 1$. How would this change affect the analysis of $\\text{RELABEL-TO-FRONT}$? If we change relabel to just increment the value of $u$, we will not be ruining the correctness of the Algorithm. This is because since it only applies when $u.h \\le v.h$, we won't be every creating a graph where $h$ ceases to be a height function, since $u.h$ will only ever be increasing by exactly $1$ whenever relabel is called, ensuring that $u.h + 1 \\le v.h$. This means that Lemmatae 26.15 and 26.16 will still hold. Even Corollary 26.21 holds since all it counts on is that relabel causes some vertex's $h$ value to increase by at least $1$, it will still work when we have all of the operations causing it to increase by exactly $1$. However, Lemma 26.28 will no longer hold. That is, it may require more than a single relabel operation to cause an admissible edge to appear, if for example, $u.h$ was strictly less than the $h$ values of all its neighbors. However, this lemma is not used in the proof of Exercise 26.4-3, which bounds the number of relabel operations. Since the number of relabel operations still have the same bound, and we know that we can simulate the old relabel operation by doing (possibly many) of these new relabel operations, we have the same bound as in the original algorithm with this different relabel operation. 26.5-4 $\\star$ Show that if we always discharge a highest overflowing vertex, we can make the push-relabel method run in $O(V^3)$ time. We'll keep track of the heights of the overflowing vertices using an array and a series of doubly linked lists. In particular, let $A$ be an array of size $|V|$, and let $A[i]$ store a list of the elements of height $i$. Now we create another list $L$, which is a list of lists. The head points to the list containing the vertices of highest height. The next pointer of this list points to the next nonempty list stored in $A$, and so on. This allows for constant time insertion of a vertex into $A$, and also constant time access to an element of largest height, and because all lists are doubly linked, we can add and delete elements in constant time. Essentially, we are implementing the algorithm of Exercise 26.5-2, but with the queue replaced by a priority queue with constant time operations. As before, it will suffice to show that there are at most $|V|$ calls to discharge between consecutive relabel operations. Consider what happens when a vertex $v$ is put into the priority queue. There must exist a vertex $u$ for which we have called $\\text{PUSH}(u, v)$. After this, no ad- missible edge is entering $v$, so it can't be added to the priority queue again until after a relabel operation has occurred on $v$. Moreover, every call to $\\text{DISCHARGE}$ terminates with a $\\text{PUSH}$, so for every call to $\\text{DISCHARGE}$ there is another vertex which can't be added until a relabel operation occurs. After $|V|$ $\\text{DISCHARGE}$ operations and no relabel operations, there are no remaining valid $\\text{PUSH}$ operations, so either the algorithm terminates, or there is a valid relabel operation which is performed. Thus, there are $O(V^3)$ calls to $\\text{DISCHARGE}$. By carrying out the rest of the analysis of Theorem 26.30, we conclude that the runtime is $O(V^3)$. 26.5-5 Suppose that at some point in the execution of a push-relabel algorithm, there exists an integer $0 < k \\le |V| - 1$ for which no vertex has $v.h = k$. Show that all vertices with $v.h > k$ are on the source side of a minimum cut. If such a $k$ exists, the gap heuristic updates every vertex $v \\in V - \\{s\\}$ for which $v.h > k$, to set $v.h = \\max(v.h, |V| + 1)$. Show that the resulting attribute $h$ is a height function. (The gap heuristic is crucial in making implementations of the push-relabel method perform well in practice.) Suppose to try and obtain a contradiction that there were some minimum cut for which a vertex that had $v.h > k$ were on the sink side of that cut. For that minimum cut, there is a residual flow network for which that cut is saturated. Then, if there were any vertices that were also on the sink side of the cut which had an edge going to $v$ in this residual flow network, since it's $h$ value cannot be equal to $k$, we know that it must be greater than $k$ since it could be only at most one less than $v$. We can continue in this way to let $S$ be the set of vertices on the sink side of the graph which have an $h$ value greater than $k$. Suppose that there were some simple path from a vertex in $S$ to $s$. Then, at each of these steps, the height could only decrease by at most $1$, since it cannot get from above $k$ to $0$ without going through $k$, we know that there is no path in the residual flow network going from a vertex in $S$ to $s$. Since a minimal cut corresponds to disconnected parts of the residual graph for a maximum flow, and we know there is no path from $S$ to $s$, there is a minimum cut for which $S$ lies entirely on the source side of the cut. This was a contradiction to how we selected $v$, and so have shown the first claim. Now we show that after updating the $h$ values as suggested, we are still left with a height function. Suppose we had an edge $(u, v)$ in the residual graph. We knew from before that $u.h \\le v.h + 1$. However, this means that if $u.h > k$, so must be $v.h$. So, if both were above $k$, we would be making them equal, causing the inequality to still hold. Also, if just $v.k$ were above $k$, then we have not decreased it's $h$ value, meaning that the inequality also still must hold. Since we have not changed the value of $s.h$, and $t.h$, we have all the required properties to have a height function after modifying the $h$ values as described.","title":"26.5 The relabel-to-front algorithm"},{"location":"Chap26/26.5/#265-1","text":"Illustrate the execution of $\\text{RELABEL-TO-FRONT}$ in the manner of Figure 26.10 for the flow network in Figure 26.1(a). Assume that the initial ordering of vertices in $L$ is $\\langle v_1, v_2, v_3, v_4 \\rangle$ and that the neighbor lists are $$ \\begin{aligned} v_1.N & = \\langle s, v_2, v_3 \\rangle, \\\\ v_2.N & = \\langle s, v_1, v_3, v_4 \\rangle, \\\\ v_3.N & = \\langle v_1, v_2, v_4, t \\rangle, \\\\ v_4.N & = \\langle v_2, v_3, t \\rangle. \\end{aligned} $$ When we initialize the preflow, we have $29$ units of flow leaving $s$. Then, we consider $v_1$ since it is the first element in the $L$ list. When we discharge it, we increase it's height to $1$ so that it can dump $12$ of it's excess along its edge to vertex $v_3$, to discharge the rest of it, it has to increase it's height to $|V| + 1$ to discharge it back to $s$. It was already at the front, so, we consider $v_2$. We increase its height to $1$. Then, we send all of its excess along its edge to $v_4$. We move it to the front, which means we next consider $v_1$, and do nothing because it is not overflowing. Up next is vertex $v_3$. After increasing its height to $1$, it can send all of its excess to $t$. This puts $v_3$ at the front, and we consider the non-overflowing vertices $v_2$ and $v_1$. Then, we consider $v_4$, it increases its height to $1$, then sends $4$ units to $t$. Since it still has an excess of $9$ units, it increases its height once again. Then it becomes valid for it to send flow back to $v_2$ or to $v_3$. It considers $v_2$ first because of the ordering of its neighbor list. This means that $9$ units of flow are pushed back to $v_2$. Since $v_4.h$ increased, it moves to the front of the list Then, we consider $v_2$ since it is the only still overflowing vertex. We increase its height to $3$. Then, it is overflowing by $9$ so it increases its height to $3$ to send $9$ units to $v_4$. It's height increased so it goes to the of the list. Then, we consider $v_4$, which is overflowing. It pushes $7$ units to $v_3$. Since it is still overflowing by $2$, it increases its height to $4$ and pushes the rest back to $v_2$ and goes to the front of the list. Up next is $v_2$, which increases its height by $2$ to sends its overflow to $v_4$. The excess flow keeps bobbing around the four vertices, each time requiring them to increase their height a bit to discharge to a neighbor only to have that neighbor increase to discharge it back until $v_2$ has increased in height enough to send all of it's excess back to s. Last but not least, $v_3$ pushes its overflow of $7$ units to $t$, and gives us a maximum flow of $23$.","title":"26.5-1"},{"location":"Chap26/26.5/#265-2-star","text":"We would like to implement a push-relabel algorithm in which we maintain a firstin, first-out queue of overflowing vertices. The algorithm repeatedly discharges the vertex at the head of the queue, and any vertices that were not overflowing before the discharge but are overflowing afterward are placed at the end of the queue. After the vertex at the head of the queue is discharged, it is removed. When the queue is empty, the algorithm terminates. Show how to implement this algorithm to compute a maximum flow in $O(V^3)$ time. Initially, the vertices adjacent to $s$ are the only ones which are overflowing. The implementation is as follows: PUSH - RELABEL - QUEUE ( G , s ) INITIALIZE - PREFLOW ( G , s ) let q be a new empty queue for v \u2208 G . Adj [ s ] PUSH ( q , v ) while q . head != NIL DISCHARGE ( q . head ) POP ( q ) Note that we need to modify the $\\text{DISCHARGE}$ algorithm to push vertices $v$ onto the queue if $v$ was not overflowing before a discharge but is overflowing after one. Between lines 7 and 8 of $\\text{DISCHARGE}(u)$, add the line \"if $v.e > 0$, $\\text{PUSH}(q, v)$.\" This is an implementation of the generic push-relabel algorithm, so we know it is correct. The analysis of runtime is almost identical to that of Theorem 26.30. We just need to verify that there are at most $|V|$ calls to $\\text{DISCHARGE}$ between two consecutive relabel operations. Observe that after calling $\\text{PUSH}(u, v)$, Corollary 26.28 tells us that no admissible edges are entering $v$. Thus, once $v$ is put into the queue because of the push, it can't be added again until it has been relabeled. Thus, at most $|V|$ vertices are added to the queue between relabel operations.","title":"26.5-2 $\\star$"},{"location":"Chap26/26.5/#265-3","text":"Show that the generic algorithm still works if $\\text{RELABEL}$ updates $u.h$ by simply computing $u.h = u.h + 1$. How would this change affect the analysis of $\\text{RELABEL-TO-FRONT}$? If we change relabel to just increment the value of $u$, we will not be ruining the correctness of the Algorithm. This is because since it only applies when $u.h \\le v.h$, we won't be every creating a graph where $h$ ceases to be a height function, since $u.h$ will only ever be increasing by exactly $1$ whenever relabel is called, ensuring that $u.h + 1 \\le v.h$. This means that Lemmatae 26.15 and 26.16 will still hold. Even Corollary 26.21 holds since all it counts on is that relabel causes some vertex's $h$ value to increase by at least $1$, it will still work when we have all of the operations causing it to increase by exactly $1$. However, Lemma 26.28 will no longer hold. That is, it may require more than a single relabel operation to cause an admissible edge to appear, if for example, $u.h$ was strictly less than the $h$ values of all its neighbors. However, this lemma is not used in the proof of Exercise 26.4-3, which bounds the number of relabel operations. Since the number of relabel operations still have the same bound, and we know that we can simulate the old relabel operation by doing (possibly many) of these new relabel operations, we have the same bound as in the original algorithm with this different relabel operation.","title":"26.5-3"},{"location":"Chap26/26.5/#265-4-star","text":"Show that if we always discharge a highest overflowing vertex, we can make the push-relabel method run in $O(V^3)$ time. We'll keep track of the heights of the overflowing vertices using an array and a series of doubly linked lists. In particular, let $A$ be an array of size $|V|$, and let $A[i]$ store a list of the elements of height $i$. Now we create another list $L$, which is a list of lists. The head points to the list containing the vertices of highest height. The next pointer of this list points to the next nonempty list stored in $A$, and so on. This allows for constant time insertion of a vertex into $A$, and also constant time access to an element of largest height, and because all lists are doubly linked, we can add and delete elements in constant time. Essentially, we are implementing the algorithm of Exercise 26.5-2, but with the queue replaced by a priority queue with constant time operations. As before, it will suffice to show that there are at most $|V|$ calls to discharge between consecutive relabel operations. Consider what happens when a vertex $v$ is put into the priority queue. There must exist a vertex $u$ for which we have called $\\text{PUSH}(u, v)$. After this, no ad- missible edge is entering $v$, so it can't be added to the priority queue again until after a relabel operation has occurred on $v$. Moreover, every call to $\\text{DISCHARGE}$ terminates with a $\\text{PUSH}$, so for every call to $\\text{DISCHARGE}$ there is another vertex which can't be added until a relabel operation occurs. After $|V|$ $\\text{DISCHARGE}$ operations and no relabel operations, there are no remaining valid $\\text{PUSH}$ operations, so either the algorithm terminates, or there is a valid relabel operation which is performed. Thus, there are $O(V^3)$ calls to $\\text{DISCHARGE}$. By carrying out the rest of the analysis of Theorem 26.30, we conclude that the runtime is $O(V^3)$.","title":"26.5-4 $\\star$"},{"location":"Chap26/26.5/#265-5","text":"Suppose that at some point in the execution of a push-relabel algorithm, there exists an integer $0 < k \\le |V| - 1$ for which no vertex has $v.h = k$. Show that all vertices with $v.h > k$ are on the source side of a minimum cut. If such a $k$ exists, the gap heuristic updates every vertex $v \\in V - \\{s\\}$ for which $v.h > k$, to set $v.h = \\max(v.h, |V| + 1)$. Show that the resulting attribute $h$ is a height function. (The gap heuristic is crucial in making implementations of the push-relabel method perform well in practice.) Suppose to try and obtain a contradiction that there were some minimum cut for which a vertex that had $v.h > k$ were on the sink side of that cut. For that minimum cut, there is a residual flow network for which that cut is saturated. Then, if there were any vertices that were also on the sink side of the cut which had an edge going to $v$ in this residual flow network, since it's $h$ value cannot be equal to $k$, we know that it must be greater than $k$ since it could be only at most one less than $v$. We can continue in this way to let $S$ be the set of vertices on the sink side of the graph which have an $h$ value greater than $k$. Suppose that there were some simple path from a vertex in $S$ to $s$. Then, at each of these steps, the height could only decrease by at most $1$, since it cannot get from above $k$ to $0$ without going through $k$, we know that there is no path in the residual flow network going from a vertex in $S$ to $s$. Since a minimal cut corresponds to disconnected parts of the residual graph for a maximum flow, and we know there is no path from $S$ to $s$, there is a minimum cut for which $S$ lies entirely on the source side of the cut. This was a contradiction to how we selected $v$, and so have shown the first claim. Now we show that after updating the $h$ values as suggested, we are still left with a height function. Suppose we had an edge $(u, v)$ in the residual graph. We knew from before that $u.h \\le v.h + 1$. However, this means that if $u.h > k$, so must be $v.h$. So, if both were above $k$, we would be making them equal, causing the inequality to still hold. Also, if just $v.k$ were above $k$, then we have not decreased it's $h$ value, meaning that the inequality also still must hold. Since we have not changed the value of $s.h$, and $t.h$, we have all the required properties to have a height function after modifying the $h$ values as described.","title":"26.5-5"},{"location":"Chap26/Problems/26-1/","text":"A$n \\times n$ grid is an undirected graph consisting of $n$ rows and $n$ columns of vertices, as shown in Figure 26.11. We denote the vertex in the $i$th row and the $j$th column by $(i, j)$. All vertices in a grid have exactly four neighbors, except for the boundary vertices, which are the points $(i, j)$ for which $i = 1$, $i = n$, $j = 1$, or $j = n$. Given $m \\le n^2$ starting points $(x_1, y_1), (x_2, y_2), \\ldots, (x_m, y_m)$ in the grid, the escape problem is to determine whether or not there are $m$ vertex-disjoint paths from the starting points to any $m$ different points on the boundary. For example, the grid in Figure 26.11(a) has an escape, but the grid in Figure 26.11(b) does not. a. Consider a flow network in which vertices, as well as edges, have capacities. That is, the total positive flow entering any given vertex is subject to a capacity constraint. Show that determining the maximum flow in a network with edge and vertex capacities can be reduced to an ordinary maximum-flow problem on a flow network of comparable size. b. Describe an efficient algorithm to solve the escape problem, and analyze its running time. a. This problem is identical to exercise 26.1-7. b. Construct a vertex constrained flow network from the instance of the escape problem by letting our flow network have a vertex (each with unit capacity) for each intersection of grid lines, and have a bidirectional edge with unit capacity for each pair of vertices that are adjacent in the grid. Then, we will put a unit capacity edge going from $s$ to each of the distinguished vertices, and a unit capacity edge going from each vertex on the sides of the grid to $t$. Then, we know that a solution to this problem will correspond to a solution to the escape problem because all of the augmenting paths will be a unit flow, because every edge has unit capacity. This means that the flows through the grid will be the paths taken. This gets us the escaping paths if the total flow is equal to $m$ (we know it cannot be greater than $m$ by looking at the cut which has $s$ by itself). And, if the max flow is less than $m$, we know that the escape problem is not solvable, because otherwise we could construct a flow with value $m$ from the list of disjoint paths that the people escaped along.","title":"26-1 Escape problem"},{"location":"Chap26/Problems/26-2/","text":"A path cover of a directed graph $G = (V, E)$ is a set $P$ of vertex-disjoint paths such that every vertex in $V$ is included in exactly one path in $P$. Paths may start and end anywhere, and they may be of any length, including $0$. A minimum path cover of $G$ is a path cover containing the fewest possible paths. a. Give an efficient algorithm to find a minimum path cover of a directed acyclic graph $G = (V, E)$. ($\\textit{Hint:}$ Assuming that $V = \\{1, 2, \\ldots, n\\}$, construct the graph $G' = (V', E')$, where $$ \\begin{aligned} V' & = \\{x_0, x_1, \\ldots, x_n\\} \\cup \\{y_0, y_1, \\ldots, y_n\\}, \\\\ E' & = \\{(x_0, x_i): i \\in V\\} \\cup \\{(y_i, y_0): i \\in V\\} \\cup \\{(x_i, y_j): (i, j) \\in E\\}, \\end{aligned} $$ and run a maximum-flow algorithm.) b. Does your algorithm work for directed graphs that contain cycles? Explain. a. Set up the graph $G'$ as defined in the problem, give each edge capacity $1$, and run a maximum-flow algorithm. I claim that if $(x_i, y_j)$ has flow $1$ in the maximum flow and we set $(i, j)$ to be an edge in our path cover, then the result is a minimum path cover. First observe that no vertex appears twice in the same path. If it did, then we would have $f(x_i, y_j) = f(x_k, y_j)$ for some $i \\ne k \\ne j$. However, this contradicts the conservation of flow, since the capacity leaving $y_j$ is only $1$. Moreover, since the capacity from $s$ to $x_i$ is $1$, we can never have two edges of the form $(x_i, y_j)$ and $(x_i, y_k)$ for $k \\ne j$. We can ensure every vertex is included in some path by asserting that if there is no edge $(x_i, y_j)$ or $(x_j, y_i)$ for some $j$, then $j$ will be on a path by itself. Thus, we are guaranteed to obtain a path cover. If there are $k$ paths in a cover of $n$ vertices, then they will consist of $n \u2212 k$ edges in total. Given a path cover, we can recover a flow by assigning edge $(x_i, y_j)$ flow $1$ if and only if $(i, j)$ is an edge in one of the paths in the cover. Suppose that the maximum flow algorithm yields a cover with $k$ paths, and hence flow $n \u2212 k$, but a minimum path cover uses strictly fewer than $k$ paths. Then it must use strictly more than $n \u2212 k$ edges, so we can recover a flow which is larger than the one previously found, contradicting the fact that the previous flow was maximal. Thus, we find a minimum path cover. Since the maximum flow in the graph corresponds to finding a maximum matching in the bipartite graph obtained by considering the induced subgraph of $G'$ on $\\{1, 2, \\dots, n\\}$, section 26.3 tells us that we can find a maximum flow in $O(V E)$. b. This doesn't work for directed graphs which contain cycles. To see this, consider the graph on $\\{1, 2, 3, 4\\}$ which contains edges $(1, 2)$, $(2, 3)$, $(3, 1)$, and $(4, 3)$. The desired output would be a single path $4$, $3$, $1$, $2$ but flow which assigns edges $(x_1, y_2)$, $(x_2, y_3)$, and $(x_3, y_1)$ flow $1$ is maximal.","title":"26-2 Minimum path cover"},{"location":"Chap26/Problems/26-3/","text":"Professor Gore wants to open up an algorithmic consulting company. He has identified n important subareas of algorithms (roughly corresponding to different portions of this textbook), which he represents by the set $A = \\{A_1, A_2, \\ldots, A_n\\}$. In each subarea $A_k$, he can hire an expert in that area for $c_k$ dollars. The consulting company has lined up a set $J = \\{J_1, J_2, \\ldots, J_m\\}$ of potential jobs. In order to perform job $J_i$, the company needs to have hired experts in a subset $R_i \\subseteq A$ of subareas. Each expert can work on multiple jobs simultaneously. If the company chooses to accept job $J_i$, it must have hired experts in all subareas in $R_i$, and it will take in revenue of $p_i$ dollars. Professor Gore's job is to determine which subareas to hire experts in and which jobs to accept in order to maximize the net revenue, which is the total income from jobs accepted minus the total cost of employing the experts. Consider the following flow network $G$. It contains a source vertex $s$, vertices $A_1, A_2, \\ldots, A_n$, vertices $J_1, J_2, \\ldots, J_m$, and a sink vertex $t$. For $k = 1, 2, \\ldots, n$, the flow network contains an edge $(s, A_k)$ with capacity $c(s, A_k) = c_k$, and for $i = 1, 2, \\ldots, m$, the flow network contains an edge $(J_i, t)$ with capacity $c(J_i, t) = p_i$. For $k = 1, 2, \\ldots, n$ and $i = 1, 2, \\ldots, m$, if $A_k \\in R_i$, then $G$ contains an edge $(A_k, J_i)$ with capacity $c(A_k, J_i) = \\infty$. a. Show that if $J_i \\in T$ for a finite-capacity cut $(S, T)$ of $G$, then $A_k \\in T$ for each $A_k \\in R_i$. b. Show how to determine the maximum net revenue from the capacity of a minimum cut of $G$ and the given $p_i$ values. c. Give an efficient algorithm to determine which jobs to accept and which experts to hire. Analyze the running time of your algorithm in terms of $m$, $n$, and $r = \\sum_{i = 1}^m |R_i|$. a. Suppose to a contradiction that there were some $J_i \\in T$, and some $A_k \\in R_i$ so that $A_k \\notin T$. However, by the definition of the flow network, there is an edge of infinite capacity going from $A_k$ to $J_i$ because $A_k \\in R_i$. This means that there is an edge of infinite capacity that is going across the given cut. This means that the capacity of the cut is infinite, a contradiction to the given fact that the cut was finite capacity. b. Though tempting, it doesn't suffice to just look at the experts that are on the $s$ side of the cut. To see why this doesn't work, imagine there's one specialized skill areal, such as \"Computer power switch operator\", that is required for every job. Then, any finite cut that would include any job getting done would requiring that this expert be hired. However, since there is an infinite capacity edge coming from him to every other job, then all of the experts need for all the other jobs would also need to be hired. So, if we have this obiquitously required employee, any minimum cut would have to be all or nothing, but it is trivial to find a counterexample to this being optimal. In order for this problem to be solvable, one must assume that for every expert you've hired, you do all of the jobs that he is required for. If this is the case, then let $S_k \\subseteq [n]$ be the indices of the experts that lie on the source side of the cut, and let $S_i \\subseteq [m]$ be the indices of jobs that lie on the source side of the cut, then the net revenue is just $$\\sum_{S_i} p_i \u2212 \\sum_{S_k} c_k$$ To see this is minimum, transferring over some set of experts and tasks from the sink side to the source side causes the capacity to go down by the cost of those experts and go up by the revenue of those jobs. If the cut was minimal than this must be a positive change, so the revenue isn't enough to justify the hire, meaning that those jobs that were on the source side in the minimal cut are exactly the jobs to attempt. c. Again, to get a solution, we must make the assumption that for every expert that is hired, all jobs that that expert is required for must be completed. Basically just run either the $O(V^3)$ relabel-to-front algorithm described in section 26.5 on the flow network, and hire the experts that are on the source side of the cut. By the previous part, we know that this gets us the best outcome. The number of edges in the flow network is $m + n + r$, and the number of vertices is $2 + m + n$, so the runtime is just $O((2 + m + n)^3)$, so it's cubic in $\\max(m, n)$. There is no dependence on $R$ using this algorithm, but this is reasonable since we have the inherent bound that $r < mn$, which is a lower order term. Without this unstated assumption, I suspect that there isn't an efficient solution possible, but cannot think of what NP-complete problem you would use for the reduction.","title":"26-3 Algorithmic consulting"},{"location":"Chap26/Problems/26-4/","text":"Let $G = (V, E)$ be a flow network with source $s$, sink $t$, and integer capacities. Suppose that we are given a maximum flow in $G$. a. Suppose that we increase the capacity of a single edge $(u, v) \\in E$ by $1$. Give an $O(V + E)$-time algorithm to update the maximum flow. b. Suppose that we decrease the capacity of a single edge $(u, v) \\in E$ by $1$. Give an $O(V + E)$-time algorithm to update the maximum flow. a. If there exists a minimum cut on which $(u, v)$ doesn't lie then the maximum flow can't be increased, so there will exist no augmenting path in the residual network. Otherwise it does cross a minimum cut, and we can possibly increase the flow by $1$. Perform one iteration of Ford-Fulkerson. If there exists an augmenting path, it will be found and increased on this iteration. Since the edge capacities are integers, the flow values are all integral. Since flow strictly increases, and by an integral amount each time, a single iteration of the while loop of line 3 of Ford-Fulkerson will increase the flow by $1$, which we know to be maximal. To find an augmenting path we use a BFS, which runs in $O(V + E') = O(V + E)$. b. If the edge's flow was already at least $1$ below capacity then nothing changes. Otherwise, find a path from $s$ to $t$ which contains $(u, v)$ using BFS in $O(V + E)$. Decrease the flow of every edge on that path by $1$. This decreases total flow by $1$. Then run one iteration of the while loop of Ford-Fulkerson in $O(V + E)$. By the argument given in part a, everything is integer valued and flow strictly increases, so we will either find no augmenting path, or will increase the flow by $1$ and then terminate. \"","title":"26-4 Updating maximum flow"},{"location":"Chap26/Problems/26-5/","text":"Let $G = (V, E)$ be a flow network with source $s$, sink $t$, and an integer capacity $c(u, v)$ on each edge $(u, v) \\in E$. Let $C = \\max_{(u, v) \\in E} c(u, v)$. a. Argue that a minimum cut of $G$ has capacity at most $C|E|$. b. For a given number $K$, show how to find an augmenting path of capacity at least $K$ in $O(E)$ time, if such a path exists. We can use the following modification of $\\text{FORD-FULKERSON-METHOD}$ to compute a maximum flow in $G$: MAX - FLOW - BY - SCALING ( G , s , t ) C = max_ {( u , v ) \u2208 E } c ( u , v ) initialize flow f to 0 K = 2 ^ { floor ( lg C )} while K \u2265 1 while there exists an augmenting path p of capacity at least K augment flow f along p K = K / 2 return f c. Argue that $\\text{MAX-FLOW-BY-SCALING}$ returns a maximum flow. d. Show that the capacity of a minimum cut of the residual network $G_f$ is at most $2K|E|$ each time line 4 is executed. e. Argue that the inner while loop of lines 5\u20136 executes $O(E)$ times for each value of $K$. f. Conclude that $\\text{MAX-FLOW-BY-SCALING}$ can be implemented so that it runs in $O(E^2\\lg C)$ time. a. Since the capacity of a cut is the sum of the capacity of the edges going from a vertex on one side to a vertex on the other, it is less than or equal to the sum of the capacities of all of the edges. Since each of the edges has a capacity that is $\\le C$, if we were to replace the capacity of each edge with $C$, we would only be potentially increasing the sum of the capacities of all the edges. After so changing the capacities of the edges, the sum of the capacities of all the edges is equal to $C|E|$, potentially an overestimate of the original capacity of any cut, and so of the minimum cut. b. Since the capacity of a path is equal to the minimum of the capacities of each of the edges along that path, we know that any edges in the residual network that have a capacity less than $K$ cannot be used in such an augmenting path. Similarly, so long as all the edges have a capacity of at least $K$, then the capacity of the augmenting path, if it is found, will be of capacity at least $K$. This means that all that needs be done is remove from the residual network those edges whose capacity is less than $K$ and then run BFS. c. Since $K$ starts out as a power of $2$, and through each iteration of the while loop on line 4, it decreases by a factor of two until it is less than $1$. There will be some iteration of that loop when $K = 1$. During this iteration, we will be using any augmenting paths of capacity at least $1$ when running the loop on line 5. Since the original capacities are all integers, the augmenting paths at each step will be integers, which means that no augmenting path will have a capacity of less than $1$. So, once the algorithm terminates, there will be no more augmenting paths since there will be no more augmenting paths of capacity at least $1$. d. Each time line 4 is executed we know that there is no augmenting path of capacity at least $2K$. To see this fact on the initial time that line 4 is executed we just note that $2K = 2 \\cdot 2^{\\lfloor \\lg C \\rfloor} > 2 \\cdot 2^{\\lg C \u2212 1} = 2^{\\lg C} = C$. Then, since an augmenting path is limited by the capacity of the smallest edge it contains, and all the edges have a capacity at most $C$, no augmenting path will have a capacity greater than that. On subsequent times executing line 4, the loop of line 5 during the previous execution of the outer loop will of already used up and capacious augmenting paths, and would only end once there are no more. Since any augmenting path must have a capacity of less than $2K$, we can look at each augmenting path $p$, and assign to it an edge $e_p$ which is any edge whose capacity is tied for smallest among all the edges along the path. Then, removing all of the edges $e_p$ would disconnect the residual network since every possible augmenting path goes through one of those edge. We know that there are at most $|E|$ of them since they are a subset of the edges. We also know that each of them has capacity at most $2K$ since that was the value of the augmenting path they were selected to be tied for cheapest in. So, the total cost of this cut is $2K|E|$. e. Each time that the inner while loop runs, we know that it adds an amount of flow that is at least $K$, since that\u2019s the value of the augmenting path. We also know that before we start that while loop, there is a cut of cost $\\le 2K|E|$. This means that the most flow we could possibly add is $2K|E|$. Combining these two facts, we get that the most cuts possible is $\\frac{2K|E|}{K} = 2|E| \\in O(|E|)$. f. We only execute the outermost for loop $\\lg C$ many times since $\\lg(2^{\\lfloor \\lg C \\rfloor}) \\le \\lg C$. The inner while loop only runs $O(|E|)$ many times by the previous part. Finally, every time the inner for loop runs, the operation it does can be done in time $O(|E|)$ by part (b). Putting it all together, the runtime is $O(|E|^2\\lg C)$.","title":"26-5 Maximum flow by scaling"},{"location":"Chap26/Problems/26-6/","text":"In this problem, we describe a faster algorithm, due to Hopcroft and Karp, for $p$ finding a maximum matching in a bipartite graph. The algorithm runs in $O(\\sqrt V E)$ time. Given an undirected, bipartite graph $G = (V, E)$, where $V = L \\cup R$ and all edges have exactly one endpoint in $L$, let $M$ be a matching in $G$. We say that a simple path $P$ in $G$ is an augmenting path with respect to $M$ if it starts at an unmatched vertex in $L$, ends at an unmatched vertex in $R$, and its edges belong alternately to $M$ and $E - M$. (This definition of an augmenting path is related to, but different from, an augmenting path in a flow network.) In this problem, we treat a path as a sequence of edges, rather than as a sequence of vertices. A shortest augmenting path with respect to a matching $M$ is an augmenting path with a minimum number of edges. Given two sets $A$ and $B$, the symmetric difference $A \\oplus B$ is defined as $(A - B) \\cup (B - A)$, that is, the elements that are in exactly one of the two sets. a. Show that if $M$ is a matching and $P$ is an augmenting path with respect to $M$, then the symmetric difference $M \\oplus P$ is a matching and $|M \\oplus P| = |M| + 1$. Show that if $P_1, P_2, \\ldots, P_k$ are vertex-disjoint augmenting paths with respect to $M$, then the symmetric difference $M \\oplus (P_1 \\cup P_2 \\cup \\cdots \\cup P_k)$ is a matching with cardinality $|M| + k$. The general structure of our algorithm is the following: HOPCROPFT - KARP ( G ) M = \u00d8 repeat let P = { P [ 1 ], P [ 2 ], ..., P [ k ]} be a maximal set of vertex - disjoint shortest augmenting paths with respect to M M = M \u2a01 ( P [ 1 ] \u222a P [ 2 ] \u222a ... \u222a P [ k ]) until P == \u00d8 return M The remainder of this problem asks you to analyze the number of iterations in the algorithm (that is, the number of iterations in the repeat loop) and to describe an implementation of line 3. b. Given two matchings $M$ and $M^*$ in $G$, show that every vertex in the graph $G' = (V, M \\oplus M^*)$ has degree at most $2$. Conclude that $G'$ is a disjoint union of simple paths or cycles. Argue that edges in each such simple path or cycle belong alternately to $M$ or $M^*$. Prove that if $|M| \\le |M^*|$, then $M \\oplus M^*$ contains at least $|M^*| - |M|$ vertex-disjoint augmenting paths with respect to $M$. Let $l$ be the length of a shortest augmenting path with respect to a matching $M$, and let $P_1, P_2, \\ldots, P_k$ be a maximal set of vertex-disjoint augmenting paths of length $l$ with respect to $M$. Let $M' = M \\oplus (P_1 \\cup \\cdots \\cup P_k)$, and suppose that $P$ is a shortest augmenting path with respect to $M'$. c. Show that if $P$ is vertex-disjoint from $P_1, P_2, \\ldots, P_k$ , then $P$ has more than $l$ edges. d. Now suppose that $P$ is not vertex-disjoint from $P_1, P_2, \\ldots, P_k$ . Let $A$ be the set of edges $(M \\oplus M') \\oplus P$. Show that $A = (P_1 \\cup P_2 \\cup \\cdots \\cup P_k) \\oplus P$ and that $|A| \\ge (k + 1)l$. Conclude that $P$ has more than $l$ edges. e. Prove that if a shortest augmenting path with respect to $M$ has $l$ edges, the size of the maximum matching is at most $|M| + |V| / (l + 1)$. f. Show that the number of repeat loop iterations in the algorithm is at most $2\\sqrt{|V|}$. ($\\textit{Hint:}$ By how much can $M$ grow after iteration number $\\sqrt{|V|}$?) g. Give an algorithm that runs in $O(E)$ time to find a maximal set of vertexdisjoint shortest augmenting paths $P_1, P_2, \\ldots, P_k$ for a given matching $M$. Conclude that the total running time of $\\text{HOPCROFT-KARP}$ is $O(\\sqrt V E)$. a. Suppose $M$ is a matching and $P$ is an augmenting path with respect to $M$. Then $P$ consists of $k$ edges in $M$, and $k + 1$ edges not in $M$. This is because the first edge of $P$ touches an unmatched vertex in $L$, so it cannot be in $M$. Similarly, the last edge in $P$ touches an unmatched vertex in $R$, so the last edge cannot be in $M$. Since the edges alternate being in or not in $M$, there must be exactly one more edge not in $M$ than in $M$. This implies that $$|M \\oplus P| = |M| + |P| - 2k = |M| + 2k + 1 - 2k = |M| + 1,$$ since we must remove each edge of $M$ which is in $P$ from both $M$ and $P$. Now suppose $P_1, P_2, \\ldots, P_k$ are vertex-disjoint augmenting paths with respect to $M$. Let $k_i$ be the number of edges in $P_i$ which are in $M$, so that $|P_i| = 2k + i + 1$. Then we have $$M \\oplus (P_1 \\cup P_2 \\cup \\cdots \\cup P_k) = |M| + |P_1| + \\cdots + |P_k| - 2k_1 - 2k_2 - \\cdots - 2k_k = |M| + k.$$ To see that we in fact get a matching, suppose that there was some vertex $v$ which had at least $2$ incident edges $e$ and $e'$. They cannot both come from $M$, since $M$ is a matching. They cannot both come from $P$ since $P$ is simple and every other edge of $P$ is removed. Thus, $e \\in M$ and $e' \\in P \\backslash M$. However, if $e \\in M$ then $e \\in P$, so $e \\notin M \\oplus P$, a contradiction. A similar argument gives the case of $M \\oplus (P_1 \\cup \\cdots \\cup P_k)$. b. Suppose some vertex in $G'$ has degree at least $3$. Since the edges of $G'$ come from $M \\oplus M^*$, at least $2$ of these edges come from the same matching. However, a matching never contains two edges with the same endpoint, so this is impossible. Thus every vertex has degree at most $2$, so $G'$ is a disjoint union of simple paths and cycles. If edge $(u, v)$ is followed by edge $(z, w)$ in a simple path or cycle then we must have $v = z$. Since two edges with the same endpoint cannot appear in a matching, they must belong alternately to $M$ and $M^*$. Since edges alternate, every cycle has the same number of edges in each matching and every path has at most one more edge in one matching than in the other. Thus, if $|M| \\le |M^*|$ there must be at least $|M^*| - |M|$ vertex-disjoint augmenting paths with respect to $M$. c. Every vertex matched by $M$ must be incident with some edge in $M'$. Since $P$ is augmenting with respect to $M$\u2032, the left endpoint of the first edge of $P$ isn't incident to a vertex touched by an edge in $M'$. In particular, $P$ starts at a vertex in $L$ which is unmatched by $M$ since every vertex of $M$ is incident with an edge in $M'$. Since $P$ is vertex disjoint from $P_1, P_2, \\ldots, P_k$, any edge of $P$ which is in $M'$ must in fact be in $M$ and any edge of $P$ which is not in $M'$ cannot be in $M$. Since $P$ has edges alternately in $M'$ and $E - M'$, $P$ must in fact have edges alternately in $M$ and $E - M$. Finally, the last edge of $P$ must be incident to a vertex in $R$ which is unmatched by $M'$. Any vertex unmatched by $M'$ is also unmatched by $M$, so $P$ is an augmenting path for $M$. $P$ must have length at least $l$ since $l$ is the length of the shortest augmenting path with respect to $M$. If $P$ had length exactly $l$, then this would contradict the fact that $P_1 \\cup \\cdots \\cup P_k$ is a maximal set of vertex disjoint paths of length $l$ because we could add $P$ to the set. Thus $P$ has more than $l$ edges. d. Any edge in $M \\oplus M'$ is in exactly one of $M$ or $M'$. Thus, the only possible contributing edges from $M'$ are from $P_1 \\cup \\cdots \\cup P_k$. An edge from $M$ can contribute if and only if it is not in exactly one of $M$ and $P_1 \\cup \\cdots \\cup P_k$, which means it must be in both. Thus, the edges from $M$ are redundant so $M \\oplus M' = (P_1 \\cup \\cdots \\cup P_k)$ which implies $A = (P_1 \\cup \\cdots \\cup P_k) \\oplus P$. Now we'll show that $P$ is edge disjoint from each $P_i$. Suppose that an edge $e$ of $P$ is also an edge of $P_i$ for some $i$. Since $P$ is an augmenting path with respect to $M'$ either $e \\in M'$ or $e \\in E - M'$. Suppose $e \\in M'$. Since $P$ is also augmenting with respect to $M$, we must have $e \\in M$. However, if $e$ is in $M$ and $M'$, then $e$ cannot be in any of the $P_i$'s by the definition of $M'$. Now suppose $e \\in E - M'$. Then $e \\in E - M$ since $P$ is augmenting with respect to $M$. Since $e$ is an edge of $P_i$, $e \\in E - M'$ implies that $e \\in M$, a contradiction. Since $P$ has edges alternately in $M'$ and $E - M'$ and is edge disjoint from $P_1 \\cup \\cdots \\cup P_k$, $P$ is also an augmenting path for $M$, which implies $|P| \\ge l$. Since every edge in $A$ is disjoint we conclude that $|A| \\ge (k + 1)l$. e. Suppose $M^*$ is a matching with strictly more than $|M| + |V| / (l + 1)$ edges. By part (b) there are strictly more than $|V| / (l + 1)$ vertex-disjoint augmenting paths with respect to $M$. Each one of these contains at least $l$ edges, so it is incident on $l + 1$ vertices. Since the paths are vertex disjoint, there are strictly more than $|V|(l + 1) / (l + 1)$ distinct vertices incident with these paths, a contradiction. Thus, the size of the maximum matching is at most $|M| + |V| / (l + 1)$. f. Consider what happens after iteration number $\\sqrt{|V|}$. Let $M^*$ be a maximal matching in $G$. Then $|M^*| \\ge |M|$ so by part (b), $M \\oplus M^*$ contains at least $|M^*| - |M|$ vertex disjoint augmenting paths with respect to $M$. By part (c), each of these is also a an augmenting path for $M$. Since each has length $\\sqrt{|V|}$, there can be at most $\\sqrt{|V|}$ such paths, so $|M^*| - |M| \\le \\sqrt{|V|}$. Thus, only $\\sqrt{|V|}$ additional iterations of the repeat loop can occur, so there are at most $2\\sqrt{|V|}$ iterations in total. g. For each unmatched vertex in $L$ we can perform a modified $\\text{BFS}$ to find the length of the shortest path to an unmatched vertex in $R$. Modify the $\\text{BFS}$ to ensure that we only traverse an edge if it causes the path to alternate between an edge in $M$ and an edge in $E - M$. The first time an unmatched vertex in $R$ is reached we know the length $k$ of a shortest augmenting path. We can use this to stop our search early if at any point we have traversed more than that number of edges. To find disjoint paths, start at the vertices of $R$ which were found at distance $k$ in the $\\text{BFS}$. Run a $\\text{DFS}$ backwards from these, which maintains the property that the next vertex we pick has distance one fewer, and the edges alternate between being in $M$ and $E - M$. As we build up a path, mark the vertices as used so that we never traverse them again. This takes $O(E)$, so by part (f) the total runtime is $O(\\sqrt VE)$.","title":"26-6 The Hopcroft-Karp bipartite matching algorithm"},{"location":"Chap27/27.1/","text":"27.1-1 Suppose that we spawn $\\text{P-FIB}(n - 2)$ in line 4 of $\\text{P-FIB}$, rather than calling it as is done in the code. What is the impact on the asymptotic work, span, and parallelism? (Removed) 27.1-2 Draw the computation dag that results from executing $\\text{P-FIB}(5)$. Assuming that each strand in the computation takes unit time, what are the work, span, and parallelism of the computation? Show how to schedule the dag on 3 processors using greedy scheduling by labeling each strand with the time step in which it is executed. Work: $T_1 = 29$. Span: $T_\\infty = 10$. Parallelism: $T_1 / T_\\infty \\approx 2.9$. 27.1-3 Prove that a greedy scheduler achieves the following time bound, which is slightly stronger than the bound proven in Theorem 27.1: $$T_P \\le \\frac{T_1 - T_\\infty}{P} + T_\\infty. \\tag{27.5}$$ Suppose that there are x incomplete steps in a run of the program. Since each of these steps causes at least one unit of work to be done, we have that there is at most $(T_1 - x)$ units of work done in the complete steps. Then, we suppose by contradiction that the number of complete steps is strictly greater than $\\lfloor (T_1 - x) / P \\rfloor$. Then, we have that the total amount of work done during the complete steps is $$P \\cdot (\\lfloor (T_1 - x) / P \\rfloor + 1) = P \\lfloor (T_1 - x) / P = (T_1 - x) - ((T_1 - x) \\mod P) + P > T_1 - x.$$ This is a contradiction because there are only $(T_1 - x)$ units of work done during complete steps, which is less than the amount we would be doing. Notice that since $T_\\infty$ is abound on the total number of both kinds of steps, it is a bound on the number of incomplete steps, $x$, so, $$T_P \\le \\lfloor (T_1 - x) / P \\rfloor + x \\le \\lfloor (T_1 - T_\\infty) / P \\rfloor + T_\\infty.$$ Where the second inequality comes by noting that the middle expression, as a function of $x$ is monotonically increasing, and so is bounded by the largest value of $x$ that is possible, namely $T_\\infty$. 27.1-4 Construct a computation dag for which one execution of a greedy scheduler can take nearly twice the time of another execution of a greedy scheduler on the same number of processors. Describe how the two executions would proceed. The computation is given in the image below. Let vertex $u$ have degree $k$, and assume that there are $m$ vertices in each vertical chain. Assume that this is executed on $k$ processors. In one execution, each strand from among the $k$ on the left is executed concurrently, and then the $m$ strands on the right are executed one at a time. If each strand takes unit time to execute, then the total computation takes $2m$ time. On the other hand, suppose that on each time step of the computation, $k - 1$ strands from the left (descendants of $u$) are executed, and one from the right (a descendant of $v$), is executed. If each strand take unit time to executed, the total computation takes $m + m / k$. Thus, the ratio of times is $2m / (m + m / k) = 2 / (1 + 1 / k)$. As $k$ gets large, this approaches $2$ as desired. 27.1-5 Professor Karan measures her deterministic multithreaded algorithm on $4$, $10$, and $64$ processors of an ideal parallel computer using a greedy scheduler. She claims that the three runs yielded $T_4 = 80$ seconds, $T_{10} = 42$ seconds, and $T_{64} = 10$ seconds. Argue that the professor is either lying or incompetent. ($\\textit{Hint:}$ Use the work law $\\text{(27.2)}$, the span law $\\text{(27.3)}$, and inequality $\\text{(27.5)}$ from Exercise 27.1-3.) (Removed) 27.1-6 Give a multithreaded algorithm to multiply an $n \\times n$ matrix by an $n$-vector that achieves $\\Theta(n^2 / \\lg n)$ parallelism while maintaining $\\Theta(n^2)$ work. (Removed) 27.1-7 Consider the following multithreaded pseudocode for transposing an $n \\times n$ matrix $A$ in place: P - TRANSPOSE ( A ) n = A . rows parallel for j = 2 to n parallel for i = 1 to j - 1 exchange a [ i , j ] with a [ j , i ] Analyze the work, span, and parallelism of this algorithm. (Removed) 27.1-8 Suppose that we replace the parallel for loop in line 3 of $\\text{P-TRANSPOSE}$ (see Exercise 27.1-7) with an ordinary for loop. Analyze the work, span, and parallelism of the resulting algorithm. (Removed) 27.1-9 For how many processors do the two versions of the chess programs run equally fast, assuming that $T_P = T_1 / P + T_\\infty$? (Removed)","title":"27.1 The basics of dynamic multithreading"},{"location":"Chap27/27.1/#271-1","text":"Suppose that we spawn $\\text{P-FIB}(n - 2)$ in line 4 of $\\text{P-FIB}$, rather than calling it as is done in the code. What is the impact on the asymptotic work, span, and parallelism? (Removed)","title":"27.1-1"},{"location":"Chap27/27.1/#271-2","text":"Draw the computation dag that results from executing $\\text{P-FIB}(5)$. Assuming that each strand in the computation takes unit time, what are the work, span, and parallelism of the computation? Show how to schedule the dag on 3 processors using greedy scheduling by labeling each strand with the time step in which it is executed. Work: $T_1 = 29$. Span: $T_\\infty = 10$. Parallelism: $T_1 / T_\\infty \\approx 2.9$.","title":"27.1-2"},{"location":"Chap27/27.1/#271-3","text":"Prove that a greedy scheduler achieves the following time bound, which is slightly stronger than the bound proven in Theorem 27.1: $$T_P \\le \\frac{T_1 - T_\\infty}{P} + T_\\infty. \\tag{27.5}$$ Suppose that there are x incomplete steps in a run of the program. Since each of these steps causes at least one unit of work to be done, we have that there is at most $(T_1 - x)$ units of work done in the complete steps. Then, we suppose by contradiction that the number of complete steps is strictly greater than $\\lfloor (T_1 - x) / P \\rfloor$. Then, we have that the total amount of work done during the complete steps is $$P \\cdot (\\lfloor (T_1 - x) / P \\rfloor + 1) = P \\lfloor (T_1 - x) / P = (T_1 - x) - ((T_1 - x) \\mod P) + P > T_1 - x.$$ This is a contradiction because there are only $(T_1 - x)$ units of work done during complete steps, which is less than the amount we would be doing. Notice that since $T_\\infty$ is abound on the total number of both kinds of steps, it is a bound on the number of incomplete steps, $x$, so, $$T_P \\le \\lfloor (T_1 - x) / P \\rfloor + x \\le \\lfloor (T_1 - T_\\infty) / P \\rfloor + T_\\infty.$$ Where the second inequality comes by noting that the middle expression, as a function of $x$ is monotonically increasing, and so is bounded by the largest value of $x$ that is possible, namely $T_\\infty$.","title":"27.1-3"},{"location":"Chap27/27.1/#271-4","text":"Construct a computation dag for which one execution of a greedy scheduler can take nearly twice the time of another execution of a greedy scheduler on the same number of processors. Describe how the two executions would proceed. The computation is given in the image below. Let vertex $u$ have degree $k$, and assume that there are $m$ vertices in each vertical chain. Assume that this is executed on $k$ processors. In one execution, each strand from among the $k$ on the left is executed concurrently, and then the $m$ strands on the right are executed one at a time. If each strand takes unit time to execute, then the total computation takes $2m$ time. On the other hand, suppose that on each time step of the computation, $k - 1$ strands from the left (descendants of $u$) are executed, and one from the right (a descendant of $v$), is executed. If each strand take unit time to executed, the total computation takes $m + m / k$. Thus, the ratio of times is $2m / (m + m / k) = 2 / (1 + 1 / k)$. As $k$ gets large, this approaches $2$ as desired.","title":"27.1-4"},{"location":"Chap27/27.1/#271-5","text":"Professor Karan measures her deterministic multithreaded algorithm on $4$, $10$, and $64$ processors of an ideal parallel computer using a greedy scheduler. She claims that the three runs yielded $T_4 = 80$ seconds, $T_{10} = 42$ seconds, and $T_{64} = 10$ seconds. Argue that the professor is either lying or incompetent. ($\\textit{Hint:}$ Use the work law $\\text{(27.2)}$, the span law $\\text{(27.3)}$, and inequality $\\text{(27.5)}$ from Exercise 27.1-3.) (Removed)","title":"27.1-5"},{"location":"Chap27/27.1/#271-6","text":"Give a multithreaded algorithm to multiply an $n \\times n$ matrix by an $n$-vector that achieves $\\Theta(n^2 / \\lg n)$ parallelism while maintaining $\\Theta(n^2)$ work. (Removed)","title":"27.1-6"},{"location":"Chap27/27.1/#271-7","text":"Consider the following multithreaded pseudocode for transposing an $n \\times n$ matrix $A$ in place: P - TRANSPOSE ( A ) n = A . rows parallel for j = 2 to n parallel for i = 1 to j - 1 exchange a [ i , j ] with a [ j , i ] Analyze the work, span, and parallelism of this algorithm. (Removed)","title":"27.1-7"},{"location":"Chap27/27.1/#271-8","text":"Suppose that we replace the parallel for loop in line 3 of $\\text{P-TRANSPOSE}$ (see Exercise 27.1-7) with an ordinary for loop. Analyze the work, span, and parallelism of the resulting algorithm. (Removed)","title":"27.1-8"},{"location":"Chap27/27.1/#271-9","text":"For how many processors do the two versions of the chess programs run equally fast, assuming that $T_P = T_1 / P + T_\\infty$? (Removed)","title":"27.1-9"},{"location":"Chap27/27.2/","text":"27.2-1 Draw the computation dag for computing $\\text{P-SQUARE-MATRIX-MULTIPLY}$ on $2 \\times 2$ matrices, labeling how the vertices in your diagram correspond to strands in the execution of the algorithm. Use the convention that spawn and call edges point downward, continuation edges point horizontally to the right, and return edges point upward. Assuming that each strand takes unit time, analyze the work, span, and parallelism of this computation. (Omit!) 27.2-2 Repeat Exercise 27.2-1 for $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$. (Omit!) 27.2-3 Give pseudocode for a multithreaded algorithm that multiplies two $n \\times n$ matrices with work $\\Theta(n^3)$ but span only $\\Theta(\\lg n)$. Analyze your algorithm. (Removed) 27.2-4 Give pseudocode for an efficient multithreaded algorithm that multiplies a $p \\times q$ matrix by a $q \\times r$ matrix. Your algorithm should be highly parallel even if any of $p$, $q$, and $r$ are $1$. Analyze your algorithm. (Removed) 27.2-5 Give pseudocode for an efficient multithreaded algorithm that transposes an $n \\times n$ matrix in place by using divide-and-conquer to divide the matrix recursively into four $n / 2 \\times n / 2$ submatrices. Analyze your algorithm. P - MATRIX - TRANSPOSE ( A ) n = A . rows if n == 1 return partition A into n / 2 \u2715 n / 2 submatrices A11 , A12 , A21 , A22 spawn P - MATRIX - TRANSPOSE ( A11 ) spawn P - MATRIX - TRANSPOSE ( A12 ) spawn P - MATRIX - TRANSPOSE ( A21 ) P - MATRIX - TRANSPOSE ( A22 ) sync // exchange A12 with A21 parallel for i = 1 to n / 2 parallel for j = 1 + n / 2 to n exchange A [ i , j ] with A [ i + n / 2 , j - n / 2 ] span: $T(n) = T(n / 2) + O(\\lg n) = O(\\lg^2 n)$. work: $T(n) = 4T(n / 2) + O(n^2) = O(n^2\\lg n)$. 27.2-6 Give pseudocode for an efficient multithreaded implementation of the Floyd-Warshall algorithm (see Section 25.2), which computes shortest paths between all pairs of vertices in an edge-weighted graph. Analyze your algorithm. (Removed)","title":"27.2 Multithreaded matrix multiplication"},{"location":"Chap27/27.2/#272-1","text":"Draw the computation dag for computing $\\text{P-SQUARE-MATRIX-MULTIPLY}$ on $2 \\times 2$ matrices, labeling how the vertices in your diagram correspond to strands in the execution of the algorithm. Use the convention that spawn and call edges point downward, continuation edges point horizontally to the right, and return edges point upward. Assuming that each strand takes unit time, analyze the work, span, and parallelism of this computation. (Omit!)","title":"27.2-1"},{"location":"Chap27/27.2/#272-2","text":"Repeat Exercise 27.2-1 for $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$. (Omit!)","title":"27.2-2"},{"location":"Chap27/27.2/#272-3","text":"Give pseudocode for a multithreaded algorithm that multiplies two $n \\times n$ matrices with work $\\Theta(n^3)$ but span only $\\Theta(\\lg n)$. Analyze your algorithm. (Removed)","title":"27.2-3"},{"location":"Chap27/27.2/#272-4","text":"Give pseudocode for an efficient multithreaded algorithm that multiplies a $p \\times q$ matrix by a $q \\times r$ matrix. Your algorithm should be highly parallel even if any of $p$, $q$, and $r$ are $1$. Analyze your algorithm. (Removed)","title":"27.2-4"},{"location":"Chap27/27.2/#272-5","text":"Give pseudocode for an efficient multithreaded algorithm that transposes an $n \\times n$ matrix in place by using divide-and-conquer to divide the matrix recursively into four $n / 2 \\times n / 2$ submatrices. Analyze your algorithm. P - MATRIX - TRANSPOSE ( A ) n = A . rows if n == 1 return partition A into n / 2 \u2715 n / 2 submatrices A11 , A12 , A21 , A22 spawn P - MATRIX - TRANSPOSE ( A11 ) spawn P - MATRIX - TRANSPOSE ( A12 ) spawn P - MATRIX - TRANSPOSE ( A21 ) P - MATRIX - TRANSPOSE ( A22 ) sync // exchange A12 with A21 parallel for i = 1 to n / 2 parallel for j = 1 + n / 2 to n exchange A [ i , j ] with A [ i + n / 2 , j - n / 2 ] span: $T(n) = T(n / 2) + O(\\lg n) = O(\\lg^2 n)$. work: $T(n) = 4T(n / 2) + O(n^2) = O(n^2\\lg n)$.","title":"27.2-5"},{"location":"Chap27/27.2/#272-6","text":"Give pseudocode for an efficient multithreaded implementation of the Floyd-Warshall algorithm (see Section 25.2), which computes shortest paths between all pairs of vertices in an edge-weighted graph. Analyze your algorithm. (Removed)","title":"27.2-6"},{"location":"Chap27/27.3/","text":"27.3-1 Explain how to coarsen the base case of $\\text{P-MERGE}$. Replace the condition on line 2 with a check that $n < k$ for some base case size $k$. And instead of just copying over the particular element of $A$ to the right spot in $B$, you would call a serial sort on the remaining segment of $A$ and copy the result of that over into the right spots in $B$. 27.3-2 Instead of finding a median element in the larger subarray, as $\\text{P-MERGE}$ does, consider a variant that finds a median element of all the elements in the two sorted subarrays using the result of Exercise 9.3-8. Give pseudocode for an efficient multithreaded merging procedure that uses this median-finding procedure. Analyze your algorithm. By a slight modification of exercise 9.3-8 we can find we can find the median of all elements in two sorted arrays of total length $n$ in $O(\\lg n)$ time. We'll modify $\\text{P-MERGE}$ to use this fact. Let $\\text{MEDIAN}(T, p_1, r_1, p_2, r_2)$ be the function which returns a pair, $q$, where $q.pos$ is the position of the median of all the elements $T$ which lie between positions $p_1$ and $r_1$, and between positions $p_2$ and $r_2$, and $q.arr$ is $1$ if the position is between $p_1$ and $r_1$, and $2$ otherwise. P - MEDIAN - MERGE ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ], A , p [ 3 ]) n [ 1 ] = r [ 1 ] - p [ 1 ] + 1 n [ 2 ] = r [ 2 ] - p [ 2 ] + 1 if n [ 1 ] < n [ 2 ] // ensure that n[1] \u2265 n[2] exchange p [ 1 ] with p [ 2 ] exchange r [ 1 ] with r [ 2 ] exchange n [ 1 ] with n [ 2 ] if n [ 1 ] == 0 // both empty? return q = MEDIAN ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ]) if q . arr == 1 q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 2 ], r [ 2 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 1 ] + q [ 2 ] - p [ 2 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q . pos - 1 , p [ 2 ], q [ 2 ] - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q . pos + 1 , r [ 1 ], q [ 2 ] + 1 , r [ 2 ], A , p [ 3 ]) sync else q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 1 ], r [ 1 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 2 ] + q [ 2 ] - p [ 1 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q [ 2 ] - 1 , p [ 2 ], q . pos - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q [ 2 ] + 1 , r [ 1 ], q . pos + 1 , r [ 2 ], A , p [ 3 ]) sync The work is characterized by the recurrence $T_1(n) = O(\\lg n) + 2T_1(n / 2)$, whose solution tells us that $T_1(n) = O(n)$. The work is at least $\\Omega(n)$ since we need to examine each element, so the work is $\\Theta(n)$. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = O(\\lg n) + O(\\lg n / 2) + T_\\infty(n / 2) \\\\ & = O(\\lg n) + T_\\infty(n / 2) \\\\ & = \\Theta(\\lg^2 n), \\end{aligned} $$ by exercise 4.6-2. 27.3-3 Give an efficient multithreaded algorithm for partitioning an array around a pivot, as is done by the $\\text{PARTITION}$ procedure on page 171. You need not partition the array in place. Make your algorithm as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ You may need an auxiliary array and may need to make more than one pass over the input elements.) Suppose that there are $c$ different processors, and the array has length $n$ and you are going to use its last element as a pivot. Then, look at each chunk of size $\\lceil \\frac{n}{c} \\rceil$ of entries before the last element, give one to each processor. Then, each counts the number of elements that are less than the pivot. Then, we compute all the running sums of these values that are returned. This can be done easily by considering all of the subarrays placed along the leaves of a binary tree, and then summing up adjacent pairs. This computation can be done in time $\\lg(\\min\\{c, n\\})$ since it's the log of the number of leaves. From there, we can compute all the running sums for each of the subarrays also in logarithmic time. This is by keeping track of the sum of all more left cousins of each internal node, which is found by adding the left sibling's sum vale to the left cousin value of the parent, with the root's left cousin value initiated to $0$. This also just takes time the depth of the tree, so is $\\lg(\\min\\{c, n\\})$. Once all of these values are computed at the root, it is the index that the subarray's elements less than the pivot should be put. To find the position where the subarray's elements larger than the root should be put, just put it at twice the sum value of the root minus the left cousin value for that subarray. Then, the time taken is just $O(\\frac{n}{c})$. By doing this procedure, the total work is just $O(n)$, and the span is $O(\\lg n)$, and so has parallelization of $O(\\frac{n}{\\lg n})$. This whole process is split across the several algoithms appearing here. 27.3-4 Give a multithreaded version of $\\text{RECURSIVE-FFT}$ on page 911. Make your implementation as parallel as possible. Analyze your algorithm. P - RECURSIVE - FFT ( a ) n = a . length if n == 1 return a w [ n ] = e ^ { 2 * \u03c0 * i / n } w = 1 a ( 0 ) = [ a [ 0 ], a [ 2 ].. a [ n - 2 ]] a ( 1 ) = [ a [ 1 ], a [ 3 ].. a [ n - 1 ]] y ( 0 ) = spawn P - RECURSIVE - FFT ( a [ 0 ]) y ( 1 ) = P - RECURSIVE - FFT ( a [ 1 ]) sync parallel for k = 0 to n / 2 - 1 y [ k ] = y [ k ]( 0 ) + w * y [ k ]( 1 ) y [ k + n / 2 ] = y [ k ]( 0 ) - w * y [ k ]( 1 ) w = w * w [ n ] return y $\\text{P-RECURSIVE-FFT}$ parallelized over the two recursive calls, having a parallel for works because each of the iterations of the for loop touch independent sets of variables. The span of the procedure is only $\\Theta(\\lg n)$ giving it a parallelization of $\\Theta(n)$. 27.3-5 $\\star$ Give a multithreaded version of $\\text{RANDOMIZED-SELECT}$ on page 216. Make your implementation as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ Use the partitioning algorithm from Exercise 27.3-3.) Randomly pick a pivot element, swap it with the last element, so that it is in the correct format for running the procedure described in 27.3-3. Run partition from problem 27.3-3. As an intermediate step, in that procedure, we compute the number of elements less than the pivot ($T.root.sum$), so keep track of that value after the end of partition. Then, if we have that it is less than $k$, recurse on the subarray that was greater than or equal to the pivot, decreasing the order statistic of the element to be selected by $T.root.sum$. If it is larger than the order statistic of the element to be selected, then leave it unchanged and recurse on the subarray that was formed to be less than the pivot. A lot of the analysis in section 9.2 still applies, except replacing the timer needed for partitioning with the runtime of the algorithm in problem 27.3-3. The work is unchanged from the serial case because when $c = 1$, the algorithm reduces to the serial algorithm for partitioning. For span, the $O(n)$ term in the equation half way down page 218 can be replaced with an $O(\\lg n)$ term. It can be seen with the substitution method that the solution to this is logarithmic $$E[T(n)] \\le \\frac{2}{n} \\sum_{k = \\lfloor n / 2 \\rfloor}^{n - 1} C\\lg k + O(\\lg n) \\le O(\\lg n).$$ So, the total span of this algorithm will still just be $O(\\lg n)$. 27.3-6 $\\star$ Show how to multithread $\\text{SELECT}$ from Section 9.3. Make your implementation as parallel as possible. Analyze your algorithm. Let $\\text{MEDIAN}(A)$ denote a brute force method which returns the median element of the array $A$. We will only use this to find the median of small arrays, in particular, those of size at most $5$, so it will always run in constant time. We also let $A[i..j]$ denote the array whose elements are $A[i], A[i + 1], \\ldots, A[j]$. The function $\\text{P-PARTITION}(A, x)$ is a multithreaded function which partitions $A$ around the input element $x$ and returns the number of elements in $A$ which are less than or equal to $x$. Using a parallel for loop, its span is logarithmic in the number of elements in $A$. The work is the same as the serialization, which is $\\Theta(n)$ according to section 9.3. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = \\Theta(lg n / 5) + T_\\infty(n / 5) + \\Theta(\\lg n) + T_\\infty(7n / 10 + 6) \\\\ & \\le \\Theta(\\lg n) + T_\\infty(n / 5) + T_\\infty(7n / 10 + 6). \\end{aligned} $$ Using the substitution method we can show that $T_\\infty(n) = O(n^\\epsilon)$ for some $\\epsilon < 1$. In particular, $\\epsilon = 0.9$ works. This gives a parallelization of $\\Omega(n^0.1)$. P - SELECT ( A , i ) if n == 1 return A [ 1 ] let T [ 1. . floor ( n / 5 )] be a new array parallel for i = 0 to floor ( n / 5 ) - 1 T [ i + 1 ] = MEDIAN ( A [ i * floor ( n / 5 ).. i * floor ( n / 5 ) + 4 ]) if n / 5 is not an integer T [ floor ( n / 5 )] = MEDIAN ( A [ 5 * floor ( n / 5 ).. n ]) x = P - SELECT ( T , ceil ( n / 5 )) k = P - PARTITION ( A , x ) if k == i return x else if i < k P - SELECT ( A [ 1. . k - 1 ], i ) else P - SELECT ( A [ k + 1. . n ], i - k )","title":"27.3 Multithreaded merge sort"},{"location":"Chap27/27.3/#273-1","text":"Explain how to coarsen the base case of $\\text{P-MERGE}$. Replace the condition on line 2 with a check that $n < k$ for some base case size $k$. And instead of just copying over the particular element of $A$ to the right spot in $B$, you would call a serial sort on the remaining segment of $A$ and copy the result of that over into the right spots in $B$.","title":"27.3-1"},{"location":"Chap27/27.3/#273-2","text":"Instead of finding a median element in the larger subarray, as $\\text{P-MERGE}$ does, consider a variant that finds a median element of all the elements in the two sorted subarrays using the result of Exercise 9.3-8. Give pseudocode for an efficient multithreaded merging procedure that uses this median-finding procedure. Analyze your algorithm. By a slight modification of exercise 9.3-8 we can find we can find the median of all elements in two sorted arrays of total length $n$ in $O(\\lg n)$ time. We'll modify $\\text{P-MERGE}$ to use this fact. Let $\\text{MEDIAN}(T, p_1, r_1, p_2, r_2)$ be the function which returns a pair, $q$, where $q.pos$ is the position of the median of all the elements $T$ which lie between positions $p_1$ and $r_1$, and between positions $p_2$ and $r_2$, and $q.arr$ is $1$ if the position is between $p_1$ and $r_1$, and $2$ otherwise. P - MEDIAN - MERGE ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ], A , p [ 3 ]) n [ 1 ] = r [ 1 ] - p [ 1 ] + 1 n [ 2 ] = r [ 2 ] - p [ 2 ] + 1 if n [ 1 ] < n [ 2 ] // ensure that n[1] \u2265 n[2] exchange p [ 1 ] with p [ 2 ] exchange r [ 1 ] with r [ 2 ] exchange n [ 1 ] with n [ 2 ] if n [ 1 ] == 0 // both empty? return q = MEDIAN ( T , p [ 1 ], r [ 1 ], p [ 2 ], r [ 2 ]) if q . arr == 1 q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 2 ], r [ 2 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 1 ] + q [ 2 ] - p [ 2 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q . pos - 1 , p [ 2 ], q [ 2 ] - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q . pos + 1 , r [ 1 ], q [ 2 ] + 1 , r [ 2 ], A , p [ 3 ]) sync else q [ 2 ] = BINARY - SEARCH ( T [ q . pos ], T , p [ 1 ], r [ 1 ]) q [ 3 ] = p [ 3 ] + q . pos - p [ 2 ] + q [ 2 ] - p [ 1 ] A [ q [ 3 ]] = T [ q . pos ] spawn P - MEDIAN - MERGE ( T , p [ 1 ], q [ 2 ] - 1 , p [ 2 ], q . pos - 1 , A , p [ 3 ]) P - MEDIAN - MERGE ( T , q [ 2 ] + 1 , r [ 1 ], q . pos + 1 , r [ 2 ], A , p [ 3 ]) sync The work is characterized by the recurrence $T_1(n) = O(\\lg n) + 2T_1(n / 2)$, whose solution tells us that $T_1(n) = O(n)$. The work is at least $\\Omega(n)$ since we need to examine each element, so the work is $\\Theta(n)$. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = O(\\lg n) + O(\\lg n / 2) + T_\\infty(n / 2) \\\\ & = O(\\lg n) + T_\\infty(n / 2) \\\\ & = \\Theta(\\lg^2 n), \\end{aligned} $$ by exercise 4.6-2.","title":"27.3-2"},{"location":"Chap27/27.3/#273-3","text":"Give an efficient multithreaded algorithm for partitioning an array around a pivot, as is done by the $\\text{PARTITION}$ procedure on page 171. You need not partition the array in place. Make your algorithm as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ You may need an auxiliary array and may need to make more than one pass over the input elements.) Suppose that there are $c$ different processors, and the array has length $n$ and you are going to use its last element as a pivot. Then, look at each chunk of size $\\lceil \\frac{n}{c} \\rceil$ of entries before the last element, give one to each processor. Then, each counts the number of elements that are less than the pivot. Then, we compute all the running sums of these values that are returned. This can be done easily by considering all of the subarrays placed along the leaves of a binary tree, and then summing up adjacent pairs. This computation can be done in time $\\lg(\\min\\{c, n\\})$ since it's the log of the number of leaves. From there, we can compute all the running sums for each of the subarrays also in logarithmic time. This is by keeping track of the sum of all more left cousins of each internal node, which is found by adding the left sibling's sum vale to the left cousin value of the parent, with the root's left cousin value initiated to $0$. This also just takes time the depth of the tree, so is $\\lg(\\min\\{c, n\\})$. Once all of these values are computed at the root, it is the index that the subarray's elements less than the pivot should be put. To find the position where the subarray's elements larger than the root should be put, just put it at twice the sum value of the root minus the left cousin value for that subarray. Then, the time taken is just $O(\\frac{n}{c})$. By doing this procedure, the total work is just $O(n)$, and the span is $O(\\lg n)$, and so has parallelization of $O(\\frac{n}{\\lg n})$. This whole process is split across the several algoithms appearing here.","title":"27.3-3"},{"location":"Chap27/27.3/#273-4","text":"Give a multithreaded version of $\\text{RECURSIVE-FFT}$ on page 911. Make your implementation as parallel as possible. Analyze your algorithm. P - RECURSIVE - FFT ( a ) n = a . length if n == 1 return a w [ n ] = e ^ { 2 * \u03c0 * i / n } w = 1 a ( 0 ) = [ a [ 0 ], a [ 2 ].. a [ n - 2 ]] a ( 1 ) = [ a [ 1 ], a [ 3 ].. a [ n - 1 ]] y ( 0 ) = spawn P - RECURSIVE - FFT ( a [ 0 ]) y ( 1 ) = P - RECURSIVE - FFT ( a [ 1 ]) sync parallel for k = 0 to n / 2 - 1 y [ k ] = y [ k ]( 0 ) + w * y [ k ]( 1 ) y [ k + n / 2 ] = y [ k ]( 0 ) - w * y [ k ]( 1 ) w = w * w [ n ] return y $\\text{P-RECURSIVE-FFT}$ parallelized over the two recursive calls, having a parallel for works because each of the iterations of the for loop touch independent sets of variables. The span of the procedure is only $\\Theta(\\lg n)$ giving it a parallelization of $\\Theta(n)$.","title":"27.3-4"},{"location":"Chap27/27.3/#273-5-star","text":"Give a multithreaded version of $\\text{RANDOMIZED-SELECT}$ on page 216. Make your implementation as parallel as possible. Analyze your algorithm. ($\\textit{Hint:}$ Use the partitioning algorithm from Exercise 27.3-3.) Randomly pick a pivot element, swap it with the last element, so that it is in the correct format for running the procedure described in 27.3-3. Run partition from problem 27.3-3. As an intermediate step, in that procedure, we compute the number of elements less than the pivot ($T.root.sum$), so keep track of that value after the end of partition. Then, if we have that it is less than $k$, recurse on the subarray that was greater than or equal to the pivot, decreasing the order statistic of the element to be selected by $T.root.sum$. If it is larger than the order statistic of the element to be selected, then leave it unchanged and recurse on the subarray that was formed to be less than the pivot. A lot of the analysis in section 9.2 still applies, except replacing the timer needed for partitioning with the runtime of the algorithm in problem 27.3-3. The work is unchanged from the serial case because when $c = 1$, the algorithm reduces to the serial algorithm for partitioning. For span, the $O(n)$ term in the equation half way down page 218 can be replaced with an $O(\\lg n)$ term. It can be seen with the substitution method that the solution to this is logarithmic $$E[T(n)] \\le \\frac{2}{n} \\sum_{k = \\lfloor n / 2 \\rfloor}^{n - 1} C\\lg k + O(\\lg n) \\le O(\\lg n).$$ So, the total span of this algorithm will still just be $O(\\lg n)$.","title":"27.3-5 $\\star$"},{"location":"Chap27/27.3/#273-6-star","text":"Show how to multithread $\\text{SELECT}$ from Section 9.3. Make your implementation as parallel as possible. Analyze your algorithm. Let $\\text{MEDIAN}(A)$ denote a brute force method which returns the median element of the array $A$. We will only use this to find the median of small arrays, in particular, those of size at most $5$, so it will always run in constant time. We also let $A[i..j]$ denote the array whose elements are $A[i], A[i + 1], \\ldots, A[j]$. The function $\\text{P-PARTITION}(A, x)$ is a multithreaded function which partitions $A$ around the input element $x$ and returns the number of elements in $A$ which are less than or equal to $x$. Using a parallel for loop, its span is logarithmic in the number of elements in $A$. The work is the same as the serialization, which is $\\Theta(n)$ according to section 9.3. The span satisfies the recurrence $$ \\begin{aligned} T_\\infty(n) & = \\Theta(lg n / 5) + T_\\infty(n / 5) + \\Theta(\\lg n) + T_\\infty(7n / 10 + 6) \\\\ & \\le \\Theta(\\lg n) + T_\\infty(n / 5) + T_\\infty(7n / 10 + 6). \\end{aligned} $$ Using the substitution method we can show that $T_\\infty(n) = O(n^\\epsilon)$ for some $\\epsilon < 1$. In particular, $\\epsilon = 0.9$ works. This gives a parallelization of $\\Omega(n^0.1)$. P - SELECT ( A , i ) if n == 1 return A [ 1 ] let T [ 1. . floor ( n / 5 )] be a new array parallel for i = 0 to floor ( n / 5 ) - 1 T [ i + 1 ] = MEDIAN ( A [ i * floor ( n / 5 ).. i * floor ( n / 5 ) + 4 ]) if n / 5 is not an integer T [ floor ( n / 5 )] = MEDIAN ( A [ 5 * floor ( n / 5 ).. n ]) x = P - SELECT ( T , ceil ( n / 5 )) k = P - PARTITION ( A , x ) if k == i return x else if i < k P - SELECT ( A [ 1. . k - 1 ], i ) else P - SELECT ( A [ k + 1. . n ], i - k )","title":"27.3-6 $\\star$"},{"location":"Chap27/Problems/27-1/","text":"Consider the following multithreaded algorithm for performing pairwise addition on $n$-element arrays $A[1..n]$ and $B[1..n]$, storing the sums in $C[1..n]$: SUM - ARRAYS ( A , B , C ) parallel for i = 1 to A . length C [ i ] = A [ i ] + B [ i ] a. Rewrite the parallel loop in $\\text{SUM-ARRAYS}$ using nested parallelism ( spawn and sync ) in the manner of $\\text{MAT-VEC-MAIN-LOOP}$. Analyze the parallelism of your implementation. Consider the following alternative implementation of the parallel loop, which contains a value $grain\\text-size$ to be specified: SUM - ARRAYS ' ( A , B , C ) n = A . length grain - size = ? // to be determined r = ceil ( n / grain - size ) for k = 0 to r - 1 spawn ADD - SUBARRAY ( A , B , C , k * grain - size + 1 , min (( k + 1 ) * grain - size , n )) sync ADD - SUBARRAY ( A , B , C , i , j ) for k = i to j C [ k ] = A [ k ] + B [ k ] b. Suppose that we set $grain\\text -size = 1$. What is the parallelism of this implementation? c. Give a formula for the span of $\\text{SUM-ARRAYS}'$ in terms of $n$ and $grain\\text-size$. Derive the best value for grain-size to maximize parallelism. a. See the algorithm $\\text{SUM-ARRAYS}(A, B, C)$. The parallelism is $O(n)$ since it's work is $n\\lg n$ and the span is $\\lg n$. b. If grainsize is $1$, this means that each call of $\\text{ADD-SUBARRAY}$ just sums a single pair of numbers. This means that since the for loop on line 4 will run $n$ times, both the span and work will be $O(n)$. So, the parallelism is just $O(1)$. SUM - ARRAYS ( A , B , C ) n = floor ( A . length / 2 ) if n == 0 C [ 1 ] = A [ 1 ] + B [ 1 ] else spawn SUM - ARRAYS ( A [ 1. . n ], B [ 1. . n ], C [ 1. . n ]) SUM - ARRAYS ( A [ n + 1. . A . length ], B [ n + 1. . A .. length ], C [ n + 1. . A . length ]) c. Let $g$ be the grainsize. The runtime of the function that spawns all the other functions is $\\left\\lceil \\frac{n}{g} \\right\\rceil$. The runtime of any particular spawned task is $g$. So, we want to minimize $$\\frac{n}{g} + g.$$ To do this we pull out our freshman calculus hat and take a derivative, we have $$0 = 1 \u2212 \\frac{n}{g^2}.$$ To solve this, we set $g = \\sqrt n$. This minimizes the quantity and makes the span $O(n / g + g) = O(\\sqrt n)$. Resulting in a parallelism of $O(\\sqrt n)$.","title":"27-1 Implementing parallel loops using nested parallelism"},{"location":"Chap27/Problems/27-2/","text":"The $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$ procedure has the disadvantage that it must allocate a temporary matrix $T$ of size $n \\times n$, which can adversely affect the constants hidden by the $\\Theta$-notation. The $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$ procedure does have high parallelism, however. For example, ignoring the constants in the $\\Theta$-notation, the parallelism for multiplying $1000 \\times 1000$ matrices comes to approximately $1000^3 / 10^2 = 10^7$, since $\\lg 1000 \\approx 10$. Most parallel computers have far fewer than 10 million processors. a. Describe a recursive multithreaded algorithm that eliminates the need for the temporary matrix $T$ at the cost of increasing the span to $\\Theta(n)$. ($\\textit{Hint:}$ Compute $C = C + AB$ following the general strategy of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$, but initialize $C$ in parallel and insert a sync in a judiciously chosen location.) b. Give and solve recurrences for the work and span of your implementation. c. Analyze the parallelism of your implementation. Ignoring the constants in the $\\Theta$-notation, estimate the parallelism on $1000 \\times 1000$ matrices. Compare with the parallelism of $\\text{P-MATRIX-MULTIPLY-RECURSIVE}$. (Removed)","title":"27-2 Saving temporary space in matrix multiplication"},{"location":"Chap27/Problems/27-3/","text":"a. Parallelize the $\\text{LU-DECOMPOSITION}$ procedure on page 821 by giving pseudocode for a multithreaded version of this algorithm. Make your implementation as parallel as possible, and analyze its work, span, and parallelism. b. Do the same for $\\text{LUP-DECOMPOSITION}$ on page 824. c. Do the same for $\\text{LUP-SOLVE}$ on page 817. d. Do the same for a multithreaded algorithm based on equation $\\text{(28.13)}$ for inverting a symmetric positive-definite matrix. a. For the algorithm $\\text{LU-DECOMPOSITION}(A)$ on page 821, the inner for loops can be parallelized, since they never update values that are read on later runs of those loops. However, the outermost for loop cannot be parallelized because across iterations of it the changes to the matrices from previous runs are used to affect the next. This means that the span will be $\\Theta(n \\lg n)$, work will still be $\\Theta(n^3)$ and, so, the parallelization will be $\\Theta(\\frac{n^3}{n\\lg n}) = \\Theta(\\frac{n^2}{\\lg n})$. b. The for loop on lines 7-10 is taking the max of a set of things, while recording the index that that max occurs. This for loop can therefor be replaced with a $\\lg n$ span parallelized procedure in which we arrange the $n$ elements into the leaves of an almost balanced binary tree, and we let each internal node be the max of its two children. Then, the span will just be the depth of this tree. This procedure can gracefully scale with the number of processors to make the span be linear, though even if it is $\\Theta(n\\lg n)$ it will be less than the $\\Theta(n^2)$ work later. The for loop on line 14-15 and the implicit for loop on line 15 have no concurrent editing, and so, can be made parallel to have a span of $lg n$. While the for loop on lines 18-19 can be made parallel, the one containing it cannot without creating data races. Therefore, the total span of the naive parallelized algorithm will be $\\Theta(n^2\\lg n)$, with a work of $\\Theta(n^3)$. So, the parallelization will be $\\Theta(\\frac{n}{\\lg n})$. Not as parallized as part (a), but still a significant improvement. c. We can parallelize the computing of the sums on lines 4 and 6, but cannot also parallize the for loops containing them without creating an issue of concurrently modifying data that we are reading. This means that the span will be $\\Theta(n\\lg n)$, work will still be $\\Theta(n^2)$, and so the parallelization will be $\\Theta(\\frac{n}{\\lg n})$. d. The recurrence governing the amount of work of implementing this procedure is given by $$I(n) \\le 2I(n / 2) + 4M(n / 2) + O(n^2).$$ However, the two inversions that we need to do are independent, and the span of parallelized matrix multiply is just $O(\\lg n)$. Also, the $n^2$ work of having to take a transpose and subtract and add matrices has a span of only $O(\\lg n)$. Therefore, the span satisfies the recurrence $$I_\\infty(n) \\le I_\\infty(n / 2) + O(\\lg n).$$ This recurrence has the solution $I_\\infty(n) \\in \\Theta(\\lg^2 n)$ by exercise 4.6-2. Therefore, the span of the inversion algorithm obtained by looking at the procedure detailed on page 830. This makes the parallelization of it equal to $\\Theta(M(n) / \\lg^2 n)$ where $M(n)$ is the time to compute matrix products.","title":"27-3 Multithreaded matrix algorithms"},{"location":"Chap27/Problems/27-4/","text":"A $\\otimes$-reduction of an array $x[1 \\ldots n]$, where $\\otimes$ is an associative operator, is the value $$y = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[n].$$ The following procedure computes the $\\otimes$-reduction of a subarray $x[i \\ldots j]$ serially. REDUCE ( x , i , j ) y = x [ i ] for k = i + 1 to j y = y \u2297 x [ k ] return y a. Use nested parallelism to implement a multithreaded algorithm $\\text{P-REDUCE}$, which performs the same function with $\\Theta(n)$ work and $\\Theta(\\lg n)$ span. Analyze your algorithm. A related problem is that of computing a $\\otimes$-prefix computation , sometimes called a $\\otimes$-scan , on an array $x[1 \\ldots n]$, where $\\otimes$ is once again an associative operator. The $\\otimes$-scan produces the array $y[1 \\ldots n]$ given by $$ \\begin{aligned} y[1] & = x[1], \\\\ y[2] & = x[1] \\otimes x[2], \\\\ y[3] & = x[1] \\otimes x[2] \\otimes x[3], \\\\ & \\vdots \\\\ y[n] & = x[1] \\otimes x[2] \\otimes x[3] \\otimes \\cdots \\otimes x[n], \\end{aligned} $$ that is, all prefixes of the array $x$ \"summed\" using $\\otimes$ operator. The following serial procedure $\\text{SCAN}$ performs a $\\otimes$-prefix computation: SCAN ( x ) n = x . length let y [ 1. . n ] be a new array y [ 1 ] = x [ 1 ] for i = 2 to n y [ i ] = y [ i - 1 ] \u2297 x [ i ] return y Unfortunately, multithreading $\\text{SCAN}$ is not straightforward. For example, changing the for loop to a parallel for loop would create races, since each iteration of the loop body depends on the previous iteration. The following procedure $\\text{P-SCAN-1}$ performs the $\\otimes$-prefix computation in parallel, albeit inefficiently. P - SCAN -1 ( x ) n = x . length let y [ 1. . n ] be a new array P - SCAN -1 - AUX ( x , y , 1 , n ) return y P - SCAN -1 - AUX ( x , y , i , j ) parallel for l = i to j y [ l ] = P - REDUCE ( x , 1 , l ) b. Analyze the work, span, and parallelism of $\\text{P-SCAN-1}$. By using nested parallelism, we can obtain a more efficient $\\otimes$-prefix computation: P - SCAN -2 ( x ) n = x . length let y [ 1. . n ] be a new array P - SCAN -2 - AUX ( x , y , 1 , n ) return y P - SCAN -2 - AUX ( x , y , i , j ) if i == j y [ i ] = x [ i ] else k = floor (( i + j ) / 2 ) spawn P - SCAN -2 - AUX ( x , y , i , k ) P - SCAN -2 - AUX ( x , y , k + 1 , j ) sync parallel for l = k + 1 to j y [ l ] = y [ k ] \u2297 y [ l ] c. Argue that $\\text{P-SCAN-2}$ is correct, and analyze its work, span, and parallelism. We can improve on both $\\text{P-SCAN-1}$ and $\\text{P-SCAN-2}$ by performing the $\\otimes$-prefix computation in two distinct passes over the data. On the first pass, we gather the terms for various contiguous subarrays of $x$ into a temporary array $t$, and on the second pass we use the terms in $t$ to compute the final result $y$. The following pseudocode implements this strategy, but certain expressions have been omitted: P - SCAN -3 ( x ) n = x . length let y [ 1. . n ] and t [ 1. . n ] be new arrays y [ 1 ] = x [ 1 ] if n > 1 P - SCAN - UP ( x , t , 2 , n ) P - SCAN - DOWN ( x [ 1 ], x , t , y , 2 , n ) return y P - SCAN - UP ( x , t , i , j ) if i == j return x [ i ] else k = floor (( i + j ) / 2 ) t [ k ] = spawn P - SCAN - UP ( x , t , i , k ) right = P - SCAN - UP ( x , t , k + 1 , j ) sync return ____ // fill in the blank P - SCAN - DOWN ( v , x , t , y , i , j ) if i == j y [ i ] = v \u2297 x [ i ] else k = floor (( i + j ) / 2 ) spawn P - SCAN - DOWN ( ____ , x , t , y , i , k ) // fill in the blank P - SCAN - DOWN ( ____ , x , t , y , k + 1 , j ) // fill in the blank sync d. Fill in the three missing expressions in line 8 of $\\text{P-SCAN-UP}$ and lines 5 and 6 of $\\text{P-SCAN-DOWN}$. Argue that with expressions you supplied, $\\text{P-SCAN-3}$ is correct. ($\\textit{Hint:}$ Prove that the value $v$ passed to $\\text{P-SCAN-DOWN}(v, x, t, y, i, j)$ satisfies $v = x[1] \\otimes x[2] \\otimes \\cdots \\otimes x[i - 1]$.) e. Analyze the work, span, and parallelism of $\\text{P-SCAN-3}$. (Removed)","title":"27-4 Multithreading reductions and prefix computations"},{"location":"Chap27/Problems/27-5/","text":"Computational science is replete with algorithms that require the entries of an array to be filled in with values that depend on the values of certain already computed neighboring entries, along with other information that does not change over the course of the computation. The pattern of neighboring entries does not change during the computation and is called a stencil . For example, Section 15.4 presents a stencil algorithm to compute a longest common subsequence, where the value in entry $c[i, j]$ depends only on the values in $c[i - 1, j]$, $c[i, j - 1]$, and $c[i - 1, j - 1]$, as well as the elements $x_i$ and $y_j$ within the two sequences given as inputs. The input sequences are fixed, but the algorithm fills in the two-dimensional array $c$ so that it computes entry $c[i, j]$ after computing all three entries $c[i - 1, j]$, $c[i, j - 1]$, and $c[i - 1, j - 1]$. In this problem, we examine how to use nested parallelism to multithread a simple stencil calculation on an $n \\times n$ array $A$ in which, of the values in $A$, the value placed into entry $A[i, j]$ depends only on values in $A[i' , j']$, where $i' \\le i$ and $j' \\le j$ (and of course, $i' \\ne i$ or $j' \\ne j$). In other words, the value in an entry depends only on values in entries that are above it and/or to its left, along with static information outside of the array. Furthermore, we assume throughout this problem that once we have filled in the entries upon which $A[i, j]$ depends, we can fill in $A[i, j]$ in $\\Theta(1)$ time (as in the $\\text{LCS-LENGTH}$ procedure of Section 15.4). We can partition the $n \\times n$ array $A$ into four $n / 2 \\times n / 2$ subarrays as follows: $$ A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\tag{27.11} \\end{pmatrix} . $$ Observe now that we can fill in subarray $A_{11}$ recursively, since it does not depend on the entries of the other three subarrays. Once $A_{11}$ is complete, we can continue to fill in $A_{12}$ and $A_{21}$ recursively in parallel, because although they both depend on $A_{11}$ , they do not depend on each other. Finally, we can fill in $A_{22}$ recursively. a. Give multithreaded pseudocode that performs this simple stencil calculation using a divide-and-conquer algorithm $\\text{SIMPLE-STENCIL}$ based on the decomposition $\\text{(27.11)}$ and the discussion above. (Don't worry about the details of the base case, which depends on the specific stencil.) Give and solve recurrences for the work and span of this algorithm in terms of $n$. What is the parallelism? b. Modify your solution to part (a) to divide an $n \\times n$ array into nine $n / 3 \\times n / 3$ subarrays, again recursing with as much parallelism as possible. Analyze this algorithm. How much more or less parallelism does this algorithm have compared with the algorithm from part (a)? c. Generalize your solutions to parts (a) and (b) as follows. Choose an integer $b \\ge 2$. Divide an $n \\times n$ array into $b^2$ subarrays, each of size $n / b \\times n / b$, recursing with as much parallelism as possible. In terms of $n$ and $b$, what are the work, span, and parallelism of your algorithm? Argue that, using this approach, the parallelism must be $o(n)$ for any choice of $b \\ge 2$. ($\\textit{Hint:}$ For this last argument, show that the exponent of $n$ in the parallelism is strictly less than $1$ for any choice of $b \\ge 2$.) d. Give pseudocode for a multithreaded algorithm for this simple stencil calculation that achieves $\\Theta(n\\lg n)$ parallelism. Argue using notions of work and span that the problem, in fact, has $\\Theta(n)$ inherent parallelism. As it turns out, the divide-and-conquer nature of our multithreaded pseudocode does not let us achieve this maximal parallelism. (Removed)","title":"27-5 Multithreading a simple stencil calculation"},{"location":"Chap27/Problems/27-6/","text":"Just as with ordinary serial algorithms, we sometimes want to implement randomized multithreaded algorithms. This problem explores how to adapt the various performance measures in order to handle the expected behavior of such algorithms. It also asks you to design and analyze a multithreaded algorithm for randomized quicksort. a. Explain how to modify the work law $\\text{(27.2)}$, span law $\\text{(27.3)}$, and greedy scheduler bound $\\text{(27.4)}$ to work with expectations when $T_P$, $T_1$, and $T_\\infty$ are all random variables. b. Consider a randomized multithreaded algorithm for which $1\\%$ of the time we have $T_1 = 10^4$ and $T_{10,000} = 1$, but for $99\\%$ of the time we have $T_1 = T_{10,000} = 10^9$. Argue that the speedup of a randomized multithreaded algorithm should be defined as $\\text E[T_1]/\\text E[T_P]$, rather than $\\text E[T_1 / T_P]$. c. Argue that the parallelism of a randomized multithreaded algorithm should be defined as the ratio $\\text E[T_1] / \\text E[T_\\infty]$. d. Multithread the $\\text{RANDOMIZED-QUICKSORT}$ algorithm on page 179 by using nested parallelism. (Do not parallelize $\\text{RANDOMIZED-PARTITION}$.) Give the pseudocode for your $\\text{P-RANDOMIZED-QUICKSORT}$ algorithm. e. Analyze your multithreaded algorithm for randomized quicksort. ($\\textit{Hint:}$ Review the analysis of $\\text{RANDOMIZED-SELECT}$ on page 216.) a. $$ \\begin{aligned} \\text E[T_P] & \\ge \\text E[T_1] / P \\\\ \\text E[T_P] & \\ge \\text E[T_\\infty] \\\\ \\text E[T_P] & \\le \\text E[T_1]/P + \\text E[T_\\infty]. \\end{aligned} $$ b. $$\\text E[T_1] \\approx \\text E[T_{10,000}] \\approx 9.9 \\times 10^8, \\text E[T_1]/\\text E[T_P] = 1.$$ $$\\text E[T_1 / T_{10,000}] = 10^4 * 0.01 + 0.99 = 100.99.$$ c. Same as the above. d. RANDOMIZED - QUICKSORT ( A , p , r ) if p < r q = RANDOM - PARTITION ( A , p , r ) spawn RANDOMIZED - QUICKSORT ( A , p , q - 1 ) RANDOMIZED - QUICKSORT ( A , q + 1 , r ) sync e. $$ \\begin{aligned} \\text E[T_1] & = O(n\\lg n) \\\\ \\text E[T_\\infty] & = O(\\lg n) \\\\ \\text E[T_1] / \\text E[T_\\infty] & = O(n). \\end{aligned} $$","title":"27-6 Randomized multithreaded algorithms"},{"location":"Chap28/28.1/","text":"28.1-1 Solve the equation $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ -6 & 5 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 14 \\\\ -7 \\end{pmatrix} $$ by using forward substitution. $$ \\begin{pmatrix} 3 \\\\ 14 - 4 \\cdot 3 \\\\ -7 - 5 \\cdot (14 - 4 \\cdot 3) - (-6) \\cdot 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\end{pmatrix} . $$ 28.1-2 Find an $\\text{LU}$ decomposition of the matrix $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} . $$ $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 3 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & -5 & 6 \\\\ 0 & 4 & -5 \\\\ 0 & 0 & 4 \\end{pmatrix} . $$ 28.1-3 Solve the equation $$ \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} $$ by using forward substitution. We have $$ \\begin{aligned} A & = \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} , \\end{aligned} $$ and we with to solve for the unknown $x$. The $\\text{LUP}$ decomposition is $$ \\begin{aligned} L & = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} , \\\\ U & = \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} , \\\\ P & = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} . \\end{aligned} $$ Using forward substitution, we solve $Ly = Pb$ for $y$: $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 12 \\\\ 9 \\end{pmatrix} , $$ obtaining $$ y = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} $$ by computing first $y_1$, then $y_2$, and finally $y_3$. Using back substitution, we solve $Ux = y$ for $x$: $$ \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} , $$ thereby obtaining the desired answer $$ x = \\begin{pmatrix} -\\frac{3}{19} \\\\ -\\frac{1}{19} \\\\ \\frac{49}{19} \\end{pmatrix} $$ by computing first $x_3$, then $x_2$, and finally $x_1$. 28.1-4 Describe the $\\text{LUP}$ decomposition of a diagonal matrix. The $\\text{LUP}$ decomposition of a diagonal matrix $D$ is $D = IDI$ where $I$ is the identity matrix. 28.1-5 Describe the $\\text{LUP}$ decomposition of a permutation matrix $A$, and prove that it is unique. (Omit!) 28.1-6 Show that for all $n \\ge 1$, there exists a singular $n \\times n$ matrix that has an $\\text{LU}$ decomposition. The zero matrix always has an $\\text{LU}$ decomposition by taking $L$ to be any unit lower-triangular matrix and $U$ to be the zero matrix, which is upper triangular. 28.1-7 In $\\text{LU-DECOMPOSITION}$, is it necessary to perform the outermost for loop iteration when $k = n$? How about in $\\text{LUP-DECOMPOSITION}$? For $\\text{LU-DECOMPOSITION}$, it is indeed necessary. If we didn't run the line 6 of the outermost for loop, $u_{nn}$ would be left its initial value of $0$ instead of being set equal to $a_{nn}$. This can clearly produce incorrect results, because the $\\text{LU-DECOMPOSITION}$ of any non-singular matrix must have both $L$ and $U$ having positive determinant. However, if $u_{nn} = 0$, the determinant of $U$ will be $0$ by problem D.2-2. For $\\text{LUP-DECOMPOSITION}$, the iteration of the outermost for loop that occurs with $k = n$ will not change the final answer. Since $\\pi$ would have to be a permutation on a single element, it cannot be non-trivial. and the for loop on line 16 will not run at all.","title":"28.1 Solving systems of linear equations"},{"location":"Chap28/28.1/#281-1","text":"Solve the equation $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 4 & 1 & 0 \\\\ -6 & 5 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 14 \\\\ -7 \\end{pmatrix} $$ by using forward substitution. $$ \\begin{pmatrix} 3 \\\\ 14 - 4 \\cdot 3 \\\\ -7 - 5 \\cdot (14 - 4 \\cdot 3) - (-6) \\cdot 3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\\\ 1 \\end{pmatrix} . $$","title":"28.1-1"},{"location":"Chap28/28.1/#281-2","text":"Find an $\\text{LU}$ decomposition of the matrix $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} . $$ $$ \\begin{pmatrix} 4 & -5 & 6 \\\\ 8 & -6 & 7 \\\\ 12 & -7 & 12 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 3 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & -5 & 6 \\\\ 0 & 4 & -5 \\\\ 0 & 0 & 4 \\end{pmatrix} . $$","title":"28.1-2"},{"location":"Chap28/28.1/#281-3","text":"Solve the equation $$ \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} $$ by using forward substitution. We have $$ \\begin{aligned} A & = \\begin{pmatrix} 1 & 5 & 4 \\\\ 2 & 0 & 3 \\\\ 5 & 8 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 12 \\\\ 9 \\\\ 5 \\end{pmatrix} , \\end{aligned} $$ and we with to solve for the unknown $x$. The $\\text{LUP}$ decomposition is $$ \\begin{aligned} L & = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} , \\\\ U & = \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} , \\\\ P & = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} . \\end{aligned} $$ Using forward substitution, we solve $Ly = Pb$ for $y$: $$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0.2 & 1 & 0 \\\\ 0.4 & -\\frac{3.2}{3.4} & 1 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 12 \\\\ 9 \\end{pmatrix} , $$ obtaining $$ y = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} $$ by computing first $y_1$, then $y_2$, and finally $y_3$. Using back substitution, we solve $Ux = y$ for $x$: $$ \\begin{pmatrix} 5 & 8 & 2 \\\\ 0 & 3.4 & 3.6 \\\\ 0 & 0 & 2.2 + \\frac{11.52}{3.4} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 11 \\\\ 7 + \\frac{35.2}{3.4} \\end{pmatrix} , $$ thereby obtaining the desired answer $$ x = \\begin{pmatrix} -\\frac{3}{19} \\\\ -\\frac{1}{19} \\\\ \\frac{49}{19} \\end{pmatrix} $$ by computing first $x_3$, then $x_2$, and finally $x_1$.","title":"28.1-3"},{"location":"Chap28/28.1/#281-4","text":"Describe the $\\text{LUP}$ decomposition of a diagonal matrix. The $\\text{LUP}$ decomposition of a diagonal matrix $D$ is $D = IDI$ where $I$ is the identity matrix.","title":"28.1-4"},{"location":"Chap28/28.1/#281-5","text":"Describe the $\\text{LUP}$ decomposition of a permutation matrix $A$, and prove that it is unique. (Omit!)","title":"28.1-5"},{"location":"Chap28/28.1/#281-6","text":"Show that for all $n \\ge 1$, there exists a singular $n \\times n$ matrix that has an $\\text{LU}$ decomposition. The zero matrix always has an $\\text{LU}$ decomposition by taking $L$ to be any unit lower-triangular matrix and $U$ to be the zero matrix, which is upper triangular.","title":"28.1-6"},{"location":"Chap28/28.1/#281-7","text":"In $\\text{LU-DECOMPOSITION}$, is it necessary to perform the outermost for loop iteration when $k = n$? How about in $\\text{LUP-DECOMPOSITION}$? For $\\text{LU-DECOMPOSITION}$, it is indeed necessary. If we didn't run the line 6 of the outermost for loop, $u_{nn}$ would be left its initial value of $0$ instead of being set equal to $a_{nn}$. This can clearly produce incorrect results, because the $\\text{LU-DECOMPOSITION}$ of any non-singular matrix must have both $L$ and $U$ having positive determinant. However, if $u_{nn} = 0$, the determinant of $U$ will be $0$ by problem D.2-2. For $\\text{LUP-DECOMPOSITION}$, the iteration of the outermost for loop that occurs with $k = n$ will not change the final answer. Since $\\pi$ would have to be a permutation on a single element, it cannot be non-trivial. and the for loop on line 16 will not run at all.","title":"28.1-7"},{"location":"Chap28/28.2/","text":"28.2-1 Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $S(n)$ denote the time required to square an $n \\times n$ matrix. Show that multiplying and squaring matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time squaring algorithm, and an $S(n)$-time squaring algorithm implies an $O(S(n))$-time matrix-multiplication algorithm. Showing that being able to multiply matrices in time $M(n)$ implies being able to square matrices in time $M(n)$ is trivial because squaring a matrix is just multiplying it by itself. The more tricky direction is showing that being able to square matrices in time $S(n)$ implies being able to multiply matrices in time $O(S(n))$. As we do this, we apply the same regularity condition that $S(2n) \\in O(S(n))$. Suppose that we are trying to multiply the matrices, $A$ and $B$, that is, find $AB$. Then, define the matrix $$ C = \\begin{pmatrix} I & A \\\\ 0 & B \\end{pmatrix} $$ Then, we can find $C^2$ in time $S(2n) \\in O(S(n))$. Since $$ C^2 = \\begin{pmatrix} I & A + AB \\\\ 0 & B \\end{pmatrix} $$ Then we can just take the upper right quarter of $C^2$ and subtract $A$ from it to obtain the desired result. Apart from the squaring, we've only done work that is $O(n^2)$. Since $S(n)$ is $\\Omega(n^2)$ anyways, we have that the total amount of work we've done is $O(n^2)$. 28.2-2 Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $L(n)$ be the time to compute the LUP decomposition of an $n \\times n$ matrix. Show that multiplying matrices and computing LUP decompositions of matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time LUP-decomposition algorithm, and an $L(n)$-time LUP-decomposition algorithm implies an $O(L(n))$-time matrix-multiplication algorithm. Let $A$ be an $n \\times n$ matrix. Without loss of generality we'll assume $n = 2^k$, and impose the regularity condition that $L(n / 2) \\le cL(n)$ where $c < 1 / 2$ and $L(n)$ is the time it takes to find an LUP decomposition of an $n \\times n$ matrix. First, decompose $A$ as $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} $$ where $A_1$ is $n / 2$ by $n$. Let $A_1 = L_1U_1P_1$ be an LUP decomposition of $A_1$, where $L_1$ is $ / 2$ by $n / 2$, $U_1$ is $n / 2$ by $n$, and $P_1$ is $n$ by $n$. Perform a block decomposition of $U_1$ and $A_2P_1^{-1}$ as $U_1 = [\\overline U_1|B]$ and $A_2P_1^{-1} = [C|D]$ where $\\overline U_1$ and $C$ are $n / 2$ by $n / 2$ matrices. Since we assume that $A$ is nonsingular, $\\overline U_1$ must also be nonsingular. Set $F = D - C\\overline U_1^{-1}B$. Then we have $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & I_{n / 2} \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & B \\\\ 0 & F \\end{pmatrix} P_1. $$ Now let $F = L_2U_2P_2$ be an LUP decomposition of $F$, and let $\\overline P = \\begin{pmatrix} I_{n / 2} & 0 \\\\ 0 & P_2 \\end{pmatrix}$. Then we may write $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & L_2 \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & BP_2^{-1} \\\\ 0 & U_2 \\end{pmatrix} \\overline P P_1. $$ This is an LUP decomposition of $A$. To achieve it, we computed two LUP decompositions of half size, a constant number of matrix multiplications, and a constant number of matrix inversions. Since matrix inversion and multiplication are computationally equivalent, we conclude that the runtime is $O(M(n))$. 28.2-3 Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $D(n)$ denote the time required to find the determinant of an $n \\times n$ matrix. Show that multiplying matrices and computing the determinant have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time determinant algorithm, and a $D(n)$-time determinant algorithm implies an $O(D(n))$-time matrix-multiplication algorithm. (Omit!) 28.2-4 Let $M(n)$ be the time to multiply two $n \\times n$ boolean matrices, and let $T(n)$ be the time to find the transitive closure of an $n \\times n$ boolean matrix. (See Section 25.2.) Show that an $M(n)$-time boolean matrix-multiplication algorithm implies an $O(M(n)\\lg n)$-time transitive-closure algorithm, and a $T(n)$-time transitive-closure algorithm implies an $O(T(n))$-time boolean matrix-multiplication algorithm. Suppose we can multiply boolean matrices in $M(n)$ time, where we assume this means that if we're multiplying boolean matrices $A$ and $B$, then $(AB)_{ij} = (a_{i1} \\wedge b_{1j}) \\vee \\dots \\vee (a_{in} \\wedge b_{nj})$. To find the transitive closure of a boolean matrix $A$ we just need to find the $n^{\\text{th}}$ power of $A$. We can do this by computing $A^2$, then $(A^2)^2$, then $((A^2)^2)^2$ and so on. This requires only $\\lg n$ multiplications, so the transitive closure can be computed in $O(M(n)\\lg n)$. For the other direction, first view $A$ and $B$ as adjacency matrices, and impose the regularity condition $T(3n) = O(T(n))$, where $T(n)$ is the time to compute the transitive closure of a graph on $n$ vertices. We will define a new graph whose transitive closure matrix contains the boolean product of $A$ and $B$. Start by placing $3n$ vertices down, labeling them $1, 2, \\dots, n, 1', 2', \\dots, n', 1'', 2'', \\dots, n''$. Connect vertex $i$ to vertex $j'$ if and only if $A_{ij} = 1$. Connect vertex $j'$ to vertex $k''$ if and only if $B_{jk} = 1$. In the resulting graph, the only way to get from the first set of $n$ vertices to the third set is to first take an edge which \"looks like\" an edge in $A$, then take an edge which \"looks like\" an edge in $B$. In particular, the transitive closure of this graph is: $$ \\begin{pmatrix} I & A & AB \\\\ 0 & I & B \\\\ 0 & 0 & I \\end{pmatrix} . $$ Since the graph is only of size $3n$, computing its transitive closure can be done in $O(T(3n)) = O(T(n))$ by the regularity condition. Therefore multiplying matrices and finding transitive closure are are equally hard. 28.2-5 Does the matrix-inversion algorithm based on Theorem 28.2 work when matrix elements are drawn from the field of integers modulo $2$? Explain. It does not work necessarily over the field of two elements. The problem comes in in applying theorem D.6 to conclude that $A^{\\text T}A$ is positive definite. In the proof of that theorem they obtain that $||Ax||^2 \\ge 0$ and only zero if every entry of $Ax$ is zero. This second part is not true over the field with two elements, all that would be required is that there is an even number of ones in $Ax$. This means that we can only say that $A^{\\text T}A$ is positive semi-definite instead of the positive definiteness that the algorithm requires. 28.2-6 $\\star$ Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices of complex numbers, and prove that your generalization works correctly. ($\\textit{Hint:}$ Instead of the transpose of $A$, use the conjugate transpose $A^*$, which you obtain from the transpose of $A$ by replacing every entry with its complex conjugate. Instead of symmetric matrices, consider Hermitian matrices, which are matrices $A$ such that $A = A^*$.) We may again assume that our matrix is a power of $2$, this time with complex entries. For the moment we assume our matrix $A$ is Hermitian and positivedefinite. The proof goes through exactly as before, with matrix transposes replaced by conjugate transposes, and using the fact that Hermitian positivedefinite matrices are invertible. Finally, we need to justify that we can obtain the same asymptotic running time for matrix multiplication as for matrix inversion when $A$ is invertible, but not Hermitian positive-definite. For any nonsingular matrix $A$, the matrix $A^*A$ is Hermitian and positive definite, since for any $x$ we have $x^*A^*Ax = \\langle Ax, Ax \\rangle > 0$ by the definition of inner product. To invert $A$, we first compute $(A^*A)^{-1} = A^{-1}(A^*)^{-1}$. Then we need only multiply this result on the right by $A^*$. Each of these steps takes $O(M(n))$ time, so we can invert any nonsingluar matrix with complex entries in $O(M(n))$ time.","title":"28.2 Inverting matrices"},{"location":"Chap28/28.2/#282-1","text":"Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $S(n)$ denote the time required to square an $n \\times n$ matrix. Show that multiplying and squaring matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time squaring algorithm, and an $S(n)$-time squaring algorithm implies an $O(S(n))$-time matrix-multiplication algorithm. Showing that being able to multiply matrices in time $M(n)$ implies being able to square matrices in time $M(n)$ is trivial because squaring a matrix is just multiplying it by itself. The more tricky direction is showing that being able to square matrices in time $S(n)$ implies being able to multiply matrices in time $O(S(n))$. As we do this, we apply the same regularity condition that $S(2n) \\in O(S(n))$. Suppose that we are trying to multiply the matrices, $A$ and $B$, that is, find $AB$. Then, define the matrix $$ C = \\begin{pmatrix} I & A \\\\ 0 & B \\end{pmatrix} $$ Then, we can find $C^2$ in time $S(2n) \\in O(S(n))$. Since $$ C^2 = \\begin{pmatrix} I & A + AB \\\\ 0 & B \\end{pmatrix} $$ Then we can just take the upper right quarter of $C^2$ and subtract $A$ from it to obtain the desired result. Apart from the squaring, we've only done work that is $O(n^2)$. Since $S(n)$ is $\\Omega(n^2)$ anyways, we have that the total amount of work we've done is $O(n^2)$.","title":"28.2-1"},{"location":"Chap28/28.2/#282-2","text":"Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $L(n)$ be the time to compute the LUP decomposition of an $n \\times n$ matrix. Show that multiplying matrices and computing LUP decompositions of matrices have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time LUP-decomposition algorithm, and an $L(n)$-time LUP-decomposition algorithm implies an $O(L(n))$-time matrix-multiplication algorithm. Let $A$ be an $n \\times n$ matrix. Without loss of generality we'll assume $n = 2^k$, and impose the regularity condition that $L(n / 2) \\le cL(n)$ where $c < 1 / 2$ and $L(n)$ is the time it takes to find an LUP decomposition of an $n \\times n$ matrix. First, decompose $A$ as $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} $$ where $A_1$ is $n / 2$ by $n$. Let $A_1 = L_1U_1P_1$ be an LUP decomposition of $A_1$, where $L_1$ is $ / 2$ by $n / 2$, $U_1$ is $n / 2$ by $n$, and $P_1$ is $n$ by $n$. Perform a block decomposition of $U_1$ and $A_2P_1^{-1}$ as $U_1 = [\\overline U_1|B]$ and $A_2P_1^{-1} = [C|D]$ where $\\overline U_1$ and $C$ are $n / 2$ by $n / 2$ matrices. Since we assume that $A$ is nonsingular, $\\overline U_1$ must also be nonsingular. Set $F = D - C\\overline U_1^{-1}B$. Then we have $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & I_{n / 2} \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & B \\\\ 0 & F \\end{pmatrix} P_1. $$ Now let $F = L_2U_2P_2$ be an LUP decomposition of $F$, and let $\\overline P = \\begin{pmatrix} I_{n / 2} & 0 \\\\ 0 & P_2 \\end{pmatrix}$. Then we may write $$ A = \\begin{pmatrix} L_1 & 0 \\\\ C\\overline U_1^{-1} & L_2 \\end{pmatrix} \\begin{pmatrix} \\overline U_1 & BP_2^{-1} \\\\ 0 & U_2 \\end{pmatrix} \\overline P P_1. $$ This is an LUP decomposition of $A$. To achieve it, we computed two LUP decompositions of half size, a constant number of matrix multiplications, and a constant number of matrix inversions. Since matrix inversion and multiplication are computationally equivalent, we conclude that the runtime is $O(M(n))$.","title":"28.2-2"},{"location":"Chap28/28.2/#282-3","text":"Let $M(n)$ be the time to multiply two $n \\times n$ matrices, and let $D(n)$ denote the time required to find the determinant of an $n \\times n$ matrix. Show that multiplying matrices and computing the determinant have essentially the same difficulty: an $M(n)$-time matrix-multiplication algorithm implies an $O(M(n))$-time determinant algorithm, and a $D(n)$-time determinant algorithm implies an $O(D(n))$-time matrix-multiplication algorithm. (Omit!)","title":"28.2-3"},{"location":"Chap28/28.2/#282-4","text":"Let $M(n)$ be the time to multiply two $n \\times n$ boolean matrices, and let $T(n)$ be the time to find the transitive closure of an $n \\times n$ boolean matrix. (See Section 25.2.) Show that an $M(n)$-time boolean matrix-multiplication algorithm implies an $O(M(n)\\lg n)$-time transitive-closure algorithm, and a $T(n)$-time transitive-closure algorithm implies an $O(T(n))$-time boolean matrix-multiplication algorithm. Suppose we can multiply boolean matrices in $M(n)$ time, where we assume this means that if we're multiplying boolean matrices $A$ and $B$, then $(AB)_{ij} = (a_{i1} \\wedge b_{1j}) \\vee \\dots \\vee (a_{in} \\wedge b_{nj})$. To find the transitive closure of a boolean matrix $A$ we just need to find the $n^{\\text{th}}$ power of $A$. We can do this by computing $A^2$, then $(A^2)^2$, then $((A^2)^2)^2$ and so on. This requires only $\\lg n$ multiplications, so the transitive closure can be computed in $O(M(n)\\lg n)$. For the other direction, first view $A$ and $B$ as adjacency matrices, and impose the regularity condition $T(3n) = O(T(n))$, where $T(n)$ is the time to compute the transitive closure of a graph on $n$ vertices. We will define a new graph whose transitive closure matrix contains the boolean product of $A$ and $B$. Start by placing $3n$ vertices down, labeling them $1, 2, \\dots, n, 1', 2', \\dots, n', 1'', 2'', \\dots, n''$. Connect vertex $i$ to vertex $j'$ if and only if $A_{ij} = 1$. Connect vertex $j'$ to vertex $k''$ if and only if $B_{jk} = 1$. In the resulting graph, the only way to get from the first set of $n$ vertices to the third set is to first take an edge which \"looks like\" an edge in $A$, then take an edge which \"looks like\" an edge in $B$. In particular, the transitive closure of this graph is: $$ \\begin{pmatrix} I & A & AB \\\\ 0 & I & B \\\\ 0 & 0 & I \\end{pmatrix} . $$ Since the graph is only of size $3n$, computing its transitive closure can be done in $O(T(3n)) = O(T(n))$ by the regularity condition. Therefore multiplying matrices and finding transitive closure are are equally hard.","title":"28.2-4"},{"location":"Chap28/28.2/#282-5","text":"Does the matrix-inversion algorithm based on Theorem 28.2 work when matrix elements are drawn from the field of integers modulo $2$? Explain. It does not work necessarily over the field of two elements. The problem comes in in applying theorem D.6 to conclude that $A^{\\text T}A$ is positive definite. In the proof of that theorem they obtain that $||Ax||^2 \\ge 0$ and only zero if every entry of $Ax$ is zero. This second part is not true over the field with two elements, all that would be required is that there is an even number of ones in $Ax$. This means that we can only say that $A^{\\text T}A$ is positive semi-definite instead of the positive definiteness that the algorithm requires.","title":"28.2-5"},{"location":"Chap28/28.2/#282-6-star","text":"Generalize the matrix-inversion algorithm of Theorem 28.2 to handle matrices of complex numbers, and prove that your generalization works correctly. ($\\textit{Hint:}$ Instead of the transpose of $A$, use the conjugate transpose $A^*$, which you obtain from the transpose of $A$ by replacing every entry with its complex conjugate. Instead of symmetric matrices, consider Hermitian matrices, which are matrices $A$ such that $A = A^*$.) We may again assume that our matrix is a power of $2$, this time with complex entries. For the moment we assume our matrix $A$ is Hermitian and positivedefinite. The proof goes through exactly as before, with matrix transposes replaced by conjugate transposes, and using the fact that Hermitian positivedefinite matrices are invertible. Finally, we need to justify that we can obtain the same asymptotic running time for matrix multiplication as for matrix inversion when $A$ is invertible, but not Hermitian positive-definite. For any nonsingular matrix $A$, the matrix $A^*A$ is Hermitian and positive definite, since for any $x$ we have $x^*A^*Ax = \\langle Ax, Ax \\rangle > 0$ by the definition of inner product. To invert $A$, we first compute $(A^*A)^{-1} = A^{-1}(A^*)^{-1}$. Then we need only multiply this result on the right by $A^*$. Each of these steps takes $O(M(n))$ time, so we can invert any nonsingluar matrix with complex entries in $O(M(n))$ time.","title":"28.2-6 $\\star$"},{"location":"Chap28/28.3/","text":"28.3-1 Prove that every diagonal element of a symmetric positive-definite matrix is positive. To see this, let $e_i$ be the vector that is $0$s except for a $1$ in the $i$th position. Then, we consider the quantity $e_i^{\\text T}Ae_i$ for every $i$. $Ae_i$ takes each row of $A$ and pulls out the $i$th column of it, and puts those values into a column vector. Then, we multiply that on the left by $e_i^{\\text T}$, pulls out the $i$th row of this quantity, which means that the quantity $e_i^{\\text T}Ae_i$ exactly the value of $A_{i, i}$. So, we have that by positive definiteness, since $e_i$ is nonzero, that quantity must be positive. Since we do this for every $i$, we have that every entry along the diagonal must be positive. 28.3-2 Let $$ A = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix} $$ be a $2 \\times 2$ symmetrix positive-definite matrix. Prove that its determinant $ac - b^2$ is positive by \"completing the square\" in a manner similar to that used in the proof of Lemma 28.5. Let $x = -by / a$. Since $A$ is positive-definite, we have $$ \\begin{aligned} 0 & < \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} A \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\ & = \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} \\begin{pmatrix} ax + by \\\\ bx + cy \\end{pmatrix} \\\\ & = ax^2 + 2bxy + cy^2 \\\\ & = cy^2 - \\frac{b^2y^2}{a} \\\\ & = (c - b^2 / a)y^2. \\end{aligned} $$ Thus, $c - b^2 / a > 0$, which implies $ac - b^2 > 0$, since $a > 0$. 28.3-3 Prove that the maximum element in a symmetric positive-definite matrix lies on the diagonal. Suppose to a contradiction that there were some element $a_{ij}$ with $i \\ne j$ so that $a_{ij}$ were a largest element. We will use $e_i$ to denote the vector that is all zeroes except for having a $1$ at position $i$. Then, we consider the value $(e_i \u2212 e_j)^{\\text T} A(e_i \u2212 e_j)$. When we compute $A(e_i - e_j)$ this will return a vector which is column $i$ minus column $j$. Then, when we do the last multiplication, we will get the quantity which is the $i$th row minus the $j$th row. So, $$ \\begin{aligned} (e_i - e_j)^{\\text T} A(e_i - e_j) & = a_{ii} - a_{ij} - a_{ji} + a_{jj} \\\\ & = a_{ii} + a_{jj} - 2a_{ij} \\le 0 \\end{aligned} $$ Where we used symmetry to get that $a_{ij} = a_{ji}$. This result contradicts the fact that $A$ was positive definite. So, our assumption that there was a element tied for largest off the diagonal must of been false. 28.3-4 Prove that the determinant of each leading submatrix of a symmetrix positive-definite matrix is positive. The claim clearly holds for matrices of size $1$ because the single entry in the matrix is positive the only leading submatrix is the matrix itself. Now suppose the claim holds for matrices of size $n$, and let $A$ be an $(n + 1) \\times (n + 1)$ symmetric positive-definite matrix. We can write $A$ as $$ A = \\left[ \\begin{array}{ccc|c} & & & \\\\ & A' & & w \\\\ & & & \\\\ \\hline & v & & c \\end{array} \\right] . $$ Then $A'$ is clearly symmetric, and for any $x$ we have $x^{\\text T} A'x = \\begin{pmatrix} x & 0 \\end{pmatrix} A \\begin{pmatrix} x \\\\ 0 \\end{pmatrix} > 0$, so $A'$ is positive-definite. By our induction hypothesis, every leading submatrix of $A'$ has positive determinant, so we are left only to show that $A$ has positive determinant. By Theorem D.4, the determinant of $A$ is equal to the determinant of the matrix $$ B = \\left[ \\begin{array}{c|ccc} c & & v & \\\\ \\hline & & & \\\\ w & & A' & \\\\ & & & \\end{array} \\right] . $$ Theorem D.4 also tells us that the determinant is unchanged if we add a multiple of one column of a matrix to another. Since $0 < e_{n + 1}^{\\text T} Ae_{n + 1} = c$, we can use multiples of the first column to zero out every entry in the first row other than $c$. Specifically, the determinant of $B$ is the same as the determinant of the matrix obtained in this way, which looks like $$ C = \\left[ \\begin{array}{c|ccc} c & & 0 & \\\\ \\hline & & & \\\\ w & & A'' & \\\\ & & & \\end{array} \\right] . $$ By definition, $\\det(A) = c\\det(A'')$. By our induction hypothesis, $\\det(A'') > 0$. Since $c > 0$ as well, we conclude that $\\det(A) > 0$, which completes the proof. 28.3-5 Let $A_k$ denote the $k$th leading submatrix of a symmetric positive-definite matrix $A$. Prove that $\\text{det}(A_k) / \\text{det}(A_{k - 1})$ is the $k$th pivot during $\\text{LU}$ decomposition, where, by convention, $\\text{det}(A_0) = 1$. When we do an LU decomposition of a positive definite symmetric matrix, we never need to permute the rows. This means that the pivot value being used from the first operation is the entry in the upper left corner. This gets us that for the case $k = 1$, it holds because we were told to define $\\det(A_0) = 1$, getting us, $a_{11} = \\det(A_1) / \\det(A_0)$. When Diagonalizing a matrix, the product of the pivot values used gives the determinant of the matrix. So, we have that the determinant of $A_k$ is a product of the $k$th pivot value with all the previous values. By induction, the product of all the previous values is $\\det(A_{k \u2212 1})$. So, we have that if $x$ is the $k$th pivot value, $\\det(A_k) = x\\det(A_{k \u2212 1})$, giving us the desired result that the $k$th pivot value is $\\det(A_k) / \\det(A_{k \u2212 1})$. 28.3-6 Find the function of the form $$F(x) = c_1 + c_2x\\lg x + c_3 e^x$$ that is the best least-squares fit to the data points $$(1, 1), (2, 1), (3, 3), (4, 8).$$ First we form the $A$ matrix $$ A = \\begin{pmatrix} 1 & 0 & e \\\\ 1 & 2 & e^2 \\\\ 1 & 3\\lg 3 & e^3 \\\\ 1 & 8 & e^4 \\end{pmatrix} . $$ We compute the pseudoinverse, then multiply it by $y$, to obtain the coefficient vector $$ c = \\begin{pmatrix} 0.411741 \\\\ -0.20487 \\\\ 0.16546 \\end{pmatrix} . $$ 28.3-7 Show that the pseudoinverse $A^+$ satisfies the following four equations: $$ \\begin{aligned} AA^+A & = A, \\\\ A^+AA^+ & = A^+, \\\\ (AA^+)^{\\text T} & = AA^+, \\\\ (A^+A)^{\\text T} & = A^+A. \\end{aligned} $$ $$ \\begin{aligned} AA^+A & = A((A^{\\text T}A)^{-1}A^{\\text T})A \\\\ & = A(A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A, \\end{aligned} $$ $$ \\begin{aligned} A^+AA^+ & = ((A^{\\text T}A)^{-1}A^{\\text T})A((A^{\\text T}A)^{-1}A^{\\text T}) \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A)(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = (A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = A^+, \\end{aligned} $$ $$ \\begin{aligned} (AA^+)^{\\text T} & = (A(A^{\\text T}A)^{-1}A^{\\text T})^{\\text T} \\\\ & = A((A^{\\text T}A)^{-1})^{\\text T}A^{\\text T} \\\\ & = A((A^{\\text T}A)^{\\text T})^{-1}A^{\\text T} \\\\ & = A(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = AA^+, \\end{aligned} $$ $$ \\begin{aligned} (A^+A)^{\\text T} & = ((A^{\\text T}A)^{-1}A^{\\text T}A)^{\\text T} \\\\ & = ((A^{\\text T}A)^{-1}(A^{\\text T}A))^{\\text T} \\\\ & = I^{\\text T} \\\\ & = I \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A^+A. \\end{aligned} $$","title":"28.3 Symmetric positive-definite matrices and least-squares approximation"},{"location":"Chap28/28.3/#283-1","text":"Prove that every diagonal element of a symmetric positive-definite matrix is positive. To see this, let $e_i$ be the vector that is $0$s except for a $1$ in the $i$th position. Then, we consider the quantity $e_i^{\\text T}Ae_i$ for every $i$. $Ae_i$ takes each row of $A$ and pulls out the $i$th column of it, and puts those values into a column vector. Then, we multiply that on the left by $e_i^{\\text T}$, pulls out the $i$th row of this quantity, which means that the quantity $e_i^{\\text T}Ae_i$ exactly the value of $A_{i, i}$. So, we have that by positive definiteness, since $e_i$ is nonzero, that quantity must be positive. Since we do this for every $i$, we have that every entry along the diagonal must be positive.","title":"28.3-1"},{"location":"Chap28/28.3/#283-2","text":"Let $$ A = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix} $$ be a $2 \\times 2$ symmetrix positive-definite matrix. Prove that its determinant $ac - b^2$ is positive by \"completing the square\" in a manner similar to that used in the proof of Lemma 28.5. Let $x = -by / a$. Since $A$ is positive-definite, we have $$ \\begin{aligned} 0 & < \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} A \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\ & = \\begin{pmatrix} x & y \\end{pmatrix}^{\\text T} \\begin{pmatrix} ax + by \\\\ bx + cy \\end{pmatrix} \\\\ & = ax^2 + 2bxy + cy^2 \\\\ & = cy^2 - \\frac{b^2y^2}{a} \\\\ & = (c - b^2 / a)y^2. \\end{aligned} $$ Thus, $c - b^2 / a > 0$, which implies $ac - b^2 > 0$, since $a > 0$.","title":"28.3-2"},{"location":"Chap28/28.3/#283-3","text":"Prove that the maximum element in a symmetric positive-definite matrix lies on the diagonal. Suppose to a contradiction that there were some element $a_{ij}$ with $i \\ne j$ so that $a_{ij}$ were a largest element. We will use $e_i$ to denote the vector that is all zeroes except for having a $1$ at position $i$. Then, we consider the value $(e_i \u2212 e_j)^{\\text T} A(e_i \u2212 e_j)$. When we compute $A(e_i - e_j)$ this will return a vector which is column $i$ minus column $j$. Then, when we do the last multiplication, we will get the quantity which is the $i$th row minus the $j$th row. So, $$ \\begin{aligned} (e_i - e_j)^{\\text T} A(e_i - e_j) & = a_{ii} - a_{ij} - a_{ji} + a_{jj} \\\\ & = a_{ii} + a_{jj} - 2a_{ij} \\le 0 \\end{aligned} $$ Where we used symmetry to get that $a_{ij} = a_{ji}$. This result contradicts the fact that $A$ was positive definite. So, our assumption that there was a element tied for largest off the diagonal must of been false.","title":"28.3-3"},{"location":"Chap28/28.3/#283-4","text":"Prove that the determinant of each leading submatrix of a symmetrix positive-definite matrix is positive. The claim clearly holds for matrices of size $1$ because the single entry in the matrix is positive the only leading submatrix is the matrix itself. Now suppose the claim holds for matrices of size $n$, and let $A$ be an $(n + 1) \\times (n + 1)$ symmetric positive-definite matrix. We can write $A$ as $$ A = \\left[ \\begin{array}{ccc|c} & & & \\\\ & A' & & w \\\\ & & & \\\\ \\hline & v & & c \\end{array} \\right] . $$ Then $A'$ is clearly symmetric, and for any $x$ we have $x^{\\text T} A'x = \\begin{pmatrix} x & 0 \\end{pmatrix} A \\begin{pmatrix} x \\\\ 0 \\end{pmatrix} > 0$, so $A'$ is positive-definite. By our induction hypothesis, every leading submatrix of $A'$ has positive determinant, so we are left only to show that $A$ has positive determinant. By Theorem D.4, the determinant of $A$ is equal to the determinant of the matrix $$ B = \\left[ \\begin{array}{c|ccc} c & & v & \\\\ \\hline & & & \\\\ w & & A' & \\\\ & & & \\end{array} \\right] . $$ Theorem D.4 also tells us that the determinant is unchanged if we add a multiple of one column of a matrix to another. Since $0 < e_{n + 1}^{\\text T} Ae_{n + 1} = c$, we can use multiples of the first column to zero out every entry in the first row other than $c$. Specifically, the determinant of $B$ is the same as the determinant of the matrix obtained in this way, which looks like $$ C = \\left[ \\begin{array}{c|ccc} c & & 0 & \\\\ \\hline & & & \\\\ w & & A'' & \\\\ & & & \\end{array} \\right] . $$ By definition, $\\det(A) = c\\det(A'')$. By our induction hypothesis, $\\det(A'') > 0$. Since $c > 0$ as well, we conclude that $\\det(A) > 0$, which completes the proof.","title":"28.3-4"},{"location":"Chap28/28.3/#283-5","text":"Let $A_k$ denote the $k$th leading submatrix of a symmetric positive-definite matrix $A$. Prove that $\\text{det}(A_k) / \\text{det}(A_{k - 1})$ is the $k$th pivot during $\\text{LU}$ decomposition, where, by convention, $\\text{det}(A_0) = 1$. When we do an LU decomposition of a positive definite symmetric matrix, we never need to permute the rows. This means that the pivot value being used from the first operation is the entry in the upper left corner. This gets us that for the case $k = 1$, it holds because we were told to define $\\det(A_0) = 1$, getting us, $a_{11} = \\det(A_1) / \\det(A_0)$. When Diagonalizing a matrix, the product of the pivot values used gives the determinant of the matrix. So, we have that the determinant of $A_k$ is a product of the $k$th pivot value with all the previous values. By induction, the product of all the previous values is $\\det(A_{k \u2212 1})$. So, we have that if $x$ is the $k$th pivot value, $\\det(A_k) = x\\det(A_{k \u2212 1})$, giving us the desired result that the $k$th pivot value is $\\det(A_k) / \\det(A_{k \u2212 1})$.","title":"28.3-5"},{"location":"Chap28/28.3/#283-6","text":"Find the function of the form $$F(x) = c_1 + c_2x\\lg x + c_3 e^x$$ that is the best least-squares fit to the data points $$(1, 1), (2, 1), (3, 3), (4, 8).$$ First we form the $A$ matrix $$ A = \\begin{pmatrix} 1 & 0 & e \\\\ 1 & 2 & e^2 \\\\ 1 & 3\\lg 3 & e^3 \\\\ 1 & 8 & e^4 \\end{pmatrix} . $$ We compute the pseudoinverse, then multiply it by $y$, to obtain the coefficient vector $$ c = \\begin{pmatrix} 0.411741 \\\\ -0.20487 \\\\ 0.16546 \\end{pmatrix} . $$","title":"28.3-6"},{"location":"Chap28/28.3/#283-7","text":"Show that the pseudoinverse $A^+$ satisfies the following four equations: $$ \\begin{aligned} AA^+A & = A, \\\\ A^+AA^+ & = A^+, \\\\ (AA^+)^{\\text T} & = AA^+, \\\\ (A^+A)^{\\text T} & = A^+A. \\end{aligned} $$ $$ \\begin{aligned} AA^+A & = A((A^{\\text T}A)^{-1}A^{\\text T})A \\\\ & = A(A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A, \\end{aligned} $$ $$ \\begin{aligned} A^+AA^+ & = ((A^{\\text T}A)^{-1}A^{\\text T})A((A^{\\text T}A)^{-1}A^{\\text T}) \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A)(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = (A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = A^+, \\end{aligned} $$ $$ \\begin{aligned} (AA^+)^{\\text T} & = (A(A^{\\text T}A)^{-1}A^{\\text T})^{\\text T} \\\\ & = A((A^{\\text T}A)^{-1})^{\\text T}A^{\\text T} \\\\ & = A((A^{\\text T}A)^{\\text T})^{-1}A^{\\text T} \\\\ & = A(A^{\\text T}A)^{-1}A^{\\text T} \\\\ & = AA^+, \\end{aligned} $$ $$ \\begin{aligned} (A^+A)^{\\text T} & = ((A^{\\text T}A)^{-1}A^{\\text T}A)^{\\text T} \\\\ & = ((A^{\\text T}A)^{-1}(A^{\\text T}A))^{\\text T} \\\\ & = I^{\\text T} \\\\ & = I \\\\ & = (A^{\\text T}A)^{-1}(A^{\\text T}A) \\\\ & = A^+A. \\end{aligned} $$","title":"28.3-7"},{"location":"Chap28/Problems/28-1/","text":"Consider the tridiagonal matrix $$ A = \\begin{pmatrix} 1 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} . $$ a. Find an $\\text{LU}$ decomposition of $A$. b. Solve the equation $Ax = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\end{pmatrix}^{\\text T}$ by using forward and back substitution. c. Find the inverse of $A$. d. Show how, for any $n \\times n$ symmetric positive-definite, tridiagonal matrix $A$ and any $n$-vector $b$, to solve the equation $Ax = b$ in $O(n)$ time by performing an $\\text{LU}$ decomposition. Argue that any method based on forming $A^{-1}$ is asymptotically more expensive in the worst case. e. Show how, for any $n \\times n$ nonsingular, tridiagonal matrix $A$ and any $n$-vector $b$, to solve the equation $Ax = b$ in $O(n)$ time by performing an $\\text{LUP}$ decomposition. a. $$ \\begin{aligned} L & = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ -1 & 1 & 0 & 0 & 0 \\\\ 0 & -1 & 1 & 0 & 0 \\\\ 0 & 0 & -1 & 1 & 0 \\\\ 0 & 0 & 0 & -1 & 1 \\end{pmatrix} , \\\\ U & = \\begin{pmatrix} 1 & -1 & 0 & 0 & 0 \\\\ 0 & 1 & -1 & 0 & 0 \\\\ 0 & 0 & 1 & -1 & 0 \\\\ 0 & 0 & 0 & 1 & -1 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix} , \\\\ P & = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix} . \\end{aligned} $$ b. We first do back substitution to obtain that $$ Ux = \\begin{pmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{pmatrix} . $$ By forward substitution, we have that $$ x = \\begin{pmatrix} 5 \\\\ 9 \\\\ 12 \\\\ 14 \\\\ 15 \\end{pmatrix} . $$ c. We will set $Ax = e_i$ for each $i$, where $e_i$ is the vector that is all zeroes except for a one in the $i$th position. Then, we will just concatenate all of these solutions together to get the desired inverse. $$ \\begin{array}{|c|c|} \\hline \\text{equation} & \\text{solution} \\\\ \\hline Ax_1 = e_1 & x_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\\\ \\hline Ax_2 = e_2 & x_2 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 2 \\\\ 2 \\end{pmatrix} \\\\ \\hline Ax_3 = e_3 & x_3 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 3 \\\\ 3 \\end{pmatrix} \\\\ \\hline Ax_4 = e_4 & x_4 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 4 \\end{pmatrix} \\\\ \\hline Ax_5 = e_5 & x_5 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} \\\\ \\hline \\end{array} $$ Thus, $$ A^{-1} = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 2 & 2 & 2 & 2 \\\\ 1 & 2 & 3 & 3 & 3 \\\\ 1 & 2 & 3 & 4 & 4 \\\\ 1 & 2 & 3 & 4 & 5 \\end{pmatrix} . $$ d. When performing the LU decomposition, we only need to take the max over at most two different rows, so the loop on line 7 of $\\text{LUP-DECOMPOSITION}$ drops to $O(1)$. There are only some constant number of nonzero entries in each row, so the loop on line 14 can also be reduced to being $O(1)$. Lastly, there are only some constant number of nonzero entries of the form $a_{ik}$ and $a_{kj}$. Since the square of a constant is also a constant, this means that the nested for loops on lines 16-19 also only take time $O(1)$ to run. Since the for loops on lines 3 and 5 both run $O(n)$ times and take $O(1)$ time each to run (provided we are smart to not consider a bunch of zero entries in the matrix), the total runtime can be brought down to $O(n)$. Since for a Tridiagonal matrix, it will only ever have finitely many nonzero entries in any row, we can do both the forward and back substitution each in time only $O(n)$. Since the asymptotics of performing the LU decomposition on a positive definite tridiagonal matrix is $O(n)$, and this decomposition can be used to solve the equation in time $O(n)$, the total time for solving it with this method is $O(n)$. However, to simply record the inverse of the tridiagonal matrix would take time $O(n^2)$ since there are that many entries, so, any method based on computing the inverse of the matrix would take time $\\Omega(n^2)$ which is clearly slower than the previous method. e. The runtime of our LUP decomposition algorithm drops to being $O(n)$ because we know there are only ever a constant number of nonzero entries in each row and column, as before. Once we have an LUP decomposition, we also know that that decomposition have both $L$ and $U$ having only a constant number of non-zero entries in each row and column. This means that when we perform the forward and backward substitution, we only spend a constant amount of time per entry in $x$, and so, only takes $O(n)$ time.","title":"28-1 Tridiagonal systems of linear equations"},{"location":"Chap28/Problems/28-2/","text":"A pratical method for interpolating a set of points with a curve is to use cubic splines . We are given a set $\\{(x_i, y_i): i = 0, 1, \\ldots, n\\}$ of $n + 1$ point-value pairs, where $x_0 < x_1 < \\cdots < x_n$. We wish to fit a piecewise-cubic curve (spline) $f(x)$ to the points. That is, the curve $f(x)$ is made up of $n$ cubic polynomials $f_i(x) = a_i + b_ix + c_ix^2 + d_ix^3$ for $i = 0, 1, \\ldots, n - 1$, where if $x$ falls in the range $x_i \\le x \\le x_{i + 1}$, then the value of the curve is given by $f(x) = f_i(x - x_i)$. The points $x_i$ at which the cubic polynomials are \"pasted\" together are called knots . For simplicity, we shall assume that $x_i = i$ for $i = 0, 1, \\ldots, n$. To ensure continuity of $f(x)$, we require that $$ \\begin{aligned} f(x_i) & = f_i(0) = y_i, \\\\ f(x_{i + 1}) & = f_i(1) = y_{i + 1} \\end{aligned} $$ for $i = 0, 1, \\ldots, n - 1$. To ensure that $f(x)$ is sufficiently smooth, we also insist that the first derivative be continuous at each knot: $$f'(x_{i + 1}) = f'_i(1) = f'_{i + 1}(0)$$ for $i = 0, 1, \\ldots, n - 2$. a. Suppose that for $i = 0, 1, \\ldots, n$, we are given not only the point-value pairs $\\{(x_i, y_i)\\}$ but also the first derivatives $D_i = f'(x_i)$ at each knot. Express each coefficient $a_i$, $b_i$, $c_i$ and $d_i$ in terms of the values $y_i$, $y_{i + 1}$, $D_i$, and $D_{i + 1}$. (Remember that $x_i = i$.) How quickly can we compute the $4n$ coefficients from the point-value pairs and first derivatives? The question remains of how to choose the first derivatives of $f(x)$ at the knots. One method is to require the second derivatives to be continuous at the knots: $$f''(x_{i + 1}) = f''_i(1) = f''_{i + 1}(0)$$ for $i = 0, 1, \\ldots, n - 2$. At the first and last knots, we assume that $f''(x_0) = f''_0(0) = 0$ and $f''(x_n) = f''_{n - 1}(1) = 0$; these assumptions make $f(x)$ a natural cubic spline. b. Use the continuity constraints on the second derivative to show that for $i = 1, 2, \\ldots, n - 1$, $$D_{i - 1} + 4D_i + D_{i + 1} = 3(y_{i + 1} - y_{i - 1}). \\tag{23.21}$$ c. Show that $$ \\begin{aligned} 2D_0 + D_1 & = 3(y_1 - y_0), & \\text{(28.22)} \\\\ D_{n - 1} + 2D_n & = 3(y_n - y_{n - 1}). & \\text{(28.23)} \\end{aligned} $$ d. Rewrite equations $\\text{(28.21)}$\u2013$\\text{(28.23)}$ as a matrix equation involving the vector $D = \\langle D_0, D_1, \\ldots, D_n \\rangle$ or unknowns. What attributes does the matrix in your equation have? e. Argue that a natural cubic spline can interpolate a set of $n + 1$ point-value pairs in $O(n)$ time (see Problem 28-1). f. Show how to determine a natural cubic spline that interpolates a set of $n + 1$ points $(x_i, y_i)$ satisfying $x_0 < x_1 < \\cdots < x_n$, even when $x_i$ is not necessarily equal to $i$. What matrix equation must your method solve, and how quickly does your algorithm run? a. We have $a_i = f_i(0) = y_i$ and $b_i = f_i'(0) = f'(x_i) = D_i$. Since $f_i(1) = a_i + b_i + c_i + d_i$ and $f_i'(1) = b_i + 2c_i + 3d_i$, we have $d_i = D_{i + 1} - 2y_{i + 1} + 2y_i + D_i$ which implies $c_i = 3y_{i + 1} - 3y_i - D_{i + 1} - 2D_i$. Since each coefficient can be computed in constant time from the known values, we can compute the $4n$ coefficients in linear time. b. By the continuity constraints, we have $f_i''(1) = f_{i + 1}''(0)$ which implies that $2c_i + 6d_i = 2c_{i + 1}$, or $c_i + 3d_i = c_{i + 1}$. Using our equations from above, this is equivalent to $$D_i + 2D_{i + 1} + 3y_i - 3y_{i + 1} = 3y_{i + 2} - 3y_{i + 1} - D_{i + 2} - 2D_{i + 1}.$$ Rearranging gives the desired equation $$D_i + 4D_{i + 1} + D_{i + 2} = 3(y_{i + 2} - y_i).$$ c. The condition on the left endpoint tells us that $f_0''(0) = 0$, which implies $2c_0 = 0$. By part (a), this means $3(y_1 \u2212 y_0) = 2D_0 + D_1$. The condition on the right endpoint tells us that $f_{n - 1}''(1) = 0$, which implies $c_{n - 1} + 3d_{n - 1} = 0$. By part (a), this means $3(y_n - y_{n - 1}) = D_{n - 1} + 2D_n$. d. The matrix equation has the form $AD = Y$, where $A$ is symmetric and tridiagonal. It looks like this: $$ \\begin{pmatrix} 2 & 1 & 0 & 0 & \\cdots & 0 \\\\ 1 & 4 & 1 & 0 & \\cdots & 0 \\\\ 0 & \\ddots & \\ddots & \\ddots & \\cdots & \\vdots \\\\ \\vdots & \\cdots & 1 & 4 & 1 & 0 \\\\ 0 & \\cdots & 0 & 1 & 4 & 1 \\\\ 0 & \\cdots & 0 & 0 & 1 & 2 \\\\ \\end{pmatrix} \\begin{pmatrix} D_0 \\\\ D_1 \\\\ D_2 \\\\ \\vdots \\\\ D_{n - 1} \\\\ D_n \\end{pmatrix} = \\begin{pmatrix} 3(y_1 - y_0) \\\\ 3(y_2 - y_0) \\\\ 3(y_3 - y_1) \\\\ \\vdots \\\\ 3(y_n - y_{n - 2}) \\\\ 3(y_n - y_{n - 1}) \\end{pmatrix} . $$ e. Since the matrix is symmetric and tridiagonal, Problem 28-1 (e) tells us that we can solve the equation in $O(n)$ time by performing an LUP decomposition. By part (a), once we know each $D_i$ we can compute each $f_i$ in $O(n)$ time. f. For the general case of solving the nonuniform natural cubic spline problem, we require that $f(x_{i + 1}) = f_i(x_{i + 1} \u2212 x_i) = y_{i + 1}$, $f'(x_{i + 1}) = f_i'(x_{i + 1} - x_i) = f_{i + 1}'(0)$ and $f''(x_{i + 1}) = f_i''(x_{i + 1} - x_i) = f_{i + 1}''(0)$. We can still solve for each of $a_i$, $b_i$, $c_i$ and $d_i$ in terms of $y_i$, $y_{i + 1}$, $D_i$ and $D_{i + 1}$, so we still get a tridiagonal matrix equation. The solution will be slightly messier, but ultimately it is solved just like the simpler case, in $O(n)$ time.","title":"28-2 Splines"},{"location":"Chap29/29.1/","text":"29.1-1 If we express the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$ in the compact notation of $\\text{(29.19)}$\u2013$\\text{(29.21)}$, what are $n$, $m$, $A$, $b$, and $c$? $$ \\begin{aligned} n = m & = 3, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} . \\end{aligned} $$ 29.1-2 Give three feasible solutions to the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$. What is the objective value of each one? $(x_1, x_2, x_3) = (6, 1, 0)$ with objective value $9$. $(x_1, x_2, x_3) = (5, 2, 0)$ with objective value $4$. $(x_1, x_2, x_3) = (4, 3, 0)$ with objective value $-1$. 29.1-3 For the slack form in $\\text{(29.38)}$\u2013$\\text{(29.41)}$, what are $N$, $B$, $A$, $b$, $c$, and $v$? $$ \\begin{aligned} N & = \\{1, 2, 3\\}, \\\\ B & = \\{4, 5, 6\\}, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} , \\\\ v & = 0. \\end{aligned} $$ 29.1-4 Convert the following linear program into standard form: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & 2x_1 & + & 7x_2 & + & x_3 & & \\\\ \\text{subject to} & \\\\ & x_1 & & & - & x_3 & = & 7 \\\\ & 3x_1 & + & x_2 & & & \\ge & 24 \\\\ & & & x_2 & & & \\ge & 0 \\\\ & & & & & x_3 & \\le & 0 & . \\end{array} $$ $$ \\begin{array}{lrcrcrcrcrl} \\text{maximize} & -2x_1 & - & 2x_2 & - & 7x_3 & + & x_4 & & \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & & & - & x_4 & \\le & -7 \\\\ & x_1 & - & x_2 & & & + & x_4 & \\le & 7 \\\\ & -3x_1 & + & 3x_2 & - & x_3 & & & \\le & -24 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\le & 0 & . \\end{array} $$ 29.1-5 Convert the following linear program into slack form: $$ \\begin{array}{lrcrcrcrl} \\text{maximize} & 2x_1 & & & - & 6x_3 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & - & x_3 & \\le & 7 \\\\ & 3x_1 & - & x_2 & & & \\ge & 8 \\\\ & -x_1 & + & 2x_2 & + & 2x_3 & \\ge & 0 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ What are the basic and nonbasic variables? First, we will multiply the second and third inequalities by minus one to make it so that they are all $\\le$ inequalities. We will introduce the three new variables $x_4$, $x_5$, $x_6$, and perform the usual procedure for rewriting in slack form $$ \\begin{array}{rcrcrcrcr} x_4 & = & 7 & - & x_1 & - & x_2 & + & x_3 \\\\ x_5 & = & -8 & + & 3x_1 & - & x_2 \\\\ x_6 & = & & - & x_1 & + & 2x_2 & + & 2x_3 \\\\ x_1, x_2, x_3, x_4, x_5, x_6 & \\ge & 0 & , \\end{array} $$ where we are still trying to maximize $2x_1 - 6x_3$. The basic variables are $x_4$, $x_5$, $x_6$ and the nonbasic variables are $x_1$, $x_2$, $x_3$. 29.1-6 Show that the following linear program is infeasible: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 3x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 2 \\\\ & -2x_1 & - & 2x_2 & \\le & -10 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ By dividing the second constraint by $2$ and adding to the first, we have $0 \\le \u22123$, which is impossible. Therefore there linear program is unfeasible. 29.1-7 Show that the following linear program is unbounded: $$ \\begin{array}{lrcrcrl} \\text{minimize} & x_1 & - & x_2 \\\\ \\text{subject to} & \\\\ & -2x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & 2x_2 & \\le & -2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ For any number $r > 1$, we can set $x_1 = 2r$ and $x_2 = r$. Then, the restaints become $$ \\begin{array}{rcrcrl} -2(2r) & + & r = -3r & \\le & -1 \\\\ -2r & - & 2r = -4r & \\le & -2 \\\\ & 2r, r & & \\ge & 0 & . \\end{array} $$ All of these inequalities are clearly satisfied because of our initial restriction in selecting $r$. Now, we look to the objective function, it is $2r - r = r$. So, since we can select $r$ to be arbitrarily large, and still satisfy all of the constraints, we can achieve an arbitrarily large value of the objective function. 29.1-8 Suppose that we have a general linear program with $n$ variables and $m$ constraints, and suppose that we convert it into standard form. Give an upper bound on the number of variables and constraints in the resulting linear program. In the worst case, we have to introduce 2 variables for every variable to ensure that we have nonnegativity constraints, so the resulting program will have $2n$ variables. If each constraint is an equality, we would have to double the number of constraints to create inequalities, resulting in $2m$ constraints. Changing minimization to maximization and greater-than signs to less-than signs don't affect the number of variables or constraints, so the upper bound is $2n$ on variables and $2m$ on constraints. 29.1-9 Give an example of a linear program for which the feasible region is not bounded, but the optimal objective value is finite. Consider the linear program where we want to maximize $x_1 \u2212 x_2$ subject to the constraints $x_1 \u2212 x_2 \\le 1$ and $x_1$, $x_2 \\ge 0$. clearly the objective value can never be greater than one, and it is easy to achieve the optimal value of $1$, by setting $x_1 = 1$ and $x_2 = 0$. Then, this feasible region is unbounded because for any number $r$, we could set $x_1 = x_2 = r$, and that would be feasible because the difference of the two would be zero which is $\\le 1$. If we further wanted it so that there was a single solution that achieved the finite optimal value, we could add the requirements that $x_1 \\le 1$.","title":"29.1 Standard and slack forms"},{"location":"Chap29/29.1/#291-1","text":"If we express the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$ in the compact notation of $\\text{(29.19)}$\u2013$\\text{(29.21)}$, what are $n$, $m$, $A$, $b$, and $c$? $$ \\begin{aligned} n = m & = 3, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} . \\end{aligned} $$","title":"29.1-1"},{"location":"Chap29/29.1/#291-2","text":"Give three feasible solutions to the linear program in $\\text{(29.24)}$\u2013$\\text{(29.28)}$. What is the objective value of each one? $(x_1, x_2, x_3) = (6, 1, 0)$ with objective value $9$. $(x_1, x_2, x_3) = (5, 2, 0)$ with objective value $4$. $(x_1, x_2, x_3) = (4, 3, 0)$ with objective value $-1$.","title":"29.1-2"},{"location":"Chap29/29.1/#291-3","text":"For the slack form in $\\text{(29.38)}$\u2013$\\text{(29.41)}$, what are $N$, $B$, $A$, $b$, $c$, and $v$? $$ \\begin{aligned} N & = \\{1, 2, 3\\}, \\\\ B & = \\{4, 5, 6\\}, \\\\ A & = \\begin{pmatrix} 1 & 1 & -1 \\\\ -1 & -1 & 1 \\\\ 1 & -2 & 2 \\end{pmatrix} , \\\\ b & = \\begin{pmatrix} 7 \\\\ -7 \\\\ 4 \\end{pmatrix} , \\\\ c & = \\begin{pmatrix} 2 \\\\ -3 \\\\ 3 \\end{pmatrix} , \\\\ v & = 0. \\end{aligned} $$","title":"29.1-3"},{"location":"Chap29/29.1/#291-4","text":"Convert the following linear program into standard form: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & 2x_1 & + & 7x_2 & + & x_3 & & \\\\ \\text{subject to} & \\\\ & x_1 & & & - & x_3 & = & 7 \\\\ & 3x_1 & + & x_2 & & & \\ge & 24 \\\\ & & & x_2 & & & \\ge & 0 \\\\ & & & & & x_3 & \\le & 0 & . \\end{array} $$ $$ \\begin{array}{lrcrcrcrcrl} \\text{maximize} & -2x_1 & - & 2x_2 & - & 7x_3 & + & x_4 & & \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & & & - & x_4 & \\le & -7 \\\\ & x_1 & - & x_2 & & & + & x_4 & \\le & 7 \\\\ & -3x_1 & + & 3x_2 & - & x_3 & & & \\le & -24 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\le & 0 & . \\end{array} $$","title":"29.1-4"},{"location":"Chap29/29.1/#291-5","text":"Convert the following linear program into slack form: $$ \\begin{array}{lrcrcrcrl} \\text{maximize} & 2x_1 & & & - & 6x_3 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & - & x_3 & \\le & 7 \\\\ & 3x_1 & - & x_2 & & & \\ge & 8 \\\\ & -x_1 & + & 2x_2 & + & 2x_3 & \\ge & 0 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ What are the basic and nonbasic variables? First, we will multiply the second and third inequalities by minus one to make it so that they are all $\\le$ inequalities. We will introduce the three new variables $x_4$, $x_5$, $x_6$, and perform the usual procedure for rewriting in slack form $$ \\begin{array}{rcrcrcrcr} x_4 & = & 7 & - & x_1 & - & x_2 & + & x_3 \\\\ x_5 & = & -8 & + & 3x_1 & - & x_2 \\\\ x_6 & = & & - & x_1 & + & 2x_2 & + & 2x_3 \\\\ x_1, x_2, x_3, x_4, x_5, x_6 & \\ge & 0 & , \\end{array} $$ where we are still trying to maximize $2x_1 - 6x_3$. The basic variables are $x_4$, $x_5$, $x_6$ and the nonbasic variables are $x_1$, $x_2$, $x_3$.","title":"29.1-5"},{"location":"Chap29/29.1/#291-6","text":"Show that the following linear program is infeasible: $$ \\begin{array}{lrcrcrl} \\text{minimize} & 3x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 2 \\\\ & -2x_1 & - & 2x_2 & \\le & -10 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ By dividing the second constraint by $2$ and adding to the first, we have $0 \\le \u22123$, which is impossible. Therefore there linear program is unfeasible.","title":"29.1-6"},{"location":"Chap29/29.1/#291-7","text":"Show that the following linear program is unbounded: $$ \\begin{array}{lrcrcrl} \\text{minimize} & x_1 & - & x_2 \\\\ \\text{subject to} & \\\\ & -2x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & 2x_2 & \\le & -2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ For any number $r > 1$, we can set $x_1 = 2r$ and $x_2 = r$. Then, the restaints become $$ \\begin{array}{rcrcrl} -2(2r) & + & r = -3r & \\le & -1 \\\\ -2r & - & 2r = -4r & \\le & -2 \\\\ & 2r, r & & \\ge & 0 & . \\end{array} $$ All of these inequalities are clearly satisfied because of our initial restriction in selecting $r$. Now, we look to the objective function, it is $2r - r = r$. So, since we can select $r$ to be arbitrarily large, and still satisfy all of the constraints, we can achieve an arbitrarily large value of the objective function.","title":"29.1-7"},{"location":"Chap29/29.1/#291-8","text":"Suppose that we have a general linear program with $n$ variables and $m$ constraints, and suppose that we convert it into standard form. Give an upper bound on the number of variables and constraints in the resulting linear program. In the worst case, we have to introduce 2 variables for every variable to ensure that we have nonnegativity constraints, so the resulting program will have $2n$ variables. If each constraint is an equality, we would have to double the number of constraints to create inequalities, resulting in $2m$ constraints. Changing minimization to maximization and greater-than signs to less-than signs don't affect the number of variables or constraints, so the upper bound is $2n$ on variables and $2m$ on constraints.","title":"29.1-8"},{"location":"Chap29/29.1/#291-9","text":"Give an example of a linear program for which the feasible region is not bounded, but the optimal objective value is finite. Consider the linear program where we want to maximize $x_1 \u2212 x_2$ subject to the constraints $x_1 \u2212 x_2 \\le 1$ and $x_1$, $x_2 \\ge 0$. clearly the objective value can never be greater than one, and it is easy to achieve the optimal value of $1$, by setting $x_1 = 1$ and $x_2 = 0$. Then, this feasible region is unbounded because for any number $r$, we could set $x_1 = x_2 = r$, and that would be feasible because the difference of the two would be zero which is $\\le 1$. If we further wanted it so that there was a single solution that achieved the finite optimal value, we could add the requirements that $x_1 \\le 1$.","title":"29.1-9"},{"location":"Chap29/29.2/","text":"29.2-1 Put the single-pair shortest-path linear program from $\\text{(29.44)}$\u2013$\\text{(29.46)}$ into standard form. The objective is already in normal form. However, some of the constraints are equality constraints instead of $\\le$ constraints. This means that we need to rewrite them as a pair of inequality constraints, the overlap of whose solutions is just the case where we have equality. we also need to deal with the fact that most of the variables can be negative. To do that, we will introduce variables for the negative part and positive part, each of which need be positive, and we'll just be sure to subtract the negative part. $d_s$ need not be changed in this way since it can never be negative since we are not assuming the existence of negative weight cycles. $$ \\begin{aligned} d_v^+ - d_v^- - d_u^+ + d_u^- \\le w(u, v) \\text{ for every edge } (u, v) \\\\ d_s \\le 0 \\end{aligned} $$ 29.2-2 Write out explicitly the linear program corresponding to finding the shortest path from node $s$ to node $y$ in Figure 24.2(a). $$ \\begin{array}{ll} \\text{minimize} & d_y \\\\ \\text{subject to} & \\\\ & d_t \\le d_s + 3 \\\\ & d_x \\le d_t + 6 \\\\ & d_y \\le d_s + 5 \\\\ & d_y \\le d_t + 2 \\\\ & d_z \\le d_x + 2 \\\\ & d_t \\le d_y + 1 \\\\ & d_x \\le d_y + 4 \\\\ & d_z \\le d_y + 1 \\\\ & d_s \\le d_z + 1 \\\\ & d_x \\le d_z + 7 \\\\ & d_2 = 0. \\end{array} $$ 29.2-3 In the single-source shortest-paths problem, we want to find the shortest-path weights from a source vertex $s$ to all vertices $v \\in V$. Given a graph $G$, write a linear program for which the solution has the property that $d_v$ is the shortest-path weight from $s$ to $v$ for each vertex $v \\in V$. We will follow a similar idea to the way to when we were finding the shortest path between two particular vertices. $$ \\begin{array}{ll} \\text{maximize} & \\sum_{v \\in V} d_v \\\\ \\text{subject to} & \\\\ & d_v \\le d_u + w(u, v) \\text{ for each edge } (u, v) \\\\ & d_s = 0. \\end{array} $$ The first type of constraint makes sure that we never say that a vertex is further away than it would be if we just took the edge corresponding to that constraint. Also, since we are trying to maximize all of the variables, we will make it so that there is no slack anywhere, and so all the dv values will correspond to lengths of shortest paths to $v$. This is because the only thing holding back the variables is the information about relaxing along the edges, which is what determines shortest paths. 29.2-4 Write out explicitly the linear program corresponding to finding the maximum flow in Figure 26.1(a). $$ \\begin{array}{lll} \\text{maximize} & f_{sv_1} + f_{sv_2} \\\\ \\text{subject to} & \\\\ & f_{sv_1} & \\le 16 \\\\ & f_{sv_2} & \\le 14 \\\\ & f_{v_1v_3} & \\le 12 \\\\ & f_{v_2v_1} & \\le 4 \\\\ & f_{v_2v_4} & \\le 14 \\\\ & f_{v_3v_2} & \\le 9 \\\\ & f_{v_3t} & \\le 20 \\\\ & f_{v_4v_3} & \\le 7 \\\\ & f_{v_4t} & \\le 4 \\\\ & f_{sv_1} + f_{v_2v_1} & = f_{v_1v_3} \\\\ & f_{sv_2} + f_{v_3v_2} & = f_{v_2v_1} + f_{v_2v_4} \\\\ & f_{v_1v_3} + f_{v_4v_3} & = f_{v_3v_2} + f_{v_3t} \\\\ & f_{v_2v_4} & = f_{v_4v_3} + f_{v_4t} \\\\ & f_{uv} & \\ge 0 \\text{ for } u, v \\in \\{s, v_1, v_2, v_3, v_4, t\\}. \\end{array} $$ 29.2-5 Rewrite the linear program for maximum flow $\\text{(29.47)}$\u2013$\\text{(29.50)}$ so that it uses only $O(V + E)$ constraints. All we need to do to bring the number of constraints down from $O(V^2)$ to $O(V + E)$ is to replace the way we index the flows. Instead of indexing it by a pair of vertices, we will index it by an edge. This won't change anything about the analysis because between pairs of vertices that don't have an edge between them, there definitely won't be any flow. Also, it brings the number of constraints of the first and third time down to $O(E)$ and the number of constraints of the second kind stays at $O(V)$. $$ \\begin{array}{lll} \\text{maximize} & \\sum_{\\text{edges $e$ leaving $s$}} f_e - \\sum_{\\text{edges $e$ entering $s$}} f_s \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le c(u, v) \\text{ for each edge } (u, v) \\\\ & \\sum_{\\text{edges $e$ leaving $u$}} f_e - \\sum_{\\text{edges $e$ entering $u$}} f_e \\text{ for each edge } u \\in V - \\{s, t\\} \\\\ & f_e \\ge 0 \\text{ for each edge } e. \\end{array} $$ 29.2-6 Write a linear program that, given a bipartite graph $G = (V, E)$ solves the maximum-bipartite-matching problem. Recall from section 26.3 that we can solve the maximum-bipartite-matching problem by viewing it as a network flow problem, where we append a source $s$ and sink $t$, each connected to every vertex is $L$ and $R$ respectively by an edge with capacity $1$, and we give every edge already in the bipartite graph capacity $1$. The integral maximum flows are in correspondence with maximum bipartite matchings. In this setup, the linear programming problem to solve is as follows: $$ \\begin{aligned} \\text{maximize} & \\sum_{v \\in L} f_{sv} \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le 1 \\text{ for each } u, v \\in \\{s\\} \\cup L \\cup R \\cup \\{t\\} = V \\\\ & \\sum_{v \\in V} f_{vu} = \\sum_{v \\in V} f_{uv} \\text{ for each } u \\in L \\cup R \\\\ & f_{uv} \\ge 0 \\text{ for each } u, v \\in V \\end{aligned} $$ 29.2-7 In the minimum-cost multicommodity-flow problem , we are given directed graph $G = (V, E)$ in which each edge $(u, v) \\in E$ has a nonnegative capacity $c(u, v) \\ge 0$ and a cost $a(u, v)$. As in the multicommodity-flow problem, we are given $k$ different commodities, $K_1, K_2, \\ldots, K_k$, where we specify commodity $i$ by the triple $K_i = (s_i, t_i, d_i)$. We define the flow $f_i$ for commodity $i$ and the aggregate flow $f_{uv}$ on edge $(u, v)$ as in the multicommodity-flow problem. A feasible flow is one in which the aggregate flow on each edge $(u, v)$ is no more than the capacity of edge $(u, v)$. The cost of a flow is $\\sum_{u, v \\in V} a(u, v)f_{uv}$, and the goal is to find the feasible flow of minimum cost. Express this problem as a linear program. As in the minimum cost flow problem, we have constraints for the edge capacities, for the conservation of flow, and nonegativity. The difference is that the restraint that before we required exactly $d$ units to flow, now, we require that for each commodity, the right amount of that commodity flows. the conservation equalities will be applied to each different type of commodity independently. If we super script $f$ that will denote the type of commodity the flow is describing, if we do not superscript it, it will denote the aggregate flow We want to minimize $$\\sum_{u, v \\in V} a(u, v) f_{uv}.$$ The capacity constraints are that $$\\sum_{i \\in [k]} \\sum_{u, v \\in V} f_{uv}^i \\le c(u, v).$$ The conservation constraints are that for every $i \\in [k]$, for every $u \\in V \\backslash \\{s_i, t_i\\}$. $$\\sum_{v \\in V} f_{uv}^i = \\sum_{v \\in V} f_{vu}^i.$$ Now, the constraints that correspond to requiring a certain amount of flow are that for every $i \\in [k]$. $$\\sum_{v \\in V} f_{s_i, v}^i - \\sum_{v \\in V} f_{v, s_i}^i = d.$$ Now, we put in the constraint that makes sure what we called the aggregate flow is actually the aggregate flow, so, for every $u, v \\in V$, $$f_{uv} = \\sum_{i \\in [k]} f_{uv}^i.$$ Finally, we get to the fact that all flows are nonnegative, for every $u, v \\in V$, $$f_{uv} \\ge 0.$$","title":"29.2 Formulating problems as linear programs"},{"location":"Chap29/29.2/#292-1","text":"Put the single-pair shortest-path linear program from $\\text{(29.44)}$\u2013$\\text{(29.46)}$ into standard form. The objective is already in normal form. However, some of the constraints are equality constraints instead of $\\le$ constraints. This means that we need to rewrite them as a pair of inequality constraints, the overlap of whose solutions is just the case where we have equality. we also need to deal with the fact that most of the variables can be negative. To do that, we will introduce variables for the negative part and positive part, each of which need be positive, and we'll just be sure to subtract the negative part. $d_s$ need not be changed in this way since it can never be negative since we are not assuming the existence of negative weight cycles. $$ \\begin{aligned} d_v^+ - d_v^- - d_u^+ + d_u^- \\le w(u, v) \\text{ for every edge } (u, v) \\\\ d_s \\le 0 \\end{aligned} $$","title":"29.2-1"},{"location":"Chap29/29.2/#292-2","text":"Write out explicitly the linear program corresponding to finding the shortest path from node $s$ to node $y$ in Figure 24.2(a). $$ \\begin{array}{ll} \\text{minimize} & d_y \\\\ \\text{subject to} & \\\\ & d_t \\le d_s + 3 \\\\ & d_x \\le d_t + 6 \\\\ & d_y \\le d_s + 5 \\\\ & d_y \\le d_t + 2 \\\\ & d_z \\le d_x + 2 \\\\ & d_t \\le d_y + 1 \\\\ & d_x \\le d_y + 4 \\\\ & d_z \\le d_y + 1 \\\\ & d_s \\le d_z + 1 \\\\ & d_x \\le d_z + 7 \\\\ & d_2 = 0. \\end{array} $$","title":"29.2-2"},{"location":"Chap29/29.2/#292-3","text":"In the single-source shortest-paths problem, we want to find the shortest-path weights from a source vertex $s$ to all vertices $v \\in V$. Given a graph $G$, write a linear program for which the solution has the property that $d_v$ is the shortest-path weight from $s$ to $v$ for each vertex $v \\in V$. We will follow a similar idea to the way to when we were finding the shortest path between two particular vertices. $$ \\begin{array}{ll} \\text{maximize} & \\sum_{v \\in V} d_v \\\\ \\text{subject to} & \\\\ & d_v \\le d_u + w(u, v) \\text{ for each edge } (u, v) \\\\ & d_s = 0. \\end{array} $$ The first type of constraint makes sure that we never say that a vertex is further away than it would be if we just took the edge corresponding to that constraint. Also, since we are trying to maximize all of the variables, we will make it so that there is no slack anywhere, and so all the dv values will correspond to lengths of shortest paths to $v$. This is because the only thing holding back the variables is the information about relaxing along the edges, which is what determines shortest paths.","title":"29.2-3"},{"location":"Chap29/29.2/#292-4","text":"Write out explicitly the linear program corresponding to finding the maximum flow in Figure 26.1(a). $$ \\begin{array}{lll} \\text{maximize} & f_{sv_1} + f_{sv_2} \\\\ \\text{subject to} & \\\\ & f_{sv_1} & \\le 16 \\\\ & f_{sv_2} & \\le 14 \\\\ & f_{v_1v_3} & \\le 12 \\\\ & f_{v_2v_1} & \\le 4 \\\\ & f_{v_2v_4} & \\le 14 \\\\ & f_{v_3v_2} & \\le 9 \\\\ & f_{v_3t} & \\le 20 \\\\ & f_{v_4v_3} & \\le 7 \\\\ & f_{v_4t} & \\le 4 \\\\ & f_{sv_1} + f_{v_2v_1} & = f_{v_1v_3} \\\\ & f_{sv_2} + f_{v_3v_2} & = f_{v_2v_1} + f_{v_2v_4} \\\\ & f_{v_1v_3} + f_{v_4v_3} & = f_{v_3v_2} + f_{v_3t} \\\\ & f_{v_2v_4} & = f_{v_4v_3} + f_{v_4t} \\\\ & f_{uv} & \\ge 0 \\text{ for } u, v \\in \\{s, v_1, v_2, v_3, v_4, t\\}. \\end{array} $$","title":"29.2-4"},{"location":"Chap29/29.2/#292-5","text":"Rewrite the linear program for maximum flow $\\text{(29.47)}$\u2013$\\text{(29.50)}$ so that it uses only $O(V + E)$ constraints. All we need to do to bring the number of constraints down from $O(V^2)$ to $O(V + E)$ is to replace the way we index the flows. Instead of indexing it by a pair of vertices, we will index it by an edge. This won't change anything about the analysis because between pairs of vertices that don't have an edge between them, there definitely won't be any flow. Also, it brings the number of constraints of the first and third time down to $O(E)$ and the number of constraints of the second kind stays at $O(V)$. $$ \\begin{array}{lll} \\text{maximize} & \\sum_{\\text{edges $e$ leaving $s$}} f_e - \\sum_{\\text{edges $e$ entering $s$}} f_s \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le c(u, v) \\text{ for each edge } (u, v) \\\\ & \\sum_{\\text{edges $e$ leaving $u$}} f_e - \\sum_{\\text{edges $e$ entering $u$}} f_e \\text{ for each edge } u \\in V - \\{s, t\\} \\\\ & f_e \\ge 0 \\text{ for each edge } e. \\end{array} $$","title":"29.2-5"},{"location":"Chap29/29.2/#292-6","text":"Write a linear program that, given a bipartite graph $G = (V, E)$ solves the maximum-bipartite-matching problem. Recall from section 26.3 that we can solve the maximum-bipartite-matching problem by viewing it as a network flow problem, where we append a source $s$ and sink $t$, each connected to every vertex is $L$ and $R$ respectively by an edge with capacity $1$, and we give every edge already in the bipartite graph capacity $1$. The integral maximum flows are in correspondence with maximum bipartite matchings. In this setup, the linear programming problem to solve is as follows: $$ \\begin{aligned} \\text{maximize} & \\sum_{v \\in L} f_{sv} \\\\ \\text{subject to} & \\\\ & f_{(u, v)} \\le 1 \\text{ for each } u, v \\in \\{s\\} \\cup L \\cup R \\cup \\{t\\} = V \\\\ & \\sum_{v \\in V} f_{vu} = \\sum_{v \\in V} f_{uv} \\text{ for each } u \\in L \\cup R \\\\ & f_{uv} \\ge 0 \\text{ for each } u, v \\in V \\end{aligned} $$","title":"29.2-6"},{"location":"Chap29/29.2/#292-7","text":"In the minimum-cost multicommodity-flow problem , we are given directed graph $G = (V, E)$ in which each edge $(u, v) \\in E$ has a nonnegative capacity $c(u, v) \\ge 0$ and a cost $a(u, v)$. As in the multicommodity-flow problem, we are given $k$ different commodities, $K_1, K_2, \\ldots, K_k$, where we specify commodity $i$ by the triple $K_i = (s_i, t_i, d_i)$. We define the flow $f_i$ for commodity $i$ and the aggregate flow $f_{uv}$ on edge $(u, v)$ as in the multicommodity-flow problem. A feasible flow is one in which the aggregate flow on each edge $(u, v)$ is no more than the capacity of edge $(u, v)$. The cost of a flow is $\\sum_{u, v \\in V} a(u, v)f_{uv}$, and the goal is to find the feasible flow of minimum cost. Express this problem as a linear program. As in the minimum cost flow problem, we have constraints for the edge capacities, for the conservation of flow, and nonegativity. The difference is that the restraint that before we required exactly $d$ units to flow, now, we require that for each commodity, the right amount of that commodity flows. the conservation equalities will be applied to each different type of commodity independently. If we super script $f$ that will denote the type of commodity the flow is describing, if we do not superscript it, it will denote the aggregate flow We want to minimize $$\\sum_{u, v \\in V} a(u, v) f_{uv}.$$ The capacity constraints are that $$\\sum_{i \\in [k]} \\sum_{u, v \\in V} f_{uv}^i \\le c(u, v).$$ The conservation constraints are that for every $i \\in [k]$, for every $u \\in V \\backslash \\{s_i, t_i\\}$. $$\\sum_{v \\in V} f_{uv}^i = \\sum_{v \\in V} f_{vu}^i.$$ Now, the constraints that correspond to requiring a certain amount of flow are that for every $i \\in [k]$. $$\\sum_{v \\in V} f_{s_i, v}^i - \\sum_{v \\in V} f_{v, s_i}^i = d.$$ Now, we put in the constraint that makes sure what we called the aggregate flow is actually the aggregate flow, so, for every $u, v \\in V$, $$f_{uv} = \\sum_{i \\in [k]} f_{uv}^i.$$ Finally, we get to the fact that all flows are nonnegative, for every $u, v \\in V$, $$f_{uv} \\ge 0.$$","title":"29.2-7"},{"location":"Chap29/29.3/","text":"29.3-1 Complete the proof of Lemma 29.4 by showing that it must be the case that $c = c'$ and $v = v'$. We subtract equation $\\text{(29.81)}$ from equation $\\text{(29.79)}$. $$z = v + \\sum_{j \\in N} c_j x_j, \\tag{29.79}$$ $$z = v' + \\sum_{j \\in N} c_j' x_j. \\tag{29.81}$$ Thus we have, $$ \\begin{aligned} 0 & = v - v' + \\sum_{j \\in N} (c_j - c_j') x_j. \\\\ \\sum_{j \\in N} c_j' x_j & = v - v' + \\sum_{j \\in N} c_j x_j. \\end{aligned} $$ By Lemma 29.3, we have $c_j = c_j'$ for every $j$ and $v = v'$ since $v - v' = 0$. 29.3-2 Show that the call to $\\text{PIVOT}$ in line 12 of $\\text{SIMPLEX}$ never decreases the value of $v$. The only time $v$ is updated in $\\text{PIVOT}$ is line 14, so it will suffice to show that $c_e \\hat b_e \\ge 0$. Prior to making the call to $\\text{PIVOT}$, we choose an index $e$ such that $c_e > 0$, and this is unchanged in $\\text{PIVOT}$. We set $\\hat b_e$ in line 3 to be $b_l / a_{le}$. The loop invariant proved in Lemma 29.2 tells us that $b_l \\ge 0$. The if-condition of line 6 of $\\text{SIMPLEX}$ tells us that only the noninfinite $\\delta_i$ must have $a_{ie} > 0$, and we choose $l$ to minimize $\\delta_l$, so we must have $a_{le} > 0$. Thus, $c_e \\hat b_e \\ge 0$, which implies $v$ can never decrease. 29.3-3 Prove that the slack form given to the $\\text{PIVOT}$ procedure and the slack form that the procedure returns are equivalent. To show that the two slack forms are equivalent, we will show both that they have equal objective functions, and their sets of feasible solutions are equal. First, we'll check that their sets of feasible solutions are equal. Basically all we do to the constraints when we pivot is take the non-basic variable, $e$, and solve the equation corresponding to the basic variable $l$ for $e$. We are then taking that expression and replacing $e$ in all the constraints with this expression we got by solving the equation corresponding to $l$. Since each of these algebraic operations are valid, the result of the sequence of them is also algebraically equivalent to the original. Next, we'll see that the objective functions are equal. We decrease each $c_j$ by $c_e \\hat a_{ej}$, which is to say that we replace the non-basic variable we are making basic with the expression we got it was equal to once we made it basic. Since the slack form returned by $\\text{PIVOT}$, has the same feasible region and an equal objective function, it is equivalent to the original slack form passed in. 29.3-4 Suppose we convert a linear program $(A, b, c)$ in standard form to slack form. Show that the basic solution is feasible if and only if $b_i \\ge 0$ for $i = 1, 2, \\ldots, m$. First suppose that the basic solution is feasible. We set each $x_i = 0$ for $1 \\le i \\le n$, so we have $x_{n + i} = b_i - \\sum_{j = 1}^n a_{ij}x_j = b_i$ as a satisfied constraint. Since we also require $x_{n + i} \\ge 0$ for all $1 \\le i \\le m$, this implies $b_i \\ge 0$. Now suppose $b_i \\ge 0$ for all $i$. In the basic solution we set $x_i = 0$ for $1 \\le i \\le n$ which satisfies the nonnegativity constraints. We set $x_{n + i} = b_i$ for $1 \\le i \\le m$ which satisfies the other constraint equations, and also the nonnegativity constraints on the basic variables since $b_i \\ge 0$. Thus, every constraint is satisfied, so the basic solution is feasible. 29.3-5 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maximize} & 18x_1 & + & 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 20 \\\\ & x_1 & & & \\le & 12 \\\\ & & & x_2 & \\le & 16 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we rewrite the linear program into it's slack form $$ \\begin{array}{lrl} \\text{maximize} & 18x_1 + 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_3 & = 20 - x_1 - x_2 \\\\ & x_4 & = 12 - x_1 \\\\ & x_5 & = 16 - x_2 \\\\ & x_1, x_2, x_3, x_4, x_5 & \\ge 0. \\end{array} $$ We now stop since no more non-basic variables appear in the objective with a positive coefficient. Our solution is $(12, 8, 0, 0, 8)$ and has a value of $316$. Going back to the standard form we started with, we just disregard the values of $x_3$ through $x_5$ and have the solution that $x_1 = 12$ and $x_2 = 8$. We can check that this is both feasible and has the objective achieve $316$. 29.3-6 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maximize} & 5x_1 & - & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 1 \\\\ & 2x_1 & + & x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we convert the linear program into it's slack form $$ \\begin{array}{rcrcrcrl} z & = & & & 5x_1 & - & 3x_2 \\\\ x_3 & = & 1 & - & x_1 & + & x_2 \\\\ x_4 & = & 2 & - & 2x_1 & - & x_2 & . \\end{array} $$ The nonbasic variables are $x_1$ and $x_2$. Of these, only $x_1$ has a positive coefficient in the objective function, so we must choose $x_e = x_1$. Both equations limit $x_1$ by $1$, so we'll choose the first one to rewrite $x_1$ with. Using $x_1 = 1 \u2212 x_3 + x_2$ we obtain the new system $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & 5x_3 & + & 2x_2 & \\\\ x_1 & = & 1 & - & x_3 & + & x_2 & \\\\ x_4 & = & & & 2x_3 & - & 3x_2 & . \\end{array} $$ Now $x_2$ is the only nonbasic variable with positive coefficient in the objective function, so we set $x_e = x_2$. The last equation limits $x_2$ by $0$ which is most restrictive, so we set $x_2 = \\frac{2}{3} x_3 \u2212 \\frac{1}{3} x_4$. Rewriting, our new system becomes $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & \\frac{11}{3} x_3 & - & \\frac{2}{3} x_4 \\\\ x_1 & = & 1 & - & \\frac{1}{3} x_3 & - & \\frac{1}{3} x_4 \\\\ x_2 & = & & & \\frac{2}{3} x_3 & - & \\frac{1}{3} x_4 & . \\end{array} $$ Every nonbasic variable now has negative coefficient in the objective function, so we take the basic solution $(x_1, x_2, x_3, x_4) = (1, 0, 0, 0)$. The objective value this achieves is $5$. 29.3-7 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & x_1 & + & x_2 & + & x_3 \\\\ \\text{subject to} & \\\\ & 2x_1 & + & 7.5x_2 & + & 3x_3 & \\ge & 10000 \\\\ & 20x_1 & & 5x_2 & + & 10x_3 & \\ge & 30000 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ First, we convert this equation to the slack form. Doing so doesn't change the objective, but the constraints become $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_1 & - & x_2 & - & x_3 \\\\ x_4 & = & -10000 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Also, since the objective is to minimize a given function, we'll change it over to maximizing the negative of that function. In particular maximize $\u2212x_1 \u2212 x_2 \u2212 x_3$. Now, we note that the initial basic solution is not feasible, because it would leave $x_4$ and $x_5$ being negative. This means that finding an initial solution requires using the method of section 29.5. The auxiliary linear program in slack form is $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_4 & = & -10000 & + & x_0 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & x_0 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We choose $x_0$ as the entering variable and $x_5$ as the leaving variable, since it is the basic variable whose value in the basic solution is most negative. After pivoting, we have the slack form $$ \\begin{array}{rcrcrcrcrcr} z & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 & - & x_5 \\\\ x_0 & = & 30000 & - & 20x_1 & - & 5x_2 & - & 10x_3 & + & x_5 \\\\ x_4 & = & 20000 & - & 18x_1 & + & 2.5x_2 & - & 7x_3 & + & x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The associated basic solution is feasible, so now we just need to repeatedly call $\\text{PIVOT}$ until we obtain an optimal solution to $L_{aux}$. We'll choose $x_2$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_2 & = & 6000 & - & 0.2x_0 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 0.5x_0 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form is the final solution to the auxiliary problem. Since this solution has $x_0 = 0$, we know that our initial problem was feasible. Furthermore, since $x_0 = 0$, we can just remove it from the set of constraints. We then restore the original objective function, with appropriate substitutions made to include only the nonbasic variables. This yields $$ \\begin{array}{rcrcrcrcr} z & = & -6000 & + & 3x_1 & + & x_3 & - & 0.2x_5 \\\\ x_2 & = & 6000 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form has a feasible basic solution, and we can return it to $\\text{SIMPLEX}$. We choose $x_1$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & -2250 & - & \\frac{2}{7} x_3 & - & \\frac{3}{28} x_4 & - & \\frac{11}{280} x_5 \\\\ x_1 & = & 1250 & - & \\frac{3}{7} x_3 & - & \\frac{1}{28} x_4 & + & \\frac{15}{280} x_5 \\\\ x_2 & = & 1000 & - & \\frac{2}{7} x_3 & + & \\frac{4}{28} x_4 & - & \\frac{4}{280} x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ At this point, all coefficients in the objective function are negative, so the basic solution is an optimal solution. This solution is $(x_1, x_2, x_3) = (1250, 1000, 0)$. 29.3-8 In the proof of Lemma 29.5, we argued that there are at most $\\binom{m + n}{n}$ ways to choose a set $B$ of basic variables. Give an example of a linear program in which there are strictly fewer than $\\binom{m + n}{n}$ ways to choose the set $B$. Consider the simple program $$ \\begin{array}{rcrcrl} z & = & & - & x_1 \\\\ x_2 & = & 1 & - & x_1 & . \\end{array} $$ In this case, we have $m = n = 1$, so $\\binom{m + n}{n} = \\binom{2}{1} = 2$, however, since the only coefficients of the objective function are negative, we can't make any other choices for basic variable. We must immediately terminate with the basic solution $(x_1, x_2) = (0, 1)$, which is optimal.","title":"29.3 The simplex algorithm"},{"location":"Chap29/29.3/#293-1","text":"Complete the proof of Lemma 29.4 by showing that it must be the case that $c = c'$ and $v = v'$. We subtract equation $\\text{(29.81)}$ from equation $\\text{(29.79)}$. $$z = v + \\sum_{j \\in N} c_j x_j, \\tag{29.79}$$ $$z = v' + \\sum_{j \\in N} c_j' x_j. \\tag{29.81}$$ Thus we have, $$ \\begin{aligned} 0 & = v - v' + \\sum_{j \\in N} (c_j - c_j') x_j. \\\\ \\sum_{j \\in N} c_j' x_j & = v - v' + \\sum_{j \\in N} c_j x_j. \\end{aligned} $$ By Lemma 29.3, we have $c_j = c_j'$ for every $j$ and $v = v'$ since $v - v' = 0$.","title":"29.3-1"},{"location":"Chap29/29.3/#293-2","text":"Show that the call to $\\text{PIVOT}$ in line 12 of $\\text{SIMPLEX}$ never decreases the value of $v$. The only time $v$ is updated in $\\text{PIVOT}$ is line 14, so it will suffice to show that $c_e \\hat b_e \\ge 0$. Prior to making the call to $\\text{PIVOT}$, we choose an index $e$ such that $c_e > 0$, and this is unchanged in $\\text{PIVOT}$. We set $\\hat b_e$ in line 3 to be $b_l / a_{le}$. The loop invariant proved in Lemma 29.2 tells us that $b_l \\ge 0$. The if-condition of line 6 of $\\text{SIMPLEX}$ tells us that only the noninfinite $\\delta_i$ must have $a_{ie} > 0$, and we choose $l$ to minimize $\\delta_l$, so we must have $a_{le} > 0$. Thus, $c_e \\hat b_e \\ge 0$, which implies $v$ can never decrease.","title":"29.3-2"},{"location":"Chap29/29.3/#293-3","text":"Prove that the slack form given to the $\\text{PIVOT}$ procedure and the slack form that the procedure returns are equivalent. To show that the two slack forms are equivalent, we will show both that they have equal objective functions, and their sets of feasible solutions are equal. First, we'll check that their sets of feasible solutions are equal. Basically all we do to the constraints when we pivot is take the non-basic variable, $e$, and solve the equation corresponding to the basic variable $l$ for $e$. We are then taking that expression and replacing $e$ in all the constraints with this expression we got by solving the equation corresponding to $l$. Since each of these algebraic operations are valid, the result of the sequence of them is also algebraically equivalent to the original. Next, we'll see that the objective functions are equal. We decrease each $c_j$ by $c_e \\hat a_{ej}$, which is to say that we replace the non-basic variable we are making basic with the expression we got it was equal to once we made it basic. Since the slack form returned by $\\text{PIVOT}$, has the same feasible region and an equal objective function, it is equivalent to the original slack form passed in.","title":"29.3-3"},{"location":"Chap29/29.3/#293-4","text":"Suppose we convert a linear program $(A, b, c)$ in standard form to slack form. Show that the basic solution is feasible if and only if $b_i \\ge 0$ for $i = 1, 2, \\ldots, m$. First suppose that the basic solution is feasible. We set each $x_i = 0$ for $1 \\le i \\le n$, so we have $x_{n + i} = b_i - \\sum_{j = 1}^n a_{ij}x_j = b_i$ as a satisfied constraint. Since we also require $x_{n + i} \\ge 0$ for all $1 \\le i \\le m$, this implies $b_i \\ge 0$. Now suppose $b_i \\ge 0$ for all $i$. In the basic solution we set $x_i = 0$ for $1 \\le i \\le n$ which satisfies the nonnegativity constraints. We set $x_{n + i} = b_i$ for $1 \\le i \\le m$ which satisfies the other constraint equations, and also the nonnegativity constraints on the basic variables since $b_i \\ge 0$. Thus, every constraint is satisfied, so the basic solution is feasible.","title":"29.3-4"},{"location":"Chap29/29.3/#293-5","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maximize} & 18x_1 & + & 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & x_2 & \\le & 20 \\\\ & x_1 & & & \\le & 12 \\\\ & & & x_2 & \\le & 16 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we rewrite the linear program into it's slack form $$ \\begin{array}{lrl} \\text{maximize} & 18x_1 + 12.5x_2 \\\\ \\text{subject to} & \\\\ & x_3 & = 20 - x_1 - x_2 \\\\ & x_4 & = 12 - x_1 \\\\ & x_5 & = 16 - x_2 \\\\ & x_1, x_2, x_3, x_4, x_5 & \\ge 0. \\end{array} $$ We now stop since no more non-basic variables appear in the objective with a positive coefficient. Our solution is $(12, 8, 0, 0, 8)$ and has a value of $316$. Going back to the standard form we started with, we just disregard the values of $x_3$ through $x_5$ and have the solution that $x_1 = 12$ and $x_2 = 8$. We can check that this is both feasible and has the objective achieve $316$.","title":"29.3-5"},{"location":"Chap29/29.3/#293-6","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maximize} & 5x_1 & - & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 1 \\\\ & 2x_1 & + & x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ First, we convert the linear program into it's slack form $$ \\begin{array}{rcrcrcrl} z & = & & & 5x_1 & - & 3x_2 \\\\ x_3 & = & 1 & - & x_1 & + & x_2 \\\\ x_4 & = & 2 & - & 2x_1 & - & x_2 & . \\end{array} $$ The nonbasic variables are $x_1$ and $x_2$. Of these, only $x_1$ has a positive coefficient in the objective function, so we must choose $x_e = x_1$. Both equations limit $x_1$ by $1$, so we'll choose the first one to rewrite $x_1$ with. Using $x_1 = 1 \u2212 x_3 + x_2$ we obtain the new system $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & 5x_3 & + & 2x_2 & \\\\ x_1 & = & 1 & - & x_3 & + & x_2 & \\\\ x_4 & = & & & 2x_3 & - & 3x_2 & . \\end{array} $$ Now $x_2$ is the only nonbasic variable with positive coefficient in the objective function, so we set $x_e = x_2$. The last equation limits $x_2$ by $0$ which is most restrictive, so we set $x_2 = \\frac{2}{3} x_3 \u2212 \\frac{1}{3} x_4$. Rewriting, our new system becomes $$ \\begin{array}{rcrcrcrl} z & = & 5 & - & \\frac{11}{3} x_3 & - & \\frac{2}{3} x_4 \\\\ x_1 & = & 1 & - & \\frac{1}{3} x_3 & - & \\frac{1}{3} x_4 \\\\ x_2 & = & & & \\frac{2}{3} x_3 & - & \\frac{1}{3} x_4 & . \\end{array} $$ Every nonbasic variable now has negative coefficient in the objective function, so we take the basic solution $(x_1, x_2, x_3, x_4) = (1, 0, 0, 0)$. The objective value this achieves is $5$.","title":"29.3-6"},{"location":"Chap29/29.3/#293-7","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrcrl} \\text{minimize} & x_1 & + & x_2 & + & x_3 \\\\ \\text{subject to} & \\\\ & 2x_1 & + & 7.5x_2 & + & 3x_3 & \\ge & 10000 \\\\ & 20x_1 & & 5x_2 & + & 10x_3 & \\ge & 30000 \\\\ & & x_1, x_2, x_3 & & & & \\ge & 0 & . \\end{array} $$ First, we convert this equation to the slack form. Doing so doesn't change the objective, but the constraints become $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_1 & - & x_2 & - & x_3 \\\\ x_4 & = & -10000 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Also, since the objective is to minimize a given function, we'll change it over to maximizing the negative of that function. In particular maximize $\u2212x_1 \u2212 x_2 \u2212 x_3$. Now, we note that the initial basic solution is not feasible, because it would leave $x_4$ and $x_5$ being negative. This means that finding an initial solution requires using the method of section 29.5. The auxiliary linear program in slack form is $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_4 & = & -10000 & + & x_0 & + & 2x_1 & + & 7.5x_2 & + & 3x_3 \\\\ x_5 & = & -30000 & + & x_0 & + & 20x_1 & + & 5x_2 & + & 10x_3 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We choose $x_0$ as the entering variable and $x_5$ as the leaving variable, since it is the basic variable whose value in the basic solution is most negative. After pivoting, we have the slack form $$ \\begin{array}{rcrcrcrcrcr} z & = & -30000 & + & 20x_1 & + & 5x_2 & + & 10x_3 & - & x_5 \\\\ x_0 & = & 30000 & - & 20x_1 & - & 5x_2 & - & 10x_3 & + & x_5 \\\\ x_4 & = & 20000 & - & 18x_1 & + & 2.5x_2 & - & 7x_3 & + & x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The associated basic solution is feasible, so now we just need to repeatedly call $\\text{PIVOT}$ until we obtain an optimal solution to $L_{aux}$. We'll choose $x_2$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcrcr} z & = & & - & x_0 \\\\ x_2 & = & 6000 & - & 0.2x_0 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 0.5x_0 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form is the final solution to the auxiliary problem. Since this solution has $x_0 = 0$, we know that our initial problem was feasible. Furthermore, since $x_0 = 0$, we can just remove it from the set of constraints. We then restore the original objective function, with appropriate substitutions made to include only the nonbasic variables. This yields $$ \\begin{array}{rcrcrcrcr} z & = & -6000 & + & 3x_1 & + & x_3 & - & 0.2x_5 \\\\ x_2 & = & 6000 & - & 4x_1 & - & 2x_3 & + & 0.2x_5 \\\\ x_4 & = & 35000 & - & 28x_1 & - & 12x_3 & + & 1.5x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ This slack form has a feasible basic solution, and we can return it to $\\text{SIMPLEX}$. We choose $x_1$ as our entering variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & -2250 & - & \\frac{2}{7} x_3 & - & \\frac{3}{28} x_4 & - & \\frac{11}{280} x_5 \\\\ x_1 & = & 1250 & - & \\frac{3}{7} x_3 & - & \\frac{1}{28} x_4 & + & \\frac{15}{280} x_5 \\\\ x_2 & = & 1000 & - & \\frac{2}{7} x_3 & + & \\frac{4}{28} x_4 & - & \\frac{4}{280} x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ At this point, all coefficients in the objective function are negative, so the basic solution is an optimal solution. This solution is $(x_1, x_2, x_3) = (1250, 1000, 0)$.","title":"29.3-7"},{"location":"Chap29/29.3/#293-8","text":"In the proof of Lemma 29.5, we argued that there are at most $\\binom{m + n}{n}$ ways to choose a set $B$ of basic variables. Give an example of a linear program in which there are strictly fewer than $\\binom{m + n}{n}$ ways to choose the set $B$. Consider the simple program $$ \\begin{array}{rcrcrl} z & = & & - & x_1 \\\\ x_2 & = & 1 & - & x_1 & . \\end{array} $$ In this case, we have $m = n = 1$, so $\\binom{m + n}{n} = \\binom{2}{1} = 2$, however, since the only coefficients of the objective function are negative, we can't make any other choices for basic variable. We must immediately terminate with the basic solution $(x_1, x_2) = (0, 1)$, which is optimal.","title":"29.3-8"},{"location":"Chap29/29.4/","text":"29.4-1 Formulate the dual of the linear program given in Exercise 29.3-5. By just transposing $A$, swapping $b$ and $c$, and switching the maximization to a minimization, we want to minimize $20y_1 + 12y_2 + 16y_3$ subject to the constrain $$ \\begin{aligned} y_1 + y_2 & \\ge 18 \\\\ y_1 + y_3 & \\ge 12.5 \\\\ y_1, y_2, y_3 & \\ge 0 \\end{aligned} $$ 29.4-2 Suppose that we have a linear program that is not in standard form. We could produce the dual by first converting it to standard form, and then taking the dual. It would be more convenient, however, to be able to produce the dual directly. Explain how we can directly take the dual of an arbitrary linear program. By working through each aspect of putting a general linear program into standard form, as outlined on page 852, we can show how to deal with transforming each into the dual individually. If the problem is a minimization instead of a maximization, replace $c_j$ by $\u2212c_j$ in $\\text{(29.84)}$. If there is a lack of nonnegativity constraint on $x_j$ we duplicate the $j$th column of $A$, which corresponds to duplicating the $j$th row of $A^{\\text T}$. If there is an equality constraint for $b_i$, we convert it to two inequalities by duplicating then negating the $i$th column of $A^{\\text T}$, duplicating then negating the $i$th entry of $b$, and adding an extra $y_i$ variable. We handle the greater-than-or-equal-to sign $\\sum_{i = 1}^n a_{ij}x_j \\ge b_i$ by negating $i$th column of $A^{\\text T}$ and negating $b_i$. Then we solve the dual problem of minimizing $b^{\\text T}y$ subject to $A^{\\text T}y$ and $y \\ge 0$. 29.4-3 Write down the dual of the maximum-flow linear program, as given in lines $\\text{(29.47)}$\u2013$\\text{(29.50)}$ on page 860. Explain how to interpret this formulation as a minimum-cut problem. First, we'll convert the linear program for maximum flow described in equation $\\text{(29.47)}$-$\\text{(29.50)}$ into standard form. The objective function says that $c$ is a vector indexed by a pair of vertices, and it is positive one if $s$ is the first index and negative one if $s$ is the second index (zero if it is both). Next, we'll modify the constraints by switching the equalities over into inequalities to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} f_{vu} & \\ge & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Then, we'll convert all but the last set of the inequalities to be $\\le$ by multiplying the third line by $-1$. $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} & \\le & \\sum_{u \\in V} -f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Finally, we'll bring all the variables over to the left to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} - \\sum_{u \\in V} f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} - \\sum_{u \\in V} -f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Now, we can finally write down our $A$ and $b$. $A$ will be a $|V|^2 \\times |V|^2 + 2|V| \u2212 4$ matrix built from smaller matrices $A_1$ and $A_2$ which correspond to the three types of constraints that we have (of course, not counting the non-negativity constraints). We will let $g(u, v)$ be any bijective mapping from $V \\times V$ to $[|V|^2]$. We'll also let $h$ be any bijection from $V - \\{s, t\\}$ to $[|V| - 2]$. $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\\\ -A_2 \\end{pmatrix}, $$ where $A_1$ is defined as having its row $g(u, v)$ be all zeroes except for having the value $1$ at at the $g(u, v)$th entry. We define $A_2$ to have it's row $h(u)$ be equal to $1$ at all columns $j$ for which $j = g(v, u)$ for some $v$ and equal to $-1$ at all columns $j$ for which $j = g(u, v)$ for some $v$. Lastly, we mention that $b$ is defined as having it's $j$th entry be equal to $c(u, v)$ if $j = g(u, v)$ and zero if $j > |V|^2$. Now that we have placed the linear program in standard form, we can take its dual. We want to minimize $\\sum_{i = 1}^{|V|^2 + 2|V| - 2} b_iy_i$ given the constraints that all the $y$ values are non-negative, and $A^{\\text T} y \\ge c$. 29.4-4 Write down the dual of the minimum-cost-flow linear program, as given in lines $\\text{(29.51)}$\u2013$\\text{(29.52)}$ on page 862. Explain how to interpret this problem in terms of graphs and flows. First we need to put the linear programming problem into standard form, as follows: $$ \\begin{array}{lrcrl} \\text{maximize} & \\sum_{(u, v) \\in E} -a(u, v) f_{uv} \\\\ \\text{subject to} & \\\\ & f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ & \\sum_{v \\in V} f_{vu} - \\sum_{v \\in V} f_{uv} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{uv} - \\sum_{v \\in V} f_{vu} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{sv} - \\sum_{v \\in V} f_{vs} & \\le & d \\\\ & \\sum_{v \\in V} f_{vs} - \\sum_{v \\in V} f_{sv} & \\le & -d \\\\ & f_{uv} & \\ge & 0 & . \\end{array} $$ We now formulate the dual problem. Let the vertices be denoted $v_1, v_2, \\dots, v_n, s, t$ and the edges be $e_1, e_2, \\dots, e_k$. Then we have $b_i = c(e_i)$ for $1 \\le i \\le k$, $b_i = 0$ for $k + 1 \\le i \\le k + 2n$, $b_{k + 2n + 1} = d$, and $b_{k + 2n + 2} = \u2212d$. We also have $c_i = \u2212a(e_i)$ for $1 \\le i \\le k$. For notation, let $j.left$ denote the tail of edge $e_j$ and $j.right$ denote the head. Let $\\chi_s(e_j) = 1$ if $e_j$ enters $s$, set it equal to $-1$ if $e_j$ leaves $s$, and set it equal to $0$ if $e_j$ is not incident with $s$. The dual problem is: $$ \\begin{array}{ll} \\text{minimize} & \\sum_{i = 1}^k c(e_j)y_i + dy_{k + 2n + 1} - dy_{k + 2n + 2} \\\\ \\text{subject to} & \\\\ & y_j + y_{k + e_j.right} - y_{k + j.left} - y_{k + n + e_j.right} + y_{k + n + e_j.left} - \\chi_s(e_j) y_{k + 2n + 1} + \\chi_s(e_j) y_{k + 2n + 2} \\ge -a(e_j), \\end{array} $$ where $j$ runs between $1$ and $k$. There is one constraint equation for each edge $e_j$. 29.4-5 Show that the dual of the dual of a linear program is the primal linear program. Suppose that our original linear program is in standard form for some $A$, $b$, $c$. Then, the dual of this is to minimize $\\sum_{i = 1}^m b_iy_i$ subject to $A^{\\text T} y \\ge c$ This can be rewritten as wanting to maximize $\\sum_{i = 1}^m (\u2212b_i)y_i$ subject to $(\u2212A)^{\\text T} y \\le \u2212c$. Since this is a standard form, we can take its dual easily, it is minimize $\\sum_{j = 1}^n (\u2212c_j)x_j$ subject to $(\u2212A)x \\ge \u2212b$. This is the same as minimizing $\\sum_{j = 1}^n c_jx_j$ subject to $Ax \\le b$, which was the original linear program. 29.4-6 Which result from Chapter 26 can be interpreted as weak duality for the maximum-flow problem? Corollary 26.5 from Chapter 26 can be interpreted as weak duality.","title":"29.4 Duality"},{"location":"Chap29/29.4/#294-1","text":"Formulate the dual of the linear program given in Exercise 29.3-5. By just transposing $A$, swapping $b$ and $c$, and switching the maximization to a minimization, we want to minimize $20y_1 + 12y_2 + 16y_3$ subject to the constrain $$ \\begin{aligned} y_1 + y_2 & \\ge 18 \\\\ y_1 + y_3 & \\ge 12.5 \\\\ y_1, y_2, y_3 & \\ge 0 \\end{aligned} $$","title":"29.4-1"},{"location":"Chap29/29.4/#294-2","text":"Suppose that we have a linear program that is not in standard form. We could produce the dual by first converting it to standard form, and then taking the dual. It would be more convenient, however, to be able to produce the dual directly. Explain how we can directly take the dual of an arbitrary linear program. By working through each aspect of putting a general linear program into standard form, as outlined on page 852, we can show how to deal with transforming each into the dual individually. If the problem is a minimization instead of a maximization, replace $c_j$ by $\u2212c_j$ in $\\text{(29.84)}$. If there is a lack of nonnegativity constraint on $x_j$ we duplicate the $j$th column of $A$, which corresponds to duplicating the $j$th row of $A^{\\text T}$. If there is an equality constraint for $b_i$, we convert it to two inequalities by duplicating then negating the $i$th column of $A^{\\text T}$, duplicating then negating the $i$th entry of $b$, and adding an extra $y_i$ variable. We handle the greater-than-or-equal-to sign $\\sum_{i = 1}^n a_{ij}x_j \\ge b_i$ by negating $i$th column of $A^{\\text T}$ and negating $b_i$. Then we solve the dual problem of minimizing $b^{\\text T}y$ subject to $A^{\\text T}y$ and $y \\ge 0$.","title":"29.4-2"},{"location":"Chap29/29.4/#294-3","text":"Write down the dual of the maximum-flow linear program, as given in lines $\\text{(29.47)}$\u2013$\\text{(29.50)}$ on page 860. Explain how to interpret this formulation as a minimum-cut problem. First, we'll convert the linear program for maximum flow described in equation $\\text{(29.47)}$-$\\text{(29.50)}$ into standard form. The objective function says that $c$ is a vector indexed by a pair of vertices, and it is positive one if $s$ is the first index and negative one if $s$ is the second index (zero if it is both). Next, we'll modify the constraints by switching the equalities over into inequalities to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} f_{vu} & \\ge & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Then, we'll convert all but the last set of the inequalities to be $\\le$ by multiplying the third line by $-1$. $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} & \\le & \\sum_{u \\in V} f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} & \\le & \\sum_{u \\in V} -f_{uv} & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Finally, we'll bring all the variables over to the left to get $$ \\begin{array}{rcll} f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ \\sum_{u \\in V} f_{vu} - \\sum_{u \\in V} f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ \\sum_{u \\in V} -f_{vu} - \\sum_{u \\in V} -f_{uv} & \\le & 0 & \\text{ for each } v \\in V - \\{s, t\\} \\\\ f_{uv} & \\ge & 0 & \\text{ for each } u, v \\in V \\end{array} $$ Now, we can finally write down our $A$ and $b$. $A$ will be a $|V|^2 \\times |V|^2 + 2|V| \u2212 4$ matrix built from smaller matrices $A_1$ and $A_2$ which correspond to the three types of constraints that we have (of course, not counting the non-negativity constraints). We will let $g(u, v)$ be any bijective mapping from $V \\times V$ to $[|V|^2]$. We'll also let $h$ be any bijection from $V - \\{s, t\\}$ to $[|V| - 2]$. $$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\\\ -A_2 \\end{pmatrix}, $$ where $A_1$ is defined as having its row $g(u, v)$ be all zeroes except for having the value $1$ at at the $g(u, v)$th entry. We define $A_2$ to have it's row $h(u)$ be equal to $1$ at all columns $j$ for which $j = g(v, u)$ for some $v$ and equal to $-1$ at all columns $j$ for which $j = g(u, v)$ for some $v$. Lastly, we mention that $b$ is defined as having it's $j$th entry be equal to $c(u, v)$ if $j = g(u, v)$ and zero if $j > |V|^2$. Now that we have placed the linear program in standard form, we can take its dual. We want to minimize $\\sum_{i = 1}^{|V|^2 + 2|V| - 2} b_iy_i$ given the constraints that all the $y$ values are non-negative, and $A^{\\text T} y \\ge c$.","title":"29.4-3"},{"location":"Chap29/29.4/#294-4","text":"Write down the dual of the minimum-cost-flow linear program, as given in lines $\\text{(29.51)}$\u2013$\\text{(29.52)}$ on page 862. Explain how to interpret this problem in terms of graphs and flows. First we need to put the linear programming problem into standard form, as follows: $$ \\begin{array}{lrcrl} \\text{maximize} & \\sum_{(u, v) \\in E} -a(u, v) f_{uv} \\\\ \\text{subject to} & \\\\ & f_{uv} & \\le & c(u, v) & \\text{ for each } u, v \\in V \\\\ & \\sum_{v \\in V} f_{vu} - \\sum_{v \\in V} f_{uv} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{uv} - \\sum_{v \\in V} f_{vu} & \\le & 0 & \\text{ for each } u \\in V - \\{s, t\\} \\\\ & \\sum_{v \\in V} f_{sv} - \\sum_{v \\in V} f_{vs} & \\le & d \\\\ & \\sum_{v \\in V} f_{vs} - \\sum_{v \\in V} f_{sv} & \\le & -d \\\\ & f_{uv} & \\ge & 0 & . \\end{array} $$ We now formulate the dual problem. Let the vertices be denoted $v_1, v_2, \\dots, v_n, s, t$ and the edges be $e_1, e_2, \\dots, e_k$. Then we have $b_i = c(e_i)$ for $1 \\le i \\le k$, $b_i = 0$ for $k + 1 \\le i \\le k + 2n$, $b_{k + 2n + 1} = d$, and $b_{k + 2n + 2} = \u2212d$. We also have $c_i = \u2212a(e_i)$ for $1 \\le i \\le k$. For notation, let $j.left$ denote the tail of edge $e_j$ and $j.right$ denote the head. Let $\\chi_s(e_j) = 1$ if $e_j$ enters $s$, set it equal to $-1$ if $e_j$ leaves $s$, and set it equal to $0$ if $e_j$ is not incident with $s$. The dual problem is: $$ \\begin{array}{ll} \\text{minimize} & \\sum_{i = 1}^k c(e_j)y_i + dy_{k + 2n + 1} - dy_{k + 2n + 2} \\\\ \\text{subject to} & \\\\ & y_j + y_{k + e_j.right} - y_{k + j.left} - y_{k + n + e_j.right} + y_{k + n + e_j.left} - \\chi_s(e_j) y_{k + 2n + 1} + \\chi_s(e_j) y_{k + 2n + 2} \\ge -a(e_j), \\end{array} $$ where $j$ runs between $1$ and $k$. There is one constraint equation for each edge $e_j$.","title":"29.4-4"},{"location":"Chap29/29.4/#294-5","text":"Show that the dual of the dual of a linear program is the primal linear program. Suppose that our original linear program is in standard form for some $A$, $b$, $c$. Then, the dual of this is to minimize $\\sum_{i = 1}^m b_iy_i$ subject to $A^{\\text T} y \\ge c$ This can be rewritten as wanting to maximize $\\sum_{i = 1}^m (\u2212b_i)y_i$ subject to $(\u2212A)^{\\text T} y \\le \u2212c$. Since this is a standard form, we can take its dual easily, it is minimize $\\sum_{j = 1}^n (\u2212c_j)x_j$ subject to $(\u2212A)x \\ge \u2212b$. This is the same as minimizing $\\sum_{j = 1}^n c_jx_j$ subject to $Ax \\le b$, which was the original linear program.","title":"29.4-5"},{"location":"Chap29/29.4/#294-6","text":"Which result from Chapter 26 can be interpreted as weak duality for the maximum-flow problem? Corollary 26.5 from Chapter 26 can be interpreted as weak duality.","title":"29.4-6"},{"location":"Chap29/29.5/","text":"29.5-1 Give detailed pseudocode to implement lines 5 and 14 of $\\text{INITIALIZE-SIMPLEX}$. For line 5, first let $(N, B, A, b, c, v)$ be the result of calling $\\text{PIVOT}$ on $L_{aux}$ using $x_0$ as the entering variable. Then repeatedly call $\\text{PIVOT}$ until an optimal solution to $L_{aux}$ is obtained, and return this to $(N, B, A, b, c, v)$. To remove $x_0$ from the constraints, set $a_{i, 0} = 0$ for all $i \\in B$, and set $N = N \\backslash \\{0\\}$. To restore the original objective function of $L$, for each $j \\in N$ and each $i \\in B$, set $c_j = c_j \u2212 c_ia_{ij}$. 29.5-2 Show that when the main loop of $\\text{SIMPLEX}$ is run by $\\text{INITIALIZE-SIMPLEX}$, it can never return \"unbounded.\" In order to enter line 10 of $\\text{INITIALIZE-SIMPLEX}$ and begin iterating the main loop of $\\text{SIMPLEX}$, we must have recovered a basic solution which is feasible for $L_{aux}$. Since $x_0 \\ge 0$ and the objective function is $\u2212x_0$, the objective value associated to this solution (or any solution) must be negative. Since the goal is to aximize, we have an upper bound of $0$ on the objective value. By Lemma 29.2, $\\text{SIMPLEX}$ correctly determines whether or not the input linear program is unbounded. Since $L_{aux}$ is not unbounded, this can never be returned by $\\text{SIMPLEX}$. 29.5-3 Suppose that we are given a linear program $L$ in standard form, and suppose that for both $L$ and the dual of $L$, the basic solutions associated with the initial slack forms are feasible. Show that the optimal objective value of $L$ is $0$. Since it is in standard form, the objective function has no constant term, it is entirely given by $\\sum_{i = 1}^n c_ix_i$, which is going to be zero for any basic solution. The same thing goes for its dual. Since there is some solution which has the objective function achieve the same value both for the dual and the primal, by the corollary to the weak duality theorem, that common value must be the optimal value of the objective function. 29.5-4 Suppose that we allow strict inequalities in a linear program. Show that in this case, the fundamental theorem of linear programming does not hold. Consider the linear program in which we wish to maximize $x_1$ subject to the constraint $x_1 < 1$ and $x_1 \\ge 0$. This has no optimal solution, but it is clearly bounded and has feasible solutions. Thus, the Fundamental theorem of linear programming does not hold in the case of strict inequalities. 29.5-5 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 8 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & - & x_2 & \\le & 8 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 8 & + & x_0 & - & x_1 & + & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 11 & - & 2x_1 & & & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, so we repeatedly call $\\text{PIVOT}$ to get the optimal solution to $L_{aux}$. We'll choose $x_1$ to be our entering variable and $x_0$ to be the leaving variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & & & -x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_0 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is now optimal for $L_{aux}$, so we return this slack form to $\\text{SIMPLEX}$, set $x_0 = 0$, and update the objective function which yields $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_5$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1 & = & 2 & + & (1 / 5)x_4 & + & (1 / 5)x_5 \\\\ x_2 & = & 1 & + & (4 / 5)x_4 & - & (1 / 5)x_5 \\\\ x_3 & = & 7 & - & (3 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_4$ as our entering variable, which makes $x_3$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & (64 / 3) & - & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & (34 / 3) & - & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & (10 / 3) & - & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & (35 / 3) & - & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Now all coefficients in the objective function are negative, so the basic solution is the optimal solution. It is $(x_1, x_2) = (34 / 3, 10 / 3)$. 29.5-6 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & 2x_2 & \\le & 4 \\\\ & -2x_1 & - & 6x_2 & \\le & -12 \\\\ & & & x_2 & \\le & 1 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & + & 2x_2 & \\le & 4 \\\\ & -x_0 & - & 2x_1 & - & 6x_2 & \\le & -12 \\\\ & -x_0 & & & + & x_2 & \\le & 1 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 4 & + & x_0 & - & x_1 & - & 2x_2 \\\\ x_4 & = & -12 & + & x_0 & + & 2x_1 & + & 6x_2 \\\\ x_5 & = & 1 & + & x_0 & & & - & x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -12 & + & 2x_1 & + & 6x_2 & - & x_4 \\\\ x_0 & = & 12 & - & 2x_1 & - & 6x_2 & + & x_4 \\\\ x_3 & = & 16 & - & 3x_1 & - & 8x_2 & + & x_4 \\\\ x_5 & = & 13 & - & 2x_1 & - & 8x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is $(x_0, x_1, x_2, x_3, x_4, x_5) = (12, 0, 0, 16, 0, 13)$ which is feasible for the auxiliary program. Now we need to run $\\text{SIMPLEX}$ to find the optimal objective value to $L_{aux}$. Let $x_1$ be our next entering variable. It is most constrained by $x_3$, which will be our leaving variable. After $\\text{PIVOT}$, the new linear program is $$ \\begin{array}{rcrcrcrcr} z & = & -(4 / 3) & + & (2 / 3)x_2 & - & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0 & = & (4 / 3) & - & (2 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_1 & = & (16 / 3) & - & (8 / 3)x_2 & - & (1 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_5 & = & (7 / 3) & - & (8 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Every coefficient in the objective function is negative, so we take the basic solution $(x_0, x_1, x_2, x_3, x_4, x_5) = (4 / 3, 16 / 3, 0, 0, 0, 7 / 3)$ which is also optimal. Since $x_0 \\ne 0$, the original linear program must be unfeasible. 29.5-7 Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & - & x_1 & + & x_2 & \\le & -1 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & -1 & + & x_0 & + & x_1 & - & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Let $x_1$ be our entering variable. Then $x_0$ is our leaving variable, and we have $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, and optimal for $L_{aux}$, so we return this and run $\\text{SIMPLEX}$. Updating the objective function and setting $x_0 = 0$ gives $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_3$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & - & x_3 & + & 2x_4 \\\\ x_1 & = & 2 & + & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_2 & = & 1 & - & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_5 & = & & & (5 / 2)x_3 & - & (3 / 2)x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we use $x_4$ as our entering variable, which makes $x_5$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & 2 & + & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & 1 & + & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & & & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Finally, we would like to choose $x_3$ as our entering variable, but every coefficient on $x_3$ is positive, so $\\text{SIMPLEX}$ returns that the linear program is unbounded. 29.5-8 Solve the linear program given in $\\text{(29.6)}$\u2013$\\text{(29.10)}$. We first put the linear program in standard form, $$ \\begin{array}{lrcrcrcrcrl} \\text{maxmize} & x_1 & + & x_2 & + & x_3 & + & x_4 \\\\ \\text{subject to} & \\\\ & 2x_1 & - & 8x_2 & & & - & 10x_4 & \\le & -50 \\\\ & -5x_1 & - & 2x_2 & & & & & \\le & -100 \\\\ & -3x_1 & + & 5x_2 & - & 10x_3 & + & 2x_4 & \\le & -25 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program. $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & & - & x_0 \\\\ x_5 & = & -50 & + & x_0 & - & 2x_1 & + & 8x_2 & & & + & 10x_4 \\\\ x_6 & = & -100 & + & x_0 & + & 5x_1 & + & 2x_2 \\\\ x_7 & = & -25 & + & x_0 & + & 3x_1 & - & 5x_2 & + & 10x_3 & - & 2x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The index of the minimum $b_i$ is $2$, so we take $x_0$ to be our entering variable and $x_6$ to be our leaving variable. The call to $\\text{PIVOT}$ on line 8 yields $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & -100 & + & 5x_1 & + & 2x_2 & & & & & - & x_6 \\\\ x_0 & = & 100 & - & 5x_1 & - & 2x_2 & & & & & + & x_6 \\\\ x_5 & = & 50 & - & 7x_1 & + & 8x_2 & & & + & 10x_4 & + & x_6 \\\\ x_7 & = & 75 & - & 2x_1 & - & 7x_2 & + & 10x_3 & - & 2x_4 & + & x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ Next we'll take $x_2$ to be our entering variable and $x_5$ to be our leaving variable. The call to $\\text{PIVOT}$ yields $$ \\begin{array}{rcrcrcrcrcrcr} z & = & -225 / 2 & + & (27 / 4)x_1 & & & - & (10 / 4)x_4 & + & (1 / 4)x_5 & - & (5 / 4)x_6 \\\\ x_0 & = & 225 / 2 & - & (27 / 4)x_1 & & & + & (10 / 4)x_4 & - & (1 / 4)x_5 & + & (5 / 4)x_6 \\\\ x_2 & = & -50 / 8 & + & (7 / 8)x_1 & & & - & (10 / 8)x_4 & + & (1 / 8)x_5 & - & (1 / 8)x_6 \\\\ x_7 & = & 475 / 4 & - & (65 / 8)x_1 & + & 10x_3 & + & (54 / 8)x_4 & - & (7 / 8)x_5 & + & (15 / 8)x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The work gets rather messy, but $\\text{INITIALIZE-SIMPLEX}$ does eventually give a feasible solution to the linear program, and after running the simplex method we find that $(x_1, x_2, x_3, x_4) = (175 / 11, 225 / 22, 125 / 44, 0)$ is an optimal solution to the original linear programming problem. 29.5-9 Consider the following $1$-variable linear program, which we call $P$: $$ \\begin{array}{lrcrl} \\text{maximize} & tx \\\\ \\text{subject to} & rx & \\le & s \\\\ & x & \\ge & 0 & , \\end{array} $$ where $r$, $s$, and $t$ are arbitrary real numbers. Let $D$ be the dual of $P$. State for which values of $r$, $s$, and $t$ you can assert that Both $P$ and $D$ have optimal solutions with finite objective values. $P$ is feasible, but $D$ is infeasible. $D$ is feasible, but $P$ is infeasible. Neither $P$ nor $D$ is feasible. One option is that $r = 0$, $s \\ge 0$ and $t \\le 0$. Suppose that $r > 0$, then, if we have that $s$ is non-negative and $t$ is non-positive, it will be as we want. We will split into two cases based on $r$. If $r = 0$, then this is exactly when $t$ is non-positive and $s$ is non-negative. The other possible case is that $r$ is negative, and $t$ is positive. In which case, because $r$ is negative, we can always get $rx$ as small as we want so s doesn't matter, however, we can never make $rx$ positive so it can never be $\\ge t$. Again, we split into two possible cases for $r$. If $r = 0$, then it is when $t$ is nonnegative and $s$ is non-positive. The other possible case is that $r$ is positive, and $s$ is negative. Since $r$ is positive, $rx$ will always be non-negative, so it cannot be $\\le s$. But since $r$ is positive, we have that we can always make $rx$ as big as we want, in particular, greater than $t$. If we have that $r = 0$ and $t$ is positive and $s$ is negative. If $r$ is nonzero, then we can always either make $rx$ really big or really small depending on the sign of $r$, meaning that either the primal or the dual would be feasable.","title":"29.5 The initial basic feasible solution"},{"location":"Chap29/29.5/#295-1","text":"Give detailed pseudocode to implement lines 5 and 14 of $\\text{INITIALIZE-SIMPLEX}$. For line 5, first let $(N, B, A, b, c, v)$ be the result of calling $\\text{PIVOT}$ on $L_{aux}$ using $x_0$ as the entering variable. Then repeatedly call $\\text{PIVOT}$ until an optimal solution to $L_{aux}$ is obtained, and return this to $(N, B, A, b, c, v)$. To remove $x_0$ from the constraints, set $a_{i, 0} = 0$ for all $i \\in B$, and set $N = N \\backslash \\{0\\}$. To restore the original objective function of $L$, for each $j \\in N$ and each $i \\in B$, set $c_j = c_j \u2212 c_ia_{ij}$.","title":"29.5-1"},{"location":"Chap29/29.5/#295-2","text":"Show that when the main loop of $\\text{SIMPLEX}$ is run by $\\text{INITIALIZE-SIMPLEX}$, it can never return \"unbounded.\" In order to enter line 10 of $\\text{INITIALIZE-SIMPLEX}$ and begin iterating the main loop of $\\text{SIMPLEX}$, we must have recovered a basic solution which is feasible for $L_{aux}$. Since $x_0 \\ge 0$ and the objective function is $\u2212x_0$, the objective value associated to this solution (or any solution) must be negative. Since the goal is to aximize, we have an upper bound of $0$ on the objective value. By Lemma 29.2, $\\text{SIMPLEX}$ correctly determines whether or not the input linear program is unbounded. Since $L_{aux}$ is not unbounded, this can never be returned by $\\text{SIMPLEX}$.","title":"29.5-2"},{"location":"Chap29/29.5/#295-3","text":"Suppose that we are given a linear program $L$ in standard form, and suppose that for both $L$ and the dual of $L$, the basic solutions associated with the initial slack forms are feasible. Show that the optimal objective value of $L$ is $0$. Since it is in standard form, the objective function has no constant term, it is entirely given by $\\sum_{i = 1}^n c_ix_i$, which is going to be zero for any basic solution. The same thing goes for its dual. Since there is some solution which has the objective function achieve the same value both for the dual and the primal, by the corollary to the weak duality theorem, that common value must be the optimal value of the objective function.","title":"29.5-3"},{"location":"Chap29/29.5/#295-4","text":"Suppose that we allow strict inequalities in a linear program. Show that in this case, the fundamental theorem of linear programming does not hold. Consider the linear program in which we wish to maximize $x_1$ subject to the constraint $x_1 < 1$ and $x_1 \\ge 0$. This has no optimal solution, but it is clearly bounded and has feasible solutions. Thus, the Fundamental theorem of linear programming does not hold in the case of strict inequalities.","title":"29.5-4"},{"location":"Chap29/29.5/#295-5","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & x_1 & - & x_2 & \\le & 8 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & - & x_2 & \\le & 8 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 8 & + & x_0 & - & x_1 & + & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 11 & - & 2x_1 & & & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, so we repeatedly call $\\text{PIVOT}$ to get the optimal solution to $L_{aux}$. We'll choose $x_1$ to be our entering variable and $x_0$ to be the leaving variable. This gives $$ \\begin{array}{rcrcrcrcr} z & = & & & -x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_0 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is now optimal for $L_{aux}$, so we return this slack form to $\\text{SIMPLEX}$, set $x_0 = 0$, and update the objective function which yields $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 5 & + & 2x_2 & - & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_5$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1 & = & 2 & + & (1 / 5)x_4 & + & (1 / 5)x_5 \\\\ x_2 & = & 1 & + & (4 / 5)x_4 & - & (1 / 5)x_5 \\\\ x_3 & = & 7 & - & (3 / 5)x_4 & - & (2 / 5)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_4$ as our entering variable, which makes $x_3$ our leaving variable. $\\text{PIVOT}$ then gives, $$ \\begin{array}{rcrcrcr} z & = & (64 / 3) & - & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & (34 / 3) & - & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & (10 / 3) & - & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & (35 / 3) & - & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Now all coefficients in the objective function are negative, so the basic solution is the optimal solution. It is $(x_1, x_2) = (34 / 3, 10 / 3)$.","title":"29.5-5"},{"location":"Chap29/29.5/#295-6","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & - & 2x_2 \\\\ \\text{subject to} & \\\\ & x_1 & + & 2x_2 & \\le & 4 \\\\ & -2x_1 & - & 6x_2 & \\le & -12 \\\\ & & & x_2 & \\le & 1 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & + & x_1 & + & 2x_2 & \\le & 4 \\\\ & -x_0 & - & 2x_1 & - & 6x_2 & \\le & -12 \\\\ & -x_0 & & & + & x_2 & \\le & 1 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & 4 & + & x_0 & - & x_1 & - & 2x_2 \\\\ x_4 & = & -12 & + & x_0 & + & 2x_1 & + & 6x_2 \\\\ x_5 & = & 1 & + & x_0 & & & - & x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -12 & + & 2x_1 & + & 6x_2 & - & x_4 \\\\ x_0 & = & 12 & - & 2x_1 & - & 6x_2 & + & x_4 \\\\ x_3 & = & 16 & - & 3x_1 & - & 8x_2 & + & x_4 \\\\ x_5 & = & 13 & - & 2x_1 & - & 8x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is $(x_0, x_1, x_2, x_3, x_4, x_5) = (12, 0, 0, 16, 0, 13)$ which is feasible for the auxiliary program. Now we need to run $\\text{SIMPLEX}$ to find the optimal objective value to $L_{aux}$. Let $x_1$ be our next entering variable. It is most constrained by $x_3$, which will be our leaving variable. After $\\text{PIVOT}$, the new linear program is $$ \\begin{array}{rcrcrcrcr} z & = & -(4 / 3) & + & (2 / 3)x_2 & - & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0 & = & (4 / 3) & - & (2 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_1 & = & (16 / 3) & - & (8 / 3)x_2 & - & (1 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_5 & = & (7 / 3) & - & (8 / 3)x_2 & + & (2 / 3)x_3 & + & (1 / 3) x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Every coefficient in the objective function is negative, so we take the basic solution $(x_0, x_1, x_2, x_3, x_4, x_5) = (4 / 3, 16 / 3, 0, 0, 0, 7 / 3)$ which is also optimal. Since $x_0 \\ne 0$, the original linear program must be unfeasible.","title":"29.5-6"},{"location":"Chap29/29.5/#295-7","text":"Solve the following linear program using $\\text{SIMPLEX}$: $$ \\begin{array}{lrcrcrl} \\text{maxmize} & x_1 & + & 3x_2 \\\\ \\text{subject to} & \\\\ & -x_1 & + & x_2 & \\le & -1 \\\\ & -x_1 & - & x_2 & \\le & -3 \\\\ & -x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_1, x_2 & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program, $$ \\begin{array}{lrcrcrcrl} \\text{maxmize} & -x_0 \\\\ \\text{subject to} & \\\\ & -x_0 & - & x_1 & + & x_2 & \\le & -1 \\\\ & -x_0 & - & x_1 & - & x_2 & \\le & -3 \\\\ & -x_0 & - & x_1 & + & 4x_2 & \\le & 2 \\\\ & & x_0, x_1, x_2 & & & & \\ge & 0 & . \\end{array} $$ Writing this linear program in slack form, $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_3 & = & -1 & + & x_0 & + & x_1 & - & x_2 \\\\ x_4 & = & -3 & + & x_0 & + & x_1 & + & x_2 \\\\ x_5 & = & 2 & + & x_0 & + & x_1 & - & 4x_2 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we make one call to $\\text{PIVOT}$ where $x_0$ is the entering variable and $x_4$ is the leaving variable. $$ \\begin{array}{rcrcrcrcr} z & = & -3 & + & x_1 & + & x_2 & - & x_4 \\\\ x_0 & = & 3 & - & x_1 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Let $x_1$ be our entering variable. Then $x_0$ is our leaving variable, and we have $$ \\begin{array}{rcrcrcrcr} z & = & & - & x_0 \\\\ x_1 & = & 3 & - & x_0 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & & & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & & & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ The basic solution is feasible, and optimal for $L_{aux}$, so we return this and run $\\text{SIMPLEX}$. Updating the objective function and setting $x_0 = 0$ gives $$ \\begin{array}{rcrcrcr} z & = & 3 & + & 2x_2 & + & x_4 \\\\ x_1 & = & 3 & - & x_2 & + & x_4 \\\\ x_3 & = & 2 & - & 2x_2 & + & x_4 \\\\ x_5 & = & 5 & - & 5x_2 & + & x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ We'll choose $x_2$ as our entering variable, which makes $x_3$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & - & x_3 & + & 2x_4 \\\\ x_1 & = & 2 & + & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_2 & = & 1 & - & (1 / 2)x_3 & + & (1 / 2)x_4 \\\\ x_5 & = & & & (5 / 2)x_3 & - & (3 / 2)x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Next we use $x_4$ as our entering variable, which makes $x_5$ our leaving variable. This gives $$ \\begin{array}{rcrcrcr} z & = & 5 & + & (7 / 3)x_3 & - & (4 / 3)x_5 \\\\ x_1 & = & 2 & + & (4 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_2 & = & 1 & + & (1 / 3)x_3 & - & (1 / 3)x_5 \\\\ x_4 & = & & & (5 / 3)x_3 & - & (2 / 3)x_5 \\\\ x_0, x_1, x_2, x_3, x_4, x_5 & \\ge & 0 & . \\end{array} $$ Finally, we would like to choose $x_3$ as our entering variable, but every coefficient on $x_3$ is positive, so $\\text{SIMPLEX}$ returns that the linear program is unbounded.","title":"29.5-7"},{"location":"Chap29/29.5/#295-8","text":"Solve the linear program given in $\\text{(29.6)}$\u2013$\\text{(29.10)}$. We first put the linear program in standard form, $$ \\begin{array}{lrcrcrcrcrl} \\text{maxmize} & x_1 & + & x_2 & + & x_3 & + & x_4 \\\\ \\text{subject to} & \\\\ & 2x_1 & - & 8x_2 & & & - & 10x_4 & \\le & -50 \\\\ & -5x_1 & - & 2x_2 & & & & & \\le & -100 \\\\ & -3x_1 & + & 5x_2 & - & 10x_3 & + & 2x_4 & \\le & -25 \\\\ & & x_1, x_2, x_3, x_4 & & & & & & \\ge & 0 & . \\end{array} $$ The initial basic solution isn't feasible, so we will need to form the auxiliary linear program. $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & & - & x_0 \\\\ x_5 & = & -50 & + & x_0 & - & 2x_1 & + & 8x_2 & & & + & 10x_4 \\\\ x_6 & = & -100 & + & x_0 & + & 5x_1 & + & 2x_2 \\\\ x_7 & = & -25 & + & x_0 & + & 3x_1 & - & 5x_2 & + & 10x_3 & - & 2x_4 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The index of the minimum $b_i$ is $2$, so we take $x_0$ to be our entering variable and $x_6$ to be our leaving variable. The call to $\\text{PIVOT}$ on line 8 yields $$ \\begin{array}{rcrcrcrcrcrrcl} z & = & -100 & + & 5x_1 & + & 2x_2 & & & & & - & x_6 \\\\ x_0 & = & 100 & - & 5x_1 & - & 2x_2 & & & & & + & x_6 \\\\ x_5 & = & 50 & - & 7x_1 & + & 8x_2 & & & + & 10x_4 & + & x_6 \\\\ x_7 & = & 75 & - & 2x_1 & - & 7x_2 & + & 10x_3 & - & 2x_4 & + & x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ Next we'll take $x_2$ to be our entering variable and $x_5$ to be our leaving variable. The call to $\\text{PIVOT}$ yields $$ \\begin{array}{rcrcrcrcrcrcr} z & = & -225 / 2 & + & (27 / 4)x_1 & & & - & (10 / 4)x_4 & + & (1 / 4)x_5 & - & (5 / 4)x_6 \\\\ x_0 & = & 225 / 2 & - & (27 / 4)x_1 & & & + & (10 / 4)x_4 & - & (1 / 4)x_5 & + & (5 / 4)x_6 \\\\ x_2 & = & -50 / 8 & + & (7 / 8)x_1 & & & - & (10 / 8)x_4 & + & (1 / 8)x_5 & - & (1 / 8)x_6 \\\\ x_7 & = & 475 / 4 & - & (65 / 8)x_1 & + & 10x_3 & + & (54 / 8)x_4 & - & (7 / 8)x_5 & + & (15 / 8)x_6 \\\\ x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7 & \\ge & 0 & . \\end{array} $$ The work gets rather messy, but $\\text{INITIALIZE-SIMPLEX}$ does eventually give a feasible solution to the linear program, and after running the simplex method we find that $(x_1, x_2, x_3, x_4) = (175 / 11, 225 / 22, 125 / 44, 0)$ is an optimal solution to the original linear programming problem.","title":"29.5-8"},{"location":"Chap29/29.5/#295-9","text":"Consider the following $1$-variable linear program, which we call $P$: $$ \\begin{array}{lrcrl} \\text{maximize} & tx \\\\ \\text{subject to} & rx & \\le & s \\\\ & x & \\ge & 0 & , \\end{array} $$ where $r$, $s$, and $t$ are arbitrary real numbers. Let $D$ be the dual of $P$. State for which values of $r$, $s$, and $t$ you can assert that Both $P$ and $D$ have optimal solutions with finite objective values. $P$ is feasible, but $D$ is infeasible. $D$ is feasible, but $P$ is infeasible. Neither $P$ nor $D$ is feasible. One option is that $r = 0$, $s \\ge 0$ and $t \\le 0$. Suppose that $r > 0$, then, if we have that $s$ is non-negative and $t$ is non-positive, it will be as we want. We will split into two cases based on $r$. If $r = 0$, then this is exactly when $t$ is non-positive and $s$ is non-negative. The other possible case is that $r$ is negative, and $t$ is positive. In which case, because $r$ is negative, we can always get $rx$ as small as we want so s doesn't matter, however, we can never make $rx$ positive so it can never be $\\ge t$. Again, we split into two possible cases for $r$. If $r = 0$, then it is when $t$ is nonnegative and $s$ is non-positive. The other possible case is that $r$ is positive, and $s$ is negative. Since $r$ is positive, $rx$ will always be non-negative, so it cannot be $\\le s$. But since $r$ is positive, we have that we can always make $rx$ as big as we want, in particular, greater than $t$. If we have that $r = 0$ and $t$ is positive and $s$ is negative. If $r$ is nonzero, then we can always either make $rx$ really big or really small depending on the sign of $r$, meaning that either the primal or the dual would be feasable.","title":"29.5-9"},{"location":"Chap29/Problems/29-1/","text":"Given a set of $m$ linear inequalities on $n$ variables $x_1, x_2, \\dots, x_n$, the linear-inequality feasibility problem asks whether there is a setting of the variables that simultaneously satisfies each of the inequalities. a. Show that if we have an algorithm for linear programming, we can use it to solve a linear-inequality feasibility problem. The number of variables and constraints that you use in the linear-programming problem should be polynomial in $n$ and $m$. b. Show that if we have an algorithm for the linear-inequality feasibility problem, we can use it to solve a linear-programming problem. The number of variables and linear inequalities that you use in the linear-inequality feasibility problem should be polynomial in $n$ and $m$, the number of variables and constraints in the linear program. a. We just let the linear inequalities that we need to satisfy be our set of constraints in the linear program. We let our function to maximize just be a constant. The solver for linear programs would fail to detect any feasible solution if the linear constraints were not feasible. If the linear programming solver returns any solution at all, we know that the linear constraints are feasible. b. Suppose that we are trying to solve the linear program in standard form with some particular $A$, $b$, $c$. That is, we want to maximize $\\sum_{j = 1}^n c_jx_j$ subject to $Ax \\le b$ and all entries of the $x$ vector are non-negative. Now, consider the dual program, that is, we want to minimize $\\sum_{i = 1}^m b_iy_i$ subject to $A^{\\text T} y \\ge c$ and all the entries in the $y$ vector are nonzero. We know by Corollary 29.9, if $x$ and $y$ are feasible solutions to their respective problems, then, if we have that their objective functions are equal, then, they are both optimal solutions. We can force their objective functions to be equal. To do this, let $c_k$ be some nonzero entry in the $c$ vector. If there are no nonzero entries, then the function we are trying to optimize is just the zero function, and it is exactly a feasibility question, so we we would be done. Then, we add two linear inequalities to require $x_k = \\frac{1}{c_k} \\Big(\\sum_{i = 1}^m b_iy_i - \\sum_{j = 1}^n c_jx_j \\Big)$. This will require that whatever values the variables take, their objective functions will be equal. Lastly, we just throw these in with the inequalities we already had. So, the constraints will be: $$ \\begin{aligned} Ax & \\le b \\\\ A^{\\text T} y & \\ge c \\\\ x_k & \\le \\frac{1}{c_k} \\Bigg(\\sum_{i = 1}^m b_iy_i - \\sum_{j = 1}^n c_jx_j \\Bigg) \\\\ x_k & \\ge \\frac{1}{c_k} \\Bigg(\\sum_{i = 1}^m b_iy_i - \\sum_{j = 1}^n c_jx_j \\Bigg) \\\\ x_1, x_2, \\dots, x_n, y_1, y_2, \\dots, y_m & \\ge 0. \\end{aligned} $$ We have a number of variables equal to $n + m$ and a number of constraints equal to $2 + 2n + 2m$, so both are polynomial in $n$ and $m$. Also, any assignment of variables which satisfy all of these constraints will be a feasible solution to both the problem and its dual that cause the respective objective functions to take the same value, and so, must be an optimal solution to both the original problem and its dual. This of course assumes that the linear inequality feasibility solver doesn't merely say that the inequalities are satisfiable, but actually returns a satisfying assignment. Lastly, it is necessary to note that if there is some optimal solution $x$, then, we can obtain an optimal solution for the dual that makes the objective functions equal by theorem 29.10. This ensures that the two constraints we added to force the objectives of the primal and the dual to be equal don't cause us to change the optimal solution to the linear program.","title":"29-1 Linear-inequality feasibility"},{"location":"Chap29/Problems/29-2/","text":"Complementary slackness describes a relationship between the values of primal variables and dual constraints and between the values of dual variables and primal constraints. Let $\\bar x$ be a feasible solution to the primal linear program given in $\\text{(29.16)\u2013(29.18)}$, and let $\\bar y$ be a feasible solution to the dual linear program given in $\\text{(29.83)\u2013(29.85)}$. Complementary slackness states that the following conditions are necessary and sufficient for $\\bar x$ and $\\bar y$ to be optimal: $$\\sum_{i = 1}^m a_{ij}\\bar y_i = c_j \\text{ or } \\bar x_j = 0 \\text{ for } j = 1, 2, \\dots, n$$ and $$\\sum_{j = 1}^m a_{ij}\\bar x_j = b_i \\text{ or } \\bar y_i = 0 \\text{ for } j = 1, 2, \\dots, m.$$ a. Verify that complementary slackness holds for the linear program in lines $\\text{(29.53)\u2013(29.57)}$. b. Prove that complementary slackness holds for any primal linear program and its corresponding dual. c. Prove that a feasible solution $\\bar x$ to a primal linear program given in lines $\\text{(29.16)\u2013(29.18)}$ is optimal if and only if there exist values $\\bar y = (\\bar y_1, \\bar y_2, \\dots, \\bar y_m)$ such that $\\bar y$ is a feasible solution to the dual linear program given in $\\text{(29.83)\u2013(29.85)}$, $\\sum_{i = 1}^m a_{ij}\\bar y_i = c_j$ for all $j$ such that $\\bar x_j > 0$, and $\\bar y_i = 0$ for all $i$ such that $\\sum_{j = 1}^n a_{ij}\\bar x_j < b_i$. a. An optimal solution to the LP program given in $\\text{(29.53)}$-$\\text{(29.57)}$ is $(x_1, x_2, x_3) = (8, 4, 0)$. An optimal solution to the dual is $(y_1, y_2, y_3) = (0, 1 / 6, 2 / 3)$. It is then straightforward to verify that the equations hold. b. First suppose that complementary slackness holds. Then the optimal objective value of the primal problem is, if it exists, $$ \\begin{aligned} \\sum_{k = 1} c_kx_k & = \\sum_{k = 1}^n \\sum_{i = 1}^m a_{ik}y_ix_k \\\\ & = \\sum_{i = 1}^m \\sum_{k = 1}^n a_{ik}x_ky_i \\\\ & = \\sum_{i = 1}^m b_iy_i, \\end{aligned} $$ which is precisely the optimal objective value of the dual problem. If any $x_j$ is $0$, then those terms drop out of them sum, so we can safely replace $c_k$ by whatever we like in those terms. Since the objective values are equal, they must be optimal. An identical argument shows that if an optimal solution exists for the dual problem then any feasible solution for the primal problem which satisfies the second equality of complementary slackness must also be optimal. Now suppose that $x$ and $y$ are optimal solutions, but that complementary slackness fails. In other words, there exists some $j$ such that $x_j \\ne 0$ but $\\sum_{i = 1}^m a_{ij}y_i > c_j$, or there exists some $i$ such that $y_i \\ne 0$ but $\\sum_{j = 1}^n a_{ij}x_j < b_i$. In the first case we have $$ \\begin{aligned} \\sum_{k = 1} c_kx_k & < \\sum_{k = 1}^n \\sum_{i = 1}^m a_{ik}y_ix_k \\\\ & = \\sum_{i = 1}^m \\sum_{k = 1}^n a_{ik}x_ky_i \\\\ & = \\sum_{i = 1}^m b_iy_i. \\end{aligned} $$ This implies that the optimal objective value of the primal solution is strictly less than the optimal value of the dual solution, a contradiction. The argument for the second case is identical. Thus, $x$ and $y$ are optimal solutions if and only if complementary slackness holds. c. This follows immediately from part (b). If $x$ is feasible and $y$ satisfies conditions 1, 2, and 3, then complementary slackness holds, so $x$ and $y$ are optimal. On the other hand, if $x$ is optimal, then the dual linear program must have an optimal solution $y$ as well, according to Theorem 29.10. Optimal solutions are feasible, and by part (b), $x$ and $y$ satisfy complementary slackness. Thus, conditions 1, 2, and 3 hold.","title":"29-2 Complementary slackness"},{"location":"Chap29/Problems/29-3/","text":"An integer linear-programming problem is a linear-programming problem with the additional constraint that the variables $x$ must take on integral values. Exercise 34.5-3 shows that just determining whether an integer linear program has a feasible solution is NP-hard, which means that there is no known polynomial-time algorithm for this problem. a. Show that weak duality (Lemma 29.8) holds for an integer linear program. b. Show that duality (Theorem 29.10) does not always hold for an integer linear program. c. Given a primal linear program in standard form, let us define $P$ to be the optimal objective value for the primal linear program, $D$ to be the optimal objective value for its dual, $IP$ to be the optimal objective value for the integer version of the primal (that is, the primal with the added constraint that the variables take on integer values), and $ID$ to be the optimal objective value for the integer version of the dual. Assuming that both the primal integer program and the dual integer program are feasible and bounded, show that $$IP \\le P = D \\le ID.$$ a. The proof for weak duality goes through identically. Nowhere in it does it use the integrality of the solutions. b. Consider the linear program given in standard form by $A = (1)$, $b = (\\frac{1}{2})$ and $c = (2)$. The highest we can get this is $0$ since that's that only value that $x$ can be. Now, consider the dual to this, that is, we are trying to minimize $\\frac{x}{2}$ subject to the constraint that $x \\ge 2$. This will be minimized when $x = 2$, so, the smallest solution we can get is $1$. Since we have just exhibited an example of a linear program that has a different optimal solution as it's dual, the duality theorem does not hold for integer linear program. c. The first inequality comes from looking at the fact that by adding the restriction that the solution must be integer valued, we obtain a set of feasible solutions that is a subset of the feasible solutions of the original primal linear program. Since, to get $IP$, we are taking the max over a subset of the things we are taking a max over to get $P$, we must get a number that is no larger. The third inequality is similar, except since we are taking min over a subset, the inequality goes the other way. The middle equality is given by Theorem 29.10.","title":"29-3 Integer linear programming"},{"location":"Chap29/Problems/29-4/","text":"Let $A$ be an $m \\times n$ matrix and $c$ be an $n$-vector. Then Farkas's lemma states that exactly one of the systems $$ \\begin{aligned} Ax & \\le 0, \\\\ c^Tx & > 0 \\end{aligned} $$ and $$ \\begin{aligned} A^Ty & = c, \\\\ y & \\le 0 \\end{aligned} $$ is solvable, where $x$ is an $n$-vector and $y$ is an $m$-vector. Prove Farkas's lemma. Suppose that both systems are solvable, let $x$ denote a solution to the first system, and $y$ denote a solution to the second. Taking transposes we have $x^{\\text T}A^{\\text T} \\le 0^{\\text T}$. Right multiplying by $y$ gives $x^{\\text T}c = x^{\\text T}A^{\\text T}y \\le 0^{\\text T}$, which is a contradiction to the fact that $c^{\\text T}x > 0$. Thus, both systems cannot be simultaneously solved. Now suppose that the second system fails. Consider the following linear program: $$\\text{maximize } 0x \\text{ subject to } A^{\\text T}y = c \\text{ and } y \\ge 0,$$ and its corresponding dual program $$\\text{minimize } -c^{\\text T}x \\text{ subject to } Ax \\le 0.$$ Since the second system fails, the primal is infeasible. However, the dual is always feasible by taking $x = 0$. If there were a finite solution to the dual, then duality says there would also be a finite solution to the primal. Thus, the dual must be unbounded. Thus, there must exist a vector $x$ which makes $\u2212c^{\\text T}x$ arbitrarily small, implying that there exist vectors $x$ for which $c^{\\text T}x$ is strictly greater than $0$. Thus, there is always at least one solution.","title":"29-4 Farkas'ss lemma"},{"location":"Chap29/Problems/29-5/","text":"In this problem, we consider a variant of the minimum-cost-flow problem from Section 29.2 in which we are not given a demand, a source, or a sink. Instead, we are given, as before, a flow network and edge costs $a(u, v)$ flow is feasible if it satisfies the capacity constraint on every edge and flow conservation at every vertex. The goal is to find, among all feasible flows, the one of minimum cost. We call this problem the minimum-cost-circulation problem . a. Formulate the minimum-cost-circulation problem as a linear program. b. Suppose that for all edges $(u, v) \\in E$, we have $a(u, v) > 0$. Characterize an optimal solution to the minimum-cost-circulation problem. c. Formulate the maximum-flow problem as a minimum-cost-circulation problem linear program. That is given a maximum-flow problem instance $G = (V, E)$ with source $s$, sink $t$ and edge capacities $c$, create a minimum-cost-circulation problem by giving a (possibly different) network $G' = (V', E')$ with edge capacities $c'$ and edge costs $a'$ such that you can discern a solution to the maximum-flow problem from a solution to the minimum-cost-circulation problem. d. Formulate the single-source shortest-path problem as a minimum-cost-circulation problem linear program. a. This is exactly the linear program given in equations $\\text{(29.51)}$-$\\text{(29.52)}$ except that the equation on the third line of the constraints should be removed, and for the equation on the second line of the constraints, $u$ should be selected from all of $V$ instead of from $V \\backslash \\{s, t\\}$. b. If $a(u, v) > 0$ for every pair of vertices, then, there is no point in sending any flow at all. So, an optimal solution is just to have no flow. This obviously satisfies the capacity constraints, it also satisfies the conservation constraints because the flow into and out of each vertex is zero. c. We assume that the edge $(t, s)$ is not in $E$ because that would be a silly edge to have for a maximum flow from $s$ to $t$. If it is there, remove it and it won't decrease the maximum flow. Let $V' = V$ and $E' = E \\cup \\{(t, s)\\}$. For the edges of $E'$ that are in $E$, let the capacity be as it is in $E$ and let the cost be $0$. For the other edge, we set $c(t, s) = \\infty$ and $a(t, s) = -1$. Then, if we have any circulation in $G'$, it will be trying to get as much flow to go across the edge $(t, s)$ in order to drive the objective function lower, the other flows will have no affect on the objective function. Then, by Kirchhoff's current law (a.k.a. common sense), the amount going across the edge $(t, s)$ is the same as the total flow in the rest of the network from $s$ to $t$. This means that maximizing the flow across the edge $(t, s)$ is also maximizing the flow from $s$ to $t$. So, all we need to do to recover the maximum flow for the original network is to keep the same flow values, but throw away the edge $(t, s)$. d. Suppose that $s$ is the vertex that we are computing shortest distance from. Then, we make the circulation network by first starting with the original graph, giving each edge a cost of whatever it was before and infinite capacity. Then, we place an edge going from every vertex that isn't $s$ to $s$ that has a capacity of $1$ and a cost of $-|E|$ times the largest cost appearing among all the edge costs already in the graph. Giving it such a negative cost ensures that placing other flow through the network in order to get a unit of flow across it will cause the total cost to decrease. Then, to recover the shortest path for any vertex, start at that vertex and then go to any vertex that is sending a unit of flow to it. Repeat this until you've reached $s$.","title":"29-5 Minimum-cost circulation"},{"location":"Chap30/30.1/","text":"30.1-1 Multiply the polynomials $A(x) = 7x^3 - x^2 + x - 10$ and $B(x) = 8x^3 - 6x + 3$ using equations $\\text{(30.1)}$ and $\\text{(30.2)}$. $$ \\begin{array}{rl} & 56x^6 - 8x^5 + (8 - 42)x^4 + (-80 + 6 + 21)x^3 + (-3 - 6)x^2 + (60 + 3)x - 30 \\\\ = & 56x^6 - 8x^5 - 34x^4 - 53x^3 - 9x^2 + 63x - 30. \\end{array} $$ 30.1-2 Another way to evaluate a polynomial $A(x)$ of degree-bound $n$ at a given point $x_0$ is to divide $A(x)$ by the polynomial $(x - x_0)$, obtaining a quotient polynomial $q(x)$ of degree-bound $n - 1$ and a remainder $r$, such that $$A(x) = q(x)(x - x_0) + r.$$ Clearly, $A(x_0) = r$. Show how to compute the remainder $r$ and the coefficients of $q(x)$ in time $\\Theta(n)$ from $x_0$ and the coefficients of $A$. Let $A$ be the matrix with $1$'s on the diagonal, $\u2212x_0$'s on the super diagonal, and $0$'s everywhere else. Let $q$ be the vector $(r, q_0, q_1, \\dots, x_{n \u2212 2})$. If $a = (a_0, a_1, \\dots, a_{n \u2212 1})$ then we need to solve the matrix equation $Aq = a$ to compute the remainder and coefficients. Since $A$ is tridiagonal, Problem 28-1 (e) tells us how to solve this equation in linear time. 30.1-3 Derive a point-value representation for $A^\\text{rev}(x) = \\sum_{j = 0}^{n - 1} a_{n - 1 - j}x^j$ from a point-value representation for $A(x) = \\sum_{j = 0}^{n - 1} a_jx^j$, assuming that none of the points is $0$. For each pair of points, $(p, A(p))$, we can compute the pair $(\\frac{1}{p}, A^{rev}(\\frac{1}{p}))$. To do this, we note that $$ \\begin{aligned} A^{rev}(\\frac{1}{p}) & = \\sum_{j = 0}^{n - 1} a_{n - 1 - j} (\\frac{1}{p})^j \\\\ & = \\sum_{j = 0}^{n - 1} a_j(\\frac{1}{p})^{n - 1 - j} \\\\ & = p^{1 - n} \\sum_{j = 0}^{n - 1} a_jp^j \\\\ & = p^{1 - n}A(p), \\end{aligned} $$ since we know what $A(p)$ is, we can compute $A^{rev}(\\frac{1}{p})$ of course, we are using the fact that $p \\ne 0$ because we are dividing by it. Also, we know that each of these points are distinct, because $\\frac{1}{p} = \\frac{1}{p'}$ implies that $p = p'$ by cross multiplication. So, since all the $x$ values were distinct in the point value representation of $A$, they will be distinct in this point value representation of $A^{rev}$ that we have made. 30.1-4 Prove that $n$ distinct point-value pairs are necessary to uniquely specify a polynomial of degree-bound $n$, that is, if fewer than $n$ distinct point-value pairs are given, they fail to specify a unique polynomial of degree-bound $n$. ($\\textit{Hint:}$ Using Theorem 30.1, what can you say about a set of $n - 1$ point-value pairs to which you add one more arbitrarily chosen point-value pair?) Suppose that just $n \u2212 1$ point-value pairs uniquely determine a polynomial $P$ which satisfies them. Append the point value pair $(x_{n - 1}, y_{n \u2212 1})$ to them, and let $P'$ be the unique polynomial which agrees with the $n$ pairs, given by Theorem 30.1. Now append instead $(x_{n - 1}, y'_{n \u2212 1})$ where $y_{n \u2212 1} \\ne y'_{n - 1}$, and let $P''$ be the polynomial obtained from these points via Theorem 30.1. Since polynomials coming from $n$ pairs are unique, $P' \\ne P''$. However, $P'$ and $P''$ agree on the original $n \u2212 1$ point-value pairs, contradicting the fact that $P$ was determined uniquely. 30.1-5 Show how to use equation $\\text{(30.5)}$ to interpolate in time $\\Theta(n^2)$. ($\\textit{Hint:}$ First compute the coefficient representation of the polynomial $\\prod_j (x - x_j)$ and then divide by $(x - x_k)$ as necessary for the numerator of each term; see Exercise 30.1-2. You can compute each of the $n$ denominators in time $O(n)$.) First, we show that we can compute the coefficient representation of $\\prod_j (x - x_j)$ in time $\\Theta(n^2)$. We will do it by recursion, showing that multiplying $\\prod_{j < k} (x \u2212 x_j)$ by $(x \u2212 x_k)$ only takes time $O(n)$, since this only needs to be done $n$ times, this gets is total runtime of $O(n)$. Suppose that $\\sum_{i = 0}^{k - 1} k_ix^i$ is a coefficient representation of $\\prod_{j < k} (x \u2212 x_j)$. To multiply this by $(x \u2212 x_k)$, we just set $(k + 1)_i = k_{i \u2212 1} - x_kk_i$ for $i = 1, \\dots, k$ and $(k + 1)_0 = \u2212x_k \\cdot k_0$. Each of these coefficients can be computed in constant time, since there are only linearly many coefficients, then, the time to compute the next partial product is just $O(n)$. Now that we have a coefficient representation of $\\prod_j (x \u2212 x_j)$, we need to compute, for each $k \\prod_{j \u2212 k} (x \u2212 x_j)$, each of which can be computed in time $\\Theta(n)$ by problem 30.1-2. Since the polynomial is defined as a product of things containing the thing we are dividing by, we have that the remainder in each case is equal to $0$. Lets call these polynomials $f_k$. Then, we need only compute the sum $\\sum_k y_k \\frac{f_k(x)}{f_k(x_k)}$. That is, we compute $f(x_k)$ each in time $\\Theta(n)$, so all told, only $\\Theta(n^2)$ time is spent computing all the $f(x_k)$ values. For each of the terms in the sum, dividing the polynomial $f_k(x)$ by the number $f_k(x_k)$ and multiplying by $y_k$ only takes time $\\Theta(n)$, so total it takes time $\\Theta(n^2)$. Lastly, we are adding up $n$ polynomials, each of degree bound $n \u2212 1$, so the total time taken there is $\\Theta(n^2)$. 30.1-6 Explain what is wrong with the \"obvious\" approach to polynomial division using a point-value representation, i.e., dividing the corresponding $y$ values. Discuss separately the case in which the division comes out exactly and the case in which it doesn't. If we wish to compute $P / Q$ but $Q$ takes on the value zero at some of these points, then we can't carry out the \"obvious\" method. However, as long as all point value pairs $(x_i, y_i)$ we choose for $Q$ are such that $y_i \\ne 0$, then the approach comes out exactly as we would like. 30.1-7 Consider two sets $A$ and $B$, each having $n$ integers in the range from $0$ to $10n$. We wish to compute the Cartesian sum of $A$ and $B$, defined by $$C = \\{x + y: x \\in A \\text{ and } y \\in B\\}.$$ Note that the integers in $C$ are in the range from $0$ to $20n$. We want to find the elements of $C$ and the number of times each element of $C$ is realized as a sum of elements in $A$ and $B$. Show how to solve the problem in $O(n\\lg n)$ time. ($\\textit{Hint:}$ Represent $A$ and $B$ as polynomials of degree at most $10n$.) For the set $A$, we define the polynomial $f_A$ to have a coefficient representation that has $a_i$ equal zero if $i \\notin A$ and equal to $1$ if $i \\in A$. Similarly define $f_B$. Then, we claim that looking at $f_C := f_A \\cdot f_B$ in coefficient form, we have that the $i$th coefficient, $c_i$ is exactly equal to the number of times that $i$ is realized as a sum of elements from $A$ and $B$. Since we can perform the polynomial multiplication in time $O(n \\lg n)$ by the methods of this chapter, we can get the final answer in time $O(n \\lg n)$. To see that $f_C$ has the nice property described, we'll look at the ways that we could end up having a term of $x^i$ appear. Each contribution to that coefficient must come from there being some $k$ so that $a_k \\ne 0$ and $b_{i \u2212 k} \\ne 0$, because the powers of $x$ attached to each are additive when we multiply. Since each of these contributions is only ever $1$, the final coefficient is counting the total number of such contributions, therefore counting the number of $k \\in A$ such that $\u2212k \\in B$, which is exactly what we claimed $f_C$ was counting.","title":"30.1 Representing polynomials"},{"location":"Chap30/30.1/#301-1","text":"Multiply the polynomials $A(x) = 7x^3 - x^2 + x - 10$ and $B(x) = 8x^3 - 6x + 3$ using equations $\\text{(30.1)}$ and $\\text{(30.2)}$. $$ \\begin{array}{rl} & 56x^6 - 8x^5 + (8 - 42)x^4 + (-80 + 6 + 21)x^3 + (-3 - 6)x^2 + (60 + 3)x - 30 \\\\ = & 56x^6 - 8x^5 - 34x^4 - 53x^3 - 9x^2 + 63x - 30. \\end{array} $$","title":"30.1-1"},{"location":"Chap30/30.1/#301-2","text":"Another way to evaluate a polynomial $A(x)$ of degree-bound $n$ at a given point $x_0$ is to divide $A(x)$ by the polynomial $(x - x_0)$, obtaining a quotient polynomial $q(x)$ of degree-bound $n - 1$ and a remainder $r$, such that $$A(x) = q(x)(x - x_0) + r.$$ Clearly, $A(x_0) = r$. Show how to compute the remainder $r$ and the coefficients of $q(x)$ in time $\\Theta(n)$ from $x_0$ and the coefficients of $A$. Let $A$ be the matrix with $1$'s on the diagonal, $\u2212x_0$'s on the super diagonal, and $0$'s everywhere else. Let $q$ be the vector $(r, q_0, q_1, \\dots, x_{n \u2212 2})$. If $a = (a_0, a_1, \\dots, a_{n \u2212 1})$ then we need to solve the matrix equation $Aq = a$ to compute the remainder and coefficients. Since $A$ is tridiagonal, Problem 28-1 (e) tells us how to solve this equation in linear time.","title":"30.1-2"},{"location":"Chap30/30.1/#301-3","text":"Derive a point-value representation for $A^\\text{rev}(x) = \\sum_{j = 0}^{n - 1} a_{n - 1 - j}x^j$ from a point-value representation for $A(x) = \\sum_{j = 0}^{n - 1} a_jx^j$, assuming that none of the points is $0$. For each pair of points, $(p, A(p))$, we can compute the pair $(\\frac{1}{p}, A^{rev}(\\frac{1}{p}))$. To do this, we note that $$ \\begin{aligned} A^{rev}(\\frac{1}{p}) & = \\sum_{j = 0}^{n - 1} a_{n - 1 - j} (\\frac{1}{p})^j \\\\ & = \\sum_{j = 0}^{n - 1} a_j(\\frac{1}{p})^{n - 1 - j} \\\\ & = p^{1 - n} \\sum_{j = 0}^{n - 1} a_jp^j \\\\ & = p^{1 - n}A(p), \\end{aligned} $$ since we know what $A(p)$ is, we can compute $A^{rev}(\\frac{1}{p})$ of course, we are using the fact that $p \\ne 0$ because we are dividing by it. Also, we know that each of these points are distinct, because $\\frac{1}{p} = \\frac{1}{p'}$ implies that $p = p'$ by cross multiplication. So, since all the $x$ values were distinct in the point value representation of $A$, they will be distinct in this point value representation of $A^{rev}$ that we have made.","title":"30.1-3"},{"location":"Chap30/30.1/#301-4","text":"Prove that $n$ distinct point-value pairs are necessary to uniquely specify a polynomial of degree-bound $n$, that is, if fewer than $n$ distinct point-value pairs are given, they fail to specify a unique polynomial of degree-bound $n$. ($\\textit{Hint:}$ Using Theorem 30.1, what can you say about a set of $n - 1$ point-value pairs to which you add one more arbitrarily chosen point-value pair?) Suppose that just $n \u2212 1$ point-value pairs uniquely determine a polynomial $P$ which satisfies them. Append the point value pair $(x_{n - 1}, y_{n \u2212 1})$ to them, and let $P'$ be the unique polynomial which agrees with the $n$ pairs, given by Theorem 30.1. Now append instead $(x_{n - 1}, y'_{n \u2212 1})$ where $y_{n \u2212 1} \\ne y'_{n - 1}$, and let $P''$ be the polynomial obtained from these points via Theorem 30.1. Since polynomials coming from $n$ pairs are unique, $P' \\ne P''$. However, $P'$ and $P''$ agree on the original $n \u2212 1$ point-value pairs, contradicting the fact that $P$ was determined uniquely.","title":"30.1-4"},{"location":"Chap30/30.1/#301-5","text":"Show how to use equation $\\text{(30.5)}$ to interpolate in time $\\Theta(n^2)$. ($\\textit{Hint:}$ First compute the coefficient representation of the polynomial $\\prod_j (x - x_j)$ and then divide by $(x - x_k)$ as necessary for the numerator of each term; see Exercise 30.1-2. You can compute each of the $n$ denominators in time $O(n)$.) First, we show that we can compute the coefficient representation of $\\prod_j (x - x_j)$ in time $\\Theta(n^2)$. We will do it by recursion, showing that multiplying $\\prod_{j < k} (x \u2212 x_j)$ by $(x \u2212 x_k)$ only takes time $O(n)$, since this only needs to be done $n$ times, this gets is total runtime of $O(n)$. Suppose that $\\sum_{i = 0}^{k - 1} k_ix^i$ is a coefficient representation of $\\prod_{j < k} (x \u2212 x_j)$. To multiply this by $(x \u2212 x_k)$, we just set $(k + 1)_i = k_{i \u2212 1} - x_kk_i$ for $i = 1, \\dots, k$ and $(k + 1)_0 = \u2212x_k \\cdot k_0$. Each of these coefficients can be computed in constant time, since there are only linearly many coefficients, then, the time to compute the next partial product is just $O(n)$. Now that we have a coefficient representation of $\\prod_j (x \u2212 x_j)$, we need to compute, for each $k \\prod_{j \u2212 k} (x \u2212 x_j)$, each of which can be computed in time $\\Theta(n)$ by problem 30.1-2. Since the polynomial is defined as a product of things containing the thing we are dividing by, we have that the remainder in each case is equal to $0$. Lets call these polynomials $f_k$. Then, we need only compute the sum $\\sum_k y_k \\frac{f_k(x)}{f_k(x_k)}$. That is, we compute $f(x_k)$ each in time $\\Theta(n)$, so all told, only $\\Theta(n^2)$ time is spent computing all the $f(x_k)$ values. For each of the terms in the sum, dividing the polynomial $f_k(x)$ by the number $f_k(x_k)$ and multiplying by $y_k$ only takes time $\\Theta(n)$, so total it takes time $\\Theta(n^2)$. Lastly, we are adding up $n$ polynomials, each of degree bound $n \u2212 1$, so the total time taken there is $\\Theta(n^2)$.","title":"30.1-5"},{"location":"Chap30/30.1/#301-6","text":"Explain what is wrong with the \"obvious\" approach to polynomial division using a point-value representation, i.e., dividing the corresponding $y$ values. Discuss separately the case in which the division comes out exactly and the case in which it doesn't. If we wish to compute $P / Q$ but $Q$ takes on the value zero at some of these points, then we can't carry out the \"obvious\" method. However, as long as all point value pairs $(x_i, y_i)$ we choose for $Q$ are such that $y_i \\ne 0$, then the approach comes out exactly as we would like.","title":"30.1-6"},{"location":"Chap30/30.1/#301-7","text":"Consider two sets $A$ and $B$, each having $n$ integers in the range from $0$ to $10n$. We wish to compute the Cartesian sum of $A$ and $B$, defined by $$C = \\{x + y: x \\in A \\text{ and } y \\in B\\}.$$ Note that the integers in $C$ are in the range from $0$ to $20n$. We want to find the elements of $C$ and the number of times each element of $C$ is realized as a sum of elements in $A$ and $B$. Show how to solve the problem in $O(n\\lg n)$ time. ($\\textit{Hint:}$ Represent $A$ and $B$ as polynomials of degree at most $10n$.) For the set $A$, we define the polynomial $f_A$ to have a coefficient representation that has $a_i$ equal zero if $i \\notin A$ and equal to $1$ if $i \\in A$. Similarly define $f_B$. Then, we claim that looking at $f_C := f_A \\cdot f_B$ in coefficient form, we have that the $i$th coefficient, $c_i$ is exactly equal to the number of times that $i$ is realized as a sum of elements from $A$ and $B$. Since we can perform the polynomial multiplication in time $O(n \\lg n)$ by the methods of this chapter, we can get the final answer in time $O(n \\lg n)$. To see that $f_C$ has the nice property described, we'll look at the ways that we could end up having a term of $x^i$ appear. Each contribution to that coefficient must come from there being some $k$ so that $a_k \\ne 0$ and $b_{i \u2212 k} \\ne 0$, because the powers of $x$ attached to each are additive when we multiply. Since each of these contributions is only ever $1$, the final coefficient is counting the total number of such contributions, therefore counting the number of $k \\in A$ such that $\u2212k \\in B$, which is exactly what we claimed $f_C$ was counting.","title":"30.1-7"},{"location":"Chap30/30.2/","text":"30.2-1 Prove Corollary 30.4. (Omit!) 30.2-2 Compute the $\\text{DFT}$ of the vector $(0, 1, 2, 3)$. (Omit!) 30.2-3 Do Exercise 30.1-1 by using the $\\Theta(n\\lg n)$-time scheme. (Omit!) 30.2-4 Write pseudocode to compute $\\text{DFT}_n^{-1}$ in $\\Theta(n\\lg n)$ time. (Omit!) 30.2-5 Describe the generalization of the $\\text{FFT}$ procedure to the case in which $n$ is a power of $3$. Give a recurrence for the running time, and solve the recurrence. (Omit!) 30.2-6 $\\star$ Suppose that instead of performing an $n$-element $\\text{FFT}$ over the field of complex numbers (where $n$ is even), we use the ring $\\mathbb Z_m$ of integers modulo $m$, where $m = 2^{tn / 2} + 1$ and $t$ is an arbitrary positive integer. Use $\\omega = 2^t$ instead of $\\omega_n$ as a principal nth root of unity, modulo $m$. Prove that the $\\text{DFT}$ and the inverse $\\text{DFT}$ are well defined in this system. (Omit!) 30.2-7 Given a list of values $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions), show how to find the coefficients of a polynomial $P(x)$ of degree-bound $n + 1$ that has zeros only at $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions). Your procedure should run in time $O(n\\lg^2 n)$. ($\\textit{Hint:}$ The polynomial $P(x)$ has a zero at $z_j$ if and only if $P(x)$ is a multiple of $(x - z_j)$.) (Omit!) 30.2-8 $\\star$ The chirp transform of a vector $a = (a_0, a_1, \\dots, a_{n - 1})$ is the vector $y = (y_0, y_1, \\dots, y_{n - 1})$, where $y_k = \\sum_{j = 0}^{n - 1} a_jz^{kj}$ and $z$ is any complex number. The $\\text{DFT}$ is therefore a special case of the chirp transform, obtained by taking $z = \\omega_n$. Show how to evaluate the chirp transform in time $O(n\\lg n)$ for any complex number $z$. ($\\textit{Hint:}$ Use the equation $$y_k = z^{k^2 / 2} \\sum_{j = 0}^{n - 1} \\Big(a_jz^{j^2 / 2}\\Big) \\Big(z^{-(k - j)^2 / 2}\\Big)$$ to view the chirp transform as a convolution.) (Omit!)","title":"30.2 The DFT and FFT"},{"location":"Chap30/30.2/#302-1","text":"Prove Corollary 30.4. (Omit!)","title":"30.2-1"},{"location":"Chap30/30.2/#302-2","text":"Compute the $\\text{DFT}$ of the vector $(0, 1, 2, 3)$. (Omit!)","title":"30.2-2"},{"location":"Chap30/30.2/#302-3","text":"Do Exercise 30.1-1 by using the $\\Theta(n\\lg n)$-time scheme. (Omit!)","title":"30.2-3"},{"location":"Chap30/30.2/#302-4","text":"Write pseudocode to compute $\\text{DFT}_n^{-1}$ in $\\Theta(n\\lg n)$ time. (Omit!)","title":"30.2-4"},{"location":"Chap30/30.2/#302-5","text":"Describe the generalization of the $\\text{FFT}$ procedure to the case in which $n$ is a power of $3$. Give a recurrence for the running time, and solve the recurrence. (Omit!)","title":"30.2-5"},{"location":"Chap30/30.2/#302-6-star","text":"Suppose that instead of performing an $n$-element $\\text{FFT}$ over the field of complex numbers (where $n$ is even), we use the ring $\\mathbb Z_m$ of integers modulo $m$, where $m = 2^{tn / 2} + 1$ and $t$ is an arbitrary positive integer. Use $\\omega = 2^t$ instead of $\\omega_n$ as a principal nth root of unity, modulo $m$. Prove that the $\\text{DFT}$ and the inverse $\\text{DFT}$ are well defined in this system. (Omit!)","title":"30.2-6 $\\star$"},{"location":"Chap30/30.2/#302-7","text":"Given a list of values $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions), show how to find the coefficients of a polynomial $P(x)$ of degree-bound $n + 1$ that has zeros only at $z_0, z_1, \\dots, z_{n - 1}$ (possibly with repetitions). Your procedure should run in time $O(n\\lg^2 n)$. ($\\textit{Hint:}$ The polynomial $P(x)$ has a zero at $z_j$ if and only if $P(x)$ is a multiple of $(x - z_j)$.) (Omit!)","title":"30.2-7"},{"location":"Chap30/30.2/#302-8-star","text":"The chirp transform of a vector $a = (a_0, a_1, \\dots, a_{n - 1})$ is the vector $y = (y_0, y_1, \\dots, y_{n - 1})$, where $y_k = \\sum_{j = 0}^{n - 1} a_jz^{kj}$ and $z$ is any complex number. The $\\text{DFT}$ is therefore a special case of the chirp transform, obtained by taking $z = \\omega_n$. Show how to evaluate the chirp transform in time $O(n\\lg n)$ for any complex number $z$. ($\\textit{Hint:}$ Use the equation $$y_k = z^{k^2 / 2} \\sum_{j = 0}^{n - 1} \\Big(a_jz^{j^2 / 2}\\Big) \\Big(z^{-(k - j)^2 / 2}\\Big)$$ to view the chirp transform as a convolution.) (Omit!)","title":"30.2-8 $\\star$"},{"location":"Chap30/30.3/","text":"30.3-1 Show how $\\text{ITERATIVE-FFT}$ computes the $\\text{DFT}$ of the input vector $(0, 2, 3, -1, 4, 5, 7, 9)$. (Omit!) 30.3-2 Show how to implement an $\\text{FFT}$ algorithm with the bit-reversal permutation occurring at the end, rather than at the beginning, of the computation. ($\\textit{Hint:}$ Consider the inverse $\\text{DFT}$.) (Omit!) 30.3-3 How many times does $\\text{ITERATIVE-FFT}$ compute twiddle factors in each stage? Rewrite $\\text{ITERATIVE-FFT}$ to compute twiddle factors only $2^{s - 1}$ times in stage $s$. (Omit!) 30.3-4 $\\star$ Suppose that the adders within the butterfly operations of the $\\text{FFT}$ circuit sometimes fail in such a manner that they always produce a zero output, independent of their inputs. Suppose that exactly one adder has failed, but that you don't know which one. Describe how you can identify the failed adder by supplying inputs to the overall $\\text{FFT}$ circuit and observing the outputs. How efficient is your method? (Omit!)","title":"30.3 Efficient FFT implementations"},{"location":"Chap30/30.3/#303-1","text":"Show how $\\text{ITERATIVE-FFT}$ computes the $\\text{DFT}$ of the input vector $(0, 2, 3, -1, 4, 5, 7, 9)$. (Omit!)","title":"30.3-1"},{"location":"Chap30/30.3/#303-2","text":"Show how to implement an $\\text{FFT}$ algorithm with the bit-reversal permutation occurring at the end, rather than at the beginning, of the computation. ($\\textit{Hint:}$ Consider the inverse $\\text{DFT}$.) (Omit!)","title":"30.3-2"},{"location":"Chap30/30.3/#303-3","text":"How many times does $\\text{ITERATIVE-FFT}$ compute twiddle factors in each stage? Rewrite $\\text{ITERATIVE-FFT}$ to compute twiddle factors only $2^{s - 1}$ times in stage $s$. (Omit!)","title":"30.3-3"},{"location":"Chap30/30.3/#303-4-star","text":"Suppose that the adders within the butterfly operations of the $\\text{FFT}$ circuit sometimes fail in such a manner that they always produce a zero output, independent of their inputs. Suppose that exactly one adder has failed, but that you don't know which one. Describe how you can identify the failed adder by supplying inputs to the overall $\\text{FFT}$ circuit and observing the outputs. How efficient is your method? (Omit!)","title":"30.3-4 $\\star$"},{"location":"Chap30/Problems/30-1/","text":"a. Show how to multiply two linear polynomials $ax + b$ and $cx + d$ using only three multiplications. ($\\textit{Hint:}$ One of the multiplications is $(a + b) \\cdot (c + d)$.) b. Give two divide-and-conquer algorithms for multiplying two polynomials of degree-bound $n$ in $\\Theta(n^{\\lg 3})$ time. The first algorithm should divide the input polynomial coefficients into a high half and a low half, and the second algorithm should divide them according to whether their index is odd or even. c. Show how to multiply two $n$-bit integers in $O(n^{\\lg 3})$ steps, where each step operates on at most a constant number of $1$-bit values. (Omit!)","title":"30-1 Divide-and-conquer multiplication"},{"location":"Chap30/Problems/30-2/","text":"A Toeplitz matrix is an $n \\times n$ matrix $A = (a_{ij})$ such that $a_{ij} = a_{i - 1, j - 1}$ for $i = 2, 3, \\dots, n$ and $j = 2, 3, \\dots, n$. a. Is the sum of two Toeplitz matrices necessarily Toeplitz? What about the product? b. Describe how to represent a Toeplitz matrix so that you can add two $n \\times n$ Toeplitz matrices in $O(n)$ time. c. Give an $O(n\\lg n)$-time algorithm for multiplying an $n \\times n$ Toeplitz matrix by a vector of length $n$. Use your representation from part (b). d. Give an efficient algorithm for multiplying two $n \\times n$ Toeplitz matrices. Analyze its running time. (Omit!)","title":"30-2 Toeplitz matrices"},{"location":"Chap30/Problems/30-3/","text":"We can generalize the $1$-dimensional discrete Fourier transform defined by equation $\\text{(30.8)}$ to $d$ dimensions. The input is a $d$-dimensional array $A = (a_{j_1, j_2, \\dots, j_d})$ whose dimensions are $n_1, n_2, \\dots, n_d$, where $n_1n_2 \\cdots n_d = n$. We define the $d$-dimensional discrete Fourier transform by the equation $$y_{k_1, k_2, \\dots, k_d} = \\sum_{j_1 = 0}^{n_1 - 1} \\sum_{j_2 = 0}^{n_2 - 1} \\cdots \\sum_{j_d = 0}^{n_d - 1} a_{j_1, j_2, \\cdots, j_d} \\omega_{n_1}^{j_1k_1}\\omega_{n_2}^{j_2k_2} \\cdots \\omega_{n_d}^{j_dk_d}$$ for $0 \\le k_1 < n_1, 0 \\le k_2 < n_2, \\dots, 0 \\le k_d < n_d$. a. Show that we can compute a $d$-dimensional $\\text{DFT}$ by computing $1$-dimensional $\\text{DFT}$s on each dimension in turn. That is, we first compute $n / n_1$ separate $1$-dimensional $\\text{DFT}$s along dimension $1$. Then, using the result of the $\\text{DFT}$s along dimension $1$ as the input, we compute $n / n_2$ separate $1$-dimensional $\\text{DFT}$s along dimension $2$. Using this result as the input, we compute $n / n_3$ separate $1$-dimensional $\\text{DFT}$s along dimension $3$, and so on, through dimension $d$. b. Show that the ordering of dimensions does not matter, so that we can compute a $d$-dimensional $\\text{DFT}$ by computing the $1$-dimensional $\\text{DFT}$s in any order of the $d$ dimensions. c. Show that if we compute each $1$-dimensional $\\text{DFT}$ by computing the fast Fourier transform, the total time to compute a $d$-dimensional $\\text{DFT}$ is $O(n\\lg n)$, independent of $d$. (Omit!)","title":"30-3 Multidimensional fast Fourier transform"},{"location":"Chap30/Problems/30-4/","text":"Given a polynomial $A(x)$ of degree-bound $n$, we define its $t$th derivative by $$ A^{(t)}(x) = \\begin{cases} A(x) & \\text{ if } t = 0, \\\\ \\frac{d}{dx} A^{(t - 1)}(x) & \\text{ if } 1 \\le t \\le n - 1, \\\\ 0 & \\text{ if } t \\ge n. \\end{cases} $$ From the coefficient representation $(a_0, a_1, \\dots, a_{n - 1})$ of $A(x)$ and a given point $x_0$, we wish to determine $A^{(t)}(x_0)$ for $t = 0, 1, \\dots, n- 1$. a. Given coefficients $b_0, b_1, \\dots, b_{n - 1}$ such that $$A(x) = \\sum_{j = 0}^{n - 1} b_j(x - x_0)^j,$$ show how to compute $A^{(t)}(x_0)$ for $t = 0, 1, \\dots, n - 1$, in $O(n)$ time. b. Explain how to find $b_0, b_1, \\dots, b_{n - 1}$ in $O(n\\lg n)$ time, given $A(x_0 + \\omega_n^k)$ for $k = 0, 1, \\dots, n - 1$. c. Prove that $$A(x_0 + \\omega_n^k) = \\sum_{r = 0}^{n - 1} \\Bigg(\\frac{\\omega_n^{kr}}{r!} \\sum_{j = 0}^{n - 1} f(j)g(r - j)\\Bigg),$$ where $f(j) = a_j \\cdot j!$ and $$ g(l) = \\begin{cases} x_0^{-l} / (-l)! & \\text{ if } -(n - 1) \\le l \\le 0, \\\\ 0 & \\text{ if } 1 \\le l \\le n - 1. \\end{cases} $$ d. Explain how to evaluate $A(x_0 + \\omega_n^k)$ for $k = 0, 1, \\dots, n - 1$ in $O(n\\lg n)$ time. Conclude that we can evaluate all nontrivial derivatives of $A(x)$ at $x_0$ in $O(n\\lg n)$ time. (Omit!)","title":"30-4 Evaluating all derivatives of a polynomial at a point"},{"location":"Chap30/Problems/30-5/","text":"We have seen how to evaluate a polynomial of degree-bound $n$ at a single point in $O(n)$ time using Horner's rule. We have also discovered how to evaluate such a polynomial at all $n$ complex roots of unity in $O(n\\lg n)$ time using the $\\text{FFT}$. We shall now show how to evaluate a polynomial of degree-bound $n$ at $n$ arbitrary points in $O(n\\lg^2 n)$ time. To do so, we shall assume that we can compute the polynomial remainder when one such polynomial is divided by another in $O(n\\lg n)$ time, a result that we state without proof. For example, the remainder of $3x^3 + x^2 - 3x + 1$ when divided by $x^2 + x + 2$ is $$(3x^3 + x^2 - 3x + 1) \\mod (x^2 + x + 2) = -7x + 5.$$ Given the coefficient representation of a polynomial $A(x) = \\sum_{k = 0}^{n - 1} a_kx^k$ and $n$ points $x_0, x_1, \\dots, x_{n - 1}$, we wish to compute the $n$ values $A(x_0), A(x_1), \\dots, A(x_{n - 1})$. For $0 \\le i \\le j \\le n - 1$, define the polynomials $P_{ij}(x) = \\prod_{k = i}^j (x - x_k)$ and $Q_{ij}(x) = A(x) \\mod P_{ij}(x)$. Note that $Q_{ij}(x)$ has degree at most $j - i$. a. Prove that $A(x) \\mod (x - z) = A(z)$ for any point $z$. b. Prove that $Q_{kk}(x) = A(x_k)$ and that $Q_{0, n - 1}(x) = A(x)$. c. Prove that for $i \\le k \\le j$, we have $Q_{ik}(x) = Q_{ij}(x) \\mod P_{ik}(x)$ and $Q_{kj}(x) = Q_{ij}(x) \\mod P_{kj}(x)$. d. Give an $O(n\\lg^2 n)$-time algorithm to evaluate $A(x_0), A(x_1), \\dots, A(x_{n - 1})$. (Omit!)","title":"30-5 Polynomial evaluation at multiple points"},{"location":"Chap30/Problems/30-6/","text":"As defined, the discrete Fourier transform requires us to compute with complex numbers, which can result in a loss of precision due to round-off errors. For some problems, the answer is known to contain only integers, and by using a variant of the $\\text{FFT}$ based on modular arithmetic, we can guarantee that the answer is calculated exactly. An example of such a problem is that of multiplying two polynomials with integer coefficients. Exercise 30.2-6 gives one approach, using a modulus of length $\\Omega(n)$ bits to handle a $\\text{DFT}$ on $n$ points. This problem gives another approach, which uses a modulus of the more reasonable length $O(\\lg n)$; it requires that you understand the material of Chapter 31. Let $n$ be a power of $2$. a. Suppose that we search for the smallest $k$ such that $p = kn + 1$ is prime. Give a simple heuristic argument why we might expect $k$ to be approximately $\\ln n$. (The value of $k$ might be much larger or smaller, but we can reasonably expect to examine $O(\\lg n)$ candidate values of $k$ on average.) How does the expected length of $p$ compare to the length of $n$? Let $g$ be a generator of $\\mathbb Z_p^*$, and let $w = g^k \\mod p$. b. Argue that the $\\text{DFT}$ and the inverse $\\text{DFT}$ are well-defined inverse operations modulo $p$, where $w$ is used as a principal $n$th root of unity. c. Show how to make the $\\text{FFT}$ and its inverse work modulo $p$ in time $O(n\\lg n)$, where operations on words of $O(\\lg n)$ bits take unit time. Assume that the algorithm is given $p$ and $w$. d. Compute the $\\text{DFT}$ modulo $p = 17$ of the vector $(0, 5, 3, 7, 7, 2, 1, 6)$. Note that $g = 3$ is a generator of $\\mathbb Z_{17}^*$. (Omit!)","title":"30-6 FFT using modular arithmetic"},{"location":"Chap31/31.1/","text":"31.1-1 Prove that if $a > b > 0$ and $c = a + b$, then $c \\mod a = b$. $$ \\begin{aligned} c \\mod a & = (a + b) \\mod a \\\\ & = (a \\mod a) + (b \\mod a) \\\\ & = 0 + b \\\\ & = b. \\end{aligned} $$ 31.1-2 Prove that there are infinitely many primes. $$ \\begin{aligned} ((p_1 p_2 \\cdots p_k) + 1) \\mod p_i & = (p_1 p_2 \\cdots p_k) \\mod p_i + (1 \\mod p_i) \\\\ & = 0 + 1 \\\\ & = 1. \\end{aligned} $$ if $p_1 p_2 \\cdots p_k$ is all prime numbers, then $(p_1 p_2 \\cdots p_k) + 1$ is a new prime number. 31.1-3 Prove that if $a \\mid b$ and $b \\mid c$, then $a \\mid c$. If $a \\mid b$, then $b = a \\cdot k_1$. If $b \\mid c$, then $c = b \\cdot k_2 = a \\cdot (k_1 \\cdot k_2) = a \\cdot k_3$, then $a \\mid c$. 31.1-4 Prove that if $p$ is prime and $0 < k < p$, then $\\gcd(k, p) = 1$. If $k \\ne 1$, then $k \\nmid p$. If $k = 1$, then the divisor is $1$. 31.1-5 Prove Corollary 31.5. For all positive integers $n$, $a$, and $b$, if $n \\mid ab$ and $\\gcd(a, n) = 1$, then $n \\mid b$. Assume for the purpose of contradiction that $n \\mid ab$ and $\\gcd(a, n) = 1$, but $n \\nmid b$, so $gcd(n, b)=1$, for theorem 31.6, $gcd(n, ab)=1$ which is contradicting our assumption. 31.1-6 Prove that if $p$ is prime and $0 < k < p$, then $p \\mid \\binom{p}{k}$. Conclude that for all integers $a$ and $b$ and all primes $p$, $(a + b)^p \\equiv a^p + b^p \\pmod p$. $$ \\begin{array}{rlll} (a + b) ^ p & \\equiv & a^p + \\binom{p}{1} a^{p - 1}b^{1} + \\cdots + \\binom{p}{p - 1} a^{1}b^{p - 1} + b^p & \\pmod p \\\\ & \\equiv & a^p + 0 + \\cdots + 0 + b^p & \\pmod p \\\\ & \\equiv & a^p + b^p & \\pmod p \\end{array} $$ 31.1-7 Prove that if $a$ and $b$ are any positive integers such that $a \\mid b$, then $$(x \\mod b) \\mod a = x \\mod a$$ for any $x$. Prove, under the same assumptions, that $x \\equiv y \\pmod b$ implies $x \\equiv y \\pmod a$ for any integers $x$ and $y$. Suppose $x = kb + c$, we have $$(x \\mod b) \\mod a = c \\mod a,$$ and $$x \\mod a = (kb + c) \\mod a = (kb \\mod a) + (c \\mod a) = c \\mod a.$$ 31.1-8 For any integer $k > 0$, an integer $n$ is a $k$th power if there exists an integer $a$ such that $a^k = n$. Furthermore, $n > 1$ is a nontrivial power if it is a $k$th power for some integer $k > 1$. Show how to determine whether a given $\\beta$-bit integer $n$ is a nontrivial power in time polynomial in $\\beta$. Because $2^\\beta > n$, we only need to test values of $k$ that satisfy $2 \\le k < \\beta$, therefore the testing procedure remains $O(\\beta)$. For any nontrivial power $k$, where $2 \\le k < \\beta$, do a binary search on $a$ that costs $$O(\\log \\sqrt n) = O(\\log \\sqrt{2^\\beta}) = O(\\frac 1 2\\log 2^\\beta) = O(\\beta).$$ Thus, the total time complexity is $$O(\\beta) \\times O(\\beta) = O(\\beta^2).$$ 31.1-9 Prove equations $\\text{(31.6)}$\u2013$\\text{(31.10)}$. (Omit!) 31.1-10 Show that the gcd operator is associative. That is, prove that for all integers $a$, $b$, and $c$, $\\gcd(a, \\gcd(b, c)) = \\gcd(\\gcd(a, b), c)$. [The following proof is provided by my friend, Tony Xiao.] Let $d = \\gcd(a, b, c)$, $a = dp$, $b = dq$ and $c = dr$. Claim $\\gcd(a, \\gcd(b, c)) = d.$ Let $e = \\gcd(b, c)$, thus $$ \\begin{aligned} b = es, \\\\ c = et. \\end{aligned} $$ Since $d \\mid b$ and $d \\mid c$, thus $d \\mid e$. Let $e = dm$, thus $$ \\begin{aligned} b = (dm)s & = dq, \\\\ c = (dm)t & = dr. \\end{aligned} $$ Suppose $k = \\gcd(p, m)$, $$ \\begin{aligned} & k \\mid p, k \\mid m, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid dm, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid (dm)s, dk \\mid (dm)t, \\\\ \\Rightarrow & dk \\mid a, dk \\mid b, dk \\mid c. \\end{aligned} $$ Since $d = \\gcd(a, b, c)$, thus $k = 1$. $$ \\begin{aligned} \\gcd(a, \\gcd(b, c)) & = \\gcd(a, e) \\\\ & = \\gcd(dp, dm) \\\\ & = d \\cdot \\gcd(p, m) \\\\ & = d \\cdot k \\\\ & = d. \\end{aligned} $$ By the claim, $$\\gcd(a, \\gcd(b, c)) = d = \\gcd(\\gcd(a, b), c).$$ 31.1-11 $\\star$ Prove Theorem 31.8. (Omit!) 31.1-12 Give efficient algorithms for the operations of dividing a $\\beta$-bit integer by a shorter integer and of taking the remainder of a $\\beta$-bit integer when divided by a shorter integer. Your algorithms should run in time $\\Theta(\\beta^2)$. Shift left until the two numbers have the same length, then repeatedly subtract with proper multiplier and shift right. 31.1-13 Give an efficient algorithm to convert a given $\\beta$-bit (binary) integer to a decimal representation. Argue that if multiplication or division of integers whose length is at most $\\beta$ takes time $M(\\beta)$, then we can convert binary to decimal in time $\\Theta(M(\\beta) \\lg\\beta)$. (Omit!)","title":"31.1 Elementary number-theoretic notions"},{"location":"Chap31/31.1/#311-1","text":"Prove that if $a > b > 0$ and $c = a + b$, then $c \\mod a = b$. $$ \\begin{aligned} c \\mod a & = (a + b) \\mod a \\\\ & = (a \\mod a) + (b \\mod a) \\\\ & = 0 + b \\\\ & = b. \\end{aligned} $$","title":"31.1-1"},{"location":"Chap31/31.1/#311-2","text":"Prove that there are infinitely many primes. $$ \\begin{aligned} ((p_1 p_2 \\cdots p_k) + 1) \\mod p_i & = (p_1 p_2 \\cdots p_k) \\mod p_i + (1 \\mod p_i) \\\\ & = 0 + 1 \\\\ & = 1. \\end{aligned} $$ if $p_1 p_2 \\cdots p_k$ is all prime numbers, then $(p_1 p_2 \\cdots p_k) + 1$ is a new prime number.","title":"31.1-2"},{"location":"Chap31/31.1/#311-3","text":"Prove that if $a \\mid b$ and $b \\mid c$, then $a \\mid c$. If $a \\mid b$, then $b = a \\cdot k_1$. If $b \\mid c$, then $c = b \\cdot k_2 = a \\cdot (k_1 \\cdot k_2) = a \\cdot k_3$, then $a \\mid c$.","title":"31.1-3"},{"location":"Chap31/31.1/#311-4","text":"Prove that if $p$ is prime and $0 < k < p$, then $\\gcd(k, p) = 1$. If $k \\ne 1$, then $k \\nmid p$. If $k = 1$, then the divisor is $1$.","title":"31.1-4"},{"location":"Chap31/31.1/#311-5","text":"Prove Corollary 31.5. For all positive integers $n$, $a$, and $b$, if $n \\mid ab$ and $\\gcd(a, n) = 1$, then $n \\mid b$. Assume for the purpose of contradiction that $n \\mid ab$ and $\\gcd(a, n) = 1$, but $n \\nmid b$, so $gcd(n, b)=1$, for theorem 31.6, $gcd(n, ab)=1$ which is contradicting our assumption.","title":"31.1-5"},{"location":"Chap31/31.1/#311-6","text":"Prove that if $p$ is prime and $0 < k < p$, then $p \\mid \\binom{p}{k}$. Conclude that for all integers $a$ and $b$ and all primes $p$, $(a + b)^p \\equiv a^p + b^p \\pmod p$. $$ \\begin{array}{rlll} (a + b) ^ p & \\equiv & a^p + \\binom{p}{1} a^{p - 1}b^{1} + \\cdots + \\binom{p}{p - 1} a^{1}b^{p - 1} + b^p & \\pmod p \\\\ & \\equiv & a^p + 0 + \\cdots + 0 + b^p & \\pmod p \\\\ & \\equiv & a^p + b^p & \\pmod p \\end{array} $$","title":"31.1-6"},{"location":"Chap31/31.1/#311-7","text":"Prove that if $a$ and $b$ are any positive integers such that $a \\mid b$, then $$(x \\mod b) \\mod a = x \\mod a$$ for any $x$. Prove, under the same assumptions, that $x \\equiv y \\pmod b$ implies $x \\equiv y \\pmod a$ for any integers $x$ and $y$. Suppose $x = kb + c$, we have $$(x \\mod b) \\mod a = c \\mod a,$$ and $$x \\mod a = (kb + c) \\mod a = (kb \\mod a) + (c \\mod a) = c \\mod a.$$","title":"31.1-7"},{"location":"Chap31/31.1/#311-8","text":"For any integer $k > 0$, an integer $n$ is a $k$th power if there exists an integer $a$ such that $a^k = n$. Furthermore, $n > 1$ is a nontrivial power if it is a $k$th power for some integer $k > 1$. Show how to determine whether a given $\\beta$-bit integer $n$ is a nontrivial power in time polynomial in $\\beta$. Because $2^\\beta > n$, we only need to test values of $k$ that satisfy $2 \\le k < \\beta$, therefore the testing procedure remains $O(\\beta)$. For any nontrivial power $k$, where $2 \\le k < \\beta$, do a binary search on $a$ that costs $$O(\\log \\sqrt n) = O(\\log \\sqrt{2^\\beta}) = O(\\frac 1 2\\log 2^\\beta) = O(\\beta).$$ Thus, the total time complexity is $$O(\\beta) \\times O(\\beta) = O(\\beta^2).$$","title":"31.1-8"},{"location":"Chap31/31.1/#311-9","text":"Prove equations $\\text{(31.6)}$\u2013$\\text{(31.10)}$. (Omit!)","title":"31.1-9"},{"location":"Chap31/31.1/#311-10","text":"Show that the gcd operator is associative. That is, prove that for all integers $a$, $b$, and $c$, $\\gcd(a, \\gcd(b, c)) = \\gcd(\\gcd(a, b), c)$. [The following proof is provided by my friend, Tony Xiao.] Let $d = \\gcd(a, b, c)$, $a = dp$, $b = dq$ and $c = dr$. Claim $\\gcd(a, \\gcd(b, c)) = d.$ Let $e = \\gcd(b, c)$, thus $$ \\begin{aligned} b = es, \\\\ c = et. \\end{aligned} $$ Since $d \\mid b$ and $d \\mid c$, thus $d \\mid e$. Let $e = dm$, thus $$ \\begin{aligned} b = (dm)s & = dq, \\\\ c = (dm)t & = dr. \\end{aligned} $$ Suppose $k = \\gcd(p, m)$, $$ \\begin{aligned} & k \\mid p, k \\mid m, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid dm, \\\\ \\Rightarrow & dk \\mid dp, dk \\mid (dm)s, dk \\mid (dm)t, \\\\ \\Rightarrow & dk \\mid a, dk \\mid b, dk \\mid c. \\end{aligned} $$ Since $d = \\gcd(a, b, c)$, thus $k = 1$. $$ \\begin{aligned} \\gcd(a, \\gcd(b, c)) & = \\gcd(a, e) \\\\ & = \\gcd(dp, dm) \\\\ & = d \\cdot \\gcd(p, m) \\\\ & = d \\cdot k \\\\ & = d. \\end{aligned} $$ By the claim, $$\\gcd(a, \\gcd(b, c)) = d = \\gcd(\\gcd(a, b), c).$$","title":"31.1-10"},{"location":"Chap31/31.1/#311-11-star","text":"Prove Theorem 31.8. (Omit!)","title":"31.1-11 $\\star$"},{"location":"Chap31/31.1/#311-12","text":"Give efficient algorithms for the operations of dividing a $\\beta$-bit integer by a shorter integer and of taking the remainder of a $\\beta$-bit integer when divided by a shorter integer. Your algorithms should run in time $\\Theta(\\beta^2)$. Shift left until the two numbers have the same length, then repeatedly subtract with proper multiplier and shift right.","title":"31.1-12"},{"location":"Chap31/31.1/#311-13","text":"Give an efficient algorithm to convert a given $\\beta$-bit (binary) integer to a decimal representation. Argue that if multiplication or division of integers whose length is at most $\\beta$ takes time $M(\\beta)$, then we can convert binary to decimal in time $\\Theta(M(\\beta) \\lg\\beta)$. (Omit!)","title":"31.1-13"},{"location":"Chap31/31.2/","text":"31.2-1 Prove that equations $\\text{(31.11)}$ and $\\text{(31.12)}$ imply equation $\\text{(31.13)}$. (Omit!) 31.2-2 Compute the values $(d, x, y)$ that the call $\\text{EXTENDED-EUCLID}(899, 493)$ returns. $(29, -6, 11)$. 31.2-3 Prove that for all integers $a$, $k$, and $n$, $\\gcd(a, n) = \\gcd(a + kn, n)$. $\\gcd(a, n) \\mid \\gcd(a + kn, n)$. Let $d = \\gcd(a, n)$, then $d \\mid a$ and $d \\mid n$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = 0$$ and $d \\mid n$, we have $$d \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a, n) \\mid \\gcd(a + kn, n).$$ $\\gcd(a + kn, n) \\mid \\gcd(a, n)$. Suppose $d = \\gcd(a + kn, n)$, we have $d \\mid n$ and $d \\mid (a + kn)$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = a \\mod d = 0,$$ we have $d \\mid a$. Since $d \\mid a$ and $d \\mid n$, we have $$d \\mid \\gcd(a, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n).$$ Since $$\\gcd(a, n) \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n),$$ we have $$\\gcd(a, n) = \\gcd(a + kn, n).$$ 31.2-4 Rewrite $\\text{EUCLID}$ in an iterative form that uses only a constant amount of memory (that is, stores only a constant number of integer values). EUCLID ( a , b ) while b != 0 t = a a = b b = t % b return a 31.2-5 If $a > b \\ge 0$, show that the call EUCLID$(a, b)$ makes at most $1 + \\log_\\phi b$ recursive calls. Improve this bound to $1 + \\log_\\phi(b / \\gcd(a, b))$. $$b \\ge F_{k + 1} \\approx \\phi^{k + 1} / \\sqrt{5}$$ $$k + 1 < \\log_\\phi \\sqrt{5} + \\log_\\phi b \\approx 1.67 + \\log_\\phi b$$ $$k < 0.67 + \\log_\\phi b < 1 + \\log_\\phi b.$$ Since $d \\cdot a \\mod d \\cdot b = d \\cdot (a \\mod b)$, $\\gcd(d \\cdot a, d \\cdot b)$ has the same number of recursive calls with $\\gcd(a, b)$, therefore we could let $b' = b / \\gcd(a, b)$, the inequality $k < 1 + \\log_\\phi(b') = 1 + \\log_\\phi(b / \\gcd(a, b))$. will holds. 31.2-6 What does $\\text{EXTENDED-EUCLID}(F_{k + 1}, F_k)$ return? Prove your answer correct. If $k$ is odd, then $(1, -F_{k-2}, F_{k - 1})$. If $k$ is even, then $(1, F_{k-2}, -F_{k - 1})$. 31.2-7 Define the $\\gcd$ function for more than two arguments by the recursive equation $\\gcd(a_0, a_1, \\cdots, a_n) = \\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n))$. Show that the $\\gcd$ function returns the same answer independent of the order in which its arguments are specified. Also show how to find integers $x_0, x_1, \\cdots, x_n$ such that $\\gcd(a_0, a_1, \\ldots, a_n) = a_0 x_0 + a_1 x_1 + \\cdots + a_n x_n$. Show that the number of divisions performed by your algorithm is $O(n + \\lg (max \\{a_0, a_1, \\cdots, a_n \\}))$. Suppose $$\\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n)) = a_0 \\cdot x + \\gcd(a_1, a_2, \\cdots, a_n) \\cdot y$$ and $$\\gcd(a_1, \\gcd(a_2, a_3, \\cdots, a_n)) = a_1 \\cdot x' + \\gcd(a_2, a_3, \\cdots, a_n) \\cdot y',$$ then the coefficient of $a_1$ is $y \\cdot x'$. EXTENDED - EUCLID ( a , b ) if b == 0 return ( a , 1 , 0 ) ( d , x , y ) = EXTENDED - EUCLID ( b , a % b ) return ( d , y , x - ( a / b ) * y ) EXTENDED - EUCLID - MULTIPLE ( a ) if a . length == 1 return ( a [ 0 ], 1 ) g = a [ a . length - 1 ] xs = [ 1 ] * a . length ys = [ 0 ] * a . length for i = a . length - 2 downto 0 ( g , xs [ i ], ys [ i + 1 ]) = EXTENDED - EUCLID ( a [ i ], g ) m = 1 for i = 1 to a . length m *= ys [ i ] xs [ i ] *= m return ( g , xs ) 31.2-8 Define $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ to be the least common multiple of the $n$ integers $a_1, a_2, \\ldots, a_n$, that is, the smallest nonnegative integer that is a multiple of each $a_i$. Show how to compute $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ efficiently using the (two-argument) $\\gcd$ operation as a subroutine. GCD ( a , b ) if b == 0 return a return GCD ( b , a % b ) LCM ( a , b ) return a / GCD ( a , b ) * b LCM - MULTIPLE ( a ) l = a [ 0 ] for i = 1 to a . length l = LCM ( l , a [ i ]) return l 31.2-9 Prove that $n_1$, $n_2$, $n_3$, and $n_4$ are pairwise relatively prime if and only if $\\gcd(n_1n_2,n_3n_4) = \\gcd(n_1n_3, n_2n_4) = 1.$ More generally, show that $n_1, n_2, \\ldots, n_k$ are pairwise relatively prime if and only if a set of $\\lceil \\lg k \\rceil$ pairs of numbers derived from the $n_i$ are relatively prime. Suppose $n_1n_2 x + n_3n_4 y = 1$, then $n_1(n_2 x) + n_3(n_4 y) = 1$, thus $n_1$ and $n_3$ are relatively prime, $n_1$ and $n_4$, $n_2$ and $n_3$, $n_2$ and $n_4$ are the all relatively prime. And since $\\gcd(n_1n_3, n_2n_4) = 1$, all the pairs are relatively prime. General: in the first round, divide the elements into two sets with equal number of elements, calculate the products of the two set separately, if the two products are relatively prime, then the element in one set is pairwise relatively prime with the element in the other set. In the next iterations, for each set, we divide the elements into two subsets, suppose we have subsets $\\{ (s_1, s_2), (s_3, s_4), \\ldots \\}$, then we calculate the products of $\\{ s_1, s_3, \\ldots \\}$ and $\\{ s_2, s_4, \\ldots \\}$, if the two products are relatively prime, then all the pairs of subset are pairwise relatively prime similar to the first round. In each iteration, the number of elements in a subset is half of the original set, thus there are $\\lceil \\lg k \\rceil$ pairs of products. To choose the subsets efficiently, in the $k$th iteration, we could divide the numbers based on the value of the index's $k$th bit.","title":"31.2 Greatest common divisor"},{"location":"Chap31/31.2/#312-1","text":"Prove that equations $\\text{(31.11)}$ and $\\text{(31.12)}$ imply equation $\\text{(31.13)}$. (Omit!)","title":"31.2-1"},{"location":"Chap31/31.2/#312-2","text":"Compute the values $(d, x, y)$ that the call $\\text{EXTENDED-EUCLID}(899, 493)$ returns. $(29, -6, 11)$.","title":"31.2-2"},{"location":"Chap31/31.2/#312-3","text":"Prove that for all integers $a$, $k$, and $n$, $\\gcd(a, n) = \\gcd(a + kn, n)$. $\\gcd(a, n) \\mid \\gcd(a + kn, n)$. Let $d = \\gcd(a, n)$, then $d \\mid a$ and $d \\mid n$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = 0$$ and $d \\mid n$, we have $$d \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a, n) \\mid \\gcd(a + kn, n).$$ $\\gcd(a + kn, n) \\mid \\gcd(a, n)$. Suppose $d = \\gcd(a + kn, n)$, we have $d \\mid n$ and $d \\mid (a + kn)$. Since $$(a + kn) \\mod d = a \\mod d + k \\cdot (n \\mod d) = a \\mod d = 0,$$ we have $d \\mid a$. Since $d \\mid a$ and $d \\mid n$, we have $$d \\mid \\gcd(a, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n).$$ Since $$\\gcd(a, n) \\mid \\gcd(a + kn, n)$$ and $$\\gcd(a + kn, n) \\mid \\gcd(a, n),$$ we have $$\\gcd(a, n) = \\gcd(a + kn, n).$$","title":"31.2-3"},{"location":"Chap31/31.2/#312-4","text":"Rewrite $\\text{EUCLID}$ in an iterative form that uses only a constant amount of memory (that is, stores only a constant number of integer values). EUCLID ( a , b ) while b != 0 t = a a = b b = t % b return a","title":"31.2-4"},{"location":"Chap31/31.2/#312-5","text":"If $a > b \\ge 0$, show that the call EUCLID$(a, b)$ makes at most $1 + \\log_\\phi b$ recursive calls. Improve this bound to $1 + \\log_\\phi(b / \\gcd(a, b))$. $$b \\ge F_{k + 1} \\approx \\phi^{k + 1} / \\sqrt{5}$$ $$k + 1 < \\log_\\phi \\sqrt{5} + \\log_\\phi b \\approx 1.67 + \\log_\\phi b$$ $$k < 0.67 + \\log_\\phi b < 1 + \\log_\\phi b.$$ Since $d \\cdot a \\mod d \\cdot b = d \\cdot (a \\mod b)$, $\\gcd(d \\cdot a, d \\cdot b)$ has the same number of recursive calls with $\\gcd(a, b)$, therefore we could let $b' = b / \\gcd(a, b)$, the inequality $k < 1 + \\log_\\phi(b') = 1 + \\log_\\phi(b / \\gcd(a, b))$. will holds.","title":"31.2-5"},{"location":"Chap31/31.2/#312-6","text":"What does $\\text{EXTENDED-EUCLID}(F_{k + 1}, F_k)$ return? Prove your answer correct. If $k$ is odd, then $(1, -F_{k-2}, F_{k - 1})$. If $k$ is even, then $(1, F_{k-2}, -F_{k - 1})$.","title":"31.2-6"},{"location":"Chap31/31.2/#312-7","text":"Define the $\\gcd$ function for more than two arguments by the recursive equation $\\gcd(a_0, a_1, \\cdots, a_n) = \\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n))$. Show that the $\\gcd$ function returns the same answer independent of the order in which its arguments are specified. Also show how to find integers $x_0, x_1, \\cdots, x_n$ such that $\\gcd(a_0, a_1, \\ldots, a_n) = a_0 x_0 + a_1 x_1 + \\cdots + a_n x_n$. Show that the number of divisions performed by your algorithm is $O(n + \\lg (max \\{a_0, a_1, \\cdots, a_n \\}))$. Suppose $$\\gcd(a_0, \\gcd(a_1, a_2, \\cdots, a_n)) = a_0 \\cdot x + \\gcd(a_1, a_2, \\cdots, a_n) \\cdot y$$ and $$\\gcd(a_1, \\gcd(a_2, a_3, \\cdots, a_n)) = a_1 \\cdot x' + \\gcd(a_2, a_3, \\cdots, a_n) \\cdot y',$$ then the coefficient of $a_1$ is $y \\cdot x'$. EXTENDED - EUCLID ( a , b ) if b == 0 return ( a , 1 , 0 ) ( d , x , y ) = EXTENDED - EUCLID ( b , a % b ) return ( d , y , x - ( a / b ) * y ) EXTENDED - EUCLID - MULTIPLE ( a ) if a . length == 1 return ( a [ 0 ], 1 ) g = a [ a . length - 1 ] xs = [ 1 ] * a . length ys = [ 0 ] * a . length for i = a . length - 2 downto 0 ( g , xs [ i ], ys [ i + 1 ]) = EXTENDED - EUCLID ( a [ i ], g ) m = 1 for i = 1 to a . length m *= ys [ i ] xs [ i ] *= m return ( g , xs )","title":"31.2-7"},{"location":"Chap31/31.2/#312-8","text":"Define $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ to be the least common multiple of the $n$ integers $a_1, a_2, \\ldots, a_n$, that is, the smallest nonnegative integer that is a multiple of each $a_i$. Show how to compute $\\text{lcm}(a_1, a_2, \\ldots, a_n)$ efficiently using the (two-argument) $\\gcd$ operation as a subroutine. GCD ( a , b ) if b == 0 return a return GCD ( b , a % b ) LCM ( a , b ) return a / GCD ( a , b ) * b LCM - MULTIPLE ( a ) l = a [ 0 ] for i = 1 to a . length l = LCM ( l , a [ i ]) return l","title":"31.2-8"},{"location":"Chap31/31.2/#312-9","text":"Prove that $n_1$, $n_2$, $n_3$, and $n_4$ are pairwise relatively prime if and only if $\\gcd(n_1n_2,n_3n_4) = \\gcd(n_1n_3, n_2n_4) = 1.$ More generally, show that $n_1, n_2, \\ldots, n_k$ are pairwise relatively prime if and only if a set of $\\lceil \\lg k \\rceil$ pairs of numbers derived from the $n_i$ are relatively prime. Suppose $n_1n_2 x + n_3n_4 y = 1$, then $n_1(n_2 x) + n_3(n_4 y) = 1$, thus $n_1$ and $n_3$ are relatively prime, $n_1$ and $n_4$, $n_2$ and $n_3$, $n_2$ and $n_4$ are the all relatively prime. And since $\\gcd(n_1n_3, n_2n_4) = 1$, all the pairs are relatively prime. General: in the first round, divide the elements into two sets with equal number of elements, calculate the products of the two set separately, if the two products are relatively prime, then the element in one set is pairwise relatively prime with the element in the other set. In the next iterations, for each set, we divide the elements into two subsets, suppose we have subsets $\\{ (s_1, s_2), (s_3, s_4), \\ldots \\}$, then we calculate the products of $\\{ s_1, s_3, \\ldots \\}$ and $\\{ s_2, s_4, \\ldots \\}$, if the two products are relatively prime, then all the pairs of subset are pairwise relatively prime similar to the first round. In each iteration, the number of elements in a subset is half of the original set, thus there are $\\lceil \\lg k \\rceil$ pairs of products. To choose the subsets efficiently, in the $k$th iteration, we could divide the numbers based on the value of the index's $k$th bit.","title":"31.2-9"},{"location":"Chap31/31.3/","text":"31.3-1 Draw the group operation tables for the groups $(\\mathbb Z_4, +_4)$ and $(\\mathbb Z_5^*, \\cdot_5)$. Show that these groups are isomorphic by exhibiting a one-to-one correspondence $\\alpha$ between their elements such that $a + b \\equiv c \\pmod 4$ if and only if $\\alpha(a) \\cdot \\alpha(b) \\equiv \\alpha(c) \\pmod 5$. $(\\mathbb Z_4, +_4)$: $\\{ 0, 1, 2, 3 \\}$. $(\\mathbb Z_5^*, \\cdot_5)$: $\\{ 1, 2, 3, 4 \\}$. $\\alpha(x) = 2^{x-1}$. 31.3-2 List all subgroups of $\\mathbb Z_9$ and of $\\mathbb Z_{13}^*$. $\\mathbb Z_9$: $\\langle 0 \\rangle = \\{ 0 \\}$, $\\langle 1 \\rangle = \\{ 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}$, $\\langle 2 \\rangle = \\{ 0, 2, 4, 6, 8 \\}$. $\\mathbb Z_{13}^*$: $\\langle 1 \\rangle = \\{ 1 \\}$, $\\langle 2 \\rangle = \\{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 \\}$. 31.3-3 Prove Theorem 31.14. A nonempty closed subset of a finite group is a subgroup. Closure: the subset is closed. Identity: suppose $a \\in S'$, then $a^{(k)} \\in S'$. Since the subset is finite, there must be a period such that $a^{(m)} = a^{(n)}$, hence $a^{(m)}a^{(-n)} = a^{(m - n)} = 1$, therefore the subset must contain the identity. Associativity: inherit from the origin group. Inverses: suppose $a^{(k)} = 1$, the inverse of element $a$ is $a^{(k - 1)}$ since $aa^{(k - 1)} = a^{(k)} = 1$. 31.3-4 Show that if $p$ is prime and $e$ is a positive integer, then $\\phi(p^e) = p^{e - 1}(p - 1)$. $\\phi(p^e) = p^e \\cdot \\left ( 1 - \\frac{1}{p} \\right ) = p^{e - 1}(p - 1)$. 31.3-5 Show that for any integer $n > 1$ and for any $a \\in \\mathbb Z_n^*$, the function $f_a : \\mathbb Z_n^* \\rightarrow \\mathbb Z_n^*$ defined by $f_a(x) = ax \\mod n$ is a permutation of $\\mathbb Z_n^*$. To prove it is a permutation, we need to prove that for each element $x \\in \\mathbb Z_n^*$, $f_a(x) \\in \\mathbb Z_n^*$, the numbers generated by $f_a$ are distinct. Since $a \\in \\mathbb Z_{n}^*$ and $x \\in \\mathbb Z_n^*$, then $f_a(x) = ax \\mod n \\in \\mathbb Z_n^*$ by the closure property. Suppose there are two distinct numbers $x \\in \\mathbb Z_n^*$ and $y \\in \\mathbb Z_n^*$ that $f_a(x) = f_a(y)$, $$ \\begin{aligned} f_a(x) & = f_a(y) \\\\ ax \\mod n & = ay \\mod n \\\\ (a \\mod n)(x \\mod n) & = (a \\mod n)(y \\mod n) \\\\ (x \\mod n) & = y \\mod n \\\\ x & \\equiv y \\mod n, \\end{aligned} $$ which contradicts the assumption, therefore the numbers generated by $f_a$ are distinct.","title":"31.3 Modular arithmetic"},{"location":"Chap31/31.3/#313-1","text":"Draw the group operation tables for the groups $(\\mathbb Z_4, +_4)$ and $(\\mathbb Z_5^*, \\cdot_5)$. Show that these groups are isomorphic by exhibiting a one-to-one correspondence $\\alpha$ between their elements such that $a + b \\equiv c \\pmod 4$ if and only if $\\alpha(a) \\cdot \\alpha(b) \\equiv \\alpha(c) \\pmod 5$. $(\\mathbb Z_4, +_4)$: $\\{ 0, 1, 2, 3 \\}$. $(\\mathbb Z_5^*, \\cdot_5)$: $\\{ 1, 2, 3, 4 \\}$. $\\alpha(x) = 2^{x-1}$.","title":"31.3-1"},{"location":"Chap31/31.3/#313-2","text":"List all subgroups of $\\mathbb Z_9$ and of $\\mathbb Z_{13}^*$. $\\mathbb Z_9$: $\\langle 0 \\rangle = \\{ 0 \\}$, $\\langle 1 \\rangle = \\{ 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}$, $\\langle 2 \\rangle = \\{ 0, 2, 4, 6, 8 \\}$. $\\mathbb Z_{13}^*$: $\\langle 1 \\rangle = \\{ 1 \\}$, $\\langle 2 \\rangle = \\{ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 \\}$.","title":"31.3-2"},{"location":"Chap31/31.3/#313-3","text":"Prove Theorem 31.14. A nonempty closed subset of a finite group is a subgroup. Closure: the subset is closed. Identity: suppose $a \\in S'$, then $a^{(k)} \\in S'$. Since the subset is finite, there must be a period such that $a^{(m)} = a^{(n)}$, hence $a^{(m)}a^{(-n)} = a^{(m - n)} = 1$, therefore the subset must contain the identity. Associativity: inherit from the origin group. Inverses: suppose $a^{(k)} = 1$, the inverse of element $a$ is $a^{(k - 1)}$ since $aa^{(k - 1)} = a^{(k)} = 1$.","title":"31.3-3"},{"location":"Chap31/31.3/#313-4","text":"Show that if $p$ is prime and $e$ is a positive integer, then $\\phi(p^e) = p^{e - 1}(p - 1)$. $\\phi(p^e) = p^e \\cdot \\left ( 1 - \\frac{1}{p} \\right ) = p^{e - 1}(p - 1)$.","title":"31.3-4"},{"location":"Chap31/31.3/#313-5","text":"Show that for any integer $n > 1$ and for any $a \\in \\mathbb Z_n^*$, the function $f_a : \\mathbb Z_n^* \\rightarrow \\mathbb Z_n^*$ defined by $f_a(x) = ax \\mod n$ is a permutation of $\\mathbb Z_n^*$. To prove it is a permutation, we need to prove that for each element $x \\in \\mathbb Z_n^*$, $f_a(x) \\in \\mathbb Z_n^*$, the numbers generated by $f_a$ are distinct. Since $a \\in \\mathbb Z_{n}^*$ and $x \\in \\mathbb Z_n^*$, then $f_a(x) = ax \\mod n \\in \\mathbb Z_n^*$ by the closure property. Suppose there are two distinct numbers $x \\in \\mathbb Z_n^*$ and $y \\in \\mathbb Z_n^*$ that $f_a(x) = f_a(y)$, $$ \\begin{aligned} f_a(x) & = f_a(y) \\\\ ax \\mod n & = ay \\mod n \\\\ (a \\mod n)(x \\mod n) & = (a \\mod n)(y \\mod n) \\\\ (x \\mod n) & = y \\mod n \\\\ x & \\equiv y \\mod n, \\end{aligned} $$ which contradicts the assumption, therefore the numbers generated by $f_a$ are distinct.","title":"31.3-5"},{"location":"Chap31/31.4/","text":"31.4-1 Find all solutions to the equation $35x \\equiv 10 \\pmod{50}$. $\\{6, 16, 26, 36, 46\\}$. 31.4-2 Prove that the equation $ax \\equiv ay \\pmod n$ implies $x \\equiv y \\pmod n$ whenever $\\gcd(a, n) = 1$. Show that the condition $\\gcd(a, n) = 1$ is necessary by supplying a counterexample with $\\gcd(a, n) > 1$. $$d = \\gcd(ax, n) = \\gcd(x, n)$$ Since $ax \\cdot x' + n \\cdot y' = d$, we have $$x \\cdot (ax') + n \\cdot y' = d.$$ $$ \\begin{aligned} x_0 & = x'(ay / d), \\\\ x_0' & = (ax')(y / d) = x'(ay / d) = x_0. \\end{aligned} $$ 31.4-3 Consider the following change to line 3 of the procedure $\\text{MODULAR-LINEAR-EQUATION-SOLVER}$: 3 x0 = x ' ( b / d ) mod ( n / d ) Will this work? Explain why or why not. Assume that $x_0 \\ge n / d$, then the largest solution is $x_0 + (d - 1) \\cdot (n / d) \\ge d \\cdot n / d \\ge n$, which is impossible, therefore $x_0 < n / d$. 31.4-4 $\\star$ Let $p$ be prime and $f(x) \\equiv f_0 + f_1 x + \\cdots + f_tx^t \\pmod p$ be a polynomial of degree $t$, with coefficients $f_i$ drawn from $\\mathbb Z_p$. We say that $a \\in \\mathbb Z_p$ is a zero of $f$ if $f(a) \\equiv 0 \\pmod p$. Prove that if $a$ is a zero of $f$, then $f(x) \\equiv (x - a) g(x) \\pmod p$ for some polynomial $g(x)$ of degree $t - 1$. Prove by induction on $t$ that if $p$ is prime, then a polynomial $f(x)$ of degree $t$ can have at most $t$ distinct zeros modulo $p$. (Omit!)","title":"31.4 Solving modular linear equations"},{"location":"Chap31/31.4/#314-1","text":"Find all solutions to the equation $35x \\equiv 10 \\pmod{50}$. $\\{6, 16, 26, 36, 46\\}$.","title":"31.4-1"},{"location":"Chap31/31.4/#314-2","text":"Prove that the equation $ax \\equiv ay \\pmod n$ implies $x \\equiv y \\pmod n$ whenever $\\gcd(a, n) = 1$. Show that the condition $\\gcd(a, n) = 1$ is necessary by supplying a counterexample with $\\gcd(a, n) > 1$. $$d = \\gcd(ax, n) = \\gcd(x, n)$$ Since $ax \\cdot x' + n \\cdot y' = d$, we have $$x \\cdot (ax') + n \\cdot y' = d.$$ $$ \\begin{aligned} x_0 & = x'(ay / d), \\\\ x_0' & = (ax')(y / d) = x'(ay / d) = x_0. \\end{aligned} $$","title":"31.4-2"},{"location":"Chap31/31.4/#314-3","text":"Consider the following change to line 3 of the procedure $\\text{MODULAR-LINEAR-EQUATION-SOLVER}$: 3 x0 = x ' ( b / d ) mod ( n / d ) Will this work? Explain why or why not. Assume that $x_0 \\ge n / d$, then the largest solution is $x_0 + (d - 1) \\cdot (n / d) \\ge d \\cdot n / d \\ge n$, which is impossible, therefore $x_0 < n / d$.","title":"31.4-3"},{"location":"Chap31/31.4/#314-4-star","text":"Let $p$ be prime and $f(x) \\equiv f_0 + f_1 x + \\cdots + f_tx^t \\pmod p$ be a polynomial of degree $t$, with coefficients $f_i$ drawn from $\\mathbb Z_p$. We say that $a \\in \\mathbb Z_p$ is a zero of $f$ if $f(a) \\equiv 0 \\pmod p$. Prove that if $a$ is a zero of $f$, then $f(x) \\equiv (x - a) g(x) \\pmod p$ for some polynomial $g(x)$ of degree $t - 1$. Prove by induction on $t$ that if $p$ is prime, then a polynomial $f(x)$ of degree $t$ can have at most $t$ distinct zeros modulo $p$. (Omit!)","title":"31.4-4 $\\star$"},{"location":"Chap31/31.5/","text":"31.5-1 Find all solutions to the equations $x \\equiv 4 \\pmod 5$ and $x \\equiv 5 \\pmod{11}$. $$ \\begin{aligned} m_1 & = 11, m_2 = 5. \\\\ m_1^{-1} & = 1, m_2^{-1} = 9. \\\\ c_1 & = 11, c_2 = 45. \\\\ a & = (c_1 \\cdot a_1 + c_2 \\cdot a_2) \\mod (n_1 \\cdot n_2) \\\\ & = (11 \\cdot 4 + 45 \\cdot 5) \\mod 55 = 49. \\end{aligned} $$ 31.5-2 Find all integers $x$ that leave remainders $1$, $2$, $3$ when divided by $9$, $8$, $7$ respectively. $10 + 504i$, $i \\in \\mathbb Z$. 31.5-3 Argue that, under the definitions of Theorem 31.27, if $\\gcd(a, n) = 1$, then $$(a^{-1} \\mod n) \\leftrightarrow ((a_1^{-1} \\mod n_1), (a_2^{-1} \\mod n_2), \\ldots, (a_k^{-1} \\mod n_k)).$$ $$\\gcd(a, n) = 1 \\rightarrow \\gcd(a, n_i) = 1.$$ 31.5-4 Under the definitions of Theorem 31.27, prove that for any polynomial $f$, the number of roots of the equation $f(x) \\equiv 0 (\\mod n)$ equals the product of the number of roots of each of the equations $$f(x) \\equiv 0 \\pmod{n_1}, f(x) \\equiv 0 \\pmod{n_2}, \\ldots, f(x) \\equiv 0 \\pmod{n_k}.$$ Based on $\\text{31.28}$\u2013$\\text{31.30}$.","title":"31.5 The Chinese remainder theorem"},{"location":"Chap31/31.5/#315-1","text":"Find all solutions to the equations $x \\equiv 4 \\pmod 5$ and $x \\equiv 5 \\pmod{11}$. $$ \\begin{aligned} m_1 & = 11, m_2 = 5. \\\\ m_1^{-1} & = 1, m_2^{-1} = 9. \\\\ c_1 & = 11, c_2 = 45. \\\\ a & = (c_1 \\cdot a_1 + c_2 \\cdot a_2) \\mod (n_1 \\cdot n_2) \\\\ & = (11 \\cdot 4 + 45 \\cdot 5) \\mod 55 = 49. \\end{aligned} $$","title":"31.5-1"},{"location":"Chap31/31.5/#315-2","text":"Find all integers $x$ that leave remainders $1$, $2$, $3$ when divided by $9$, $8$, $7$ respectively. $10 + 504i$, $i \\in \\mathbb Z$.","title":"31.5-2"},{"location":"Chap31/31.5/#315-3","text":"Argue that, under the definitions of Theorem 31.27, if $\\gcd(a, n) = 1$, then $$(a^{-1} \\mod n) \\leftrightarrow ((a_1^{-1} \\mod n_1), (a_2^{-1} \\mod n_2), \\ldots, (a_k^{-1} \\mod n_k)).$$ $$\\gcd(a, n) = 1 \\rightarrow \\gcd(a, n_i) = 1.$$","title":"31.5-3"},{"location":"Chap31/31.5/#315-4","text":"Under the definitions of Theorem 31.27, prove that for any polynomial $f$, the number of roots of the equation $f(x) \\equiv 0 (\\mod n)$ equals the product of the number of roots of each of the equations $$f(x) \\equiv 0 \\pmod{n_1}, f(x) \\equiv 0 \\pmod{n_2}, \\ldots, f(x) \\equiv 0 \\pmod{n_k}.$$ Based on $\\text{31.28}$\u2013$\\text{31.30}$.","title":"31.5-4"},{"location":"Chap31/31.6/","text":"31.6-1 Draw a table showing the order of every element in $\\mathbb Z_{11}^*$. Pick the smallest primitive root $g$ and compute a table giving $\\text{ind}_{11, g}(x)$ for all $x \\in \\mathbb Z_{11}^*$. $g = 2$, $\\{1, 2, 4, 8, 5, 10, 9, 7, 3, 6\\}$. 31.6-2 Give a modular exponentiation algorithm that examines the bits of $b$ from right to left instead of left to right. MODULAR - EXPONENTIATION ( a , b , n ) i = 0 d = 1 while ( 1 << i ) \u2264 b if ( b & ( 1 << i )) > 0 d = ( d * a ) % n a = ( a * a ) % n i = i + 1 return d 31.6-3 Assuming that you know $\\phi(n)$, explain how to compute $a^{-1} \\mod n$ for any $a \\in \\mathbb Z_n^*$ using the procedure $\\text{MODULAR-EXPONENTIATION}$. $$ \\begin{array}{rlll} a^{\\phi(n)} & \\equiv & 1 & \\pmod n, \\\\ a\\cdot a^{\\phi(n) - 1} & \\equiv & 1 & \\pmod n, \\\\ a^{-1} & \\equiv & a^{\\phi(n)-1} & \\pmod n. \\end{array} $$","title":"31.6 Powers of an element"},{"location":"Chap31/31.6/#316-1","text":"Draw a table showing the order of every element in $\\mathbb Z_{11}^*$. Pick the smallest primitive root $g$ and compute a table giving $\\text{ind}_{11, g}(x)$ for all $x \\in \\mathbb Z_{11}^*$. $g = 2$, $\\{1, 2, 4, 8, 5, 10, 9, 7, 3, 6\\}$.","title":"31.6-1"},{"location":"Chap31/31.6/#316-2","text":"Give a modular exponentiation algorithm that examines the bits of $b$ from right to left instead of left to right. MODULAR - EXPONENTIATION ( a , b , n ) i = 0 d = 1 while ( 1 << i ) \u2264 b if ( b & ( 1 << i )) > 0 d = ( d * a ) % n a = ( a * a ) % n i = i + 1 return d","title":"31.6-2"},{"location":"Chap31/31.6/#316-3","text":"Assuming that you know $\\phi(n)$, explain how to compute $a^{-1} \\mod n$ for any $a \\in \\mathbb Z_n^*$ using the procedure $\\text{MODULAR-EXPONENTIATION}$. $$ \\begin{array}{rlll} a^{\\phi(n)} & \\equiv & 1 & \\pmod n, \\\\ a\\cdot a^{\\phi(n) - 1} & \\equiv & 1 & \\pmod n, \\\\ a^{-1} & \\equiv & a^{\\phi(n)-1} & \\pmod n. \\end{array} $$","title":"31.6-3"},{"location":"Chap31/31.7/","text":"31.7-1 Consider an RSA key set with $p = 11$, $q = 29$, $n = 319$, and $e = 3$. What value of $d$ should be used in the secret key? What is the encryption of the message $M = 100$? $\\phi(n) = (p - 1) \\cdot (q - 1) = 280$. $d = e^{-1} \\mod \\phi(n) = 187$. $P(M) = M^e \\mod n = 254$. $S(C) = C^d \\mod n = 254^{187} \\mod n = 100$. 31.7-2 Prove that if Alice's public exponent $e$ is $3$ and an adversary obtains Alice's secret exponent $d$, where $0 < d < \\phi(n)$, then the adversary can factor Alice's modulus $n$ in time polynomial in the number of bits in $n$. (Although you are not asked to prove it, you may be interested to know that this result remains true even if the condition $e = 3$ is removed. See Miller [255].) $$ed \\equiv 1 \\mod \\phi(n)$$ $$ed - 1 = 3d - 1 = k \\phi(n)$$ If $p, q < n / 4$, then $$\\phi(n) = n - (p + q) + 1 > n - n / 2 + 1 = n / 2 + 1 > n / 2.$$ $kn / 2 < 3d - 1 < 3d < 3n$, then $k < 6$, then we can solve $3d - 1 = n - p - n / p + 1$. 31.7-3 $\\star$ Prove that RSA is multiplicative in the sense that $P_A(M_1) P_A(M_2) \\equiv P_A(M_1, M_2) \\pmod n$. Use this fact to prove that if an adversary had a procedure that could efficiently decrypt $1$ percent of messages from $\\mathbb Z_n$ encrypted with $P_A$, then he could employ a probabilistic algorithm to decrypt every message encrypted with $P_A$ with high probability. Multiplicative: $e$ is relatively prime to $n$. Decrypt: In each iteration randomly choose a prime number $m$ that $m$ is relatively prime to $n$, if we can decrypt $m \\cdot M$, then we can return $m^{-1}M$ since $m^{-1} = m^{n - 2}$.","title":"31.7 The RSA public-key cryptosystem"},{"location":"Chap31/31.7/#317-1","text":"Consider an RSA key set with $p = 11$, $q = 29$, $n = 319$, and $e = 3$. What value of $d$ should be used in the secret key? What is the encryption of the message $M = 100$? $\\phi(n) = (p - 1) \\cdot (q - 1) = 280$. $d = e^{-1} \\mod \\phi(n) = 187$. $P(M) = M^e \\mod n = 254$. $S(C) = C^d \\mod n = 254^{187} \\mod n = 100$.","title":"31.7-1"},{"location":"Chap31/31.7/#317-2","text":"Prove that if Alice's public exponent $e$ is $3$ and an adversary obtains Alice's secret exponent $d$, where $0 < d < \\phi(n)$, then the adversary can factor Alice's modulus $n$ in time polynomial in the number of bits in $n$. (Although you are not asked to prove it, you may be interested to know that this result remains true even if the condition $e = 3$ is removed. See Miller [255].) $$ed \\equiv 1 \\mod \\phi(n)$$ $$ed - 1 = 3d - 1 = k \\phi(n)$$ If $p, q < n / 4$, then $$\\phi(n) = n - (p + q) + 1 > n - n / 2 + 1 = n / 2 + 1 > n / 2.$$ $kn / 2 < 3d - 1 < 3d < 3n$, then $k < 6$, then we can solve $3d - 1 = n - p - n / p + 1$.","title":"31.7-2"},{"location":"Chap31/31.7/#317-3-star","text":"Prove that RSA is multiplicative in the sense that $P_A(M_1) P_A(M_2) \\equiv P_A(M_1, M_2) \\pmod n$. Use this fact to prove that if an adversary had a procedure that could efficiently decrypt $1$ percent of messages from $\\mathbb Z_n$ encrypted with $P_A$, then he could employ a probabilistic algorithm to decrypt every message encrypted with $P_A$ with high probability. Multiplicative: $e$ is relatively prime to $n$. Decrypt: In each iteration randomly choose a prime number $m$ that $m$ is relatively prime to $n$, if we can decrypt $m \\cdot M$, then we can return $m^{-1}M$ since $m^{-1} = m^{n - 2}$.","title":"31.7-3 $\\star$"},{"location":"Chap31/31.8/","text":"31.8-1 Prove that if an odd integer $n > 1$ is not a prime or a prime power, then there exists a nontrivial square root of $1$ modulo $n$. (Omit!) 31.8-2 $\\star$ It is possible to strengthen Euler's theorem slightly to the form $a^{\\lambda(n)} \\equiv 1 \\pmod n$ for all $a \\in \\mathbb Z_n^*$, where $n = p_1^{e_1} \\cdots p_r^{e_r}$ and $\\lambda(n)$ is defined by $$\\lambda(n) = \\text{lcm}(\\phi(p_1^{e_1}), \\ldots, \\phi(p_r^{e_r})). \\tag{31.42}$$ Prove that $\\lambda(n) \\mid \\phi(n)$. A composite number $n$ is a Carmichael number if \u0005$\\lambda(n) \\mid n - 1$. The smallest Carmichael number is $561 = 3 \\cdot 11 \\cdot 17$; here, $\\lambda(n) = \\text{lcm}(2, 10, 16) = 80$, which divides $560$. Prove that Carmichael numbers must be both \"square-free\" (not divisible by the square of any prime) and the product of at least three primes. (For this reason, they are not very common.) Prove that $\\lambda(n) \\mid \\phi(n)$. We have $$ \\begin{aligned} n & = p_1^{e_1} \\cdots p_r^{e_r} \\\\ \\phi(n) & = \\phi(p_1^{e_1}) \\times \\dots \\times \\phi(p_r^{e_r}). \\end{aligned} $$ Thus, $$ \\begin{aligned} \\lambda(n) & \\mid \\phi(n) \\\\ \\Rightarrow \\text{lcm}(\\phi(p_1^{e_1}, \\ldots, \\phi(p_r^{e_r})) & \\mid (\\phi(p_1^{e_1}) \\times \\dots \\times \\phi(p_r^{e_r})) \\end{aligned} $$ Prove that Carmichael numbers must be \"square-free\" (not divisible by the square of any prime). Assume $n = p^\\alpha m$ is a Carmichael number, where $\\alpha \\ge 2$ and $p \\nmid m$. By the Chinese Remainder Theorem, the system of congruences $$ \\begin{aligned} x & \\equiv 1 + p \\pmod p^\\alpha, \\\\ x & \\equiv 1 \\pmod m \\end{aligned} $$ has a solution $a$. Note that $\\gcd(a, n) = 1$. Since $n$ is a Carmichael number, we have $a^{n - 1} \\equiv 1 \\pmod n$. In particular, $a^{n - 1} \\equiv 1 \\pmod {p^\\alpha}$; therefore, $a^n \\equiv a \\pmod {p^2}$. So, $(1 + p)^n \\equiv 1 + p \\pmod {p^2}$. Expand $(1 + p)^n$ modulo $p^2$ using the binomial theorem. We have $$(1 + p)^n \\equiv 1 \\pmod {p^2},$$ since the first two terms of the expansion are $1$ and $np$, and the rest of the terms are divisible by $p^2$. Thus, $1 \\equiv 1 + p \\pmod {p^2}$. This is impossible. Stack Exchange Reference Prove that Carmichael numbers must be the product of at least three primes. Assume that $n = pq$ is a Carmichael number, where $p$ and $q$ are distant primes and $p < q$. Then we have $$ \\begin{aligned} & q \\equiv 1 \\pmod{q - 1} \\\\ \\Rightarrow & n \\equiv pq \\equiv p \\pmod{q - 1} \\\\ \\Rightarrow & n - 1 \\equiv p - 1 \\pmod{q - 1}, \\end{aligned} $$ Here, $0 < p \u2212 1 < q \u2212 1$ , so $n \u2212 1$ is not divisible by $q \u2212 1$. Stack Exchange Reference 31.8-3 Prove that if $x$ is a nontrivial square root of $1$, modulo $n$, then $\\gcd(x - 1, n)$ and $\\gcd(x + 1, n)$ are both nontrivial divisors of $n$. $$ \\begin{array}{rlll} x^2 & \\equiv & 1 & \\pmod n, \\\\ x^2 - 1 & \\equiv & 0 & \\pmod n, \\\\ (x + 1)(x - 1) & \\equiv & 0 & \\pmod n. \\end{array} $$ $n \\mid (x + 1)(x - 1)$, suppose $\\gcd(x - 1, n) = 1$, then $n \\mid (x + 1)$, then $x \\equiv -1 \\pmod n$ which is trivial, it contradicts the fact that $x$ is nontrivial, therefore $\\gcd(x - 1, n) \\ne 1$, $\\gcd(x + 1, n) \\ne 1$.","title":"31.8 Primality testing"},{"location":"Chap31/31.8/#318-1","text":"Prove that if an odd integer $n > 1$ is not a prime or a prime power, then there exists a nontrivial square root of $1$ modulo $n$. (Omit!)","title":"31.8-1"},{"location":"Chap31/31.8/#318-2-star","text":"It is possible to strengthen Euler's theorem slightly to the form $a^{\\lambda(n)} \\equiv 1 \\pmod n$ for all $a \\in \\mathbb Z_n^*$, where $n = p_1^{e_1} \\cdots p_r^{e_r}$ and $\\lambda(n)$ is defined by $$\\lambda(n) = \\text{lcm}(\\phi(p_1^{e_1}), \\ldots, \\phi(p_r^{e_r})). \\tag{31.42}$$ Prove that $\\lambda(n) \\mid \\phi(n)$. A composite number $n$ is a Carmichael number if \u0005$\\lambda(n) \\mid n - 1$. The smallest Carmichael number is $561 = 3 \\cdot 11 \\cdot 17$; here, $\\lambda(n) = \\text{lcm}(2, 10, 16) = 80$, which divides $560$. Prove that Carmichael numbers must be both \"square-free\" (not divisible by the square of any prime) and the product of at least three primes. (For this reason, they are not very common.) Prove that $\\lambda(n) \\mid \\phi(n)$. We have $$ \\begin{aligned} n & = p_1^{e_1} \\cdots p_r^{e_r} \\\\ \\phi(n) & = \\phi(p_1^{e_1}) \\times \\dots \\times \\phi(p_r^{e_r}). \\end{aligned} $$ Thus, $$ \\begin{aligned} \\lambda(n) & \\mid \\phi(n) \\\\ \\Rightarrow \\text{lcm}(\\phi(p_1^{e_1}, \\ldots, \\phi(p_r^{e_r})) & \\mid (\\phi(p_1^{e_1}) \\times \\dots \\times \\phi(p_r^{e_r})) \\end{aligned} $$ Prove that Carmichael numbers must be \"square-free\" (not divisible by the square of any prime). Assume $n = p^\\alpha m$ is a Carmichael number, where $\\alpha \\ge 2$ and $p \\nmid m$. By the Chinese Remainder Theorem, the system of congruences $$ \\begin{aligned} x & \\equiv 1 + p \\pmod p^\\alpha, \\\\ x & \\equiv 1 \\pmod m \\end{aligned} $$ has a solution $a$. Note that $\\gcd(a, n) = 1$. Since $n$ is a Carmichael number, we have $a^{n - 1} \\equiv 1 \\pmod n$. In particular, $a^{n - 1} \\equiv 1 \\pmod {p^\\alpha}$; therefore, $a^n \\equiv a \\pmod {p^2}$. So, $(1 + p)^n \\equiv 1 + p \\pmod {p^2}$. Expand $(1 + p)^n$ modulo $p^2$ using the binomial theorem. We have $$(1 + p)^n \\equiv 1 \\pmod {p^2},$$ since the first two terms of the expansion are $1$ and $np$, and the rest of the terms are divisible by $p^2$. Thus, $1 \\equiv 1 + p \\pmod {p^2}$. This is impossible. Stack Exchange Reference Prove that Carmichael numbers must be the product of at least three primes. Assume that $n = pq$ is a Carmichael number, where $p$ and $q$ are distant primes and $p < q$. Then we have $$ \\begin{aligned} & q \\equiv 1 \\pmod{q - 1} \\\\ \\Rightarrow & n \\equiv pq \\equiv p \\pmod{q - 1} \\\\ \\Rightarrow & n - 1 \\equiv p - 1 \\pmod{q - 1}, \\end{aligned} $$ Here, $0 < p \u2212 1 < q \u2212 1$ , so $n \u2212 1$ is not divisible by $q \u2212 1$. Stack Exchange Reference","title":"31.8-2 $\\star$"},{"location":"Chap31/31.8/#318-3","text":"Prove that if $x$ is a nontrivial square root of $1$, modulo $n$, then $\\gcd(x - 1, n)$ and $\\gcd(x + 1, n)$ are both nontrivial divisors of $n$. $$ \\begin{array}{rlll} x^2 & \\equiv & 1 & \\pmod n, \\\\ x^2 - 1 & \\equiv & 0 & \\pmod n, \\\\ (x + 1)(x - 1) & \\equiv & 0 & \\pmod n. \\end{array} $$ $n \\mid (x + 1)(x - 1)$, suppose $\\gcd(x - 1, n) = 1$, then $n \\mid (x + 1)$, then $x \\equiv -1 \\pmod n$ which is trivial, it contradicts the fact that $x$ is nontrivial, therefore $\\gcd(x - 1, n) \\ne 1$, $\\gcd(x + 1, n) \\ne 1$.","title":"31.8-3"},{"location":"Chap31/31.9/","text":"31.9-1 Referring to the execution history shown in Figure 31.7(a), when does \\text{POLLARDRHO} print the factor $73$ of $1387$? $x = 84$, $y = 814$. 31.9-2 Suppose that we are given a function $f : \\mathbb Z_n \\rightarrow \\mathbb Z_n$ and an initial value $x_0 \\in \\mathbb Z_n$. Define $x_i = f(x_{i - 1})$ for $i = 1, 2, \\ldots$. Let $t$ and $u > 0$ be the smallest values such that $x_{t + i} = x_{t + u + i}$ for $i = 0, 1, \\ldots$. In the terminology of Pollard's rho algorithm, $t$ is the length of the tail and $u$ is the length of the cycle of the rho. Give an efficient algorithm to determine $t$ and $u$ exactly, and analyze its running time. (Omit!) 31.9-3 How many steps would you expect $\\text{POLLARD-RHO}$ to require to discover a factor of the form $p^e$, where $p$ is prime and $e > 1$? $\\Theta(\\sqrt p)$. 31.9-4 $\\star$ One disadvantage of $\\text{POLLARD-RHO}$ as written is that it requires one gcd computation for each step of the recurrence. Instead, we could batch the gcd computations by accumulating the product of several $x_i$ values in a row and then using this product instead of $x_i$ in the gcd computation. Describe carefully how you would implement this idea, why it works, and what batch size you would pick as the most effective when working on a $\\beta$-bit number $n$. (Omit!)","title":"31.9 Integer factorization"},{"location":"Chap31/31.9/#319-1","text":"Referring to the execution history shown in Figure 31.7(a), when does \\text{POLLARDRHO} print the factor $73$ of $1387$? $x = 84$, $y = 814$.","title":"31.9-1"},{"location":"Chap31/31.9/#319-2","text":"Suppose that we are given a function $f : \\mathbb Z_n \\rightarrow \\mathbb Z_n$ and an initial value $x_0 \\in \\mathbb Z_n$. Define $x_i = f(x_{i - 1})$ for $i = 1, 2, \\ldots$. Let $t$ and $u > 0$ be the smallest values such that $x_{t + i} = x_{t + u + i}$ for $i = 0, 1, \\ldots$. In the terminology of Pollard's rho algorithm, $t$ is the length of the tail and $u$ is the length of the cycle of the rho. Give an efficient algorithm to determine $t$ and $u$ exactly, and analyze its running time. (Omit!)","title":"31.9-2"},{"location":"Chap31/31.9/#319-3","text":"How many steps would you expect $\\text{POLLARD-RHO}$ to require to discover a factor of the form $p^e$, where $p$ is prime and $e > 1$? $\\Theta(\\sqrt p)$.","title":"31.9-3"},{"location":"Chap31/31.9/#319-4-star","text":"One disadvantage of $\\text{POLLARD-RHO}$ as written is that it requires one gcd computation for each step of the recurrence. Instead, we could batch the gcd computations by accumulating the product of several $x_i$ values in a row and then using this product instead of $x_i$ in the gcd computation. Describe carefully how you would implement this idea, why it works, and what batch size you would pick as the most effective when working on a $\\beta$-bit number $n$. (Omit!)","title":"31.9-4 $\\star$"},{"location":"Chap31/Problems/31-1/","text":"Most computers can perform the operations of subtraction, testing the parity (odd or even) of a binary integer, and halving more quickly than computing remainders. This problem investigates the binary gcd algorithm , which avoids the remainder computations used in Euclid's algorithm. a. Prove that if $a$ and $b$ are both even, then $\\gcd(a, b) = 2 \\cdot \\gcd(a / 2, b / 2)$. b. Prove that if $a$ is odd and $b$ is even, then $\\gcd(a, b) = \\gcd(a, b / 2)$. c. Prove that if $a$ and $b$ are both odd, then $\\gcd(a, b) = \\gcd((a - b) / 2, b)$. d. Design an efficient binary gcd algorithm for input integers $a$ and $b$, where $a \\ge b$, that runs in $O(\\lg a)$ time. Assume that each subtraction, parity test, and halving takes unit time. (Omit!) d. BINARY - GCD ( a , b ) if a < b return BINARY - GCD ( b , a ) if b == 0 return a if ( a & 1 == 1 ) and ( b & 1 == 1 ) return BINARY - GCD (( a - b ) >> 1 , b ) if ( a & 1 == 0 ) and ( b & 1 == 0 ) return BINARY - GCD ( a >> 1 , b >> 1 ) << 1 if a & 1 == 1 return BINARY - GCD ( a , b >> 1 ) return BINARY - GCD ( a >> 1 , b )","title":"31-1 Binary gcd algorithm"},{"location":"Chap31/Problems/31-2/","text":"a. Consider the ordinary \"paper and pencil\" algorithm for long division: dividing $a$ by $b$, which yields a quotient $q$ and remainder $r$. Show that this method requires $O((1 + \\lg q) \\lg b)$ bit operations. b. Define $\\mu(a, b) = (1 + \\lg a)(1 + \\lg b)$. Show that the number of bit operations performed by $\\text{EUCLID}$ in reducing the problem of computing $\\gcd(a, b)$ to that of computing $\\gcd(b, a \\mod b)$ is at most $c(\\mu(a, b) - \\mu(b, a \\mod b))$ for some sufficiently large constant $c > 0$. c. Show that $\\text{EUCLID}(a, b)$ requires $O(\\mu(a, b))$ bit operations in general and $O(\\beta^2)$ bit operations when applied to two $\\beta$-bit inputs. a. Number of comparisons and subtractions: $\\lceil \\lg a \\rceil - \\lceil \\lg b \\rceil + 1 = \\lceil \\lg q \\rceil$. Length of subtraction: $\\lceil \\lg b \\rceil$. Total: $O((1 + \\lg q) \\lg b)$. b. $$ \\begin{array}{rlll} & \\mu(a, b) - \\mu(b, a \\mod b) \\\\ = & \\mu(a, b) - \\mu(b, r) \\\\ = & (1 + \\lg a)(1 + \\lg b) - (1 + \\lg b)(1 + \\lg r) \\\\ = & (1 + \\lg b)(\\lg a - \\lg r) & (\\lg r \\le \\lg b) \\\\ \\ge & (1 + \\lg b)(\\lg a - \\lg b) \\\\ = & (1 + \\lg b)(\\lg q + 1) \\\\ \\ge & (1 + \\lg q) \\lg b \\end{array} $$ c. $\\mu(a, b) = (1 + \\lg a)(1 + \\lg b) \\approx \\beta^2$","title":"31-2 Analysis of bit operations in Euclid's algorithm"},{"location":"Chap31/Problems/31-3/","text":"This problem compares the efficiency of three methods for computing the $n$th Fibonacci number $F_n$, given $n$. Assume that the cost of adding, subtracting, or multiplying two numbers is $O(1)$, independent of the size of the numbers. a. Show that the running time of the straightforward recursive method for computing $F_n$ based on recurrence $\\text{(3.22)}$ is exponential in $n$. (See, for example, the FIB procedure on page 775.) b. Show how to compute $F_n$ in $O(n)$ time using memoization. c. Show how to compute $F_n$ in $O(\\lg n)$ time using only integer addition and multiplication. ($\\textit{Hint:}$ Consider the matrix $$ \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} $$ and its powers.) d. Assume now that adding two $\\beta$-bit numbers takes $\\Theta(\\beta)$ time and that multiplying two $\\beta$-bit numbers takes $\\Theta(\\beta^2)$ time. What is the running time of these three methods under this more reasonable cost measure for the elementary arithmetic operations? a. In order to solve $\\text{FIB}(n)$, we need to compute $\\text{FIB}(n - 1)$ and $\\text{FIB}(n - 1)$. Therefore we have the recurrence $$T(n) = T(n - 1) + T(n - 2) + \\Theta(1).$$ We can get the upper bound of Fibonacci as $O(2^n)$, but this is not the tight upper bound. The Fibonacci recurrence is defined as $$F(n) = F(n - 1) + F(n - 2).$$ The characteristic equation for this function will be $$ \\begin{aligned} x^2 & = x + 1 \\\\ x^2 - x - 1 & = 0. \\end{aligned} $$ We can get the roots by quadratic formula: $x = \\frac{1 \\pm \\sqrt 5}{2}$. We know the solution of a linear recursive function is given as $$ \\begin{aligned} F(n) & = \\alpha_1^n + \\alpha_2^n \\\\ & = \\bigg(\\frac{1 + \\sqrt 5}{2}\\bigg)^n + \\bigg(\\frac{1 - \\sqrt 5}{2}\\bigg)^n, \\end{aligned} $$ where $\\alpha_1$ and $\\alpha_2$ are the roots of the characteristic equation. Since both $T(n)$ and $F(n)$ are representing the same thing, they are asymptotically the same. Hence, $$ \\begin{aligned} T(n) & = \\bigg(\\frac{1 + \\sqrt 5}{2}\\bigg)^n + \\bigg(\\frac{1 - \\sqrt 5}{2}\\bigg)^n \\\\ & = \\bigg(\\frac{1 + \\sqrt 5}{2}\\bigg)^n \\\\ & \\approx O(1.618)^n. \\end{aligned} $$ b. This is same as 15.1-5 . c. Assume that all integer multiplications and additions can be done in $O(1)$. First, we want to show that $$ \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix}^k = \\begin{pmatrix} F_{k - 1} & F_k \\\\ F_k & F_{k + 1} \\end{pmatrix} . $$ By induction, $$ \\begin{aligned} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix}^{k + 1} & = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix}^k \\\\ & = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} F_{k - 1} & F_k \\\\ F_k & F_{k + 1} \\end{pmatrix}^k \\\\ & = \\begin{pmatrix} F_k & F_{k + 1} \\\\ F_{k - 1} + F_k & F_k + F_{k + 1} \\end{pmatrix} \\\\ & = \\begin{pmatrix} F_k & F_{k + 1} \\\\ F_{k + 1} & F_{k + 2} \\end{pmatrix} . \\end{aligned} $$ We show that we can compute the given matrix to the power $n - 1$ in time $O(\\lg n)$, and the bottom right entry is $F_n$. We should note that by 8 multiplications and 4 additions, we can multiply any two $2$ by $2$ matrices, that means matrix multiplications can be done in constant time. Thus we only need to bound the number of those in our algorithm. It takes $O(\\lg n)$ to run the algorithm $\\text{MATRIX-POW}(A, n - 1)$ becasue we half the value of $n$ in each step, and within each step, we perform a constant amount of calculation. The recurrence is $$T(n) = T(n / 2) + \\Theta(1).$$ MATRIX - POW ( A , n ) if n % 2 == 1 return A * MATRIX - POW ( A ^ 2 , ( n - 1 ) / 2 ) return MATRIX - POW ( A ^ 2 , n / 2 ) d. First, we should note that $\\beta = O(\\log n)$. For part (a), We naively add a $\\beta$-bit number which is growing exponentially each time, so the recurrence becomes $$ \\begin{aligned} T(n) & = T(n - 1) + T(n - 2) + \\Theta(\\beta) \\\\ & = T(n - 1) + T(n - 2) + \\Theta(\\log n), \\end{aligned} $$ which has the same solution $T(n) = O\\Big(\\frac{1 + \\sqrt 5}{2}\\Big)^n$ since $\\Theta(\\log n)$ can be absorbed by exponential time. For part (b), The recurrence of the memoized verstion becomes $$M(n) = M(n - 1) + \\Theta(\\beta).$$ This has a solution of $\\sum_{i = 2}^n \\beta = \\Theta(n\\beta) = \\Theta(2^\\beta \\cdot \\beta)$ or $\\Theta(n \\log n)$. For part (c), We perform a constant number of both additions and multiplications. The recurrence becomes $$P(n) = P(n / 2) + \\Theta(\\beta^2),$$ which has a solution of $\\Theta(\\log n \\cdot \\beta^2) = \\Theta(\\beta^3)$ or $\\Theta(\\log^3 n)$.","title":"31-3 Three algorithms for Fibonacci numbers"},{"location":"Chap31/Problems/31-4/","text":"Let $p$ be an odd prime. A number $a \\in Z_p^*$ is a quadratic residue if the equation $x^2 = a \\pmod p$ has a solution for the unknown $x$. a. Show that there are exactly $(p - 1) / 2$ quadratic residues, modulo $p$. b. If $p$ is prime, we define the Legendre symbol $(\\frac{a}{p})$, for $a \\in \\mathbb Z_p^*$, to be $1$ if $a$ is a quadratic residue modulo $p$ and $-1$ otherwise. Prove that if $a \\in \\mathbb Z_p^*$, then $$\\left(\\frac{a}{p}\\right) \\equiv a^{(p - 1) / 2} \\pmod p.$$ Give an efficient algorithm that determines whether a given number $a$ is a quadratic residue modulo $p$. Analyze the efficiency of your algorithm. c. Prove that if $p$ is a prime of the form $4k + 3$ and $a$ is a quadratic residue in $\\mathbb Z_b^*$, then $a^{k + 1} \\mod p$ is a square root of $a$, modulo $p$. How much time is required to find the square root of a quadratic residue $a$ modulo $p$? d. Describe an efficient randomized algorithm for finding a nonquadratic residue, modulo an arbitrary prime $p$, that is, a member of $\\mathbb Z_p^*$ that is not a quadratic residue. How many arithmetic operations does your algorithm require on average? (Omit!)","title":"31-4 Quadratic residues"},{"location":"Chap32/32.1/","text":"32.1-1 Show the comparisons the naive string matcher makes for the pattern $P = 0001$ in the text $T = 000010001010001$. STRING - MATCHER ( P , T , i ) for j = i to i + P . length if P [ j - i + 1 ] != T [ j ] return false return true 32.1-2 Suppose that all characters in the pattern $P$ are different. Show how to accelerate $\\text{NAIVE-STRING-MATCHER}$ to run in time $O(n)$ on an $n$-character text $T$. Suppose $T[i] \\ne P[j]$, then for $k \\in [1, j)$, $T[i - k] = P[j - k] \\ne P[0]$, the $[i - k, i)$ are all invalid shifts which could be skipped, therefore we can compare $T[i]$ with $P[0]$ in the next iteration. 32.1-3 Suppose that pattern $P$ and text $T$ are randomly chosen strings of length $m$ and $n$, respectively, from the $d$-ary alphabet $\\Sigma_d = \\{ 0, 1, \\ldots, d - 1 \\}$, where $d \\ge 2$. Show that the expected number of character-to-character comparisons made by the implicit loop in line 4 of the naive algorithm is $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2(n - m + 1)$$ over all executions of this loop. (Assume that the naive algorithm stops comparing characters for a given shift once it finds a mismatch or matches the entire pattern.) Thus, for randomly chosen strings, the naive algorithm is quite efficient. Suppose for each shift, the number of compared characters is $L$, then: $$ \\begin{aligned} \\text E[L] & = 1 \\cdot \\frac{d - 1}{d} + 2 \\cdot (\\frac{1}{d})^1 \\frac{d - 1}{d} + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} S & = 1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\\\ \\frac{1}{d}S & = 1 \\cdot (\\frac{1}{d})^1 + \\cdots + (m - 1) \\cdot (\\frac{1}{d})^{m - 1} + m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = 1 + (\\frac{1}{d})^1 + \\cdots + \\cdot (\\frac{1}{d})^{m - 1} - m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} \\text E[L] & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}}. \\end{aligned} $$ There are $n - m + 1$ shifts, therefore the expected number of comparisons is: $$(n - m + 1) \\cdot \\text E[L] = (n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}}$$ Since $d \\ge 2$, $1 - d^{-1} \\ge 0.5$, $1 - d^{-m} < 1$, and $ \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2$, therefore $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2 (n - m + 1).$$ 32.1-4 Suppose we allow the pattern $P$ to contain occurrences of a gap character $\\diamond$ that can match an arbitrary string of characters (even one of zero length). For example, the pattern $ab\\diamond ba\\diamond c$ occurs in the text $cabccbacbacab$ as $$c \\underbrace{ab}_{ab} \\underbrace{cc}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{cba}_{\\diamond} \\underbrace{c}_{c} ab$$ and as $$c \\underbrace{ab}_{ab} \\underbrace{ccbac}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{\\text{ }}_{\\diamond} \\underbrace{c}_{c} ab$$ Note that the gap character may occur an arbitrary number of times in the pattern but not at all in the text. Give a polynomial-time algorithm to determine whether such a pattern $P$ occurs in a given text $T$, and analyze the running time of your algorithm. By using dynamic programming, the time complexity is $O(mn)$ where $m$ is the length of the text $T$ and $n$ is the length of the pattern $P$; the space complexity is $O(mn)$, too. This problem is similar to LeetCode 44. WildCard Matching , except that it has no question mark ( ? ) requirement. You can see my naive DP implementation here .","title":"32.1 The naive string-matching algorithm"},{"location":"Chap32/32.1/#321-1","text":"Show the comparisons the naive string matcher makes for the pattern $P = 0001$ in the text $T = 000010001010001$. STRING - MATCHER ( P , T , i ) for j = i to i + P . length if P [ j - i + 1 ] != T [ j ] return false return true","title":"32.1-1"},{"location":"Chap32/32.1/#321-2","text":"Suppose that all characters in the pattern $P$ are different. Show how to accelerate $\\text{NAIVE-STRING-MATCHER}$ to run in time $O(n)$ on an $n$-character text $T$. Suppose $T[i] \\ne P[j]$, then for $k \\in [1, j)$, $T[i - k] = P[j - k] \\ne P[0]$, the $[i - k, i)$ are all invalid shifts which could be skipped, therefore we can compare $T[i]$ with $P[0]$ in the next iteration.","title":"32.1-2"},{"location":"Chap32/32.1/#321-3","text":"Suppose that pattern $P$ and text $T$ are randomly chosen strings of length $m$ and $n$, respectively, from the $d$-ary alphabet $\\Sigma_d = \\{ 0, 1, \\ldots, d - 1 \\}$, where $d \\ge 2$. Show that the expected number of character-to-character comparisons made by the implicit loop in line 4 of the naive algorithm is $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2(n - m + 1)$$ over all executions of this loop. (Assume that the naive algorithm stops comparing characters for a given shift once it finds a mismatch or matches the entire pattern.) Thus, for randomly chosen strings, the naive algorithm is quite efficient. Suppose for each shift, the number of compared characters is $L$, then: $$ \\begin{aligned} \\text E[L] & = 1 \\cdot \\frac{d - 1}{d} + 2 \\cdot (\\frac{1}{d})^1 \\frac{d - 1}{d} + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} S & = 1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m - 1} \\\\ \\frac{1}{d}S & = 1 \\cdot (\\frac{1}{d})^1 + \\cdots + (m - 1) \\cdot (\\frac{1}{d})^{m - 1} + m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = 1 + (\\frac{1}{d})^1 + \\cdots + \\cdot (\\frac{1}{d})^{m - 1} - m \\cdot (\\frac{1}{d})^{m} \\\\ \\frac{d - 1}{d}S & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m}. \\end{aligned} $$ $$ \\begin{aligned} \\text E[L] & = (1 + 2 \\cdot (\\frac{1}{d})^1 + \\cdots + m \\cdot (\\frac{1}{d})^{m}) \\frac{d - 1}{d} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}} - m \\cdot (\\frac{1}{d})^{m} + m \\cdot (\\frac{1}{d})^{m} \\\\ & = \\frac{1 - d^{-m}}{1 - d^{-1}}. \\end{aligned} $$ There are $n - m + 1$ shifts, therefore the expected number of comparisons is: $$(n - m + 1) \\cdot \\text E[L] = (n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}}$$ Since $d \\ge 2$, $1 - d^{-1} \\ge 0.5$, $1 - d^{-m} < 1$, and $ \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2$, therefore $$(n - m + 1) \\frac{1 - d^{-m}}{1 - d^{-1}} \\le 2 (n - m + 1).$$","title":"32.1-3"},{"location":"Chap32/32.1/#321-4","text":"Suppose we allow the pattern $P$ to contain occurrences of a gap character $\\diamond$ that can match an arbitrary string of characters (even one of zero length). For example, the pattern $ab\\diamond ba\\diamond c$ occurs in the text $cabccbacbacab$ as $$c \\underbrace{ab}_{ab} \\underbrace{cc}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{cba}_{\\diamond} \\underbrace{c}_{c} ab$$ and as $$c \\underbrace{ab}_{ab} \\underbrace{ccbac}_{\\diamond} \\underbrace{ba}_{ba} \\underbrace{\\text{ }}_{\\diamond} \\underbrace{c}_{c} ab$$ Note that the gap character may occur an arbitrary number of times in the pattern but not at all in the text. Give a polynomial-time algorithm to determine whether such a pattern $P$ occurs in a given text $T$, and analyze the running time of your algorithm. By using dynamic programming, the time complexity is $O(mn)$ where $m$ is the length of the text $T$ and $n$ is the length of the pattern $P$; the space complexity is $O(mn)$, too. This problem is similar to LeetCode 44. WildCard Matching , except that it has no question mark ( ? ) requirement. You can see my naive DP implementation here .","title":"32.1-4"},{"location":"Chap32/32.2/","text":"32.2-1 Working modulo $q = 11$, how many spurious hits does the Rabin-Karp matcher encounter in the text $T = 3141592653589793$ when looking for the pattern $P = 26$? $|\\{15, 59, 92\\}| = 3$. 32.2-2 How would you extend the Rabin-Karp method to the problem of searching a text string for an occurrence of any one of a given set of $k$ patterns? Start by assuming that all $k$ patterns have the same length. Then generalize your solution to allow the patterns to have different lengths. Truncation. 32.2-3 Show how to extend the Rabin-Karp method to handle the problem of looking for a given $m \\times m$ pattern in an $n \\times n$ array of characters. (The pattern may be shifted vertically and horizontally, but it may not be rotated.) Calculate the hashes in each column just like the Rabin-Karp in one-dimension, then treat the hashes in each row as the characters and hashing again. 32.2-4 Alice has a copy of a long $n$-bit file $A = \\langle a_{n - 1}, a_{n - 2}, \\ldots, a_0 \\rangle$, and Bob similarly has an $n$-bit file $B = \\langle b_{n - 1}, b_{n - 2}, \\ldots, b_0 \\rangle$. Alice and Bob wish to know if their files are identical. To avoid transmitting all of $A$ or $B$, they use the following fast probabilistic check. Together, they select a prime $q > 1000n$ and randomly select an integer $x$ from $\\{ 0, 1, \\ldots, q - 1 \\}$. Then, Alice evaluates $$A(x) = (\\sum_{i = 0}^{n - 1} a_i x^i) \\mod q$$ and Bob similarly evaluates $B(x)$. Prove that if $A \\ne B$, there is at most one chance in $1000$ that $A(x) = B(x)$, whereas if the two files are the same, $A(x)$ is necessarily the same as $B(x)$. ($\\textit{Hint:}$ See Exercise 31.4-4.) (Omit!)","title":"32.2 The Rabin-Karp algorithm"},{"location":"Chap32/32.2/#322-1","text":"Working modulo $q = 11$, how many spurious hits does the Rabin-Karp matcher encounter in the text $T = 3141592653589793$ when looking for the pattern $P = 26$? $|\\{15, 59, 92\\}| = 3$.","title":"32.2-1"},{"location":"Chap32/32.2/#322-2","text":"How would you extend the Rabin-Karp method to the problem of searching a text string for an occurrence of any one of a given set of $k$ patterns? Start by assuming that all $k$ patterns have the same length. Then generalize your solution to allow the patterns to have different lengths. Truncation.","title":"32.2-2"},{"location":"Chap32/32.2/#322-3","text":"Show how to extend the Rabin-Karp method to handle the problem of looking for a given $m \\times m$ pattern in an $n \\times n$ array of characters. (The pattern may be shifted vertically and horizontally, but it may not be rotated.) Calculate the hashes in each column just like the Rabin-Karp in one-dimension, then treat the hashes in each row as the characters and hashing again.","title":"32.2-3"},{"location":"Chap32/32.2/#322-4","text":"Alice has a copy of a long $n$-bit file $A = \\langle a_{n - 1}, a_{n - 2}, \\ldots, a_0 \\rangle$, and Bob similarly has an $n$-bit file $B = \\langle b_{n - 1}, b_{n - 2}, \\ldots, b_0 \\rangle$. Alice and Bob wish to know if their files are identical. To avoid transmitting all of $A$ or $B$, they use the following fast probabilistic check. Together, they select a prime $q > 1000n$ and randomly select an integer $x$ from $\\{ 0, 1, \\ldots, q - 1 \\}$. Then, Alice evaluates $$A(x) = (\\sum_{i = 0}^{n - 1} a_i x^i) \\mod q$$ and Bob similarly evaluates $B(x)$. Prove that if $A \\ne B$, there is at most one chance in $1000$ that $A(x) = B(x)$, whereas if the two files are the same, $A(x)$ is necessarily the same as $B(x)$. ($\\textit{Hint:}$ See Exercise 31.4-4.) (Omit!)","title":"32.2-4"},{"location":"Chap32/32.3/","text":"32.3-1 Construct the string-matching automaton for the pattern $P = aabab$ and illustrate its operation on the text string $T = \\text{aaababaabaababaab}$. $$0 \\rightarrow 1 \\rightarrow 2 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3.$$ 32.3-2 Draw a state-transition diagram for a string-matching automaton for the pattern $ababbabbababbababbabb$ over the alphabet $\\sigma = \\{a, b\\}$. $$ \\begin{array}{c|c|c} 0 & 1 & 0 \\\\ 1 & 1 & 2 \\\\ 2 & 3 & 0 \\\\ 3 & 1 & 4 \\\\ 4 & 3 & 5 \\\\ 5 & 6 & 0 \\\\ 6 & 1 & 7 \\\\ 7 & 3 & 8 \\\\ 8 & 9 & 0 \\\\ 9 & 1 & 10 \\\\ 10 & 11 & 0 \\\\ 11 & 1 & 12 \\\\ 12 & 3 & 13 \\\\ 13 & 14 & 0 \\\\ 14 & 1 & 15 \\\\ 15 & 16 & 8 \\\\ 16 & 1 & 17 \\\\ 17 & 3 & 18 \\\\ 18 & 19 & 0 \\\\ 19 & 1 & 20 \\\\ 20 & 3 & 21 \\\\ 21 & 9 & 0 \\end{array} $$ 32.3-3 We call a pattern $P$ nonoverlappable if $P_k \\sqsupset P_q$ implies $k = 0$ or $k = q$. Describe the state-transition diagram of the string-matching automaton for a nonoverlappable pattern. $\\delta(q, a) \\in \\{0, 1, q + 1\\}$. 32.3-4 $\\star$ Given two patterns $P$ and $P'$, describe how to construct a finite automaton that determines all occurrences of either pattern. Try to minimize the number of states in your automaton. Combine the common prefix and suffix. 32.3-5 Given a pattern $P$ containing gap characters (see Exercise 32.1-4), show how to build a finite automaton that can find an occurrence of $P$ in a text $T$ in $O(n)$ matching time, where $n = |T|$. Split the string with the gap characters, build finite automatons for each substring. When a substring is matched, moved to the next finite automaton.","title":"32.3 String matching with finite automata"},{"location":"Chap32/32.3/#323-1","text":"Construct the string-matching automaton for the pattern $P = aabab$ and illustrate its operation on the text string $T = \\text{aaababaabaababaab}$. $$0 \\rightarrow 1 \\rightarrow 2 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4 \\rightarrow 5 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3.$$","title":"32.3-1"},{"location":"Chap32/32.3/#323-2","text":"Draw a state-transition diagram for a string-matching automaton for the pattern $ababbabbababbababbabb$ over the alphabet $\\sigma = \\{a, b\\}$. $$ \\begin{array}{c|c|c} 0 & 1 & 0 \\\\ 1 & 1 & 2 \\\\ 2 & 3 & 0 \\\\ 3 & 1 & 4 \\\\ 4 & 3 & 5 \\\\ 5 & 6 & 0 \\\\ 6 & 1 & 7 \\\\ 7 & 3 & 8 \\\\ 8 & 9 & 0 \\\\ 9 & 1 & 10 \\\\ 10 & 11 & 0 \\\\ 11 & 1 & 12 \\\\ 12 & 3 & 13 \\\\ 13 & 14 & 0 \\\\ 14 & 1 & 15 \\\\ 15 & 16 & 8 \\\\ 16 & 1 & 17 \\\\ 17 & 3 & 18 \\\\ 18 & 19 & 0 \\\\ 19 & 1 & 20 \\\\ 20 & 3 & 21 \\\\ 21 & 9 & 0 \\end{array} $$","title":"32.3-2"},{"location":"Chap32/32.3/#323-3","text":"We call a pattern $P$ nonoverlappable if $P_k \\sqsupset P_q$ implies $k = 0$ or $k = q$. Describe the state-transition diagram of the string-matching automaton for a nonoverlappable pattern. $\\delta(q, a) \\in \\{0, 1, q + 1\\}$.","title":"32.3-3"},{"location":"Chap32/32.3/#323-4-star","text":"Given two patterns $P$ and $P'$, describe how to construct a finite automaton that determines all occurrences of either pattern. Try to minimize the number of states in your automaton. Combine the common prefix and suffix.","title":"32.3-4 $\\star$"},{"location":"Chap32/32.3/#323-5","text":"Given a pattern $P$ containing gap characters (see Exercise 32.1-4), show how to build a finite automaton that can find an occurrence of $P$ in a text $T$ in $O(n)$ matching time, where $n = |T|$. Split the string with the gap characters, build finite automatons for each substring. When a substring is matched, moved to the next finite automaton.","title":"32.3-5"},{"location":"Chap32/32.4/","text":"32.4-1 Compute the prefix function $\\pi$ for the pattern $\\text{ababbabbabbababbabb}$. $$\\pi = \\{ 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}.$$ 32.4-2 Give an upper bound on the size of $\\pi^*[q]$ as a function of $q$. Give an example to show that your bound is tight. $|\\pi^*[q]| < q$. 32.4-3 Explain how to determine the occurrences of pattern $P$ in the text $T$ by examining the $\\pi$ function for the string $PT$ (the string of length $m + n$ that is the concatenation of $P$ and $T$). $\\{ q \\mid \\pi[q] = m \\text{ and } q \\ge 2m \\}$. 32.4-4 Use an aggregate analysis to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta$. The number of $q = q + 1$ is at most $n$. 32.4-5 Use a potential function to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta(n)$. $\\Phi = p.$ 32.4-6 Show how to improve $\\text{KMP-MATCHER}$ by replacing the occurrence of $\\pi$ in line 7 (but not line 12) by $\\pi'$, where $\\pi'$ is defined recursively for $q = 1, 2, \\ldots, m - 1$ by the equation $$ \\pi'[q] = \\begin{cases} 0 & \\text{ if } \\pi[q] = 0, \\\\ \\pi'[\\pi[q]] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] = P[q + 1] \\\\ \\pi[q] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] \\ne P[q + 1]. \\end{cases} $$ Explain why the modified algorithm is correct, and explain in what sense this change constitutes an improvement. If $P[q + 1] \\ne T[i]$, then if $P[\\pi[q] + q] = P[q + 1] \\ne T[i]$, there is no need to compare $P[\\pi[q] + q]$ with $T[i]$. 32.4-7 Give a linear-time algorithm to determine whether a text $T$ is a cyclic rotation of another string $T'$. For example, $\\text{arc}$ and $\\text{car}$ are cyclic rotations of each other. Find $T'$ in $TT$. 32.4-8 $\\star$ Give an $O(m|\\Sigma|)$-time algorithm for computing the transition function $\\delta$ for the string-matching automaton corresponding to a given pattern $P$. (Hint: Prove that $\\delta(q, a) = \\delta(\\pi[q], a)$ if $q = m$ or $P[q + 1] \\ne a$.) Compute the prefix function $m$ times.","title":"32.4 The Knuth-Morris-Pratt algorithm"},{"location":"Chap32/32.4/#324-1","text":"Compute the prefix function $\\pi$ for the pattern $\\text{ababbabbabbababbabb}$. $$\\pi = \\{ 0, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8 \\}.$$","title":"32.4-1"},{"location":"Chap32/32.4/#324-2","text":"Give an upper bound on the size of $\\pi^*[q]$ as a function of $q$. Give an example to show that your bound is tight. $|\\pi^*[q]| < q$.","title":"32.4-2"},{"location":"Chap32/32.4/#324-3","text":"Explain how to determine the occurrences of pattern $P$ in the text $T$ by examining the $\\pi$ function for the string $PT$ (the string of length $m + n$ that is the concatenation of $P$ and $T$). $\\{ q \\mid \\pi[q] = m \\text{ and } q \\ge 2m \\}$.","title":"32.4-3"},{"location":"Chap32/32.4/#324-4","text":"Use an aggregate analysis to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta$. The number of $q = q + 1$ is at most $n$.","title":"32.4-4"},{"location":"Chap32/32.4/#324-5","text":"Use a potential function to show that the running time of $\\text{KMP-MATCHER}$ is $\\Theta(n)$. $\\Phi = p.$","title":"32.4-5"},{"location":"Chap32/32.4/#324-6","text":"Show how to improve $\\text{KMP-MATCHER}$ by replacing the occurrence of $\\pi$ in line 7 (but not line 12) by $\\pi'$, where $\\pi'$ is defined recursively for $q = 1, 2, \\ldots, m - 1$ by the equation $$ \\pi'[q] = \\begin{cases} 0 & \\text{ if } \\pi[q] = 0, \\\\ \\pi'[\\pi[q]] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] = P[q + 1] \\\\ \\pi[q] & \\text{ if } \\pi[q] \\ne 0 \\text{ and } P[\\pi[q] + 1] \\ne P[q + 1]. \\end{cases} $$ Explain why the modified algorithm is correct, and explain in what sense this change constitutes an improvement. If $P[q + 1] \\ne T[i]$, then if $P[\\pi[q] + q] = P[q + 1] \\ne T[i]$, there is no need to compare $P[\\pi[q] + q]$ with $T[i]$.","title":"32.4-6"},{"location":"Chap32/32.4/#324-7","text":"Give a linear-time algorithm to determine whether a text $T$ is a cyclic rotation of another string $T'$. For example, $\\text{arc}$ and $\\text{car}$ are cyclic rotations of each other. Find $T'$ in $TT$.","title":"32.4-7"},{"location":"Chap32/32.4/#324-8-star","text":"Give an $O(m|\\Sigma|)$-time algorithm for computing the transition function $\\delta$ for the string-matching automaton corresponding to a given pattern $P$. (Hint: Prove that $\\delta(q, a) = \\delta(\\pi[q], a)$ if $q = m$ or $P[q + 1] \\ne a$.) Compute the prefix function $m$ times.","title":"32.4-8 $\\star$"},{"location":"Chap32/Problems/32-1/","text":"Let $y^i$ denote the concatenation of string $y$ with itself $i$ times. For example, $(\\text{ab})^3 = \\text{ababab}$. We say that a string $x \\in \\Sigma^*$ has repetition factor $r$ if $x = y ^ r$ for some string $y \\in \\Sigma^*$ and some $r > 0$. Let $\\rho(x)$ denote the largest $r$ such that $x$ has repetition factor $r$. a. Give an efficient algorithm that takes as input a pattern $P[1 \\ldots m]$ and computes the value $\\rho(P_i)$ for $i = 1, 2, \\ldots, m$. What is the running time of your algorithm? b. For any pattern $P[1 \\ldots m]$, let $\\rho^*(P)$ be defined as $\\max_{1 \\le i \\le m} \\rho(P_i)$. Prove that if the pattern $P$ is chosen randomly from the set of all binary strings of length $m$, then the expected value of $\\rho^*(P)$ is $O(1)$. c. Argue that the following string-matching algorithm correctly finds all occurrences of pattern $P$ in a text $T[1 \\ldots n]$ in time $O(\\rho^*(P)n + m)$: REPETITION_MATCHER ( P , T ) m = P . length n = T . length k = 1 + \u03c1 * ( P ) q = 0 s = 0 while s \u2264 n - m if T [ s + q + 1 ] == P [ q + 1 ] q = q + 1 if q == m print \"Pattern occurs with shift\" s if q == m or T [ s + q + 1 ] != P [ q + 1 ] s = s + max ( 1 , ceil ( q / k )) q = 0 This algorithm is due to Galil and Seiferas. By extending these ideas greatly, they obtained a linear-time string-matching algorithm that uses only $O(1)$ storage beyond what is required for $P$ and $T$. a. Compute $\\pi$, let $l = m - \\pi[m]$, if $m ~\\text{mod}~ l = 0$ and for all $p = m - i \\cdot l > 0$, $p - \\pi[p] = l$, then $\\rho(P_i) = m / l$, otherwise $\\rho(P_i) = 1$. The running time is $\\Theta(n)$. b. $$ \\begin{aligned} P(\\rho^*(P) \\ge 2) & = \\frac{1}{2} + \\frac{1}{8} + \\frac{1}{32} + \\cdots \\approx \\frac{2}{3} \\\\ P(\\rho^*(P) \\ge 3) & = \\frac{1}{4} + \\frac{1}{32} + \\frac{1}{256} + \\cdots \\approx \\frac{2}{7} \\\\ P(\\rho^*(P) \\ge 4) & = \\frac{1}{8} + \\frac{1}{128} + \\frac{1}{2048} + \\cdots \\approx \\frac{2}{15} \\\\ P(\\rho^*(P) = 1) & = \\frac{1}{3} \\\\ P(\\rho^*(P) = 2) & = \\frac{8}{21} \\\\ P(\\rho^*(P) = 3) & = \\frac{16}{105} \\\\ \\text E[\\rho^*(P)] & = 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{8}{21} + 3 \\cdot \\frac{16}{105} + \\ldots \\approx 2.21 \\end{aligned} $$ c. (Omit!)","title":"32-1 String matching based on repetition factors"},{"location":"Chap33/33.1/","text":"33.1-1 Prove that if $p_1 \\times p_2$ is positive, then vector $p_1$ is clockwise from vector $p_2$ with respect to the origin $(0, 0)$ and that if this cross product is negative, then $p_1$ is counterclockwise from $p_2$. (Omit!) 33.1-2 Professor van Pelt proposes that only the $x$-dimension needs to be tested in line 1 of ON-SEGMENT. Show why the professor is wrong. $(0, 0), (5, 5), (10, 0)$. 33.1-3 The polar angle of a point $p_1$ with respect to an origin point $p_0$ is the angle of the vector $p_1 - p_0$ in the usual polar coordinate system. For example, the polar angle of $(3, 5)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $45$ degrees or $\\pi / 4$ radians. The polar angle of $(3, 3)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $315$ degrees or $7\\pi / 4$ radians. Write pseudocode to sort a sequence $\\langle p_1, p_2, \\ldots, p_n \\rangle$ of $n$ points according to their polar angles with respect to a given origin point $p_0$. Your procedure should take $O(n\\lg n)$ time and use cross products to compare angles. (Omit!) 33.1-4 Show how to determine in $O(n^2 \\lg n)$ time whether any three points in a set of $n$ points are colinear. Based on exercise 33.1-3, for each point $p_i$, let $p_i$ be $p_0$ and sort other points according to their polar angles mod $\\pi$. Then scan linearly to see whether two points have the same polar angle. $O(n \\cdot n\\lg n) = O(n^2 \\lg n)$. 33.1-5 A polygon is a piecewise-linear, closed curve in the plane. That is, it is a curve ending on itself that is formed by a sequence of straight-line segments, called the sides of the polygon. A point joining two consecutive sides is a vertex of the polygon. If the polygon is simple , as we shall generally assume, it does not cross itself. The set of points in the plane enclosed by a simple polygon forms the interior of the polygon, the set of points on the polygon itself forms its boundary , and the set of points surrounding the polygon forms its exterior . A simple polygon is convex if, given any two points on its boundary or in its interior, all points on the line segment drawn between them are contained in the polygon's boundary or interior. A vertex of a convex polygon cannot be expressed as a convex combination of any two distinct points on the boundary or in the interior of the polygon. Professor Amundsen proposes the following method to determine whether a sequence $\\langle p_1, p_2, \\ldots, p_{n - 1} \\rangle$ of $n$ points forms the consecutive vertices of a convex polygon. Output \"yes\" if the set ${ \\angle p_i p_{i + 1} p_{i + 2}: i = 0, 1, \\ldots, n - 1 }$, where subscript addition is performed modulo $n$, does not contain both left turns and right turns; otherwise, output \"no.\" Show that although this method runs in linear time, it does not always produce the correct answer. Modify the professor's method so that it always produces the correct answer in linear time. A line. 33.1-6 Given a point $p_0 = (x_0, y_0)$, the right horizontal ray from $p_0$ is the set of points ${ p_i = (x_i, y_i) : x_i \\ge x_0 ~\\text{and}~ y_i = y_0 }$, that is, it is the set of points due right of $p_0$ along with $p_0$ itself. Show how to determine whether a given right horizontal ray from $p_0$ intersects a line segment $\\overline{p_1 p_2}$ in $O(1)$ time by reducing the problem to that of determining whether two line segments intersect. $p_1.y = p_2.y = 0$ and $\\max(p_1.x, p_2.x) \\ge 0$. or $\\text{sign}(p_1.y) \\ne \\text{sign}(p_2.y)$ and \\$\\displaystyle p_1.y \\cdot \\frac{p_1.x - p_2.x}{p_1.y - p_2.y} \\ge 0{equation} 33.1-7 One way to determine whether a point $p_0$ is in the interior of a simple, but not necessarily convex, polygon $P$ is to look at any ray from $p_0$ and check that the ray intersects the boundary of $P$ an odd number of times but that $p_0$ itself is not on the boundary of $P$. Show how to compute in $\\Theta(n)$ time whether a point $p_0$ is in the interior of an $n$-vertex polygon $P$. (Hint: Use Exercise 33.1-6. Make sure your algorithm is correct when the ray intersects the polygon boundary at a vertex and when the ray overlaps a side of the polygon.) Based on exercise 33.1-6, use $p_i - p_0$ as $p_i$. 33.1-8 Show how to compute the area of an $n$-vertex simple, but not necessarily convex, polygon in $\\Theta(n)$ time. (See Exercise 33.1-5 for definitions pertaining to polygons.) Half of the sum of the cross products of ${\\overline{p_1 p_i}, \\overline{p_1 p_{i + 1}} ~|~ i \\in [2, n - 1] }$.","title":"33.1 Line-segment properties"},{"location":"Chap33/33.1/#331-1","text":"Prove that if $p_1 \\times p_2$ is positive, then vector $p_1$ is clockwise from vector $p_2$ with respect to the origin $(0, 0)$ and that if this cross product is negative, then $p_1$ is counterclockwise from $p_2$. (Omit!)","title":"33.1-1"},{"location":"Chap33/33.1/#331-2","text":"Professor van Pelt proposes that only the $x$-dimension needs to be tested in line 1 of ON-SEGMENT. Show why the professor is wrong. $(0, 0), (5, 5), (10, 0)$.","title":"33.1-2"},{"location":"Chap33/33.1/#331-3","text":"The polar angle of a point $p_1$ with respect to an origin point $p_0$ is the angle of the vector $p_1 - p_0$ in the usual polar coordinate system. For example, the polar angle of $(3, 5)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $45$ degrees or $\\pi / 4$ radians. The polar angle of $(3, 3)$ with respect to $(2, 4)$ is the angle of the vector $(1, 1)$, which is $315$ degrees or $7\\pi / 4$ radians. Write pseudocode to sort a sequence $\\langle p_1, p_2, \\ldots, p_n \\rangle$ of $n$ points according to their polar angles with respect to a given origin point $p_0$. Your procedure should take $O(n\\lg n)$ time and use cross products to compare angles. (Omit!)","title":"33.1-3"},{"location":"Chap33/33.1/#331-4","text":"Show how to determine in $O(n^2 \\lg n)$ time whether any three points in a set of $n$ points are colinear. Based on exercise 33.1-3, for each point $p_i$, let $p_i$ be $p_0$ and sort other points according to their polar angles mod $\\pi$. Then scan linearly to see whether two points have the same polar angle. $O(n \\cdot n\\lg n) = O(n^2 \\lg n)$.","title":"33.1-4"},{"location":"Chap33/33.1/#331-5","text":"A polygon is a piecewise-linear, closed curve in the plane. That is, it is a curve ending on itself that is formed by a sequence of straight-line segments, called the sides of the polygon. A point joining two consecutive sides is a vertex of the polygon. If the polygon is simple , as we shall generally assume, it does not cross itself. The set of points in the plane enclosed by a simple polygon forms the interior of the polygon, the set of points on the polygon itself forms its boundary , and the set of points surrounding the polygon forms its exterior . A simple polygon is convex if, given any two points on its boundary or in its interior, all points on the line segment drawn between them are contained in the polygon's boundary or interior. A vertex of a convex polygon cannot be expressed as a convex combination of any two distinct points on the boundary or in the interior of the polygon. Professor Amundsen proposes the following method to determine whether a sequence $\\langle p_1, p_2, \\ldots, p_{n - 1} \\rangle$ of $n$ points forms the consecutive vertices of a convex polygon. Output \"yes\" if the set ${ \\angle p_i p_{i + 1} p_{i + 2}: i = 0, 1, \\ldots, n - 1 }$, where subscript addition is performed modulo $n$, does not contain both left turns and right turns; otherwise, output \"no.\" Show that although this method runs in linear time, it does not always produce the correct answer. Modify the professor's method so that it always produces the correct answer in linear time. A line.","title":"33.1-5"},{"location":"Chap33/33.1/#331-6","text":"Given a point $p_0 = (x_0, y_0)$, the right horizontal ray from $p_0$ is the set of points ${ p_i = (x_i, y_i) : x_i \\ge x_0 ~\\text{and}~ y_i = y_0 }$, that is, it is the set of points due right of $p_0$ along with $p_0$ itself. Show how to determine whether a given right horizontal ray from $p_0$ intersects a line segment $\\overline{p_1 p_2}$ in $O(1)$ time by reducing the problem to that of determining whether two line segments intersect. $p_1.y = p_2.y = 0$ and $\\max(p_1.x, p_2.x) \\ge 0$. or $\\text{sign}(p_1.y) \\ne \\text{sign}(p_2.y)$ and \\$\\displaystyle p_1.y \\cdot \\frac{p_1.x - p_2.x}{p_1.y - p_2.y} \\ge 0{equation}","title":"33.1-6"},{"location":"Chap33/33.1/#331-7","text":"One way to determine whether a point $p_0$ is in the interior of a simple, but not necessarily convex, polygon $P$ is to look at any ray from $p_0$ and check that the ray intersects the boundary of $P$ an odd number of times but that $p_0$ itself is not on the boundary of $P$. Show how to compute in $\\Theta(n)$ time whether a point $p_0$ is in the interior of an $n$-vertex polygon $P$. (Hint: Use Exercise 33.1-6. Make sure your algorithm is correct when the ray intersects the polygon boundary at a vertex and when the ray overlaps a side of the polygon.) Based on exercise 33.1-6, use $p_i - p_0$ as $p_i$.","title":"33.1-7"},{"location":"Chap33/33.1/#331-8","text":"Show how to compute the area of an $n$-vertex simple, but not necessarily convex, polygon in $\\Theta(n)$ time. (See Exercise 33.1-5 for definitions pertaining to polygons.) Half of the sum of the cross products of ${\\overline{p_1 p_i}, \\overline{p_1 p_{i + 1}} ~|~ i \\in [2, n - 1] }$.","title":"33.1-8"},{"location":"Chap33/33.2/","text":"33.2-1 Show that a set of $n$ line segments may contain $\\Theta(n ^ 2)$ intersections. Star. 33.2-2 Given two segments $a$ and $b$ that are comparable at $x$, show how to determine in $O(1)$ time which of $a \\succeq_x b$ or $b \\succeq_x a$ holds. Assume that neither segment is vertical. (Omit!) 33.2-3 Professor Mason suggests that we modify $\\text{ANY-SEGMENTS-INTERSECT}$ so that instead of returning upon finding an intersection, it prints the segments that intersect and continues on to the next iteration of the for loop. The professor calls the resulting procedure $\\text{PRINT-INTERSECTING-SEGMENTS}$ and claims that it prints all intersections, from left to right, as they occur in the set of line segments. Professor Dixon disagrees, claiming that Professor Mason's idea is incorrect. Which professor is right? Will $\\text{PRINT-INTERSECTING-SEGMENTS}$ always find the leftmost intersection first? Will it always find all the intersections? No. 33.2-4 Give an $O(n\\lg n)$-time algorithm to determine whether an n-vertex polygon is simple. Same as $\\text{ANY-SEGMENTS-INTERSECT}$. 33.2-5 Give an $O(n\\lg n)$-time algorithm to determine whether two simple polygons with a total of $n$ vertices intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$. 33.2-6 A disk consists of a circle plus its interior and is represented by its center point and radius. Two disks intersect if they have any point in common. Give an $O(n\\lg n)$- time algorithm to determine whether any two disks in a set of $n$ intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$. 33.2-7 Given a set of $n$ line segments containing a total of $k$ intersections, show how to output all $k$ intersections in $O((n + k) \\lg)$ time. Treat the intersection points as event points. 33.2-8 Argue that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly even if three or more segments intersect at the same point. (Omit!) 33.2-9 Show that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly in the presence of vertical segments if we treat the bottom endpoint of a vertical segment as if it were a left endpoint and the top endpoint as if it were a right endpoint. How does your answer to Exercise 33.2-2 change if we allow vertical segments? (Omit!)","title":"33.2 Determining whether any pair of segments intersects"},{"location":"Chap33/33.2/#332-1","text":"Show that a set of $n$ line segments may contain $\\Theta(n ^ 2)$ intersections. Star.","title":"33.2-1"},{"location":"Chap33/33.2/#332-2","text":"Given two segments $a$ and $b$ that are comparable at $x$, show how to determine in $O(1)$ time which of $a \\succeq_x b$ or $b \\succeq_x a$ holds. Assume that neither segment is vertical. (Omit!)","title":"33.2-2"},{"location":"Chap33/33.2/#332-3","text":"Professor Mason suggests that we modify $\\text{ANY-SEGMENTS-INTERSECT}$ so that instead of returning upon finding an intersection, it prints the segments that intersect and continues on to the next iteration of the for loop. The professor calls the resulting procedure $\\text{PRINT-INTERSECTING-SEGMENTS}$ and claims that it prints all intersections, from left to right, as they occur in the set of line segments. Professor Dixon disagrees, claiming that Professor Mason's idea is incorrect. Which professor is right? Will $\\text{PRINT-INTERSECTING-SEGMENTS}$ always find the leftmost intersection first? Will it always find all the intersections? No.","title":"33.2-3"},{"location":"Chap33/33.2/#332-4","text":"Give an $O(n\\lg n)$-time algorithm to determine whether an n-vertex polygon is simple. Same as $\\text{ANY-SEGMENTS-INTERSECT}$.","title":"33.2-4"},{"location":"Chap33/33.2/#332-5","text":"Give an $O(n\\lg n)$-time algorithm to determine whether two simple polygons with a total of $n$ vertices intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$.","title":"33.2-5"},{"location":"Chap33/33.2/#332-6","text":"A disk consists of a circle plus its interior and is represented by its center point and radius. Two disks intersect if they have any point in common. Give an $O(n\\lg n)$- time algorithm to determine whether any two disks in a set of $n$ intersect. Same as $\\text{ANY-SEGMENTS-INTERSECT}$.","title":"33.2-6"},{"location":"Chap33/33.2/#332-7","text":"Given a set of $n$ line segments containing a total of $k$ intersections, show how to output all $k$ intersections in $O((n + k) \\lg)$ time. Treat the intersection points as event points.","title":"33.2-7"},{"location":"Chap33/33.2/#332-8","text":"Argue that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly even if three or more segments intersect at the same point. (Omit!)","title":"33.2-8"},{"location":"Chap33/33.2/#332-9","text":"Show that $\\text{ANY-SEGMENTS-INTERSECT}$ works correctly in the presence of vertical segments if we treat the bottom endpoint of a vertical segment as if it were a left endpoint and the top endpoint as if it were a right endpoint. How does your answer to Exercise 33.2-2 change if we allow vertical segments? (Omit!)","title":"33.2-9"},{"location":"Chap33/33.3/","text":"33.3-1 Prove that in the procedure $\\text{GRAHAM-SCAN}$, points $p_1$ and $p_m$ must be vertices of $\\text{CH}(Q)$. To see this, note that $p_1$ and $p_m$ are the points with the lowest and highest polar angle with respect to $p_0$. By symmetry, we may just show it for $p_1$ and we would also have it for $p_m$ just by reflecting the set of points across a vertical line. To see a contradiction, suppose that we have the convex hull doesn't contain $p_1$. Then, let $p$ be the point in the convex hull that has the lowest polar angle with respect to $p_0$. If $p$ is on the line from $p_0$ to $p_1$, we could replace it with $p_1$ and have a convex hull, meaning we didn't start with a convex hull. If we have that it is not on that line, then there is no way that the convex hull given contains $p_1$, also contradicting the fact that we had selected a convex hull 33.3-2 Consider a model of computation that supports addition, comparison, and multiplication and for which there is a lower bound of $\\Omega(n\\lg n)$ to sort $n$ numbers. Prove that $\\Omega(n\\lg n)$ is a lower bound for computing, in order, the vertices of the convex hull of a set of $n$ points in such a model. Let our $n$ numbers be $a_1, a_2, \\dots, a_n$ and $f$ be a strictly convex function, such as $e^x$. Let $p_i = (a_i, f(a_i))$. Compute the convex hull of $p_1, p_2, \\dots, p_n$. Then every point is in the convex hull. We can recover the numbers themselves by looking at the $x$-coordinates of the points in the order returned by the convex-hull algorithm, which will necessarily be a cyclic shift of the numbers in increasing order, so we can recover the proper order in linear time. In an algorithm such as $\\text{GRAHAM-SCAN}$ which starts with the point with minimum $y$-coordinate, the order returned actually gives the numbers in increasing order. 33.3-3 Given a set of points $Q$, prove that the pair of points farthest from each other must be vertices of $\\text{CH}(Q)$. Suppose that $p$ and $q$ are the two furthest apart points. Also, to a contradiction, suppose, without loss of generality that $p$ is on the interior of the convex hull. Then, construct the circle whose center is $q$ and which has $p$ on the circle. Then, if we have that there are any vertices of the convex hull that are outside this circle, we could pick that vertex and $q$, they would have a higher distance than between $p$ and $q$. So, we know that all of the vertices of the convex hull lie inside the circle. This means that the sides of the convex hull consist of line segments that are contained within the circle. So, the only way that they could contain $p$, a point on the circle is if it was a vertex, but we suppsed that $p$ wasn't a vertex of the convex hull, giving us our contradiction. 33.3-4 For a given polygon $P$ and a point $q$ on its boundary, the shadow of $q$ is the set of points $r$ such that the segment $\\overline{qr}$ is entirely on the boundary or in the interior of $P$. As Figure 33.10 illustrates, a polygon $P$ is star-shaped if there exists a point $p$ in the interior of $P$ that is in the shadow of every point on the boundary of $P$. The set of all such points $p$ is called the kernel of $P$. Given an $n$-vertex, star-shaped polygon $P$ specified by its vertices in counterclockwise order, show how to compute $\\text{CH}(P)$ in $O(n)$ time. We simply run $\\text{GRAHAM-SCAN}$ but without sorting the points, so the runtime becomes $O(n)$. To prove this, we'll prove the following loop invariant: At the start of each iteration of the for loop of lines 7-10, stack $S$ consists of, from bottom to top, exactly the vertices of $\\text{CH}(Q_{i - 1})$. The proof is quite similar to the proof of correctness. The invariant holds the first time we execute line 7 for the same reasons outline in the section. At the start of the $i$th iteration, $S$ contains $\\text{CH}(Q_{i - 1})$. Let $p_j$ be the top point on $S$ after executing the while loop of lines 8-9, but before $p_i$ is pushed, and let pk be the point just below $p_j$ on $S$. At this point, $S$ contains $\\text{CH}(Q_j)$ in counterclockwise order from bottom to top. Thus, when we push $p_i$, $S$ contains exactly the vertices of $\\text{CH}(Q_j \\cup \\{p_i\\})$. We now show that this is the same set of points as $\\text{CH}(Q_i)$. Let $p_t$ be any point that was popped from $S$ during iteration $i$ and $p_r$ be the point just below $p_t$ on stack $S$ at the time $p_t$ was popped. Let $p$ be a point in the kernel of $P$. Since the angle $\\angle p_rp_tp_i$ makes a nonelft turn and $P$ is star shaped, $p_t$ must be in the interior or on the boundary of the triangle formed by $p_r$, $p_i$, and $p$. Thus, $p_t$ is not in the convex hull of $Q_i$, so we have $\\text{CH}(Q_i - \\{p_t\\}) = \\text{CH}(Q_i)$. Applying this equality repeatedly for each point removed from $S$ in the while loop of lines 8-9, we have $\\text{CH}(Q_j \\cup \\{p_i\\}) = \\text{CH}(Q_i)$. When the loop terminates, the loop invariant implies that $S$ consists of exactly the vertices of $\\text{CH}(Q_m)$ in counterclockwise order, proving correctness. 33.3-5 In the on-line convex-hull problem , we are given the set $Q$ of $n$ points one point at a time. After receiving each point, we compute the convex hull of the points seen so far. Obviously, we could run Graham's scan once for each point, with a total running time of $O(n^2\\lg n)$. Show how to solve the on-line convex-hull problem in a total of $O(n^2)$ time. Suppose that we have a convex hull computed from the previous stage $\\{q_0, q_1, \\dots, q_m\\}$, and we want to add a new vertex, $p$ in and keep track of how we should change the convex hull. First, process the vertices in a clockwise manner, and look for the first time that we would have to make a non-left to get to $p$. This tells us where to start cutting vertices out of the convex hull. To find out the upper bound on the vertices that we need to cut out, turn around, start processing vertices in a clockwise manner and see the first time that we would need to make a non-right. Then, we just remove the vertices that are in this set of vertices and replace the with $p$. There is one last case to consider, which is when we end up passing ourselves when we do our clockwise sweep. Then we just remove no vertices and add $p$ in in between the two vertices that we had found in the two sweeps. Since for each vertex we add we are only considering each point in the previous step's convex hull twice, the runtime is $O(nh) = O(n^2)$ where h is the number of points in the convex hull. 33.3-6 $\\star$ Show how to implement the incremental method for computing the convex hull of $n$ points so that it runs in $O(n\\lg n)$ time. (Omit!)","title":"33.3 Finding the convex hull"},{"location":"Chap33/33.3/#333-1","text":"Prove that in the procedure $\\text{GRAHAM-SCAN}$, points $p_1$ and $p_m$ must be vertices of $\\text{CH}(Q)$. To see this, note that $p_1$ and $p_m$ are the points with the lowest and highest polar angle with respect to $p_0$. By symmetry, we may just show it for $p_1$ and we would also have it for $p_m$ just by reflecting the set of points across a vertical line. To see a contradiction, suppose that we have the convex hull doesn't contain $p_1$. Then, let $p$ be the point in the convex hull that has the lowest polar angle with respect to $p_0$. If $p$ is on the line from $p_0$ to $p_1$, we could replace it with $p_1$ and have a convex hull, meaning we didn't start with a convex hull. If we have that it is not on that line, then there is no way that the convex hull given contains $p_1$, also contradicting the fact that we had selected a convex hull","title":"33.3-1"},{"location":"Chap33/33.3/#333-2","text":"Consider a model of computation that supports addition, comparison, and multiplication and for which there is a lower bound of $\\Omega(n\\lg n)$ to sort $n$ numbers. Prove that $\\Omega(n\\lg n)$ is a lower bound for computing, in order, the vertices of the convex hull of a set of $n$ points in such a model. Let our $n$ numbers be $a_1, a_2, \\dots, a_n$ and $f$ be a strictly convex function, such as $e^x$. Let $p_i = (a_i, f(a_i))$. Compute the convex hull of $p_1, p_2, \\dots, p_n$. Then every point is in the convex hull. We can recover the numbers themselves by looking at the $x$-coordinates of the points in the order returned by the convex-hull algorithm, which will necessarily be a cyclic shift of the numbers in increasing order, so we can recover the proper order in linear time. In an algorithm such as $\\text{GRAHAM-SCAN}$ which starts with the point with minimum $y$-coordinate, the order returned actually gives the numbers in increasing order.","title":"33.3-2"},{"location":"Chap33/33.3/#333-3","text":"Given a set of points $Q$, prove that the pair of points farthest from each other must be vertices of $\\text{CH}(Q)$. Suppose that $p$ and $q$ are the two furthest apart points. Also, to a contradiction, suppose, without loss of generality that $p$ is on the interior of the convex hull. Then, construct the circle whose center is $q$ and which has $p$ on the circle. Then, if we have that there are any vertices of the convex hull that are outside this circle, we could pick that vertex and $q$, they would have a higher distance than between $p$ and $q$. So, we know that all of the vertices of the convex hull lie inside the circle. This means that the sides of the convex hull consist of line segments that are contained within the circle. So, the only way that they could contain $p$, a point on the circle is if it was a vertex, but we suppsed that $p$ wasn't a vertex of the convex hull, giving us our contradiction.","title":"33.3-3"},{"location":"Chap33/33.3/#333-4","text":"For a given polygon $P$ and a point $q$ on its boundary, the shadow of $q$ is the set of points $r$ such that the segment $\\overline{qr}$ is entirely on the boundary or in the interior of $P$. As Figure 33.10 illustrates, a polygon $P$ is star-shaped if there exists a point $p$ in the interior of $P$ that is in the shadow of every point on the boundary of $P$. The set of all such points $p$ is called the kernel of $P$. Given an $n$-vertex, star-shaped polygon $P$ specified by its vertices in counterclockwise order, show how to compute $\\text{CH}(P)$ in $O(n)$ time. We simply run $\\text{GRAHAM-SCAN}$ but without sorting the points, so the runtime becomes $O(n)$. To prove this, we'll prove the following loop invariant: At the start of each iteration of the for loop of lines 7-10, stack $S$ consists of, from bottom to top, exactly the vertices of $\\text{CH}(Q_{i - 1})$. The proof is quite similar to the proof of correctness. The invariant holds the first time we execute line 7 for the same reasons outline in the section. At the start of the $i$th iteration, $S$ contains $\\text{CH}(Q_{i - 1})$. Let $p_j$ be the top point on $S$ after executing the while loop of lines 8-9, but before $p_i$ is pushed, and let pk be the point just below $p_j$ on $S$. At this point, $S$ contains $\\text{CH}(Q_j)$ in counterclockwise order from bottom to top. Thus, when we push $p_i$, $S$ contains exactly the vertices of $\\text{CH}(Q_j \\cup \\{p_i\\})$. We now show that this is the same set of points as $\\text{CH}(Q_i)$. Let $p_t$ be any point that was popped from $S$ during iteration $i$ and $p_r$ be the point just below $p_t$ on stack $S$ at the time $p_t$ was popped. Let $p$ be a point in the kernel of $P$. Since the angle $\\angle p_rp_tp_i$ makes a nonelft turn and $P$ is star shaped, $p_t$ must be in the interior or on the boundary of the triangle formed by $p_r$, $p_i$, and $p$. Thus, $p_t$ is not in the convex hull of $Q_i$, so we have $\\text{CH}(Q_i - \\{p_t\\}) = \\text{CH}(Q_i)$. Applying this equality repeatedly for each point removed from $S$ in the while loop of lines 8-9, we have $\\text{CH}(Q_j \\cup \\{p_i\\}) = \\text{CH}(Q_i)$. When the loop terminates, the loop invariant implies that $S$ consists of exactly the vertices of $\\text{CH}(Q_m)$ in counterclockwise order, proving correctness.","title":"33.3-4"},{"location":"Chap33/33.3/#333-5","text":"In the on-line convex-hull problem , we are given the set $Q$ of $n$ points one point at a time. After receiving each point, we compute the convex hull of the points seen so far. Obviously, we could run Graham's scan once for each point, with a total running time of $O(n^2\\lg n)$. Show how to solve the on-line convex-hull problem in a total of $O(n^2)$ time. Suppose that we have a convex hull computed from the previous stage $\\{q_0, q_1, \\dots, q_m\\}$, and we want to add a new vertex, $p$ in and keep track of how we should change the convex hull. First, process the vertices in a clockwise manner, and look for the first time that we would have to make a non-left to get to $p$. This tells us where to start cutting vertices out of the convex hull. To find out the upper bound on the vertices that we need to cut out, turn around, start processing vertices in a clockwise manner and see the first time that we would need to make a non-right. Then, we just remove the vertices that are in this set of vertices and replace the with $p$. There is one last case to consider, which is when we end up passing ourselves when we do our clockwise sweep. Then we just remove no vertices and add $p$ in in between the two vertices that we had found in the two sweeps. Since for each vertex we add we are only considering each point in the previous step's convex hull twice, the runtime is $O(nh) = O(n^2)$ where h is the number of points in the convex hull.","title":"33.3-5"},{"location":"Chap33/33.3/#333-6-star","text":"Show how to implement the incremental method for computing the convex hull of $n$ points so that it runs in $O(n\\lg n)$ time. (Omit!)","title":"33.3-6 $\\star$"},{"location":"Chap33/33.4/","text":"33.4-1 Professor Williams comes up with a scheme that allows the closest-pair algorithm to check only $5$ points following each point in array $Y'$. The idea is always to place points on line $l$ into set $P_L$. Then, there cannot be pairs of coincident points on line $l$ with one point in $P_L$ and one in $P_R$. Thus, at most $6$ points can reside in the $\\delta \\times 2\\delta$ rectangle. What is the flaw in the professor's scheme? In particular, when we select line $l$, we may be unable perform an even split of the vertices. So, we don't neccesarily have that both the left set of points and right set of points have fallen to roughly half. For example, suppose that the points are all arranged on a vertical line, then, when we recurse on the the left set of points, we haven't reduced the problem size at all, let alone by a factor of two. There is also the issue in this setup that you may end up asking about a set of size less than two when looking at the right set of points. 33.4-2 Show that it actually suffices to check only the points in the $5$ array positions following each point in the array $Y'$. Since we only care about the shortest distance, the distance $\\delta'$ must be strictly less than $\\delta$. The picture in Figure 33.11(b) only illustrates the case of a nonstrict inequality. If we exclude the possibility of points whose $x$ coordinate differs by exactly $\\delta$ from $l$, then it is only possible to place at most $6$ points in the $\\delta \\times 2\\delta$ rectangle, so it suffices to check on the points in the $5$ array positions following each point in the array $Y'$. 33.4-3 We can define the distance between two points in ways other than euclidean. In the plane, the $L_m$-distance between points $p_1$ and $p_2$ is given by the expression $(|x_1 - x_2|^m + |y_1 - y_2|^m)^{1 / m}$. Euclidean distance, therefore, is $L_2$-distance. Modify the closest-pair algorithm to use the $L_1$-distance, which is also known as the Manhattan distance . In the analysis of the algorithm, most of it goes through just based on the triangle inequality. The only main point of difference is in looking at the number of points that can be fit into a $\\delta \\times 2\\delta$ rectangle. In particular, we can cram in two more points than the eight shown into the rectangle by placing points at the centers of the two squares that the rectangle breaks into. This means that we need to consider points up to $9$ away in $Y'$ instead of $7$ away. This has no impact on the asymptotics of the algorithm and it is the only correction to the algorithm that is needed if we switch from $L_2$ to $L_1$. 33.4-4 Given two points $p_1$ and $p_2$ in the plane, the $L_\\infty$-distance between them is given by $\\max(|x_1 - x_2|, |y_1 - y_2|)$. Modify the closest-pair algorithm to use the $L_\\infty$-distance. We can simply run the divide and conquer algorithm described in the section, modifying the brute force search for $|P| \\le 3$ and the check against the next $7$ points in $Y'$ to use the $L_\\infty$ distance. Since the $L_\\infty$ distance between two points is always less than the euclidean distance, there can be at most $8$ points in the $\\delta \\times 2\\delta$ rectangle which we need to examine in order to determine whether the closest pair is in that box. Thus, the modified algorithm is still correct and has the same runtime. 33.4-5 Suppose that $\\Omega(n)$ of the points given to the closest-pair algorithm are covertical. Show how to determine the sets $P_L$ and $P_R$ and how to determine whether each point of $Y$ is in $P_L$ or $P_R$ so that the running time for the closest-pair algorithm remains $O(n\\lg n)$. We select the line $l$ so that it is roughly equal, and then, we won't run into any issue if we just pick an arbitrary subset of the vertices that are on the line to go to one side or the other. Since the analysis of the algorithm allowed for both elements from $P_L$ and $P_R$ to be on the line, we still have correctness if we do this. To determine what values of $Y$ belong to which of the set can be made easier if we select our set going to $P_L$ to be the lowest however many points are needed, and the $P_R$ to be the higher points. Then, just knowing the index of $Y$ that we are looking at, we know whether that point belonged to $P_L$ or to $P_R$. 33.4-6 Suggest a change to the closest-pair algorithm that avoids presorting the $Y$ array but leaves the running time as $O(n\\lg n)$. ($\\textit{Hint:}$ Merge sorted arrays $Y_L$ and $Y_R$ to form the sorted array $Y$.) In addition to returning the distance of the closest pair, the modify the algorithm to also return the points passed to it, sorted by $y$-coordinate, as $Y$. To do this, merge $Y_L$ and $Y_R$ returned by each of its recursive calls. If we are at the base case, when $n \\le 3$, simply use insertion sort to sort the elements by y-coordinate directly. Since each merge takes linear time, this doesn't affect the recursive equation for the runtime.","title":"33.4 Finding the closest pair of points"},{"location":"Chap33/33.4/#334-1","text":"Professor Williams comes up with a scheme that allows the closest-pair algorithm to check only $5$ points following each point in array $Y'$. The idea is always to place points on line $l$ into set $P_L$. Then, there cannot be pairs of coincident points on line $l$ with one point in $P_L$ and one in $P_R$. Thus, at most $6$ points can reside in the $\\delta \\times 2\\delta$ rectangle. What is the flaw in the professor's scheme? In particular, when we select line $l$, we may be unable perform an even split of the vertices. So, we don't neccesarily have that both the left set of points and right set of points have fallen to roughly half. For example, suppose that the points are all arranged on a vertical line, then, when we recurse on the the left set of points, we haven't reduced the problem size at all, let alone by a factor of two. There is also the issue in this setup that you may end up asking about a set of size less than two when looking at the right set of points.","title":"33.4-1"},{"location":"Chap33/33.4/#334-2","text":"Show that it actually suffices to check only the points in the $5$ array positions following each point in the array $Y'$. Since we only care about the shortest distance, the distance $\\delta'$ must be strictly less than $\\delta$. The picture in Figure 33.11(b) only illustrates the case of a nonstrict inequality. If we exclude the possibility of points whose $x$ coordinate differs by exactly $\\delta$ from $l$, then it is only possible to place at most $6$ points in the $\\delta \\times 2\\delta$ rectangle, so it suffices to check on the points in the $5$ array positions following each point in the array $Y'$.","title":"33.4-2"},{"location":"Chap33/33.4/#334-3","text":"We can define the distance between two points in ways other than euclidean. In the plane, the $L_m$-distance between points $p_1$ and $p_2$ is given by the expression $(|x_1 - x_2|^m + |y_1 - y_2|^m)^{1 / m}$. Euclidean distance, therefore, is $L_2$-distance. Modify the closest-pair algorithm to use the $L_1$-distance, which is also known as the Manhattan distance . In the analysis of the algorithm, most of it goes through just based on the triangle inequality. The only main point of difference is in looking at the number of points that can be fit into a $\\delta \\times 2\\delta$ rectangle. In particular, we can cram in two more points than the eight shown into the rectangle by placing points at the centers of the two squares that the rectangle breaks into. This means that we need to consider points up to $9$ away in $Y'$ instead of $7$ away. This has no impact on the asymptotics of the algorithm and it is the only correction to the algorithm that is needed if we switch from $L_2$ to $L_1$.","title":"33.4-3"},{"location":"Chap33/33.4/#334-4","text":"Given two points $p_1$ and $p_2$ in the plane, the $L_\\infty$-distance between them is given by $\\max(|x_1 - x_2|, |y_1 - y_2|)$. Modify the closest-pair algorithm to use the $L_\\infty$-distance. We can simply run the divide and conquer algorithm described in the section, modifying the brute force search for $|P| \\le 3$ and the check against the next $7$ points in $Y'$ to use the $L_\\infty$ distance. Since the $L_\\infty$ distance between two points is always less than the euclidean distance, there can be at most $8$ points in the $\\delta \\times 2\\delta$ rectangle which we need to examine in order to determine whether the closest pair is in that box. Thus, the modified algorithm is still correct and has the same runtime.","title":"33.4-4"},{"location":"Chap33/33.4/#334-5","text":"Suppose that $\\Omega(n)$ of the points given to the closest-pair algorithm are covertical. Show how to determine the sets $P_L$ and $P_R$ and how to determine whether each point of $Y$ is in $P_L$ or $P_R$ so that the running time for the closest-pair algorithm remains $O(n\\lg n)$. We select the line $l$ so that it is roughly equal, and then, we won't run into any issue if we just pick an arbitrary subset of the vertices that are on the line to go to one side or the other. Since the analysis of the algorithm allowed for both elements from $P_L$ and $P_R$ to be on the line, we still have correctness if we do this. To determine what values of $Y$ belong to which of the set can be made easier if we select our set going to $P_L$ to be the lowest however many points are needed, and the $P_R$ to be the higher points. Then, just knowing the index of $Y$ that we are looking at, we know whether that point belonged to $P_L$ or to $P_R$.","title":"33.4-5"},{"location":"Chap33/33.4/#334-6","text":"Suggest a change to the closest-pair algorithm that avoids presorting the $Y$ array but leaves the running time as $O(n\\lg n)$. ($\\textit{Hint:}$ Merge sorted arrays $Y_L$ and $Y_R$ to form the sorted array $Y$.) In addition to returning the distance of the closest pair, the modify the algorithm to also return the points passed to it, sorted by $y$-coordinate, as $Y$. To do this, merge $Y_L$ and $Y_R$ returned by each of its recursive calls. If we are at the base case, when $n \\le 3$, simply use insertion sort to sort the elements by y-coordinate directly. Since each merge takes linear time, this doesn't affect the recursive equation for the runtime.","title":"33.4-6"},{"location":"Chap33/Problems/33-1/","text":"Given a set $Q$ of points in the plane, we define the convex layers of $Q$ inductively. The first convex layer of $Q$ consists of those points in $Q$ that are vertices of $\\text{CH}(Q)$. For $i > 1$, define $Q_i$ to consist of the points of $Q$ with all points in convex layers $i, 2, \\dots, i - 1$ removed. Then, the $i$th convex layer of $Q$ is $\\text{CH}(Q_i)$ if $Q_i \\ne \\emptyset$ and is undefined otherwise. a. Give an $O(n^2)$- time algorithm to find the convex layers of a set of $n$ points. b. Prove that $\\Omega(n\\lg n)$ time is required to compute the convex layers of a set of $n$ points with any model of computation that requires $\\Omega(n\\lg n)$ time to sort $n$ real numbers. (Omit!)","title":"33-1 Convex layers"},{"location":"Chap33/Problems/33-2/","text":"Let $Q$ be a set of $n$ points in the plane. We say that point $(x, y)$ dominates point $(x', y')$ if $x \\ge x'$ and $y \\ge y'$. A point in $Q$ that is dominated by no other points in $Q$ is said to be maximal . Note that $Q$ may contain many maximal points, which can be organized into maximal layers as follows. The first maximal layer $L_1$ is the set of maximal points of $Q$. For $i > 1$, the $i$th maximal layer $L_i$ is the set of maximal points in $Q - \\bigcup_{j = 1}^{i - 1} L_j$. Suppose that $Q$ has $k$ nonempty maximal layers, and let $y_i$ be the $y$-coordinate of the leftmost point in $L_i$ for $i = 1, 2, \\dots, k$. For now, assume that no two points in $Q$ have the same $x$- or $y$-coordinate. a. Show that $y_1 > y_2 > \\cdots > y_k$. Consider a point $(x, y)$ that is to the left of any point in $Q$ and for which $y$ is distinct from the $y$-coordinate of any point in $Q$. Let $Q' = Q \\cup \\{(w, y)\\}$. b. Let $j$ be the minimum index such that $y_j < y$, unless $y < y_k$, in which case we let $j = k + 1$. Show that the maximal layers of $Q'$ are as follows: If $j \\le k$, then the maximal layers of $Q'$ are the same as the maximal layers of $Q$, except that $L_j$ also includes $(x, y)$ as its new leftmost point. If $j = k + 1$, then the first $k$ maximal layers of $Q'$ are the same as for $Q$, but in addition, $Q'$ has a nonempty $(k + 1)$st maximal layer: $L_{k + 1} = \\{(x, y)\\}$. c. Describe an $O(n\\lg n)$-time algorithm to compute the maximal layers of a set $Q$ of $n$ points. ($\\textit{Hint:}$ Move a sweep line from right to left.) d. Do any difficulties arise if we now allow input points to have the same $x$- or $y$-coordinate? Suggest a way to resolve such problems. (Omit!)","title":"33-2 Maximal layers"},{"location":"Chap33/Problems/33-3/","text":"A group of $n$ Ghostbusters is battling n ghosts. Each Ghostbuster carries a proton pack, which shoots a stream at a ghost, eradicating it. A stream goes in a straight line and terminates when it hits the ghost. The Ghostbusters decide upon the following strategy. They will pair off with the ghosts, forming $n$ Ghostbuster-ghost pairs, and then simultaneously each Ghostbuster will shoot a stream at his chosen ghost. As we all know, it is very dangerous to let streams cross, and so the Ghostbusters must choose pairings for which no streams will cross. Assume that the position of each Ghostbuster and each ghost is a fixed point in the plane and that no three positions are colinear. a. Argue that there exists a line passing through one Ghostbuster and one ghost such that the number of Ghostbusters on one side of the line equals the number of ghosts on the same side. Describe how to find such a line in $O(n\\lg n)$ time. b. Give an $O(n^2\\lg n)$-time algorithm to pair Ghostbusters with ghosts in such a way that no streams cross. (Omit!)","title":"33-3 Ghostbusters and ghosts"},{"location":"Chap33/Problems/33-4/","text":"Professor Charon has a set of $n$ sticks, which are piled up in some configuration. Each stick is specified by its endpoints, and each endpoint is an ordered triple giving its $(x, y, z)$ coordinates. No stick is vertical. He wishes to pick up all the sticks, one at a time, subject to the condition that he may pick up a stick only if there is no other stick on top of it. a. Give a procedure that takes two sticks $a$ and $b$ and reports whether $a$ is above, below, or unrelated to $b$. b. Describe an efficient algorithm that determines whether it is possible to pick up all the sticks, and if so, provides a legal order in which to pick them up. (Omit!)","title":"33-4 Picking up sticks"},{"location":"Chap33/Problems/33-5/","text":"Consider the problem of computing the convex hull of a set of points in the plane that have been drawn according to some known random distribution. Sometimes, the number of points, or size, of the convex hull of $n$ points drawn from such a distribution has expectation $O(n^{1 - \\epsilon})$ for some constant $\\epsilon > 0$. We call such a distribution sparse-hulled . Sparse-hulled distributions include the following: Points drawn uniformly from a unit-radius disk. The convex hull has expected size $\\Theta(n^{1 / 3})$. Points drawn uniformly from the interior of a convex polygon with $k$ sides, for any constant $k$. The convex hull has expected size $\\Theta(\\lg n)$. Points drawn according to a two-dimensional normal distribution. The convex $p$ hull has expected size $\\Theta(\\sqrt{\\lg n})$. a. Given two convex polygons with $n_1$ and $n_2$ vertices respectively, show how to compute the convex hull of all $n_1 + n_2$ points in $O(n_1 + n_2)$ time. (The polygons may overlap.) b. Show how to compute the convex hull of a set of $n$ points drawn independently according to a sparse-hulled distribution in $O(n)$ average-case time. ($\\textit{Hint:}$ Recursively find the convex hulls of the first $n / 2$ points and the second $n / 2$ points, and then combine the results.) (Omit!)","title":"33-5 Sparse-hulled distributions"},{"location":"Chap34/34.1/","text":"34.1-1 Define the optimization problem $\\text{LONGEST-PATH-LENGTH}$ as the relation that associates each instance of an undirected graph and two vertices with the number of edges in a longest simple path between the two vertices. Define the decision problem $\\text{LONGEST-PATH}$ $= \\{\\langle G, u, v, k\\rangle: G = (V, E)$ is an undirected graph, $u, v \\in V, k \\ge 0$ is an integer, and there exists a simple path from $u$ to $v$ in $G$ consisting of at least $k$ edges $\\}$. Show that the optimization problem $\\text{LONGEST-PATH-LENGTH}$ can be solved in polynomial time if and only if $\\text{LONGEST-PATH} \\in P$. Showing that $\\text{LONGEST-PATH-LENGTH}$ being polynomial implies that $\\text{LONGEST-PATH}$ is polynomial is trivial, because we can just compute the length of the longest path and reject the instance of $\\text{LONGEST-PATH}$ if and only if $k$ is larger than the number we computed as the length of the longest path. Since we know that the number of edges in the longest path length is between $0$ and $|E|$, we can perform a binary search for it's length. That is, we construct an instance of $\\text{LONGEST-PATH}$ with the given parameters along with $k = \\frac{|E|}{2}$. If we hear yes, we know that the length of the longest path is somewhere above the halfway point. If we hear no, we know it is somewhere below. Since each time we are halving the possible range, we have that the procedure can require $O(\\lg |E|)$ many steps. However, running a polynomial time subroutine $\\lg n$ many times still gets us a polynomial time procedure, since we know that with this procedure we will never be feeding output of one call of $\\text{LONGEST-PATH}$ into the next. 34.1-2 Give a formal definition for the problem of finding the longest simple cycle in an undirected graph. Give a related decision problem. Give the language corresponding to the decision problem. The problem $\\text{LONGST-SIMPLE-CYCLE}$ is the relation that associates each instance of a graph with the longest simple cycle contained in that graph. The decision problem is, given $k$, to determine whether or not the instance graph has a simple cycle of length at least $k$. If yes, output $1$. Otherwise output $0$. The language corresponding to the decision problem is the set of all $\\langle G, k\\rangle$ such that $G = (V, E)$ is an undirected graph, $k \\ge 0$ is an integer, and there exists a simple cycle in $G$ consisting of at least $k$ edges. 34.1-3 Give a formal encoding of directed graphs as binary strings using an adjacencymatrix representation. Do the same using an adjacency-list representation. Argue that the two representations are polynomially related. (Omit!) 34.1-4 Is the dynamic-programming algorithm for the 0-1 knapsack problem that is asked for in Exercise 16.2-2 a polynomial-time algorithm? Explain your answer. This isn't a polynomial-time algorithm. Recall that the algorithm from Exercise 16.2-2 had running time $\\Theta(nW)$ where $W$ was the maximum weight supported by the knapsack. Consider an encoding of the problem. There is a polynomial encoding of each item by giving the binary representation of its index, worth, and weight, represented as some binary string of length $a = \\Omega(n)$. We then encode $W$, in polynomial time. This will have length $\\Theta(\\lg W) = b$. The solution to this problem of length $a + b$ is found in time $\\Theta(nW) = \\Theta(a \\cdot 2^b)$. Thus, the algorithm is actually exponential. 34.1-5 Show that if an algorithm makes at most a constant number of calls to polynomial-time subroutines and performs an additional amount of work that also takes polynomial time, then it runs in polynomial time. Also show that a polynomial number of calls to polynomial-time subroutines may result in an exponential-time algorithm. (Omit!) 34.1-6 Show that the class $P$, viewed as a set of languages, is closed under union, intersection, concatenation, complement, and Kleene star. That is, if $L_1, L_2 \\in P$, then $L_1 \\cup L_2 \\in P$, $L_1 \\cap L_2 \\in P$, $L_1L_2 \\in P$, $\\bar L_1 \\in P$, and $L_1^* \\in P$. (Omit!)","title":"34.1 Polynomial time"},{"location":"Chap34/34.1/#341-1","text":"Define the optimization problem $\\text{LONGEST-PATH-LENGTH}$ as the relation that associates each instance of an undirected graph and two vertices with the number of edges in a longest simple path between the two vertices. Define the decision problem $\\text{LONGEST-PATH}$ $= \\{\\langle G, u, v, k\\rangle: G = (V, E)$ is an undirected graph, $u, v \\in V, k \\ge 0$ is an integer, and there exists a simple path from $u$ to $v$ in $G$ consisting of at least $k$ edges $\\}$. Show that the optimization problem $\\text{LONGEST-PATH-LENGTH}$ can be solved in polynomial time if and only if $\\text{LONGEST-PATH} \\in P$. Showing that $\\text{LONGEST-PATH-LENGTH}$ being polynomial implies that $\\text{LONGEST-PATH}$ is polynomial is trivial, because we can just compute the length of the longest path and reject the instance of $\\text{LONGEST-PATH}$ if and only if $k$ is larger than the number we computed as the length of the longest path. Since we know that the number of edges in the longest path length is between $0$ and $|E|$, we can perform a binary search for it's length. That is, we construct an instance of $\\text{LONGEST-PATH}$ with the given parameters along with $k = \\frac{|E|}{2}$. If we hear yes, we know that the length of the longest path is somewhere above the halfway point. If we hear no, we know it is somewhere below. Since each time we are halving the possible range, we have that the procedure can require $O(\\lg |E|)$ many steps. However, running a polynomial time subroutine $\\lg n$ many times still gets us a polynomial time procedure, since we know that with this procedure we will never be feeding output of one call of $\\text{LONGEST-PATH}$ into the next.","title":"34.1-1"},{"location":"Chap34/34.1/#341-2","text":"Give a formal definition for the problem of finding the longest simple cycle in an undirected graph. Give a related decision problem. Give the language corresponding to the decision problem. The problem $\\text{LONGST-SIMPLE-CYCLE}$ is the relation that associates each instance of a graph with the longest simple cycle contained in that graph. The decision problem is, given $k$, to determine whether or not the instance graph has a simple cycle of length at least $k$. If yes, output $1$. Otherwise output $0$. The language corresponding to the decision problem is the set of all $\\langle G, k\\rangle$ such that $G = (V, E)$ is an undirected graph, $k \\ge 0$ is an integer, and there exists a simple cycle in $G$ consisting of at least $k$ edges.","title":"34.1-2"},{"location":"Chap34/34.1/#341-3","text":"Give a formal encoding of directed graphs as binary strings using an adjacencymatrix representation. Do the same using an adjacency-list representation. Argue that the two representations are polynomially related. (Omit!)","title":"34.1-3"},{"location":"Chap34/34.1/#341-4","text":"Is the dynamic-programming algorithm for the 0-1 knapsack problem that is asked for in Exercise 16.2-2 a polynomial-time algorithm? Explain your answer. This isn't a polynomial-time algorithm. Recall that the algorithm from Exercise 16.2-2 had running time $\\Theta(nW)$ where $W$ was the maximum weight supported by the knapsack. Consider an encoding of the problem. There is a polynomial encoding of each item by giving the binary representation of its index, worth, and weight, represented as some binary string of length $a = \\Omega(n)$. We then encode $W$, in polynomial time. This will have length $\\Theta(\\lg W) = b$. The solution to this problem of length $a + b$ is found in time $\\Theta(nW) = \\Theta(a \\cdot 2^b)$. Thus, the algorithm is actually exponential.","title":"34.1-4"},{"location":"Chap34/34.1/#341-5","text":"Show that if an algorithm makes at most a constant number of calls to polynomial-time subroutines and performs an additional amount of work that also takes polynomial time, then it runs in polynomial time. Also show that a polynomial number of calls to polynomial-time subroutines may result in an exponential-time algorithm. (Omit!)","title":"34.1-5"},{"location":"Chap34/34.1/#341-6","text":"Show that the class $P$, viewed as a set of languages, is closed under union, intersection, concatenation, complement, and Kleene star. That is, if $L_1, L_2 \\in P$, then $L_1 \\cup L_2 \\in P$, $L_1 \\cap L_2 \\in P$, $L_1L_2 \\in P$, $\\bar L_1 \\in P$, and $L_1^* \\in P$. (Omit!)","title":"34.1-6"},{"location":"Chap34/34.2/","text":"34.2-1 Consider the language $\\text{GRAPH-ISOMORPHISM}$ $= \\{\\langle G_1, G_2 \\rangle: G_1$ and $G_2$ are isomorphic graphs$\\}$. Prove that $\\text{GRAPH-ISOMORPHISM} \\in \\text{NP}$ by describing a polynomial-time algorithm to verify the language. (Omit!) 34.2-2 Prove that if $G$ is an undirected bipartite graph with an odd number of vertices, then $G$ is nonhamiltonian. (Omit!) 34.2-3 Show that if $\\text{HAM-CYCLE} \\in P$, then the problem of listing the vertices of a hamiltonian cycle, in order, is polynomial-time solvable. (Omit!) 34.2-4 Prove that the class $\\text{NP}$ of languages is closed under union, intersection, concatenation, and Kleene star. Discuss the closure of $\\text{NP}$ under complement. (Omit!) 34.2-5 Show that any language in $\\text{NP}$ can be decided by an algorithm running in time $2^{O(n^k)}$ for some constant $k$. (Omit!) 34.2-6 A hamiltonian path in a graph is a simple path that visits every vertex exactly once. Show that the language $\\text{HAM-PATH}$ $= \\{\\langle G, u, v \\rangle:$ there is a hamiltonian path from $u$ to $v$ in graph $G\\}$ belongs to $\\text{NP}$. (Omit!) 34.2-7 Show that the hamiltonian-path problem from Exercise 34.2-6 can be solved in polynomial time on directed acyclic graphs. Give an efficient algorithm for the problem. (Omit!) 34.2-8 Let $\\phi$ be a boolean formula constructed from the boolean input variables $x_1, x_2, \\dots, x_k$, negations ($\\neg$), ANDs ($\\vee$), ORs ($\\wedge$), and parentheses. The formula $\\phi$ is a tautology if it evaluates to $1$ for every assignment of $1$ and $0$ to the input variables. Define $\\text{TAUTOLOGY}$ as the language of boolean formulas that are tautologies. Show that $\\text{TAUTOLOGY} \\in \\text{co-NP}$. (Omit!) 34.2-9 Prove that $\\text P \\subseteq \\text{co-NP}$. (Omit!) 34.2-10 Prove that if $\\text{NP} \\ne \\text{co-NP}$, then $\\text P \\ne \\text{NP}$. (Omit!) 34.2-11 Let $G$ be a connected, undirected graph with at least $3$ vertices, and let $G^3$ be the graph obtained by connecting all pairs of vertices that are connected by a path in $G$ of length at most $3$. Prove that $G^3$ is hamiltonian. ($\\textit{Hint:}$ Construct a spanning tree for $G$, and use an inductive argument.) (Omit!)","title":"34.2 Polynomial-time verification"},{"location":"Chap34/34.2/#342-1","text":"Consider the language $\\text{GRAPH-ISOMORPHISM}$ $= \\{\\langle G_1, G_2 \\rangle: G_1$ and $G_2$ are isomorphic graphs$\\}$. Prove that $\\text{GRAPH-ISOMORPHISM} \\in \\text{NP}$ by describing a polynomial-time algorithm to verify the language. (Omit!)","title":"34.2-1"},{"location":"Chap34/34.2/#342-2","text":"Prove that if $G$ is an undirected bipartite graph with an odd number of vertices, then $G$ is nonhamiltonian. (Omit!)","title":"34.2-2"},{"location":"Chap34/34.2/#342-3","text":"Show that if $\\text{HAM-CYCLE} \\in P$, then the problem of listing the vertices of a hamiltonian cycle, in order, is polynomial-time solvable. (Omit!)","title":"34.2-3"},{"location":"Chap34/34.2/#342-4","text":"Prove that the class $\\text{NP}$ of languages is closed under union, intersection, concatenation, and Kleene star. Discuss the closure of $\\text{NP}$ under complement. (Omit!)","title":"34.2-4"},{"location":"Chap34/34.2/#342-5","text":"Show that any language in $\\text{NP}$ can be decided by an algorithm running in time $2^{O(n^k)}$ for some constant $k$. (Omit!)","title":"34.2-5"},{"location":"Chap34/34.2/#342-6","text":"A hamiltonian path in a graph is a simple path that visits every vertex exactly once. Show that the language $\\text{HAM-PATH}$ $= \\{\\langle G, u, v \\rangle:$ there is a hamiltonian path from $u$ to $v$ in graph $G\\}$ belongs to $\\text{NP}$. (Omit!)","title":"34.2-6"},{"location":"Chap34/34.2/#342-7","text":"Show that the hamiltonian-path problem from Exercise 34.2-6 can be solved in polynomial time on directed acyclic graphs. Give an efficient algorithm for the problem. (Omit!)","title":"34.2-7"},{"location":"Chap34/34.2/#342-8","text":"Let $\\phi$ be a boolean formula constructed from the boolean input variables $x_1, x_2, \\dots, x_k$, negations ($\\neg$), ANDs ($\\vee$), ORs ($\\wedge$), and parentheses. The formula $\\phi$ is a tautology if it evaluates to $1$ for every assignment of $1$ and $0$ to the input variables. Define $\\text{TAUTOLOGY}$ as the language of boolean formulas that are tautologies. Show that $\\text{TAUTOLOGY} \\in \\text{co-NP}$. (Omit!)","title":"34.2-8"},{"location":"Chap34/34.2/#342-9","text":"Prove that $\\text P \\subseteq \\text{co-NP}$. (Omit!)","title":"34.2-9"},{"location":"Chap34/34.2/#342-10","text":"Prove that if $\\text{NP} \\ne \\text{co-NP}$, then $\\text P \\ne \\text{NP}$. (Omit!)","title":"34.2-10"},{"location":"Chap34/34.2/#342-11","text":"Let $G$ be a connected, undirected graph with at least $3$ vertices, and let $G^3$ be the graph obtained by connecting all pairs of vertices that are connected by a path in $G$ of length at most $3$. Prove that $G^3$ is hamiltonian. ($\\textit{Hint:}$ Construct a spanning tree for $G$, and use an inductive argument.) (Omit!)","title":"34.2-11"},{"location":"Chap34/34.3/","text":"34.3-1 Verify that the circuit in Figure 34.8(b) is unsatisfiable. (Omit!) 34.3-2 Show that the $\\le_\\text P$ relation is a transitive relation on languages. That is, show that if $L_1 \\le_\\text P L_2$ and $L_2 \\le_\\text P L_3$, then $L_1 \\le_\\text P L_3$. (Omit!) 34.3-3 Prove that $L \\le_\\text P \\bar L$ if and only if $\\bar L \\le_\\text P L$. (Omit!) 34.3-4 Show that we could have used a satisfying assignment as a certificate in an alternative proof of Lemma 34.5. Which certificate makes for an easier proof? (Omit!) 34.3-5 The proof of Lemma 34.6 assumes that the working storage for algorithm A occupies a contiguous region of polynomial size. Where in the proof do we exploit this assumption? Argue that this assumption does not involve any loss of generality. (Omit!) 34.3-6 A language $L$ is complete for a language class $C$ with respect to polynomial-time reductions if $L \\in C$ and $L' \\le_\\text P L$ for all $L' \\in C$. Show that $\\emptyset$ and $\\{0, 1\\}^*$ are the only languages in $\\text P$ that are not complete for $\\text P$ with respect to polynomial-time reductions. (Omit!) 34.3-7 Show that, with respect to polynomial-time reductions (see Exercise 34.3-6), $L$ is complete for $\\text{NP}$ if and only if $\\bar L$ is complete for \\text{co-NP}\\$. (Omit!) 34.3-8 The reduction algorithm $F$ in the proof of Lemma 34.6 constructs the circuit $C = f(x)$ based on knowledge of $x$, $A$, and $k$. Professor Sartre observes that the string $x$ is input to $F$, but only the existence of $A$, $k$, and the constant factor implicit in the $O(n^k)$ running time is known to $F$ (since the language $L$ belongs to $\\text{NP}$), not their actual values. Thus, the professor concludes that $F$ can't possibly construct the circuit $C$ and that the language $\\text{CIRCUIT-SAT}$ is not necessarily $\\text{NP-hard}$. Explain the flaw in the professor's reasoning. (Omit!)","title":"34.3 NP-completeness and reducibility"},{"location":"Chap34/34.3/#343-1","text":"Verify that the circuit in Figure 34.8(b) is unsatisfiable. (Omit!)","title":"34.3-1"},{"location":"Chap34/34.3/#343-2","text":"Show that the $\\le_\\text P$ relation is a transitive relation on languages. That is, show that if $L_1 \\le_\\text P L_2$ and $L_2 \\le_\\text P L_3$, then $L_1 \\le_\\text P L_3$. (Omit!)","title":"34.3-2"},{"location":"Chap34/34.3/#343-3","text":"Prove that $L \\le_\\text P \\bar L$ if and only if $\\bar L \\le_\\text P L$. (Omit!)","title":"34.3-3"},{"location":"Chap34/34.3/#343-4","text":"Show that we could have used a satisfying assignment as a certificate in an alternative proof of Lemma 34.5. Which certificate makes for an easier proof? (Omit!)","title":"34.3-4"},{"location":"Chap34/34.3/#343-5","text":"The proof of Lemma 34.6 assumes that the working storage for algorithm A occupies a contiguous region of polynomial size. Where in the proof do we exploit this assumption? Argue that this assumption does not involve any loss of generality. (Omit!)","title":"34.3-5"},{"location":"Chap34/34.3/#343-6","text":"A language $L$ is complete for a language class $C$ with respect to polynomial-time reductions if $L \\in C$ and $L' \\le_\\text P L$ for all $L' \\in C$. Show that $\\emptyset$ and $\\{0, 1\\}^*$ are the only languages in $\\text P$ that are not complete for $\\text P$ with respect to polynomial-time reductions. (Omit!)","title":"34.3-6"},{"location":"Chap34/34.3/#343-7","text":"Show that, with respect to polynomial-time reductions (see Exercise 34.3-6), $L$ is complete for $\\text{NP}$ if and only if $\\bar L$ is complete for \\text{co-NP}\\$. (Omit!)","title":"34.3-7"},{"location":"Chap34/34.3/#343-8","text":"The reduction algorithm $F$ in the proof of Lemma 34.6 constructs the circuit $C = f(x)$ based on knowledge of $x$, $A$, and $k$. Professor Sartre observes that the string $x$ is input to $F$, but only the existence of $A$, $k$, and the constant factor implicit in the $O(n^k)$ running time is known to $F$ (since the language $L$ belongs to $\\text{NP}$), not their actual values. Thus, the professor concludes that $F$ can't possibly construct the circuit $C$ and that the language $\\text{CIRCUIT-SAT}$ is not necessarily $\\text{NP-hard}$. Explain the flaw in the professor's reasoning. (Omit!)","title":"34.3-8"},{"location":"Chap34/34.4/","text":"34.4-1 Consider the straightforward (nonpolynomial-time) reduction in the proof of Theorem 34.9. Describe a circuit of size $n$ that, when converted to a formula by this method, yields a formula whose size is exponential in $n$. (Omit!) 34.4-2 Show the $\\text{3-CNF}$ formula that results when we use the method of Theorem 34.10 on the formula $\\text{(34.3)}$. (Omit!) 34.4-3 Professor Jagger proposes to show that $\\text{SAT} \\le_\\text P \\text{3-CNF-SAT}$ by using only the truth-table technique in the proof of Theorem 34.10, and not the other steps. That is, the professor proposes to take the boolean formula $\\phi$, form a truth table for its variables, derive from the truth table a formula in $\\text{3-DNF}$ that is equivalent to $\\neg\\phi$, and then negate and apply DeMorgan's laws to produce a $\\text{3-CNF}$ formula equivalent to $\\phi$. Show that this strategy does not yield a polynomial-time reduction. (Omit!) 34.4-4 Show that the problem of determining whether a boolean formula is a tautology is complete for $\\text{co-NP}$. ($\\textit{Hint:}$ See Exercise 34.3-7.) (Omit!) 34.4-5 Show that the problem of determining the satisfiability of boolean formulas in disjunctive normal form is polynomial-time solvable. (Omit!) 34.4-6 Suppose that someone gives you a polynomial-time algorithm to decide formula satisfiability. Describe how to use this algorithm to find satisfying assignments in polynomial time. (Omit!) 34.4-7 Let $\\text{2-CNF-SAT}$ be the set of satisfiable boolean formulas in $\\text{CNF}$ with exactly $2$ literals per clause. Show that $\\text{2-CNF-SAT} \\in P$. Make your algorithm as efficient as possible. ($\\textit{Hint:}$ Observe that $x \\vee y$ is equivalent to $\\neg x \\to y$. Reduce $\\text{2-CNF-SAT}$ to an efficiently solvable problem on a directed graph.) (Omit!)","title":"34.4 NP-completeness proofs"},{"location":"Chap34/34.4/#344-1","text":"Consider the straightforward (nonpolynomial-time) reduction in the proof of Theorem 34.9. Describe a circuit of size $n$ that, when converted to a formula by this method, yields a formula whose size is exponential in $n$. (Omit!)","title":"34.4-1"},{"location":"Chap34/34.4/#344-2","text":"Show the $\\text{3-CNF}$ formula that results when we use the method of Theorem 34.10 on the formula $\\text{(34.3)}$. (Omit!)","title":"34.4-2"},{"location":"Chap34/34.4/#344-3","text":"Professor Jagger proposes to show that $\\text{SAT} \\le_\\text P \\text{3-CNF-SAT}$ by using only the truth-table technique in the proof of Theorem 34.10, and not the other steps. That is, the professor proposes to take the boolean formula $\\phi$, form a truth table for its variables, derive from the truth table a formula in $\\text{3-DNF}$ that is equivalent to $\\neg\\phi$, and then negate and apply DeMorgan's laws to produce a $\\text{3-CNF}$ formula equivalent to $\\phi$. Show that this strategy does not yield a polynomial-time reduction. (Omit!)","title":"34.4-3"},{"location":"Chap34/34.4/#344-4","text":"Show that the problem of determining whether a boolean formula is a tautology is complete for $\\text{co-NP}$. ($\\textit{Hint:}$ See Exercise 34.3-7.) (Omit!)","title":"34.4-4"},{"location":"Chap34/34.4/#344-5","text":"Show that the problem of determining the satisfiability of boolean formulas in disjunctive normal form is polynomial-time solvable. (Omit!)","title":"34.4-5"},{"location":"Chap34/34.4/#344-6","text":"Suppose that someone gives you a polynomial-time algorithm to decide formula satisfiability. Describe how to use this algorithm to find satisfying assignments in polynomial time. (Omit!)","title":"34.4-6"},{"location":"Chap34/34.4/#344-7","text":"Let $\\text{2-CNF-SAT}$ be the set of satisfiable boolean formulas in $\\text{CNF}$ with exactly $2$ literals per clause. Show that $\\text{2-CNF-SAT} \\in P$. Make your algorithm as efficient as possible. ($\\textit{Hint:}$ Observe that $x \\vee y$ is equivalent to $\\neg x \\to y$. Reduce $\\text{2-CNF-SAT}$ to an efficiently solvable problem on a directed graph.) (Omit!)","title":"34.4-7"},{"location":"Chap34/34.5/","text":"34.5-1 The subgraph-isomorphism problem takes two undirected graphs $G_1$ and $G_2$, and it asks whether $G_1$ is isomorphic to a subgraph of $G_2$. Show that the subgraphisomorphism problem is $\\text{NP-complete}$. (Omit!) 34.5-2 Given an integer $m \\times n$ matrix $A$ and an integer $m$-vector $b$, the 0-1 integerprogramming problem asks whether there exists an integer $n$-vector $x$ with elements in the set $\\{0, 1\\}$ such that $Ax \\le b$. Prove that 0-1 integer programming is $\\text{NP-complete}$. ($\\textit{Hint:}$ Reduce from $\\text{3-CNF-SAT}$.) (Omit!) 34.5-3 The integer linear-programming problem is like the 0-1 integer-programming problem given in Exercise 34.5-2, except that the values of the vector $x$ may be any integers rather than just $0$ or $1$. Assuming that the 0-1 integer-programming problem is $\\text{NP-hard}$, show that the integer linear-programming problem is $\\text{NP-complete}$. (Omit!) 34.5-4 Show how to solve the subset-sum problem in polynomial time if the target value $t$ is expressed in unary. (Omit!) 34.5-5 The set-partition problem takes as input a set $S$ of numbers. The question is whether the numbers can be partitioned into two sets $A$ and $\\bar A = S - A$ such that $\\sum_{x \\in A} x = \\sum_{x \\in \\bar A} x$. Show that the set-partition problem is $\\text{NP-complete}$. (Omit!) 34.5-6 Show that the hamiltonian-path problem is $\\text{NP-complete}$. (Omit!) 34.5-7 The longest-simple-cycle problem is the problem of determining a simple cycle (no repeated vertices) of maximum length in a graph. Formulate a related decision problem, and show that the decision problem is $\\text{NP-complete}$. (Omit!) 34.5-8 In the half 3-CNF satisfiability problem, we are given a $\\text{3-CNF}$ formula $\\phi$ with $n$ variables and $m$ clauses, where $m$ is even. We wish to determine whether there exists a truth assignment to the variables of $\\phi$ such that exactly half the clauses evaluate to $0$ and exactly half the clauses evaluate to $1$. Prove that the half $\\text{3-CNF}$ satisfiability problem is $\\text{NP-complete}$. (Omit!)","title":"34.5 NP-complete problems"},{"location":"Chap34/34.5/#345-1","text":"The subgraph-isomorphism problem takes two undirected graphs $G_1$ and $G_2$, and it asks whether $G_1$ is isomorphic to a subgraph of $G_2$. Show that the subgraphisomorphism problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-1"},{"location":"Chap34/34.5/#345-2","text":"Given an integer $m \\times n$ matrix $A$ and an integer $m$-vector $b$, the 0-1 integerprogramming problem asks whether there exists an integer $n$-vector $x$ with elements in the set $\\{0, 1\\}$ such that $Ax \\le b$. Prove that 0-1 integer programming is $\\text{NP-complete}$. ($\\textit{Hint:}$ Reduce from $\\text{3-CNF-SAT}$.) (Omit!)","title":"34.5-2"},{"location":"Chap34/34.5/#345-3","text":"The integer linear-programming problem is like the 0-1 integer-programming problem given in Exercise 34.5-2, except that the values of the vector $x$ may be any integers rather than just $0$ or $1$. Assuming that the 0-1 integer-programming problem is $\\text{NP-hard}$, show that the integer linear-programming problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-3"},{"location":"Chap34/34.5/#345-4","text":"Show how to solve the subset-sum problem in polynomial time if the target value $t$ is expressed in unary. (Omit!)","title":"34.5-4"},{"location":"Chap34/34.5/#345-5","text":"The set-partition problem takes as input a set $S$ of numbers. The question is whether the numbers can be partitioned into two sets $A$ and $\\bar A = S - A$ such that $\\sum_{x \\in A} x = \\sum_{x \\in \\bar A} x$. Show that the set-partition problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-5"},{"location":"Chap34/34.5/#345-6","text":"Show that the hamiltonian-path problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-6"},{"location":"Chap34/34.5/#345-7","text":"The longest-simple-cycle problem is the problem of determining a simple cycle (no repeated vertices) of maximum length in a graph. Formulate a related decision problem, and show that the decision problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-7"},{"location":"Chap34/34.5/#345-8","text":"In the half 3-CNF satisfiability problem, we are given a $\\text{3-CNF}$ formula $\\phi$ with $n$ variables and $m$ clauses, where $m$ is even. We wish to determine whether there exists a truth assignment to the variables of $\\phi$ such that exactly half the clauses evaluate to $0$ and exactly half the clauses evaluate to $1$. Prove that the half $\\text{3-CNF}$ satisfiability problem is $\\text{NP-complete}$. (Omit!)","title":"34.5-8"},{"location":"Chap34/Problems/34-1/","text":"An independent set of a graph $G = (V, E)$ is a subset $V' \\subseteq V$ of vertices such that each edge in $E$ is incident on at most one vertex in $V'$. The independent-set problem is to find a maximum-size independent set in $G$. a. Formulate a related decision problem for the independent-set problem, and prove that it is $\\text{NP-complete}$. ($\\textit{Hint:}$ Reduce from the clique problem.) b. Suppose that you are given a \"black-box\" subroutine to solve the decision problem you defined in part (a). Give an algorithm to find an independent set of maximum size. The running time of your algorithm should be polynomial in $|V|$ and $|E|$, counting queries to the black box as a single step. Although the independent-set decision problem is $\\text{NP-complete}$, certain special cases are polynomial-time solvable. c. Give an efficient algorithm to solve the independent-set problem when each vertex in $G$ has degree $2$. Analyze the running time, and prove that your algorithm works correctly. d. Give an efficient algorithm to solve the independent-set problem when $G$ is bipartite. Analyze the running time, and prove that your algorithm works correctly. ($\\text{Hint:}$ Use the results of Section 26.3.) (Omit!)","title":"34-1 Independent set"},{"location":"Chap34/Problems/34-2/","text":"Bonnie and Clyde have just robbed a bank. They have a bag of money and want to divide it up. For each of the following scenarios, either give a polynomial-time algorithm, or prove that the problem is $\\text{NP-complete}$. The input in each case is a list of the $n$ items in the bag, along with the value of each. a. The bag contains $n$ coins, but only $2$ different denominations: some coins are worth $x$ dollars, and some are worth $y$ dollars. Bonnie and Clyde wish to divide the money exactly evenly. b. The bag contains $n$ coins, with an arbitrary number of different denominations, but each denomination is a nonnegative integer power of $2$, i.e., the possible denominations are $1$ dollar, $2$ dollars, $4$ dollars, etc. Bonnie and Clyde wish to divide the money exactly evenly. c. The bag contains $n$ checks, which are, in an amazing coincidence, made out to \"Bonnie or Clyde.\" They wish to divide the checks so that they each get the exact same amount of money. d. The bag contains $n$ checks as in part (c), but this time Bonnie and Clyde are willing to accept a split in which the difference is no larger than $100$ dollars. (Omit!)","title":"34-2 Bonnie and Clyde"},{"location":"Chap34/Problems/34-3/","text":"Mapmakers try to use as few colors as possible when coloring countries on a map, as long as no two countries that share a border have the same color. We can model this problem with an undirected graph $G = (V, E)$ in which each vertex represents a country and vertices whose respective countries share a border are adjacent. Then, a $k$-coloring is a function $c: V \\to \\{1, 2, \\dots, k \\}$ such that $c(u) \\ne c(v)$ for every edge $(u, v) \\in E$. In other words, the numbers $1, 2, \\dots, k$ represent the $k$ colors, and adjacent vertices must have different colors. The graph-coloring problem is to determine the minimum number of colors needed to color a given graph. a. Give an efficient algorithm to determine a $2$-coloring of a graph, if one exists. b. Cast the graph-coloring problem as a decision problem. Show that your decision problem is solvable in polynomial time if and only if the graph-coloring problem is solvable in polynomial time. c. Let the language $\\text{3-COLOR}$ be the set of graphs that can be $3$-colored. Show that if $\\text{3-COLOR}$ is $\\text{NP-complete}$, then your decision problem from part (b) is $\\text{NP-complete}$. To prove that $\\text{3-COLOR}$ is $\\text{NP-complete}$, we use a reduction from $\\text{3-CNF-SAT}$. Given a formula $\\phi$ of $m$ clauses on $n$ variables $x_1, x_2, \\dots, x_n$, we construct a graph $G = (V, E)$ as follows. The set $V$ consists of a vertex for each variable, a vertex for the negation of each variable, $5$ vertices for each clause, and $3$ special vertices: $\\text{TRUE}$, $\\text{FALSE}$, and $\\text{RED}$. The edges of the graph are of two types: \"literal\" edges that are independent of the clauses and \"clause\" edges that depend on the clauses. The literal edges form a triangle on the special vertices and also form a triangle on $x_i, \\neg x_i$, and $\\text{RED}$ for $i = 1, 2, \\dots, n$. d. Argue that in any $3$-coloring $c$ of a graph containing the literal edges, exactly one of a variable and its negation is colored $c(\\text{TRUE})$ and the other is colored $c(\\text{FALSE})$. Argue that for any truth assignment for $\\phi$, there exists a $3$-coloring of the graph containing just the literal edges. The widget shown in Figure 34.20 helps to enforce the condition corresponding to a clause $(x \\vee y \\vee z)$. Each clause requires a unique copy of the $5$ vertices that are heavily shaded in the figure; they connect as shown to the literals of the clause and the special vertex $\\text{TRUE}$. e. Argue that if each of $x$, $y$, and $z$ is colored $c(\\text{TRUE})$ or $c(\\text{FALSE})$, then the widget is $3$-colorable if and only if at least one of $x$, $y$, or $z$ is colored $c(\\text{TRUE})$. f. Complete the proof that $\\text{3-COLOR}$ is $\\text{NP-complete}$. (Omit!)","title":"34-3 Graph coloring"},{"location":"Chap34/Problems/34-4/","text":"Suppose that we have one machine and a set of $n$ tasks $a_1, a_2, \\dots, a_n$, each of which requires time on the machine. Each task $a_j$ requires $t_j$ time units on the machine (its processing time), yields a profit of $p_j$, and has a deadline $d_j$. The machine can process only one task at a time, and task $a_j$ must run without interruption for $t_j$ consecutive time units. If we complete task $a_j$ by its deadline $d_j$, we receive a profit $p_j$, but if we complete it after its deadline, we receive no profit. As an optimization problem, we are given the processing times, profits, and deadlines for a set of $n$ tasks, and we wish to find a schedule that completes all the tasks and returns the greatest amount of profit. The processing times, profits, and deadlines are all nonnegative numbers. a. State this problem as a decision problem. b. Show that the decision problem is $\\text{NP-complete}$. c. Give a polynomial-time algorithm for the decision problem, assuming that all processing times are integers from $1$ to $n$. ($\\textit{Hint:}$ Use dynamic programming.) d. Give a polynomial-time algorithm for the optimization problem, assuming that all processing times are integers from $1$ to $n$. (Omit!)","title":"34-4 Scheduling with profits and deadlines"},{"location":"Chap35/35.1/","text":"35.1-1 Give an example of a graph for which $\\text{APPROX-VERTEX-COVER}$ always yields a suboptimal solution. (Omit!) 35.1-2 Prove that the set of edges picked in line 4 of $\\text{APPROX-VERTEX-COVER}$ forms a maximal matching in the graph $G$. (Omit!) 35.1-3 $\\star$ Professor B\u00fcndchen proposes the following heuristic to solve the vertex-cover problem. Repeatedly select a vertex of highest degree, and remove all of its incident edges. Give an example to show that the professor's heuristic does not have an approximation ratio of $2$. ($\\textit{Hint:}$ Try a bipartite graph with vertices of uniform degree on the left and vertices of varying degree on the right.) Consider a bibartite graph with left part $L$ and right part $R$ such that $L$ has $5$ vertices of degrees $(5, 5, 5, 5, 5)$ and $R$ has $11$ vertices of degrees $(5, 4, 4, 3, 2, 2, 1, 1, 1, 1, 1)$ (the graph is easy to draw and the figure is omitted here). Clearly there exists a vertex-cover of size $5$ (the left vertices). The idea is to show that the proposed algorithm chooses all the vertices on the right part, resulting in the approximation ratio of $11 / 5 > 2$. After choosing the first vertex in $R$, the degrees on $L$ decrease to $(4, 4, 4, 4, 4)$. After choosing the second vertex in $R$, the degrees on $L$ decrease to $(4, 3, 3, 3, 3)$. After choosing the third vertex in $R$, the degrees on $L$ decrease to $(3, 3, 2, 2, 2)$. After choosing the fourth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 2, 2, 1)$. After choosing the fifth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 1, 1, 1)$. After choosing the sixth vertex in $R$, the degrees on $L$ decrease to $(1, 1, 1, 1, 1)$. Now the algorithm still has to choose $5$ more vertices. 35.1-4 Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in linear time. (Omit!) 35.1-5 From the proof of Theorem 34.12, we know that the vertex-cover problem and the $\\text{NP-complete}$ clique problem are complementary in the sense that an optimal vertex cover is the complement of a maximum-size clique in the complement graph. Does this relationship imply that there is a polynomial-time approximation algorithm with a constant approximation ratio for the clique problem? Justify your answer. (Omit!)","title":"35.1 The vertex-cover problem"},{"location":"Chap35/35.1/#351-1","text":"Give an example of a graph for which $\\text{APPROX-VERTEX-COVER}$ always yields a suboptimal solution. (Omit!)","title":"35.1-1"},{"location":"Chap35/35.1/#351-2","text":"Prove that the set of edges picked in line 4 of $\\text{APPROX-VERTEX-COVER}$ forms a maximal matching in the graph $G$. (Omit!)","title":"35.1-2"},{"location":"Chap35/35.1/#351-3-star","text":"Professor B\u00fcndchen proposes the following heuristic to solve the vertex-cover problem. Repeatedly select a vertex of highest degree, and remove all of its incident edges. Give an example to show that the professor's heuristic does not have an approximation ratio of $2$. ($\\textit{Hint:}$ Try a bipartite graph with vertices of uniform degree on the left and vertices of varying degree on the right.) Consider a bibartite graph with left part $L$ and right part $R$ such that $L$ has $5$ vertices of degrees $(5, 5, 5, 5, 5)$ and $R$ has $11$ vertices of degrees $(5, 4, 4, 3, 2, 2, 1, 1, 1, 1, 1)$ (the graph is easy to draw and the figure is omitted here). Clearly there exists a vertex-cover of size $5$ (the left vertices). The idea is to show that the proposed algorithm chooses all the vertices on the right part, resulting in the approximation ratio of $11 / 5 > 2$. After choosing the first vertex in $R$, the degrees on $L$ decrease to $(4, 4, 4, 4, 4)$. After choosing the second vertex in $R$, the degrees on $L$ decrease to $(4, 3, 3, 3, 3)$. After choosing the third vertex in $R$, the degrees on $L$ decrease to $(3, 3, 2, 2, 2)$. After choosing the fourth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 2, 2, 1)$. After choosing the fifth vertex in $R$, the degrees on $L$ decrease to $(2, 2, 1, 1, 1)$. After choosing the sixth vertex in $R$, the degrees on $L$ decrease to $(1, 1, 1, 1, 1)$. Now the algorithm still has to choose $5$ more vertices.","title":"35.1-3 $\\star$"},{"location":"Chap35/35.1/#351-4","text":"Give an efficient greedy algorithm that finds an optimal vertex cover for a tree in linear time. (Omit!)","title":"35.1-4"},{"location":"Chap35/35.1/#351-5","text":"From the proof of Theorem 34.12, we know that the vertex-cover problem and the $\\text{NP-complete}$ clique problem are complementary in the sense that an optimal vertex cover is the complement of a maximum-size clique in the complement graph. Does this relationship imply that there is a polynomial-time approximation algorithm with a constant approximation ratio for the clique problem? Justify your answer. (Omit!)","title":"35.1-5"},{"location":"Chap35/35.2/","text":"35.2-1 Suppose that a complete undirected graph $G = (V, E)$ with at least $3$ vertices has a cost function $c$ that satisfies the triangle inequality. Prove that $c(u, v) \\ge 0$ for all $u, v \\in V$. Let $u, v \\in V$. Let $w$ be any other vertex in $V$. Then by the triangle inequality we have $$ \\begin{aligned} c(w, u) + c(u, v) &\\ge c(w, v) \\\\ c(w, v) + c(v, u) &\\ge c(w, u). \\end{aligned} $$ Adding the above inequalities together gives $$ \\cancel{c(w, u)} + \\cancel{c(w, v)} + c(u, v) + c(v, u) \\ge \\cancel{c(w, v)} + \\cancel{c(w, u)}. $$ Since $G$ is undirected we have $c(u, v) = c(v, u)$, and so the above inequality gives $2c(u, v) \\ge 0$ from which the result follows. 35.2-2 Show how in polynomial time we can transform one instance of the traveling-salesman problem into another instance whose cost function satisfies the triangle inequality. The two instances must have the same set of optimal tours. Explain why such a polynomial-time transformation does not contradict Theorem 35.3, assuming that $\\text P \\ne \\text{NP}$. (Omit!) 35.2-3 Consider the following closest-point heuristic for building an approximate traveling-salesman tour whose cost function satisfies the triangle inequality. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex $u$ that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest $u$ is vertex $v$. Extend the cycle to include $u$ by inserting $u$ just after $v$. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour. (Omit!) 35.2-4 In the bottleneck traveling-salesman problem , we wish to find the hamiltonian cycle that minimizes the cost of the most costly edge in the cycle. Assuming that the cost function satisfies the triangle inequality, show that there exists a polynomial-time approximation algorithm with approximation ratio $3$ for this problem. ($\\textit{Hint:}$ Show recursively that we can visit all the nodes in a bottleneck spanning tree, as discussed in Problem 23-3, exactly once by taking a full walk of the tree and skipping nodes, but without skipping more than two consecutive intermediate nodes. Show that the costliest edge in a bottleneck spanning tree has a cost that is at most the cost of the costliest edge in a bottleneck hamiltonian cycle.) (Omit!) 35.2-5 Suppose that the vertices for an instance of the traveling-salesman problem are points in the plane and that the cost $c(u, v)$ is the euclidean distance between points $u$ and $v$. Show that an optimal tour never crosses itself. (Omit!)","title":"35.2 The traveling-salesman problem"},{"location":"Chap35/35.2/#352-1","text":"Suppose that a complete undirected graph $G = (V, E)$ with at least $3$ vertices has a cost function $c$ that satisfies the triangle inequality. Prove that $c(u, v) \\ge 0$ for all $u, v \\in V$. Let $u, v \\in V$. Let $w$ be any other vertex in $V$. Then by the triangle inequality we have $$ \\begin{aligned} c(w, u) + c(u, v) &\\ge c(w, v) \\\\ c(w, v) + c(v, u) &\\ge c(w, u). \\end{aligned} $$ Adding the above inequalities together gives $$ \\cancel{c(w, u)} + \\cancel{c(w, v)} + c(u, v) + c(v, u) \\ge \\cancel{c(w, v)} + \\cancel{c(w, u)}. $$ Since $G$ is undirected we have $c(u, v) = c(v, u)$, and so the above inequality gives $2c(u, v) \\ge 0$ from which the result follows.","title":"35.2-1"},{"location":"Chap35/35.2/#352-2","text":"Show how in polynomial time we can transform one instance of the traveling-salesman problem into another instance whose cost function satisfies the triangle inequality. The two instances must have the same set of optimal tours. Explain why such a polynomial-time transformation does not contradict Theorem 35.3, assuming that $\\text P \\ne \\text{NP}$. (Omit!)","title":"35.2-2"},{"location":"Chap35/35.2/#352-3","text":"Consider the following closest-point heuristic for building an approximate traveling-salesman tour whose cost function satisfies the triangle inequality. Begin with a trivial cycle consisting of a single arbitrarily chosen vertex. At each step, identify the vertex $u$ that is not on the cycle but whose distance to any vertex on the cycle is minimum. Suppose that the vertex on the cycle that is nearest $u$ is vertex $v$. Extend the cycle to include $u$ by inserting $u$ just after $v$. Repeat until all vertices are on the cycle. Prove that this heuristic returns a tour whose total cost is not more than twice the cost of an optimal tour. (Omit!)","title":"35.2-3"},{"location":"Chap35/35.2/#352-4","text":"In the bottleneck traveling-salesman problem , we wish to find the hamiltonian cycle that minimizes the cost of the most costly edge in the cycle. Assuming that the cost function satisfies the triangle inequality, show that there exists a polynomial-time approximation algorithm with approximation ratio $3$ for this problem. ($\\textit{Hint:}$ Show recursively that we can visit all the nodes in a bottleneck spanning tree, as discussed in Problem 23-3, exactly once by taking a full walk of the tree and skipping nodes, but without skipping more than two consecutive intermediate nodes. Show that the costliest edge in a bottleneck spanning tree has a cost that is at most the cost of the costliest edge in a bottleneck hamiltonian cycle.) (Omit!)","title":"35.2-4"},{"location":"Chap35/35.2/#352-5","text":"Suppose that the vertices for an instance of the traveling-salesman problem are points in the plane and that the cost $c(u, v)$ is the euclidean distance between points $u$ and $v$. Show that an optimal tour never crosses itself. (Omit!)","title":"35.2-5"},{"location":"Chap35/35.3/","text":"35.3-1 Consider each of the following words as a set of letters: $\\{\\text{arid}$, $\\text{dash}$, $\\text{drain}$, $\\text{heard}$, $\\text{lost}$, $\\text{nose}$, $\\text{shun}$, $\\text{slate}$, $\\text{snare}$, $\\text{thread}\\}$. Show which set cover $\\text{GREEDY-SET-COVER}$ produces when we break ties in favor of the word that appears \ufb01rst in the dictionary. (Omit!) 35.3-2 Show that the decision version of the set-covering problem is $\\text{NP-complete}$ by reducing it from the vertex-cover problem. (Omit!) 35.3-3 Show how to implement $\\text{GREEDY-SET-COVER}$ in such a way that it runs in time $O\\Big(\\sum_{S \\in \\mathcal F} |S|\\Big)$. (Omit!) 35.3-4 Show that the following weaker form of Theorem 35.4 is trivially true: $$|\\mathcal C| \\le |\\mathcal C^*| \\max\\{|S|: S \\in \\mathcal F\\}.$$ (Omit!) 35.3-5 $\\text{GREEDY-SET-COVER}$ can return a number of different solutions, depending on how we break ties in line 4. Give a procedure $\\text{BAD-SET-COVER-INSTANCE}(n)$ that returns an $n$-element instance of the set-covering problem for which, depending on how we break ties in line 4, $\\text{GREEDY-SET-COVER}$ can return a number of different solutions that is exponential in $n$. (Omit!)","title":"35.3 The set-covering problem"},{"location":"Chap35/35.3/#353-1","text":"Consider each of the following words as a set of letters: $\\{\\text{arid}$, $\\text{dash}$, $\\text{drain}$, $\\text{heard}$, $\\text{lost}$, $\\text{nose}$, $\\text{shun}$, $\\text{slate}$, $\\text{snare}$, $\\text{thread}\\}$. Show which set cover $\\text{GREEDY-SET-COVER}$ produces when we break ties in favor of the word that appears \ufb01rst in the dictionary. (Omit!)","title":"35.3-1"},{"location":"Chap35/35.3/#353-2","text":"Show that the decision version of the set-covering problem is $\\text{NP-complete}$ by reducing it from the vertex-cover problem. (Omit!)","title":"35.3-2"},{"location":"Chap35/35.3/#353-3","text":"Show how to implement $\\text{GREEDY-SET-COVER}$ in such a way that it runs in time $O\\Big(\\sum_{S \\in \\mathcal F} |S|\\Big)$. (Omit!)","title":"35.3-3"},{"location":"Chap35/35.3/#353-4","text":"Show that the following weaker form of Theorem 35.4 is trivially true: $$|\\mathcal C| \\le |\\mathcal C^*| \\max\\{|S|: S \\in \\mathcal F\\}.$$ (Omit!)","title":"35.3-4"},{"location":"Chap35/35.3/#353-5","text":"$\\text{GREEDY-SET-COVER}$ can return a number of different solutions, depending on how we break ties in line 4. Give a procedure $\\text{BAD-SET-COVER-INSTANCE}(n)$ that returns an $n$-element instance of the set-covering problem for which, depending on how we break ties in line 4, $\\text{GREEDY-SET-COVER}$ can return a number of different solutions that is exponential in $n$. (Omit!)","title":"35.3-5"},{"location":"Chap35/35.4/","text":"35.4-1 Show that even if we allow a clause to contain both a variable and its negation, randomly setting each variable to 1 with probability $1 / 2$ and to $0$ with probability $1 / 2$ still yields a randomized $8 / 7$-approximation algorithm. (Omit!) 35.4-2 The MAX-CNF satisfiability problem is like the $\\text{MAX-3-CNF}$ satisfiability problem, except that it does not restrict each clause to have exactly $3$ literals. Give a randomized $2$-approximation algorithm for the $\\text{MAX-CNF}$ satisfiability problem. (Omit!) 35.4-3 In the $\\text{MAX-CUT}$ problem, we are given an unweighted undirected graph $G = (V, E)$. We define a cut $(S, V - S)$ as in Chapter 23 and the weight of a cut as the number of edges crossing the cut. The goal is to find a cut of maximum weight. Suppose that for each vertex $v$, we randomly and independently place $v$ in $S$ with probability $1 / 2$ and in $V - S$ with probability $1 / 2$. Show that this algorithm is a randomized $2$-approximation algorithm. $\\gdef\\Ex{\\mathbf{E}}$ $\\gdef\\Prob{\\mathbf{P}}$ $\\gdef\\opt{\\texttt{opt}}$ We first rewrite the algorithm for clarity. APPROX - MAX - CUT ( G ) for each v in V flip a fair coin if heads add v to S else add v to V - S This algorithm clearly runs in linear time. For each edge $(u, v) \\in E$, define the event $A_{uv}$ to be the event where edge $(i, j)$ crosses the cut $(S, V - S)$, and let $1_{A_{uv}}$ be the indicator random variable for $A_{uv}$. The event $A_{ij}$ occurs if and only if the vertices $u$ and $v$ are placed in different sets during the main loop in $\\text{APPROX-MAX-CUT}$. Hence, $$ \\begin{aligned} \\Prob \\{A_{uv}\\} &= \\Prob \\{u \\in S \\wedge v \\in V - S\\} + P\\{u \\in V - S \\wedge v\\in S\\} \\\\ &= \\frac{1}{2}\\cdot\\frac{1}{2} + \\frac{1}{2}\\cdot\\frac{1}{2} \\\\ &= \\frac{1}{2}. \\end{aligned} $$ Let $\\opt$ denote the cost of a maximum cut in $G$, and let $c = |(S, V - S)|$, that is, the size of the cut produced by $\\text{APPROX-MAX-CUT}$. Clearly $c = \\sum_{(u, v) \\in E} 1_{A_{uv}}$. Also, note that $\\texttt{opt} \\leq |E|$ (this is tight iff $G$ is bipartite). Hence, $$ \\begin{aligned} \\Ex[c] &= \\Ex\\left[\\sum_{(u, v) \\in E} 1_{A_{uv}}\\right]\\\\ &= \\sum_{(u, v)\\in E}\\Ex[1_{A_{uv}}]\\\\ &= \\sum_{(u, v)\\in E}\\Prob\\{A_{uv}\\}\\\\ &= \\frac{1}{2}|E|\\\\ &\\ge \\frac{1}{2}\\opt \\end{aligned} $$ Hence, $\\Ex\\left[\\frac{\\opt}{c}\\right] \\le \\frac{|E|}{1 / 2|E|} = 2$, and so $\\text{APPROX-MAX-CUT}$ is a randomized $2$-approximation algorithm. 35.4-4 Show that the constraints in line $\\text{(35.19)}$ are redundant in the sense that if we remove them from the linear program in lines $\\text{(35.17)}\u2013\\text{(35.20)}$, any optimal solution to the resulting linear program must satisfy $x(v) \\le 1$ for each $v \\in V$. (Omit!)","title":"35.4 Randomization and linear programming"},{"location":"Chap35/35.4/#354-1","text":"Show that even if we allow a clause to contain both a variable and its negation, randomly setting each variable to 1 with probability $1 / 2$ and to $0$ with probability $1 / 2$ still yields a randomized $8 / 7$-approximation algorithm. (Omit!)","title":"35.4-1"},{"location":"Chap35/35.4/#354-2","text":"The MAX-CNF satisfiability problem is like the $\\text{MAX-3-CNF}$ satisfiability problem, except that it does not restrict each clause to have exactly $3$ literals. Give a randomized $2$-approximation algorithm for the $\\text{MAX-CNF}$ satisfiability problem. (Omit!)","title":"35.4-2"},{"location":"Chap35/35.4/#354-3","text":"In the $\\text{MAX-CUT}$ problem, we are given an unweighted undirected graph $G = (V, E)$. We define a cut $(S, V - S)$ as in Chapter 23 and the weight of a cut as the number of edges crossing the cut. The goal is to find a cut of maximum weight. Suppose that for each vertex $v$, we randomly and independently place $v$ in $S$ with probability $1 / 2$ and in $V - S$ with probability $1 / 2$. Show that this algorithm is a randomized $2$-approximation algorithm. $\\gdef\\Ex{\\mathbf{E}}$ $\\gdef\\Prob{\\mathbf{P}}$ $\\gdef\\opt{\\texttt{opt}}$ We first rewrite the algorithm for clarity. APPROX - MAX - CUT ( G ) for each v in V flip a fair coin if heads add v to S else add v to V - S This algorithm clearly runs in linear time. For each edge $(u, v) \\in E$, define the event $A_{uv}$ to be the event where edge $(i, j)$ crosses the cut $(S, V - S)$, and let $1_{A_{uv}}$ be the indicator random variable for $A_{uv}$. The event $A_{ij}$ occurs if and only if the vertices $u$ and $v$ are placed in different sets during the main loop in $\\text{APPROX-MAX-CUT}$. Hence, $$ \\begin{aligned} \\Prob \\{A_{uv}\\} &= \\Prob \\{u \\in S \\wedge v \\in V - S\\} + P\\{u \\in V - S \\wedge v\\in S\\} \\\\ &= \\frac{1}{2}\\cdot\\frac{1}{2} + \\frac{1}{2}\\cdot\\frac{1}{2} \\\\ &= \\frac{1}{2}. \\end{aligned} $$ Let $\\opt$ denote the cost of a maximum cut in $G$, and let $c = |(S, V - S)|$, that is, the size of the cut produced by $\\text{APPROX-MAX-CUT}$. Clearly $c = \\sum_{(u, v) \\in E} 1_{A_{uv}}$. Also, note that $\\texttt{opt} \\leq |E|$ (this is tight iff $G$ is bipartite). Hence, $$ \\begin{aligned} \\Ex[c] &= \\Ex\\left[\\sum_{(u, v) \\in E} 1_{A_{uv}}\\right]\\\\ &= \\sum_{(u, v)\\in E}\\Ex[1_{A_{uv}}]\\\\ &= \\sum_{(u, v)\\in E}\\Prob\\{A_{uv}\\}\\\\ &= \\frac{1}{2}|E|\\\\ &\\ge \\frac{1}{2}\\opt \\end{aligned} $$ Hence, $\\Ex\\left[\\frac{\\opt}{c}\\right] \\le \\frac{|E|}{1 / 2|E|} = 2$, and so $\\text{APPROX-MAX-CUT}$ is a randomized $2$-approximation algorithm.","title":"35.4-3"},{"location":"Chap35/35.4/#354-4","text":"Show that the constraints in line $\\text{(35.19)}$ are redundant in the sense that if we remove them from the linear program in lines $\\text{(35.17)}\u2013\\text{(35.20)}$, any optimal solution to the resulting linear program must satisfy $x(v) \\le 1$ for each $v \\in V$. (Omit!)","title":"35.4-4"},{"location":"Chap35/35.5/","text":"35.5-1 Prove equation $\\text{(35.23)}$. Then show that after executing line 5 of $\\text{EXACT-SUBSET-SUM}$, $L_i$ is a sorted list containing every element of $P_i$ whose value is not more than $t$. (Omit!) 35.5-2 Using induction on $i$, prove inequality $\\text{(35.26)}$. (Omit!) 35.5-3 Prove inequality $\\text{(35.29)}$. (Omit!) 35.5-4 How would you modify the approximation scheme presented in this section to \ufb01nd a good approximation to the smallest value not less than $t$ that is a sum of some subset of the given input list? (Omit!) 35.5-5 Modify the $\\text{APPROX-SUBSET-SUM}$ procedure to also return the subset of $S$ that sums to the value $z^*$. (Omit!)","title":"35.5 The subset-sum problem"},{"location":"Chap35/35.5/#355-1","text":"Prove equation $\\text{(35.23)}$. Then show that after executing line 5 of $\\text{EXACT-SUBSET-SUM}$, $L_i$ is a sorted list containing every element of $P_i$ whose value is not more than $t$. (Omit!)","title":"35.5-1"},{"location":"Chap35/35.5/#355-2","text":"Using induction on $i$, prove inequality $\\text{(35.26)}$. (Omit!)","title":"35.5-2"},{"location":"Chap35/35.5/#355-3","text":"Prove inequality $\\text{(35.29)}$. (Omit!)","title":"35.5-3"},{"location":"Chap35/35.5/#355-4","text":"How would you modify the approximation scheme presented in this section to \ufb01nd a good approximation to the smallest value not less than $t$ that is a sum of some subset of the given input list? (Omit!)","title":"35.5-4"},{"location":"Chap35/35.5/#355-5","text":"Modify the $\\text{APPROX-SUBSET-SUM}$ procedure to also return the subset of $S$ that sums to the value $z^*$. (Omit!)","title":"35.5-5"},{"location":"Chap35/Problems/35-1/","text":"Suppose that we are given a set of $n$ objects, where the size $s_i$ of the $i$th object satisfies $0 < s_i < 1$. We wish to pack all the objects into the minimum number of unit-size bins. Each bin can hold any subset of the objects whose total size does not exceed $1$. a. Prove that the problem of determining the minimum number of bins required is $\\text{NP-hard}$. ($\\textit{Hint:}$ Reduce from the subset-sum problem.) The first-fit heuristic takes each object in turn and places it into the first bin that can accommodate it. Let $S = \\sum_{i = 1}^n s_i$. b. Argue that the optimal number of bins required is at least $\\lceil S \\rceil$. c. Argue that the first-fit heuristic leaves at most one bin less than half full. d. Prove that the number of bins used by the first-fit heuristic is never more than $\\lceil 2S \\rceil$. e. Prove an approximation ratio of $2$ for the first-fit heuristic. f. Give an efficient implementation of the first-fit heuristic, and analyze its running time. (Omit!)","title":"35-1 Bin packing"},{"location":"Chap35/Problems/35-2/","text":"Let $G = (V, E)$ be an undirected graph. For any $k \\ge 1$, define $G^{(k)}$ to be the undirected graph $(V^{(k)}, E^{(k)})$, where $V^{(k)}$ is the set of all ordered $k$-tuples of vertices from $V$ and $E^{(k)}$ is defined so that $(v_1, v_2, \\dots, v_k)$ is adjacent to $(w_1, w_2, \\dots, w_k)$ if and only if for $i = 1, 2, \\dots, k$, either vertex $v_i$ is adjacent to $w_i$ in $G$, or else $v_i = w_i$. a. Prove that the size of the maximum clique in $G^{(k)}$ is equal to the $k$th power of the size of the maximum clique in $G$. b. Argue that if there is an approximation algorithm that has a constant approximation ratio for finding a maximum-size clique, then there is a polynomial-time approximation scheme for the problem. (Omit!)","title":"35-2 Approximating the size of a maximum clique"},{"location":"Chap35/Problems/35-3/","text":"Suppose that we generalize the set-covering problem so that each set $S_i$ in the family $\\mathcal F$ has an associated weight $w_i$ and the weight of a cover $\\mathcal C$ is $\\sum_{S_i \\in \\mathcal C} w_i$. We wish to determine a minimum-weight cover. (Section 35.3 handles the case in which $w_i = 1$ for all $i$.) Show how to generalize the greedy set-covering heuristic in a natural manner to provide an approximate solution for any instance of the weighted set-covering problem. Show that your heuristic has an approximation ratio of $H(d)$, where $d$ is the maximum size of any set $S_i$. (Omit!)","title":"35-3 Weighted set-covering problem"},{"location":"Chap35/Problems/35-4/","text":"Recall that for an undirected graph $G$, a matching is a set of edges such that no two edges in the set are incident on the same vertex. In Section 26.3, we saw how to find a maximum matching in a bipartite graph. In this problem, we will look at matchings in undirected graphs in general (i.e., the graphs are not required to be bipartite). a. A maximal matching is a matching that is not a proper subset of any other matching. Show that a maximal matching need not be a maximum matching by exhibiting an undirected graph $G$ and a maximal matching $M$ in $G$ that is not a maximum matching. ($\\textit{Hint:}$ You can find such a graph with only four vertices.) b. Consider an undirected graph $G = (V, E)$. Give an $O(E)$-time greedy algorithm to find a maximal matching in $G$. In this problem, we shall concentrate on a polynomial-time approximation algorithm for maximum matching. Whereas the fastest known algorithm for maximum matching takes superlinear (but polynomial) time, the approximation algorithm here will run in linear time. You will show that the linear-time greedy algorithm for maximal matching in part (b) is a $2$-approximation algorithm for maximum matching. c. Show that the size of a maximum matching in $G$ is a lower bound on the size of any vertex cover for $G$. d. Consider a maximal matching $M$ in $G = (V, E)$. Let $$T = \\{v \\in V: \\text{ some edge in } M \\text{ is incident on } v\\}.$$ What can you say about the subgraph of $G$ induced by the vertices of $G$ that are not in $T$? e. Conclude from part (d) that $2|M|$ is the size of a vertex cover for $G$. f. Using parts (c) and (e), prove that the greedy algorithm in part (b) is a $2$-approximation algorithm for maximum matching. (Omit!)","title":"35-4 Maximum matching"},{"location":"Chap35/Problems/35-5/","text":"In the parallel-machine-scheduling problem , we are given $n$ jobs, $J_1, J_2, \\dots, J_n$, where each job $J_k$ has an associated nonnegative processing time of $p_k$. We are also given $m$ identical machines, $M_1, M_2, \\dots, M_m$. Any job can run on any machine. A schedule specifies, for each job $J_k$, the machine on which it runs and the time period during which it runs. Each job $J_k$ must run on some machine $M_i$ for $p_k$ consecutive time units, and during that time period no other job may run on $M_i$. Let $C_k$ denote the completion time of job $J_k$, that is, the time at which job $J_k$ completes processing. Given a schedule, we define $C_{\\max} = \\max_{1 \\le j \\le n} C_j$ to be the makespan of the schedule. The goal is to find a schedule whose makespan is minimum. For example, suppose that we have two machines $M_1$ and $M_2$ and that we have four jobs $J_1, J_2, J_3, J_4$, with $p_1 = 2$, $p_2 = 12$, $p_3 = 4$, and $p_4 = 5$. Then one possible schedule runs, on machine $M_1$, job $J_1$ followed by job $J_2$, and on machine $M_2$, it runs job $J_4$ followed by job $J_3$. For this schedule, $C_1 = 2$, $C_2 = 14$, $C_3 = 9$, $C_4 = 5$, and $C_{\\max} = 14$. An optimal schedule runs $J_2$ on machine $M_1$, and it runs jobs $J_1$, $J_3$, and $J_4$ on machine $M_2$. For this schedule, $C_1 = 2$, $C_2 = 12$, $C_3 = 6$, $C_4 = 11$, and $C_{\\max} = 12$. Given a parallel-machine-scheduling problem, we let $C_{\\max}^*$ denote the makespan of an optimal schedule. a. Show that the optimal makespan is at least as large as the greatest processing time, that is, $$C_{\\max}^* \\ge \\max_{1 \\le k \\le n} p_k.$$ b. Show that the optimal makespan is at least as large as the average machine load, that is, $$C_{\\max}^* \\ge \\frac 1 m \\sum_{1 \\le k \\le n} p_k.$$ Suppose that we use the following greedy algorithm for parallel machine scheduling: whenever a machine is idle, schedule any job that has not yet been scheduled. c. Write pseudocode to implement this greedy algorithm. What is the running time of your algorithm? d. For the schedule returned by the greedy algorithm, show that $$C_{\\max} \\le \\frac 1 m \\sum_{1 \\le k \\le n} p_k + \\max_{1 \\le k \\le n} p_k.$$ Conclude that this algorithm is a polynomial-time $2$-approximation algorithm. (Omit!)","title":"35-5 Parallel machine scheduling"},{"location":"Chap35/Problems/35-6/","text":"Let $G = (V, E)$ be an undirected graph with distinct edge weights $w(u, v)$ on each edge $(u, v) \\in E$. For each vertex $v \\in V$, let $\\max(v) = \\max_{(u, v) \\in E} \\{w(u, v)\\}$ be the maximum-weight edge incident on that vertex. Let $S_G = \\{\\max(v): v \\in V\\}$ be the set of maximum-weight edges incident on each vertex, and let $T_G$ be the maximum-weight spanning tree of $G$, that is, the spanning tree of maximum total weight. For any subset of edges $E' \\subseteq E$, define $w(E') = \\sum_{(u, v) \\in E'} w(u, v)$. a. Give an example of a graph with at least $4$ vertices for which $S_G = T_G$. b. Give an example of a graph with at least $4$ vertices for which $S_G \\ne T_G$. c. Prove that $S_G \\subseteq T_G$ for any graph $G$. d. Prove that $w(T_G) \\ge w(S_G) / 2$ for any graph $G$. e. Give an $O(V + E)$-time algorithm to compute a $2$-approximation to the maximum spanning tree. (Omit!)","title":"35-6 Approximating a maximum spanning tree"},{"location":"Chap35/Problems/35-7/","text":"Recall the knapsack problem from Section 16.2. There are $n$ items, where the $i$th item is worth $v_i$ dollars and weighs $w_i$ pounds. We are also given a knapsack that can hold at most $W$ pounds. Here, we add the further assumptions that each weight $w_i$ is at most $W$ and that the items are indexed in monotonically decreasing order of their values: $v_1 \\ge v_2 \\ge \\cdots \\ge v_n$. In the 0-1 knapsack problem, we wish to find a subset of the items whose total weight is at most $W$ and whose total value is maximum. The fractional knapsack problem is like the 0-1 knapsack problem, except that we are allowed to take a fraction of each item, rather than being restricted to taking either all or none of each item. If we take a fraction $x_i$ of item $i$, where $0 \\le x_i \\le 1$, we contribute $x_iw_i$ to the weight of the knapsack and receive value $x_iv_i$. Our goal is to develop a polynomial-time $2$-approximation algorithm for the 0-1 knapsack problem. In order to design a polynomial-time algorithm, we consider restricted instances of the 0-1 knapsack problem. Given an instance $I$ of the knapsack problem, we form restricted instances $I_j$, for $j = 1, 2, \\dots, n$, by removing items $1, 2, \\dots, j - 1$ and requiring the solution to include item $j$ (all of item $j$ in both the fractional and 0-1 knapsack problems). No items are removed in instance $I_1$. For instance $I_j$, let $P_j$ denote an optimal solution to the 0-1 problem and $Q_j$ denote an optimal solution to the fractional problem. a. Argue that an optimal solution to instance $I$ of the 0-1 knapsack problem is one of $\\{P_1, P_2, \\dots, P_n\\}$. b. Prove that we can find an optimal solution $Q_j$ to the fractional problem for instance $I_j$ by including item $j$ and then using the greedy algorithm in which at each step, we take as much as possible of the unchosen item in the set $\\{j + 1, j + 2, \\dots, n\\}$ with maximum value per pound $v_i / w_i$. c. Prove that we can always construct an optimal solution $Q_j$ to the fractional problem for instance $I_j$ that includes at most one item fractionally. That is, for all items except possibly one, we either include all of the item or none of the item in the knapsack. d. Given an optimal solution $Q_j$ to the fractional problem for instance $I_j$, form solution $R_j$ from $Q_j$ by deleting any fractional items from $Q_j$. Let $v(S)$ denote the total value of items taken in a solution $S$. Prove that $v(R_j) \\ge v(Q_j) / 2 \\ge v(P_j) / 2$. e. Give a polynomial-time algorithm that returns a maximum-value solution from the set $\\{R_1, R_2, \\dots, R_n\\}$, and prove that your algorithm is a polynomial-time $2$-approximation algorithm for the 0-1 knapsack problem. (Omit!)","title":"35-7 An approximation algorithm for the 0-1 knapsack problem"}]}